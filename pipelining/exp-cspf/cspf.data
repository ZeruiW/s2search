{"id": 1382, "s2_id": "bdc51d171df27b115b8188841195c3d298cd9dd8", "title": "Joint performance analysis of ages of information in a multi-source pushout server", "abstract": "Age of information (AoI) has been widely accepted as a measure quantifying freshness of status information in real-time status update systems. In many of such systems, multiple sources share a limited network resource and therefore the AoIs defined for the individual sources should be correlated with each other. However, there are not found any results studying the correlation of two or more AoIs in a status update system with multiple sources. In this work, we consider a multi-source system sharing a common service facility and provide a framework to investigate joint performance of the multiple AoIs. We then apply our framework to a simple pushout server with multiple sources and derive a closed-form formula of the joint Laplace transform of the AoIs in the case with independent M/G inputs. We further show some properties of the correlation coefficient of AoIs in the two-source system.", "venue": "IEEE Transactions on Information Theory", "authors": ["Yukang  Jiang", "Naoto  Miyoshi"], "year": 2021, "n_citations": 1}
{"id": 2463, "s2_id": "6136cd85755dcc395a0a48bf1fd64962daac4bc8", "title": "Asymptotic Miss Ratio of LRU Caching with Consistent Hashing", "abstract": "To efficiently scale data caching infrastructure to support emerging big data applications, many caching systems rely on consistent hashing to group a large number of servers to form a cooperative cluster. These servers are organized together according to a random hash function. They jointly provide a unified but distributed hash table to serve swift and voluminous data item requests. Different from the single least-recently-used (LRU) server that has already been extensively studied, theoretically characterizing a cluster that consists of multiple LRU servers remains yet to be explored. These servers are not simply added together; the random hashing complicates the behavior. To this end, we derive the asymptotic miss ratio of data item requests on a LRU cluster with consistent hashing. We show that these individual cache spaces on different servers can be effectively viewed as if they could be pooled together to form a single virtual LRU cache space parametrized by an appropriate cache size. This equivalence can be established rigorously under the condition that the cache sizes of the individual servers are large. For typical data caching systems this condition is common. Our theoretical framework provides a convenient abstraction that can directly apply the results from the simpler single LRU cache to the more complex LRU cluster with consistent hashing.", "venue": "IEEE INFOCOM 2018 - IEEE Conference on Computer Communications", "authors": ["Kaiyi  Ji", "Guocong  Quan", "Jian  Tan"], "year": 2018, "n_citations": 7}
{"id": 11504, "s2_id": "1b7e82a931685f23fd3454385859c222b435b888", "title": "A simple proof of the Moore-Hodgson Algorithm for minimizing the number of late jobs", "abstract": "The Moore-Hodgson Algorithm minimizes the number of late jobs on a single machine. That is, it finds an optimal schedule for the classical problem 1 | | \u2211 Uj . Several proofs of the correctness of this algorithm have been published. We present a new short proof.", "venue": "Oper. Res. Lett.", "authors": ["Joseph  Cheriyan", "R.  Ravi", "Martin  Skutella"], "year": 2021, "n_citations": 0}
{"id": 13628, "s2_id": "655024a89f9459554318352734b65d9d83dd98cc", "title": "Robust stability of moving horizon estimation for nonlinear systems with bounded disturbances using adaptive arrival cost", "abstract": "In this paper, the robust stability and convergence to the true state of moving horizon estimator based on an adaptive arrival cost are established for nonlinear detectable systems. Robust global asymptotic stability is shown for the case of non-vanishing bounded disturbances whereas the convergence to the true state is proved for the case of vanishing disturbances. Several simulations were made in order to show the estimator behaviour under different operational conditions and to compare it with the state of the art estimation methods.", "venue": "ArXiv", "authors": ["Nestor N. Deniz", "Marina H. Murillo", "Guido  Sanchez", "Lucas M. Genzelis", "Leonardo L. Giovanini"], "year": 2019, "n_citations": 1}
{"id": 13845, "s2_id": "51696520b83f20e9e3a8031aaa22a535f00fc61a", "title": "Exposing Provenance Metadata Using Different RDF Models", "abstract": "A standard model for exposing structured provenance metadata of scientific assertions on the Semantic Web would increase interoperability, discoverability, reliability, as well as reproducibility for scientific discourse and evidence-based knowledge discovery. Several Resource Description Framework (RDF) models have been proposed to track provenance. However, provenance metadata may not only be verbose, but also significantly redundant. Therefore, an appropriate RDF provenance model should be efficient for publishing, querying, and reasoning over Linked Data. In the present work, we have collected millions of pairwise relations between chemicals, genes, and diseases from multiple data sources, and demonstrated the extent of redundancy of provenance information in the life science domain. We also evaluated the suitability of several RDF provenance models for this crowdsourced data set, including the N-ary model, the Singleton Property model, and the Nanopublication model. We examined query performance against three commonly used large RDF stores, including Virtuoso, Stardog, and Blazegraph. Our experiments demonstrate that query performance depends on both RDF store as well as the RDF provenance model.", "venue": "SWAT4LS", "authors": ["Gang  Fu", "Evan  Bolton", "N\u00faria  Queralt-Rosinach", "Laura In\u00e9s Furlong", "Vinh  Nguyen", "Amit P. Sheth", "Olivier  Bodenreider", "Michel  Dumontier"], "year": 2015, "n_citations": 10}
{"id": 16157, "s2_id": "3dd065768db41d3a9e7bcba0d6400d3de2db4d3d", "title": "Performance Evaluation of Multiparty Authentication in 5G IIoT Environments", "abstract": "With the rapid development of various emerging technologies such as the Industrial Internet of Things (IIoT), there is a need to secure communications between such devices. Communication system delays are one of the factors that adversely affect the performance of an authentication system. 5G networks enable greater data throughput and lower latency, which presents new opportunities for the secure authentication of business transactions between IIoT devices. We evaluate an approach to developing a flexible and secure model for authenticating IIoT components in dynamic 5G environments.", "venue": "Communications in Computer and Information Science", "authors": ["Hussain  Al-Aqrabi", "Phil  Lane", "Richard  Hill"], "year": 2019, "n_citations": 3}
{"id": 17578, "s2_id": "28533c62fb20270ab8772258239487b2fa1cf75a", "title": "Graph500 from OCaml-Multicore Perspective", "abstract": "OCaml is an industrial-strength, multi-paradigm programming language, widely used in industry and academia. OCaml was developed for solving numerical and scientific problems involving large scale dataintensive operations and one such classic application set is Graph Algorithms, which are a core part of most analytics workloads. In this paper, we aim to implement the graph benchmarks along with the performance analysis. Graph500 is one such serious benchmark which aims at developing data intensive applications requiring extreme computational power. We try to implement Graph Construction, BFS, Shortest-Path problems using the desired specifications and rules posed by graph500. This paper aims at providing a clear direction of choices of several data structures used, algorithms developed and pose a reason behind every step of program. The first few sections of the paper discusses a formal approach to the problem with a small guide for starters in OCaml. The latter sections describe the algorithms in detail with the possibilities of future exploration and several mistakes which we committed or encountered whilst approaching the solution. All performance metrics were tested on Intel(R) Xeon(R) Gold 5120 CPU @ 2.20GHz 24 core machine. Every section talks about the initial performance failures encountered, which will help analyse and prioritise our preferred implementation from a performance perspective. Additional", "venue": "ArXiv", "authors": ["Shubhendra Pal Singhal"], "year": 2020, "n_citations": 0}
{"id": 17778, "s2_id": "42fcc632393d922155414f79534a5e30b9a226c6", "title": "A Comparative Measurement Study of Deep Learning as a Service Framework", "abstract": "Big data powered Deep Learning (DL) and its applications have blossomed in recent years, fueled by three technological trends: a large amount of digitized data openly accessible, a growing number of DL software frameworks in open source and commercial markets, and a selection of affordable parallel computing hardware devices. However, no single DL framework, to date, dominates in terms of performance and accuracy even for baseline classification tasks on standard datasets, making the selection of a DL framework an overwhelming task. This paper takes a holistic approach to conduct empirical comparison and analysis of four representative DL frameworks with three unique contributions. First, given a selection of CPU-GPU configurations, we show that for a specific DL framework, different configurations of its hyper-parameters may have a significant impact on both performance and accuracy of DL applications. Second, to the best of our knowledge, this study is the first to identify the opportunities for improving the training time performance and the accuracy of DL frameworks by configuring parallel computing libraries and tuning individual and multiple hyper-parameters. Third, we also conduct a comparative measurement study on the resource consumption patterns of four DL frameworks and their performance and accuracy implications, including CPU and memory usage, and their correlations to varying settings of hyper-parameters under different configuration combinations of hardware, parallel computing libraries. We argue that this measurement study provides in-depth empirical comparison and analysis of four representative DL frameworks, and offers practical guidance for service providers to deploying and delivering DL as a Service (DLaaS) and for application developers and DLaaS consumers to select the right DL frameworks for the right DL workloads.", "venue": "IEEE Transactions on Services Computing", "authors": ["Yanzhao  Wu", "Ling  Liu", "Calton  Pu", "Wenqi  Cao", "Semih  Sahin", "Wenqi  Wei", "Qi  Zhang"], "year": 2019, "n_citations": 16}
{"id": 19076, "s2_id": "65a7f6ce17c453440cf9982ef7958e668ac369db", "title": "Zero-bias Deep Learning Enabled Quick and Reliable Abnormality Detection in IoT", "abstract": "Abnormality detection is essential to the performance of safety-critical and latency-constrained systems. However, as systems are becoming increasingly complicated with a large quantity of heterogeneous data, conventional statistical change point detection methods are becoming less effective and efficient. Although Deep Learning (DL) and Deep Neural Networks (DNNs) are increasingly employed to handle heterogeneous data, they still lack theoretic assurable performance and explainability. This paper integrates zero-bias DNN and Quickest Event Detection algorithms to provide a holistic framework for quick and reliable detection of both abnormalities and time-dependent abnormal events in the Internet of Things (IoT). We first use the zero-bias dense layer to increase the explainability of DNN. We provide a solution to convert zero-bias DNN classifiers into performance assured binary abnormality detectors. Using the converted abnormality detector, we then present a sequential quickest detection scheme that provides the theoretically assured lowest abnormal event detection delay under false alarm constraints. Finally, we demonstrate the effectiveness of the framework using both massive signal records from real-world aviation communication systems and simulated data. Code and data of our work is available at \\url{https://github.com/pcwhy/AbnormalityDetectionInZbDNN}", "venue": "ArXiv", "authors": ["Yongxin  Liu", "Jian  Wang", "Jianqiang  Li", "Shuteng  Niu", "Houbing  Song"], "year": 2021, "n_citations": 0}
{"id": 21732, "s2_id": "ebacca5cffcb004a6ed7b4c0295b60aa81586369", "title": "WEP: An Energy Efficient Protocol for Cluster Based Heterogeneous Wireless Sensor Network", "abstract": "We develop an energy-efficient routing protocol in order to enhance the stability period of wireless sensor networks. This protocol is called weighted election protocol (WEP). It introduces a scheme to combine clustering strategy with chain routing algorithm for satisfy both energy and stable period constrains under heterogeneous environment in WSNs. Simulation results show that new one performs better than LEACH, SEP and HEARP in terms of stability period and network lifetime. It is also found that longer stability period strongly depend on higher values of extra energy during its heterogeneous settings.", "venue": "ArXiv", "authors": ["Md. Golam Rashed", "M. Hasnat Kabir", "Shaikh Enayet Ullah"], "year": 2012, "n_citations": 46}
{"id": 24441, "s2_id": "98d82deb4e58ad5bc4665a599002fff0616c407f", "title": "A Rank Based Replacement Policy for Multimedia Server Cache Using Zipf-Like Law", "abstract": "The cache replacement algorithm plays an important role in the overall performance of Proxy-Server system. In this paper we have proposed VoD cache memory replacement algorithm for a multimedia server system. We propose a Rank based cache replacement policy to manage the cache space in individual proxy server cache. Proposed replacement strategy incorporates in a simple way the most important characteristics of the video and its accesses such as its size, access frequency, recentness of the last access and the cost incurred while transferring the requested video from the server to the proxy. We compare our algorithm with some popular cache replacement algorithm using simulation. The video objects are ranked based on the access trend by considering the factors such as size, frequency and cost. Many studies have demonstrated that Zipf's-like law can govern many features of the VoD and is used to describe the popularity of the video. In this paper, we have designed a model, which ranks the video on the basis of its popularity using the Zipf-like law. The video with higher ranking is named \"hot\", while the video with lower ranking is named \"cold\". The result show that the proposed rank based algorithm improves cache hit ratio, cache byte ratio and average request latencies compared to other algorithms. Our experimental results indicate that Rank based cache replacement algorithm outperforms LRU, LFU and Greedy Dual.", "venue": "ArXiv", "authors": ["T. R. Gopalakrishnan Nair", "P.  Jayarekha"], "year": 2010, "n_citations": 33}
{"id": 32309, "s2_id": "f4055e8f1cb52e55774964d2cc3b0cc89edac18f", "title": "Assessing Performance Implications of Deep Copy Operations via Microbenchmarking", "abstract": "As scientific frameworks become sophisticated, so do their data structures. Current data structures are no longer simple in design and they have been progressively complicated. The typical trend in designing data structures in scientific applications are basically nested data structures: pointing to a data structure within another one. Managing nested data structures on a modern heterogeneous system requires tremendous effort due to the separate memory space design. \nIn this paper, we will discuss the implications of deep copy on data transfers on current heterogeneous. Then, we will discuss the two options that are currently available to perform the memory copy operations on complex structures and will introduce pointerchain directive that we proposed. Afterwards, we will introduce a set of extensive benchmarks to compare the available approaches. Our goal is to make our proposed benchmarks a basis to examine the efficiency of upcoming approaches that address the challenge of deep copy operations.", "venue": "ArXiv", "authors": ["Millad  Ghane", "Sunita  Chandrasekaran", "Margaret S. Cheung"], "year": 2019, "n_citations": 0}
{"id": 32952, "s2_id": "48d534c2937e0487458a0adfad8b72f29f7096fd", "title": "Near-Optimal Virtual Machine Packing Based on Resource Requirement of Service Demands Using Pattern Clustering", "abstract": "Upon the expansion of Cloud Computing and the positive outlook of organizations with regard to the movements towards using cloud computing and their expanding utilization of such valuable processing method, as well as the solutions provided by the cloud infrastructure providers with regard to the reduction of the costs of processing resources, the problem of organizing resources in a cloud environment gained a high importance. One of the major preoccupations of the minds of cloud infrastructure clients is their lack of knowledge on the quantity of their required processing resources in different periods of time. The managers and technicians are trying to make the most use of scalability and the flexibility of the resources in cloud computing. The main challenge is with calculating the amount of the required processing resources per moment with regard to the quantity of incoming requests of the service. Through deduction of the accurate amount of these items, one can have an accurate estimation of the requests per moment. This paper aims at introducing a model for automatic scaling of the cloud resources that would reduce the cost of renting the resources for the clients of cloud infrastructure. Thus, first we start with a thorough explanation of the proposal and the major components of the model. Then through calculating the incomings of the model through clustering and introducing the way that each of these components work in different phases,...", "venue": "ArXiv", "authors": ["Yaghoob  Siahmargooei", "Mohammad Kazem Akbari", "Seyyed Alireza Hashemi Golpayegani", "Saeed  Sharifian"], "year": 2014, "n_citations": 0}
{"id": 33343, "s2_id": "51fc8321e4d2dd0ba90231d77bb6eac7773de0fe", "title": "Best-Effort FPGA Programming: A Few Steps Can Go a Long Way", "abstract": "FPGA-based heterogeneous architectures provide programmers with the ability to customize their hardware accelerators for flexible acceleration of many workloads. Nonetheless, such advantages come at the cost of sacrificing programmability. FPGA vendors and researchers attempt to improve the programmability through high-level synthesis (HLS) technologies that can directly generate hardware circuits from high-level language descriptions. However, reading through recent publications on FPGA designs using HLS, one often gets the impression that FPGA programming is still hard in that it leaves programmers to explore a very large design space with many possible combinations of HLS optimization strategies. \nIn this paper we make two important observations and contributions. First, we demonstrate a rather surprising result: FPGA programming can be made easy by following a simple best-effort guideline of five refinement steps using HLS. We show that for a broad class of accelerator benchmarks from MachSuite, the proposed best-effort guideline improves the FPGA accelerator performance by 42-29,030x. Compared to the baseline CPU performance, the FPGA accelerator performance is improved from an average 292.5x slowdown to an average 34.4x speedup. Moreover, we show that the refinement steps in the best-effort guideline, consisting of explicit data caching, customized pipelining, processing element duplication, computation/communication overlapping and scratchpad reorganization, correspond well to the best practice guidelines for multicore CPU programming. Although our best-effort guideline may not always lead to the optimal solution, it substantially simplifies the FPGA programming effort, and will greatly support the wide adoption of FPGA-based acceleration by the software programming community.", "venue": "ArXiv", "authors": ["Jason  Cong", "Zhenman  Fang", "Yuchen  Hao", "Peng  Wei", "Cody Hao Yu", "Chen  Zhang", "Peipei  Zhou"], "year": 2018, "n_citations": 12}
{"id": 33459, "s2_id": "638648aa8033ea536b56f9f43e32311de874247a", "title": "Exact Simulation for Assemble-To-Order Systems", "abstract": "We develop exact simulation (also known as perfect sampling) algorithms for a family of assemble-to-order systems. Due to the nite capacity, and coupling in demands and replenishments, known solving techniques are inecient for larger problem instances. We rst consider the case with individual replenishments of items, and derive an event based representation of the Markov chain that allows applying existing exact simulation techniques, using the monotonicity properties or bounding chains. In the case of joint replenishments, the state space becomes intractable for the existing methods. We propose new exact simulation algorithms, based on aggregation and bounding chains, that allow a signicant reduction of the state space of the Markov chain. We also discuss the coupling times of considered models and provide sucient conditions for linear (in the single server replenishment case) or quadratic (many server case) complexity of our algorithms in terms of the total capacity in the system.", "venue": "ArXiv", "authors": ["Ana  Busic", "Emilie  Coupechoux"], "year": 2014, "n_citations": 1}
{"id": 36821, "s2_id": "092d0ba602a8275254c0ac5718bf176526b12eaa", "title": "LSTM-Sharp: An Adaptable, Energy-Efficient Hardware Accelerator for Long Short-Term Memory", "abstract": "The effectiveness of LSTM neural networks for popular tasks such as Automatic Speech Recognition has fostered an increasing interest in LSTM inference acceleration. Due to the recurrent nature and data dependencies of LSTM computations, designing a customized architecture specifically tailored to its computation pattern is crucial for efficiency. Since LSTMs are used for a variety of tasks, generalizing this efficiency to diverse configurations, i.e., adaptiveness, is another key feature of these accelerators. In this work, we first show the problem of low resource-utilization and adaptiveness for the state-of-the-art LSTM implementations on GPU, FPGA and ASIC architectures. To solve these issues, we propose an intelligent tiled-based dispatching mechanism that efficiently handles the data dependencies and increases the adaptiveness of LSTM computation. To do so, we propose LSTM-Sharp as a hardware accelerator, which pipelines LSTM computation using an effective scheduling scheme to hide most of the dependent serialization. Furthermore, LSTM-Sharp employs dynamic reconfigurable architecture to adapt to the model's characteristics. LSTM-Sharp achieves 1.5x, 2.86x, and 82x speedups on average over the state-of-the-art ASIC, FPGA, and GPU implementations respectively, for different LSTM models and resource budgets. Furthermore, we provide significant energy-reduction with respect to the previous solutions, due to the low power dissipation of LSTM-Sharp (383 GFLOPs/Watt).", "venue": "ArXiv", "authors": ["Reza  Yazdani", "Olatunji  Ruwase", "Minjia  Zhang", "Yuxiong  He", "Jose-Maria  Arnau", "Antonio  Gonzalez"], "year": 2019, "n_citations": 7}
{"id": 37142, "s2_id": "ad7a6247d3eac20f17ff0c43ee41e055073126dc", "title": "Serving Recurrent Neural Networks Efficiently with a Spatial Accelerator", "abstract": "Recurrent Neural Network (RNN) applications form a major class of AI-powered, low-latency data center workloads. Most execution models for RNN acceleration break computation graphs into BLAS kernels, which lead to significant inter-kernel data movement and resource underutilization. We show that by supporting more general loop constructs that capture design parameters in accelerators, it is possible to improve resource utilization using cross-kernel optimization without sacrificing programmability. Such abstraction level enables a design space search that can lead to efficient usage of on-chip resources on a spatial architecture across a range of problem sizes. We evaluate our optimization strategy on such abstraction with DeepBench using a configurable spatial accelerator. We demonstrate that this implementation provides a geometric speedup of 30x in performance, 1.6x in area, and 2x in power efficiency compared to a Tesla V100 GPU, and a geometric speedup of 2x compared to Microsoft Brainwave implementation on a Stratix 10 FPGA.", "venue": "MLSys", "authors": ["Tian  Zhao", "Yaqi  Zhang", "Kunle  Olukotun"], "year": 2019, "n_citations": 10}
{"id": 40004, "s2_id": "8d4b8bb90ed3fd462519c39b1d1957f1ca0524e2", "title": "Dominant block guided optimal cache size estimation to maximize IPC of embedded software", "abstract": "Embedded system software is highly constrained from performance, memory footprint, energy consumption and implementing cost view point. It is always desirable to obtain better Instructions per Cycle. Instruction cache has major contribution in improving IPC. Cache memories are realized on the same chip where the processor is running. This considerably increases the system cost as well. Hence, it is required to maintain a trade off between cache sizes and performance improvement offered. Determining the number of cache lines and size of cache line are important parameters for cache designing. The design space for cache is quite large. It is time taking to execute the given application with different cache sizes on an instruction set simulator to figure out the optimal cache size. In this paper, a technique is proposed to identify a number of cache lines and cache line size for the L1 instruction cache that will offer best or nearly best IPC. Cache size is derived, at a higher abstraction level, from basic block analysis in the Low Level Virtual Machine environment. The cache size estimated is cross validated by simulating the set of benchmark applications with different cache sizes in simple scalar simulator. The proposed method seems to be superior in terms of estimation accuracy and estimation time as compared to the existing methods for estimation of optimal cache size parameters like cache line size, number of cache lines.", "venue": "ArXiv", "authors": ["Rajendra  Patel", "Arvind  Rajawat"], "year": 2013, "n_citations": 1}
{"id": 41413, "s2_id": "a4901727c359ab19ef69b393b4fb08173d6eefdc", "title": "High-Performance Tensor Contraction without Transposition", "abstract": "Tensor computations---in particular tensor contraction (TC)---are important kernels in many scientific computing applications. Due to the fundamental similarity of TC to matrix multiplication and to the availability of optimized implementations such as the BLAS, tensor operations have traditionally been implemented in terms of BLAS operations, incurring both a performance and a storage overhead. Instead, we implement TC using the flexible BLAS-like Instantiation Software (BLIS) framework, which allows for transposition (reshaping) of the tensor to be fused with internal partitioning and packing operations, requiring no explicit transposition operations or additional workspace. This implementation, TBLIS, achieves performance approaching that of matrix multiplication, and in some cases considerably higher than that of traditional TC. Our implementation supports multithreading using an approach identical to that used for matrix multiplication in BLIS, with similar performance characteristics. The complexity...", "venue": "SIAM J. Sci. Comput.", "authors": ["Devin A. Matthews"], "year": 2018, "n_citations": 40}
{"id": 41594, "s2_id": "267c2cfbef7621be090cffd0b404224fc7092793", "title": "Fast Mapping onto Census Blocks", "abstract": "Pandemic measures such as social distancing and contact tracing can be enhanced by rapidly integrating dynamic location data and demographic data. Projecting billions of longitude and latitude locations onto hundreds of thousands of highly irregular demographic census block polygons is computationally challenging in both research and deployment contexts. This paper describes two approaches labeled \u201csimple\u201d and \u201cfast\u201d. The simple approach can be implemented in any scripting language (Matlab/Octave, Python, Julia, R) and is easily integrated and customized to a variety of research goals. This simple approach uses a novel combination of hierarchy, sparse bounding boxes, polygon crossing-number, vectorization, and parallel processing to achieve 100,000,000+ projections per second on 100 servers. The simple approach is compact, does not increase data storage requirements, and is applicable to any country or region. The fast approach exploits the thread, vector, and memory optimizations that are possible using a low-level language (C++) and achieves similar performance on a single server. This paper details these approaches with the goal of enabling the broader community to quickly integrate location and demographic data.", "venue": "2020 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Jeremy  Kepner", "Darren  Engwirda", "Vijay  Gadepally", "Chris  Hill", "Tim  Kraska", "Michael  Jones", "Andreas  Kipf", "Lauren  Milechin", "Navin  Vembar"], "year": 2020, "n_citations": 1}
{"id": 43299, "s2_id": "36c950b1d440b620f598492f1098b6c86b2d7e3a", "title": "Modern Multicore CPUs are not Energy Proportional: Opportunity for Bi-objective Optimization for Performance and Energy", "abstract": "Energy proportionality is the key design goal followed by architects of modern multicore CPUs. One of its implications is that optimization of an application for performance will also optimize it for energy. In this work, we show that energy proportionality does not hold true for multicore CPUs. This finding creates the opportunity for bi-objective optimization of applications for performance and energy. We propose and study the first application-level method for bi-objective optimization of multithreaded data-parallel applications for performance and energy. The method uses two decision variables, the number of identical multithreaded kernels (threadgroups) executing the application and the number of threads in each threadgroup, with the workload always partitioned equally between the threadgroups. We experimentally demonstrate the efficiency of the method using four highly optimized multithreaded data-parallel applications, 2D fast Fourier transform based on FFTW and Intel MKL, and dense matrix-matrix multiplication using OpenBLAS and Intel MKL. Four modern multicore CPUs are used in the experiments. The experiments show that optimization for performance alone results in the increase in dynamic energy consumption by up to 89% and optimization for dynamic energy alone degrades the performance by up to 49%. By solving the bi-objective optimization problem, the method determines up to 11 globally Pareto-optimal solutions. Finally, we propose a qualitative dynamic energy model employing performance monitoring counters as parameters, which we use to explain the discovered energy nonproportionality and the Pareto-optimal solutions determined by our method. The model shows that the energy nonproportionality in our case is due to the activity of the data translation lookaside buffer (dTLB), which is disproportionately energy expensive.", "venue": "ArXiv", "authors": ["Semyon  Khokhriakov", "Ravi  Reddy", "Alexey L. Lastovetsky"], "year": 2019, "n_citations": 0}
{"id": 43743, "s2_id": "a119b783940e198e51c622ab0159afda53314d60", "title": "TTC: a tensor transposition compiler for multiple architectures", "abstract": "We consider the problem of transposing tensors of arbitrary dimension and describe TTC, an open source domain-specific parallel compiler. TTC generates optimized parallel C++/CUDA C code that achieves a significant fraction of the system's peak memory bandwidth. TTC exhibits high performance across multiple architectures, including modern AVX-based systems (e.g., Intel Haswell, AMD Steamroller), Intel's Knights Corner as well as different CUDA-based GPUs such as NVIDIA's Kepler and Maxwell architectures. We report speedups of TTC over a meaningful baseline implementation generated by external C++ compilers; the results suggest that a domain-specific compiler can outperform its general purpose counterpart significantly: For instance, comparing with Intel's latest C++ compiler on the Haswell and Knights Corner architecture, TTC yields speedups of up to 8x and 32x, respectively. We also showcase TTC's support for multiple leading dimensions, making it a suitable candidate for the generation of performance-critical packing functions that are at the core of the ubiquitous BLAS 3 routines.", "venue": "ARRAY@PLDI", "authors": ["Paul  Springer", "Aravind  Sankaran", "Paolo  Bientinesi"], "year": 2016, "n_citations": 16}
{"id": 49141, "s2_id": "b09141eed768224c78e19a1710ba4b82cd9f15ff", "title": "Markovian Testing Equivalence and Exponentially Timed Internal Actions", "abstract": "In the theory of testing for Markovian processes developed so far, exponentially timed internal actions are not admitted within processes. When present, these actions cannot be abstracted away, because their execution takes a nonzero amount of time and hence can be observed. On the other hand, they must be carefully taken into account, in order not to equate processes that are distinguishable from a timing viewpoint. In this paper, we recast the definition of Markovian testing equivalence in the framework of a Markovian process calculus including exponentially timed internal actions. Then, we show that the resulting behavioral equivalence is a congruence, has a sound and complete axiomatization, has a modal logic characterization, and can be decided in polynomial time.", "venue": "QFM", "authors": ["Marco  Bernardo"], "year": 2009, "n_citations": 4}
{"id": 49173, "s2_id": "d0f56e92cb03f6f8e46034d79c963bd1af02dcc6", "title": "Probabilistic Bounds on the End-to-End Delay of Service Function Chains using Deep MDN", "abstract": "Ensuring the conformance of a service system\u2019s end-to-end delay to service level agreement (SLA) constraints is a challenging task that requires statistical measures beyond the average delay. In this paper, we study the real-time prediction of the end-to-end delay distribution in systems with composite services such as service function chains. In order to have a general framework, we use queueing theory to model service systems, while also adopting a statistical learning approach to avoid the limitations of queueing-theoretic methods such as stationarity assumptions or other approximations that are often used to make the analysis mathematically tractable. Specifically, we use deep mixture density networks (MDN) to predict the end-to-end distribution of the delay given the network\u2019s state. As a result, our method is sufficiently general to be applied in different contexts and applications. Our evaluations show a good match between the learned distributions and the simulations, which suggest that the proposed method is a good candidate for providing probabilistic bounds on the end-to-end delay of more complex systems where simulations or theoretical methods are not applicable.", "venue": "2020 IEEE 31st Annual International Symposium on Personal, Indoor and Mobile Radio Communications", "authors": ["Majid  Raeis", "Ali  Tizghadam", "Alberto  Leon-Garcia"], "year": 2020, "n_citations": 1}
{"id": 49706, "s2_id": "3fa8e223732112e2f676e47b9b66287861de7399", "title": "Performance Evaluation: Ball-Treeand KD-Tree in the Context of MST", "abstract": "Now a day\u2019s many algorithms are invented / being inventing to find the solution for Euclidean Minimum Spanning Tree (EMST) problem, as its applicability is increasing in much wide range of fields containing spatial / spatio \u2013 temporal data viz. astronomy which consists of millions of spatial data. To solve this problem, we are presenting a technique by adopting the dual tree algorithm for finding efficient EMST and experimented on a variety of real time and synthetic datasets. This paper presents the observed experimental observations and the efficiency of the dual tree framework,in the context of kd-tree and ball-tree on spatial datasets of different dimensions.", "venue": "SPIT/IPC", "authors": ["Hazarath  Munaga", "Venkata  Jarugumalli"], "year": 2011, "n_citations": 6}
{"id": 53181, "s2_id": "8ce6969e7d1d7d914e1c0e468cf9922ad2849017", "title": "The Petascale DTN Project: High Performance Data Transfer for HPC Facilities", "abstract": "The movement of large-scale (tens of Terabytes and larger) data sets between high performance computing (HPC) facilities is an important and increasingly critical capability. A growing number of scientific collaborations rely on HPC facilities for tasks which either require large-scale data sets as input or produce large-scale data sets as output. In order to enable the transfer of these data sets as needed by the scientific community, HPC facilities must design and deploy the appropriate data transfer capabilities to allow users to do data placement at scale. This paper describes the Petascale DTN Project, an effort undertaken by four HPC facilities, which succeeded in achieving routine data transfer rates of over 1PB/week between the This work was supported by the Director, Office of Science, Office of Advanced Scientific Computing Research (ASCR), of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. This manuscript has been authored by an author at Lawrence Berkeley National Laboratory under Contract No. DE-AC02-05CH11231 with the U.S. Department of Energy. The U.S. Government retains, and the publisher, by accepting the article for publication, acknowledges, that the U.S. Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for U.S. Government purposes. Disclaimer: This document was prepared as an account of work sponsored by the United States Government. While this document is believed to contain correct information, neither the United States Government nor any agency thereof, nor the Regents of the University of California, nor any of their employees, makes any warranty, express or implied, or assumes any legal responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by its trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof, or the Regents of the University of California. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof or the Regents of the University of California. facilities. We describe the design and configuration of the Data Transfer Node (DTN) clusters used for large-scale data transfers at these facilities, the software tools used, and the performance tuning that enabled this capability.", "venue": "ArXiv", "authors": ["Eli  Dart", "William  Allcock", "Wahid  Bhimji", "Tim  Boerner", "Ravinderjeet  Cheema", "Andrew  Cherry", "Brent  Draney", "Salman  Habib", "Damian  Hazen", "Jason  Hill", "Matt  Kollross", "Suzanne  Parete-Koon", "Daniel  Pelfrey", "Adrian  Pope", "Jeff  Porter", "David  Wheeler"], "year": 2021, "n_citations": 0}
{"id": 55553, "s2_id": "0c9c304eddf4fe82cd8ced5f0adc182fd97d0aaf", "title": "A hybrid performance analysis technique for distributed real-time embedded systems", "abstract": "It remains a challenging problem to tightly estimate the worst-case response time of an application in a distributed embedded system, especially when there are dependencies between tasks. Recently, a holistic worst-case response time analysis approach called scheduling time bound analysis has been proposed to find a tight upper bound of the worst-case response times of applications specified by a set of task graphs. Since it assumes that the starting offsets of applications are known and fixed, it fails to make a tight estimation despite increased computation time when the starting offsets are dynamic. To overcome this problem, we propose a novel conservative performance analysis, called hybrid performance analysis, combining the response time analysis technique and the scheduling time bound analysis technique to compute a tighter bound faster. The proposed scheme is proven to be conservative formally. Through extensive experiments with real-life benchmarks and synthetic examples, the superior performance of our proposed approach compared with previous methods is confirmed.", "venue": "Real-Time Systems", "authors": ["Junchul  Choi", "Hyunok  Oh", "Soonhoi  Ha"], "year": 2018, "n_citations": 3}
{"id": 57256, "s2_id": "181b0720dba96ef2f049b73c9592f2d581954ca6", "title": "A linear algebra approach to fast DNA mixture analysis using GPUs", "abstract": "Analysis of DNA samples is an important tool in forensics, and the speed of analysis can impact investigations. Comparison of DNA sequences is based on the analysis of short tandem repeats (STRs), which are short DNA sequences of 2\u20135 base pairs. Current forensics approaches use 20 STR loci for analysis. The use of single nucleotide polymorphisms (SNPs) has utility for analysis of complex DNA mixtures. The use of tens of thousands of SNPs loci for analysis poses significant computational challenges because the forensic analysis scales by the product of the loci count and number of DNA samples to be analyzed. In this paper, we discuss the implementation of a DNA sequence comparison algorithm by re-casting the algorithm in terms of linear algebra primitives. By developing an overloaded matrix multiplication approach to DNA comparisons, we can leverage advances in GPU hardware and algoithms for dense matrix multiplication (DGEMM) to speed up DNA sample comparisons. We show that it is possible to compare 2048 unknown DNA samples with 20 million known samples in under 6 seconds using a NVIDIA K80 GPU.", "venue": "2017 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Siddharth  Samsi", "Brian S. Helfer", "Jeremy  Kepner", "Albert  Reuther", "Darrell O. Ricke"], "year": 2017, "n_citations": 2}
{"id": 58805, "s2_id": "21e7023c4ed4448137e6f2f8f41e1adcc3ddcfb2", "title": "Simulation-Based Performance Prediction of HPC Applications: A Case Study of HPL", "abstract": "We propose a simulation-based approach for performance modeling of parallel applications on high-performance computing platforms. Our approach enables full-system performance modeling: (1) the hardware platform is represented by an abstract yet high-fidelity model; (2) the computation and communication components are simulated at a functional level, where the simulator allows the use of the components native interface; this results in a (3) fast and accurate simulation of full HPC applications with minimal modifications to the application source code. This hardware/software hybrid modeling methodology allows for low overhead, fast, and accurate exascale simulation and can be easily carried out on a standard client platform (desktop or laptop). We demonstrate the capability and scalability of our approach with High Performance LINPACK (HPL), the benchmark used to rank supercomputers in the TOP500 list. Our results show that our modeling approach can accurately and efficiently predict the performance of HPL at the scale of the TOP500 list supercomputers. For instance, the simulation of HPL on Frontera takes less than five hours with an error rate of four percent.", "venue": "2020 IEEE/ACM International Workshop on HPC User Support Tools (HUST) and Workshop on Programming and Performance Visualization Tools (ProTools)", "authors": ["Gen  Xu", "Huda  Ibeid", "Xin  Jiang", "Vjekoslav  Svilan", "Zhaojuan  Bian"], "year": 2020, "n_citations": 1}
{"id": 60207, "s2_id": "69a98cb1e955f0431b61717ed42983e4c3fe069c", "title": "Analyzing DISH for multi-channel MAC protocols in wireless networks", "abstract": "For long, node cooperation has been exploited as a data relaying mechanism. However, the wireless channel allows for much richer interaction between nodes. One such scenario is in a multi-channel environment, where transmitter-receiver pairs may make incorrect decisions (e.g., in selecting channels) but idle neighbors could help by sharing information to prevent undesirable consequences (e.g., data collisions). This represents a Distributed Information SHaring (DISH) mechanism for cooperation and suggests new ways of designing cooperative protocols. However, what is lacking is a theoretical understanding of this new notion of cooperation. In this paper, we view cooperation as a network resource and evaluate the availability of cooperation via a metric, pco, the probability of obtaining cooperation. First, we analytically evaluate pco in the context of multi-channel multi-hop wireless networks. Second, we verify our analysis via simulations and the results show that our analysis accurately characterizes the behavior of pco as a function of underlying network parameters. This step also yields important insights into DISH with respect to network dynamics. Third, we investigate the correlation between pco and network performance in terms of collision rate, packet delay, and throughput. The results indicate a near-linear relationship, which may significantly simplify performance analysis for cooperative networks and suggests that pco be used as an appropriate performance indicator itself. Throughout this work, we utilize, as appropriate, three different DISH contexts - model-based DISH, ideal DISH, and real DISH - to explore pco.", "venue": "MobiHoc '08", "authors": ["Tie  Luo", "Mehul  Motani", "Vikram  Srinivasan"], "year": 2008, "n_citations": 10}
{"id": 63390, "s2_id": "f50f921bfed3d54e687766767f4c685ffd38c7bd", "title": "Improving TCP/IP Performance over Wireless IEEE 802.11 Link", "abstract": "Cellular phones, wireless laptops, personal portable devices that supports both voice and data access are all examples of communicating devices that uses wireless communication. Sine TCP/IP (and UDP) is the dominant technology in use in the internet, it is expected that they will be used (and they are currently) over wireless connections. In this paper, we investigate the performance of the TCP (and UDP) over IEEE802.11 wireless MAC protocol. We investigate the performance of the TCP and UDP assuming three different traffic patterns. First bulk transmission where the main concern is the throughput. Second real-time audio (using UDP) in the existence of bulk TCP transmission where the main concern is the packet loss for audio traffic. Finally web traffic where the main concern is the response time. We also investigate the effect of using forward Error Correction (FEC) technique and the MAC sublayer parameters on the throughput and response time.", "venue": "ArXiv", "authors": ["Milenko  Petrovic", "Mokhtar  Aboelaze"], "year": 2003, "n_citations": 0}
{"id": 65905, "s2_id": "b9b0cec67238d2dd0fccedaae758d7aa2fda2c68", "title": "On The Modeling of OpenFlow-based SDNs: The Single Node Case", "abstract": "OpenFlow is one of the most commonly used protocols for communication between the controller and the forwarding element in a software defined network (SDN). A model based on M/M/1 queues is proposed in [1] to capture the communication between the forwarding element and the controller. Albeit the model provides useful insight, it is accurate only for the case when the probability of expecting a new flow is small. Secondly, it is not straight forward to extend the model in [1] to more than one forwarding element in the data plane. In this work we propose a model which addresses both these challenges. The model is based on Jackson assumption but with corrections tailored to the OpenFlow based SDN network. Performance analysis using the proposed model indicates that the model is accurate even for the case when the probability of new flow is quite large. Further we show by a toy example that the model can be extended to more than one node in the data plane.", "venue": "ArXiv", "authors": ["Kashif  Mahmood", "Ameen  Chilwan", "Olav N. \u00d8sterb\u00f8", "Michael  Jarschel"], "year": 2014, "n_citations": 37}
{"id": 67964, "s2_id": "0e6a8329afe4d0471c2d11db86364dc86bb34b29", "title": "Mirrored and Hybrid Disk Arrays: Organization, Scheduling, Reliability, and Performance", "abstract": "Basic mirroring (BM) classified as RAID level 1 replicates data on two disks, thus doubling disk access bandwidth for read requests. RAID1/0 is an array of BM pairs with balanced loads due to striping. When a disk fails the read load on its pair is doubled, which results in halving the maximum attainable bandwidth. We review RAID1 organizations which attain a balanced load upon disk failure, but as shown by reliability analysis tend to be less reliable than RAID1/0. Hybrid disk arrays which store XORed instead of replicated data tend to have a higher reliability than mirrored disks, but incur a higher overhead in updating data. Read request response time can be improved by processing them at a higher priority than writes, since they have a direct effect on application response time. Shortest seek distance and affinity based routing both shorten seek time. Anticipatory arm placement places arms optimally to minimize the seek distance. The analysis of RAID1 in normal, degraded, and rebuild mode is provided to quantify RAID1/0 performance. We compare the reliability of mirrored disk organizations against each other and hybrid disks and erasure coded disk arrays.", "venue": "ArXiv", "authors": ["Alexander  Thomasian"], "year": 2018, "n_citations": 1}
{"id": 70074, "s2_id": "67263d09a76925ac13fffe7b410115b1cf7f4108", "title": "Squeezing Out the Cloud via Profit-Maximizing Resource Allocation Policies", "abstract": "We study the problem of maximizing the average hourly profit earned by a Software-as-a-Service (SaaS) provider who runs a software service on behalf of a customer using servers rented from an Infrastructure-as-a-Service (IaaS) provider. The SaaS provider earns a fee per successful transaction and incurs costs pro-portional to the number of server-hours it uses. A number of resource allocation policies for this or similar problems have been proposed in previous work. However, to the best of our knowledge, these policies have not been comparatively evaluated in a cloud environment. This paper reports on an empirical evaluation of three policies using a replica of Wikipedia deployed on the Amazon EC2 cloud. Experimental results show that a policy based on a solution to an optimization problem derived from the SaaS provider's utility function outperforms well-known heuristics that have been proposed for similar problems. It is also shown that all three policies outperform a \"reactive\" allocation approach based on Amazon's auto-scaling feature.", "venue": "2012 IEEE 20th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems", "authors": ["Michele  Mazzucco", "Martti  Vasar", "Marlon  Dumas"], "year": 2012, "n_citations": 15}
{"id": 70828, "s2_id": "3b1689700c66abcfc539ae212fc42f6e18ab50e5", "title": "A High-Performance Energy Management System Based on Evolving Graph", "abstract": "As the fast growth and large integration of distributed generation, renewable energy resource, energy storage system, and load response, the modern power system operation becomes much more complicated with increasing uncertainties and frequent changes. Increased operation risks are introduced to the existing commercial energy management system (EMS), due to its limited computational capability. In this brief, a high-performance EMS analysis framework based on the evolving graph is developed. A power grid is first modeled as an evolving graph and then the power system dynamic analysis applications, like network topology processing (NTP), state estimation (SE), power flow (PF), and contingency analysis (CA), are efficiently implemented on the system evolving graph to build a high-performance EMS analysis framework. Its computation performance is field tested using a 2749-bus power system in Sichuan, China. The results illustrate that the proposed EMS remarkably speeds up the computation performance and reaches the goal of real-time power system analysis.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Guangyi  Liu", "Chen  Yuan", "Xi  Chen", "Jingjin  Wu", "Renchang  Dai", "Zhiwei  Wang"], "year": 2020, "n_citations": 5}
{"id": 77254, "s2_id": "c56ca1e11e740ca53cefe868f181de8f6d2a466d", "title": "Parallel Accelerated Vector Similarity Calculations for Genomics Applications", "abstract": "The surge in availability of genomic data holds promise for enabling determination of genetic causes of observed individual traits, with applications to problems such as discovery of the genetic roots of phenotypes, be they molecular phenotypes such as gene expression or metabolite concentrations, or complex phenotypes such as diseases. However, the growing sizes of these datasets and the quadratic, cubic or higher scaling characteristics of the relevant algorithms pose a serious computational challenge necessitating use of leadership scale computing. In this paper we describe a new approach to performing vector similarity metrics calculations, suitable for parallel systems equipped with graphics processing units (GPUs) or Intel Xeon Phi processors. Our primary focus is the Proportional Similarity metric applied to Genome Wide Association Studies (GWAS) and Phenome Wide Association Studies (PheWAS). We describe the implementation of the algorithms on accelerated processors, methods used for eliminating redundant calculations due to symmetries, and techniques for efficient mapping of the calculations to many-node parallel systems. Results are presented demonstrating high per-node performance and parallel scalability with rates of more than five quadrillion elementwise comparisons achieved per second on the ORNL Titan system. In a companion paper we describe corresponding techniques applied to calculations of the Custom Correlation Coefficient for comparative genomics applications.", "venue": "Parallel Comput.", "authors": ["Wayne  Joubert", "James  Nance", "Deborah A. Weighill", "Daniel A. Jacobson"], "year": 2018, "n_citations": 9}
{"id": 79968, "s2_id": "80b108d973f3c6e118d1bd628b4b62c210491ee7", "title": "Stochastic Service Placement", "abstract": "Resource allocation for cloud services is a complex task due to the diversity of the services and the dynamic workloads. One way to address this is by overprovisioning which results in high cost due to the unutilized resources. A much more economical approach, relying on the stochastic nature of the demand, is to allocate just the right amount of resources and use additional more expensive mechanisms in case of overflow situations where demand exceeds the capacity. In this paper we study this approach and show both by comprehensive analysis for independent normal distributed demands and simulation on synthetic data that it is significantly better than currently deployed methods.", "venue": "ArXiv", "authors": ["Galia  Shabtai", "Danny  Raz", "Yuval  Shavitt"], "year": 2015, "n_citations": 1}
{"id": 80901, "s2_id": "2f1af7f4464dda311bb24c2939965225f33d6976", "title": "Movers and Shakers: Kinetic Energy Harvesting for the Internet of Things", "abstract": "Numerous energy harvesting wireless devices that will serve as building blocks for the Internet of Things (IoT) are currently under development. However, there is still only limited understanding of the properties of various energy sources and their impact on energy harvesting adaptive algorithms. Hence, we focus on characterizing the kinetic (motion) energy that can be harvested by a wireless node with an IoT form factor and on developing energy allocation algorithms for such nodes. In this paper, we describe methods for estimating harvested energy from acceleration traces. To characterize the energy availability associated with specific human activities (e.g., relaxing, walking, cycling), we analyze a motion dataset with over 40 participants. Based on acceleration measurements that we collected for over 200 hours, we study energy generation processes associated with day-long human routines. We also briefly summarize our experiments with moving objects. We develop energy allocation algorithms that take into account practical IoT node design considerations, and evaluate the algorithms using the collected measurements. Our observations provide insights into the design of motion energy harvesters, IoT nodes, and energy harvesting adaptive algorithms.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Maria  Gorlatova", "John  Sarik", "Guy  Grebla", "Mina  Cong", "Ioannis  Kymissis", "Gil  Zussman"], "year": 2015, "n_citations": 32}
{"id": 82082, "s2_id": "be8bda6ab6fd70c8ad0d077c3a262caecd9b1ab0", "title": "Block-Structured Double-Ended Queues and Bilateral QBD Processes", "abstract": "This paper studies a block-structured double-ended queue, whose block structure comes from two independent Markovian arrival processes (MAPs), and its stability is guaranteed by customers' impatient behaviors. We show that such a queue can be expressed as a new bilateral quasi birth-and-death (QBD) process. For this purpose, we provide a detailed analysis for the bilateral QBD process, including the system stability, the stationary probability vector, the sojourn time, and so forth. Furthermore, we develop three effective algorithms for computing the performance measures (i.e., the probabilities of stationary queue lengths, the average stationary queue lengths, and the average sojourn times) of the block-structured double-ended queue. Finally, numerical examples are employed to verify the correctness of our theoretical results, and illustrate how the performance measures of this queue are influenced by key system parameters. We believe that the methodology and results described in this paper can be applied to deal with general matching queues (e.g., bilateral Markov processes of GI/M/1 type and those of M/G/1 type) via developing their corresponding bilateral block-structured Markov processes, which are very useful in analyzing many practical issues, such as those encountered in sharing economy, organ transplantation, intelligent manufacturing, intelligent transportation, and so on.", "venue": "ArXiv", "authors": ["Heng-Li  Liu", "Quan-Lin  Li", "Yan-Xia  Chang", "Chi  Zhang"], "year": 2020, "n_citations": 3}
{"id": 87824, "s2_id": "8e265aa572f5e1fe9533e5c65641bb159eab803c", "title": "Cost minimization of network services with buffer and end-to-end deadline constraints", "abstract": "Cloud computing technology provides the means to share physical resources among multiple users and data center tenants by exposing them as virtual resources. There is a strong industrial drive to use similar technology and concepts to provide timing sensitive services. One such is virtual networking services, so called services chains, which consist of several interconnected virtual network functions. This allows for the capacity to be scaled up and down by adding or removing virtual resources. In this work, we develop a model of a service chain and pose the dynamic allocation of resources as an optimization problem. We design and present a set of strategies to allot virtual network nodes in an optimal fashion subject to latency and buffer constraints.", "venue": "SIGBED", "authors": ["Victor  Millnert", "Enrico  Bini", "Johan  Eker"], "year": 2018, "n_citations": 3}
{"id": 90829, "s2_id": "e871e0340e7fdbd9655a687f93277a4feb1bdb97", "title": "Relativistic hydrodynamics on graphic cards", "abstract": "Abstract We show how to accelerate relativistic hydrodynamics simulations using graphic cards (graphic processing units, GPUs). These improvements are of highest relevance e.g. to the field of high-energetic nucleus\u2013nucleus collisions at RHIC and LHC where (ideal and dissipative) relativistic hydrodynamics is used to calculate the evolution of hot and dense QCD matter. The results reported here are based on the Sharp And Smooth Transport Algorithm (SHASTA), which is employed in many hydrodynamical models and hybrid simulation packages, e.g. the Ultrarelativistic Quantum Molecular Dynamics model (UrQMD). We have redesigned the SHASTA using the OpenCL computing framework to work on accelerators like graphic processing units (GPUs) as well as on multi-core processors. With the redesign of the algorithm the hydrodynamic calculations have been accelerated by a factor 160 allowing for event-by-event calculations and better statistics in hybrid calculations.", "venue": "Comput. Phys. Commun.", "authors": ["Jochen  Gerhard", "Volker  Lindenstruth", "Marcus  Bleicher"], "year": 2013, "n_citations": 11}
{"id": 95870, "s2_id": "b4cc6d84341a57d628c05439c6bd13757976a60a", "title": "Restructuring Batch Normalization to Accelerate CNN Training", "abstract": "Because CNN models are compute-intensive, where billions of operations can be required just for an inference over a single input image, a variety of CNN accelerators have been proposed and developed. For the early CNN models, the research mostly focused on convolutional and fully-connected layers because the two layers consumed most of the computation cycles. For more recent CNN models, however, non-convolutional layers have become comparably important because of the popular use of newly designed non-convolutional layers and because of the reduction in the number and size of convolutional filters. Non-convolutional layers, including batch normalization (BN), typically have relatively lower computational intensity compared to the convolutional or fully-connected layers, and hence are often constrained by main-memory bandwidth. In this paper, we focus on accelerating the BN layers among the non-convolutional layers, as BN has become a core design block of modern CNNs. A typical modern CNN has a large number of BN layers. BN requires mean and variance calculations over each mini-batch during training. Therefore, the existing memory-access reduction techniques, such as fusing multiple CONV layers, are not effective for accelerating BN due to their inability to optimize mini-batch related calculations. To address this increasingly important problem, we propose to restructure BN layers by first splitting it into two sub-layers and then combining the first sub-layer with its preceding convolutional layer and the second sub-layer with the following activation and convolutional layers. The proposed solution can significantly reduce main-memory accesses while training the latest CNN models, and the experiments on a chip multiprocessor with our modified Caffe implementation show that the proposed BN restructuring can improve the performance of DenseNet with 121 convolutional layers by 28.4%.", "venue": "MLSys", "authors": ["Daejin  Jung", "Wonkyung  Jung", "Byeongho  Kim", "Sunjung  Lee", "Wonjong  Rhee", "Jung Ho Ahn"], "year": 2019, "n_citations": 29}
{"id": 97599, "s2_id": "5edbccff8abdf1c4509e27cf9c626815b6ec75f0", "title": "FPGA-based CNN inference accelerator synthesized from multi-threaded C software", "abstract": "A deep-learning inference accelerator is synthesized from a C-language software program parallelized with Pthreads. The software implementation uses the well-known producer/consumer model with parallel threads interconnected by FIFO queues. The LegUp high-level synthesis (HLS) [1] tool synthesizes threads into parallel FPGA hardware, translating software parallelism into spatial parallelism. A complete system is generated where convolution, pooling and padding are realized in the synthesized accelerator, with remaining tasks executing on an embedded ARM processor. The accelerator incorporates reduced precision, and a novel approach for zero-weight-skipping in convolution. On a mid-sized Intel Arria 10 SoC FPGA, peak performance on VGG-16 is 138 effective GOPS.", "venue": "2017 30th IEEE International System-on-Chip Conference (SOCC)", "authors": ["Jin Hee Kim", "Brett  Grady", "Ruolong  Lian", "John  Brothers", "Jason Helge Anderson"], "year": 2017, "n_citations": 28}
{"id": 97664, "s2_id": "307288723e46cd5683c5ceeba5e08a144004ec42", "title": "Extending Firewall Session Table to Accelerate NAT, QoS Classification and Routing", "abstract": "security and QoS are the two most precious objectives for network systems to be attained. Unfortunately, they are in conflict, while QoS tries to minimize processing delay, strong security protection requires more processing time and cause packet delay. This article is a step towards resolving this conflict by extending the firewall session table to accelerate NAT, QoS classification, and routing processing time while providing the same level of security protection. Index Terms ? stateful packet filtering; firewall; session/state table; QoS; NAT; Routing.", "venue": "ArXiv", "authors": ["Mahmoud  Mostafa", "Anas Abou El Kalam", "Christian  Fraboul"], "year": 2009, "n_citations": 2}
{"id": 97738, "s2_id": "707f9a3f72413e6ca5a8718a1a04afc14085bbea", "title": "One Size Does Not Fit All: Quantifying and Exposing the Accuracy-Latency Trade-Off in Machine Learning Cloud Service APIs via Tolerance Tiers", "abstract": "Today's cloud service architectures follow a \u201cone size fits all\u201d deployment strategy where the same service version instantiation is provided to the end users. However, consumers are broad and different applications have different accuracy and responsiveness requirements, which as we demonstrate renders the \u201cone size fits all\u201d approach inefficient in practice. We use a production grade speech recognition engine, which serves several thousands of users, and an open source computer vision based system, to explain our point. To overcome the limitations of the \u201cone size fits all\u201d approach, we recommend Tolerance Tiers where each MLaaS tier exposes an accuracy/responsiveness characteristic, and consumers can programmatically select a tier. We evaluate our proposal on the CPU-based automatic speech recognition (ASR) engine and cutting-edge neural networks for image classification deployed on both CPUs and GPUs. The results show that our proposed approach provides a MLaaS cloud service architecture that can be tuned by the end API user or consumer to outperform the conventional \u201cone size fits all\u201d approach.", "venue": "2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Matthew  Halpern", "Behzad  Boroujerdian", "Todd  Mummert", "Evelyn  Duesterwald", "Vijay Janapa Reddi"], "year": 2019, "n_citations": 6}
{"id": 102171, "s2_id": "e3cd3485afbe1f0ff044080e7ab11db0faffa396", "title": "Fundamental Limits of Online Network-Caching", "abstract": "Optimal caching of files in a content distribution network (CDN) is a problem of fundamental and growing commercial interest. Although many different caching algorithms are in use today, the fundamental performance limits of network caching algorithms from an online learning point-of-view remain poorly understood to date. In this paper, we resolve this question in the following two settings: (1) a single user connected to a single cache, and (2) a set of users and a set of caches interconnected through a bipartite network. Recently, an online gradient-based coded caching policy was shown to enjoy sub-linear regret. However, due to the lack of known regret lower bounds, the question of the optimality of the proposed policy was left open. In this paper, we settle this question by deriving tight non-asymptotic regret lower bounds in both of the above settings. In addition to that, we propose a new Follow-the-Perturbed-Leader-based uncoded caching policy with near-optimal regret. Technically, the lower-bounds are obtained by relating the online caching problem to the classic probabilistic paradigm of balls-into-bins. Our proofs make extensive use of a new result on the expected load in the most populated half of the bins, which might also be of independent interest. We evaluate the performance of the caching policies by experimenting with the popular MovieLens dataset and conclude the paper with design recommendations and a list of open problems.", "venue": "ArXiv", "authors": ["Rajarshi  Bhattacharjee", "Subhankar  Banerjee", "Abhishek  Sinha"], "year": 2020, "n_citations": 0}
{"id": 102592, "s2_id": "b873ab5a2a13143517a638b0d46ded876203cf41", "title": "On the Impact of Memory Allocation on High-Performance Query Processing", "abstract": "Somewhat surprisingly, the behavior of analytical query engines is crucially affected by the dynamic memory allocator used. Memory allocators highly influence performance, scalability, memory efficiency and memory fairness to other processes. In this work, we provide the first comprehensive experimental analysis on the impact of memory allocation for high-performance query engines. We test five state-of-the-art dynamic memory allocators and discuss their strengths and weaknesses within our DBMS. The right allocator can increase the performance of TPC-DS (SF 100) by 2.7x on a 4-socket Intel Xeon server.", "venue": "DaMoN", "authors": ["Dominik  Durner", "Viktor  Leis", "Thomas  Neumann"], "year": 2019, "n_citations": 9}
{"id": 102795, "s2_id": "cb3d214af27c4e25c7314319eaee3dbff583c275", "title": "How is a data-driven approach better than random choice in label space division for multi-label classification?", "abstract": "We propose using five data-driven community detection approaches from social networks to partition the label space for the task of multi-label classification as an alternative to random partitioning into equal subsets as performed by RAkELd: modularity-maximizing fastgreedy and leading eigenvector, infomap, walktrap and label propagation algorithms. We construct a label co-occurence graph (both weighted an unweighted versions) based on training data and perform community detection to partition the label set. We include Binary Relevance and Label Powerset classification methods for comparison. We use gini-index based Decision Trees as the base classifier. We compare educated approaches to label space divisions against random baselines on 12 benchmark data sets over five evaluation measures. We show that in almost all cases seven educated guess approaches are more likely to outperform RAkELd than otherwise in all measures, but Hamming Loss. We show that fastgreedy and walktrap community detection methods on weighted label co-occurence graphs are 85-92% more likely to yield better F1 scores than random partitioning. Infomap on the unweighted label co-occurence graphs is on average 90% of the times better than random paritioning in terms of Subset Accuracy and 89% when it comes to Jaccard similarity. Weighted fastgreedy is better on average than RAkELd when it comes to Hamming Loss.", "venue": "Entropy", "authors": ["Piotr  Szymanski", "Tomasz  Kajdanowicz", "Kristian  Kersting"], "year": 2016, "n_citations": 33}
{"id": 102868, "s2_id": "ce1d8bbf720fba99de2b102864cb476878e42b96", "title": "Energy Efficiency in Wireless Sensor Networks", "abstract": "Unlike most of the current research that focuses on a single aspect of WSNs, we present an Energy Driven Architecture (EDA) as a new architecture for minimising the total energy consumption of WSNs. EDA as a constituent-based architecture is used to deploy WSNs according to energy dissipation through their constituents. This view of overall energy consumption in WSNs can be applied to optimising and balancing energy consumption and increasing the network lifetime. Refer back to the architecture, we introduce a single overall model and propose a feasible formulation to express the overall energy consumption of a generic wireless sensor network application in terms of its energy constituents. The formulation offers a concrete expression for evaluating the performance of WSN application, optimising its constituents operations, and designing more energy-efficient applications. The ultimate aim is to produce an energy map architecture of a generic WSN application that comprises essential and definable energy constituents and the relationships between these constituents to explore strategies for minimising the overall energy consumption of the application. Later, parameters affecting energy in WSNs are extracted. The dependency between these parameters and the average energy consumption of an application is then investigated. A few statistical tools are applied for parameter reduction followed by random forest regression to model energy consumption per delivered packet with and without parameter reduction to determine the reduction in accuracy due to reduction. Finally, an energy-efficient dynamic topology management algorithm is proposed based on the EDA model and the prevalent parameters. The performance of the new topology management algorithm, which employs Dijkstra to find energy-efficient lowest cost paths among nodes, is compared to similar topology management algorithms.", "venue": "ArXiv", "authors": ["Najmeh Kamyab Pour"], "year": 2016, "n_citations": 41}
{"id": 103346, "s2_id": "028821c2f74bce87b58d99cf63a204fe5cce94e9", "title": "Optimal content placement for peer-to-peer video-on-demand systems", "abstract": "In this paper, we address the problem of content placement in peer-to-peer systems, with the objective of maximizing the utilization of peers' uplink bandwidth resources. We consider system performance under a many-user asymptotic. We distinguish two scenarios, namely \u201cDistributed Server Networks\u201d (DSN) for which requests are exogenous to the system, and \u201cPure P2P Networks\u201d (PP2PN) for which requests emanate from the peers themselves. For both scenarios, we consider a loss network model of performance, and determine asymptotically optimal content placement strategies in the case of a limited content catalogue. We then turn to an alternative \u201clarge catalogue\u201d scaling where the catalogue size scales with the peer population. Under this scaling, we establish that storage space per peer must necessarily grow unboundedly if bandwidth utilization is to be maximized. Relating the system performance to properties of a specific random graph model, we then identify a content placement strategy and a request acceptance policy which jointly maximize bandwidth utilization, provided storage space per peer grows unboundedly, although arbitrarily slowly, with system size.", "venue": "2011 Proceedings IEEE INFOCOM", "authors": ["Bo  Tan", "Laurent  Massouli\u00e9"], "year": 2011, "n_citations": 63}
{"id": 105821, "s2_id": "1c36fb88e2e663d18790357425f646ad5ac7dac3", "title": "Approximating Activation Functions", "abstract": "ReLU is widely seen as the default choice for activation functions in neural networks. However, there are cases where more complicated functions are required. In particular, recurrent neural networks (such as LSTMs) make extensive use of both hyperbolic tangent and sigmoid functions. These functions are expensive to compute. We used function approximation techniques to develop replacements for these functions and evaluated them empirically on three popular network configurations. We find safe approximations that yield a 10% to 37% improvement in training times on the CPU. These approximations were suitable for all cases we considered and we believe are appropriate replacements for all networks using these activation functions. We also develop ranged approximations which only apply in some cases due to restrictions on their input domain. Our ranged approximations yield a performance improvement of 20% to 53% in network training time. Our functions also match or considerably out perform the ad-hoc approximations used in Theano and the implementation of Word2Vec.", "venue": "ArXiv", "authors": ["Nicholas Gerard Timmons", "Andrew  Rice"], "year": 2020, "n_citations": 2}
{"id": 105959, "s2_id": "3ff814e4a4a01beef80334e25a51335f4016d085", "title": "Stacked-VLAN-Based Modeling of Hybrid ISP Traffic Control Schemes and Service Plans Exploiting Excess Bandwidth in Shared Access Networks", "abstract": "The current practice of shaping subscriber traffic using a token bucket filter by Internet service providers may result in a severe waste of network resources in shared access networks; except for a short period of time proportional to the size of a token bucket, it cannot allocate excess bandwidth among active subscribers even when there are only a few active subscribers. To better utilize the network resources in shared access networks, therefore, we recently proposed and analyzed the performance of access traffic control schemes, which can allocate excess bandwidth among active subscribers proportional to their token generation rates. Also, to exploit the excess bandwidth allocation enabled by the proposed traffic control schemes, we have been studying flexible yet practical service plans under a hybrid traffic control architecture, which are attractive to both an Internet service provider and its subscribers in terms of revenue and quality of service. In this paper we report the current status of our modeling of the hybrid traffic control schemes and service plans with OMNeT++/INET-HNRL based on IEEE standard 802.1Q stacked VLANs.", "venue": "ArXiv", "authors": ["Kyeong Soo Kim"], "year": 2016, "n_citations": 0}
{"id": 110226, "s2_id": "4b798d2886e1f3fc214676b731e6e306eb1cfa74", "title": "PyTorch-Direct: Enabling GPU Centric Data Access for Very Large Graph Neural Network Training with Irregular Accesses", "abstract": "With the increasing adoption of graph neural networks (GNNs) in the machine learning community, GPUs have become an essential tool to accelerate GNN training. However, training GNNs on very large graphs that do not fit in GPU memory is still a challenging task. Unlike conventional neural networks, mini-batching input samples in GNNs requires complicated tasks such as traversing neighboring nodes and gathering their feature values. While this process accounts for a significant portion of the training time, we find existing GNN implementations using popular deep neural network (DNN) libraries such as PyTorch are limited to a CPU-centric approach for the entire data preparation step. This \"all-in-CPU\" approach has negative impact on the overall GNN training performance as it over-utilizes CPU resources and hinders GPU acceleration of GNN training. To overcome such limitations, we introduce PyTorch-Direct, which enables a GPU-centric data accessing paradigm for GNN training. In PyTorch-Direct, GPUs are capable of efficiently accessing complicated data structures in host memory directly without CPU intervention. Our microbenchmark and end-to-end GNN training results show that PyTorch-Direct reduces data transfer time by 47.1% on average and speeds up GNN training by up to 1.6x. Furthermore, by reducing CPU utilization, PyTorch-Direct also saves system power by 12.4% to 17.5% during training. To minimize programmer effort, we introduce a new \"unified tensor\" type along with necessary changes to the PyTorch memory allocator, dispatch logic, and placement rules. As a result, users need to change at most two lines of their PyTorch GNN training code for each tensor object to take advantage of PyTorch-Direct.", "venue": "ArXiv", "authors": ["Seung Won Min", "Kun  Wu", "Sitao  Huang", "Mert  Hidayetouglu", "Jinjun  Xiong", "Eiman  Ebrahimi", "Deming  Chen", "Wen-mei  Hwu"], "year": 2021, "n_citations": 0}
{"id": 117475, "s2_id": "dcd144c62ff0819eb39352e6b801086cf6134d03", "title": "Parallel computing environments and methods for power distribution system simulation", "abstract": "The development of cost-effective high-performance parallel computing on multi-processor supercomputers makes it attractive to port excessively time consuming simulation software from personal computers (PC) to super computes. The power distribution system simulator (PDSS) takes a bottom-up approach and simulates load at the appliance level, where detailed thermal models for appliances are used. This approach works well for a small power distribution system consisting of a few thousand appliances. When the number of appliances increases, the simulation uses up the PC memory and its runtime increases to a point where the approach is no longer feasible to model a practical large power distribution system. This paper presents an effort made to port a PC-based power distribution system simulator to a 128-processor shared-memory supercomputer. The paper offers an overview of the parallel computing environment and a description of the modification made to the PDSS model. The performance of the PDSS running on a standalone PC and on the supercomputer is compared. Future research direction of utilizing parallel computing in the power distribution system simulation is also addressed.", "venue": "IEEE Power Engineering Society General Meeting, 2005", "authors": ["Ning  Lu", "Z. Todd Taylor", "David P. Chassin", "Ross T. Guttromson", "R. Scott Studham"], "year": 2005, "n_citations": 3}
{"id": 117645, "s2_id": "05f04918e235cecdf4a76abf499dd7b5f98e5641", "title": "Task parallel implementation of a solver for electromagnetic scattering problems", "abstract": "Electromagnetic computations, where the wavelength is small in relation to the geometry of interest, become computationally demanding. In order to manage computations for realistic problems like electromagnetic scattering from aircraft, the use of parallel computing is essential. In this paper, we describe how a solver based on a hierarchical nested equivalent source approximation can be implemented in parallel using a task based programming model. We show that the effort for moving from the serial implementation to a parallel implementation is modest due to the task based programming paradigm, and that the performance achieved on a multicore system is excellent provided that the task size, depending on the method parameters, is large enough.", "venue": "ArXiv", "authors": ["Afshin  Zafari", "Elisabeth  Larsson", "Marco  Righero", "Matteo Alessandro Francavilla", "Giorgio  Giordanengo", "Francesca  Vipiana", "Giuseppe  Vecchi"], "year": 2018, "n_citations": 5}
{"id": 121967, "s2_id": "05313e1290182beb7e06f6f527d144cf70e77cdb", "title": "DCCast: Efficient Point to Multipoint Transfers Across Datacenters", "abstract": "Using multiple datacenters allows for higher availability, load balancing and reduced latency to customers of cloud services. To distribute multiple copies of data, cloud providers depend on inter-datacenter WANs that ought to be used efficiently considering their limited capacity and the ever-increasing data demands. In this paper, we focus on applications that transfer objects from one datacenter to several datacenters over dedicated inter-datacenter networks. We present DCCast, a centralized Point to Multi-Point (P2MP) algorithm that uses forwarding trees to efficiently deliver an object from a source datacenter to required destination datacenters. With low computational overhead, DCCast selects forwarding trees that minimize bandwidth usage and balance load across all links. With simulation experiments on Google's GScale network, we show that DCCast can reduce total bandwidth usage and tail Transfer Completion Times (TCT) by up to $50\\%$ compared to delivering the same objects via independent point-to-point (P2P) transfers.", "venue": "HotCloud", "authors": ["Mohammad  Noormohammadpour", "Cauligi S. Raghavendra", "Sriram  Rao", "Srikanth  Kandula"], "year": 2017, "n_citations": 30}
{"id": 122372, "s2_id": "b89259fae1b4e132799deb416bf58a75dad55760", "title": "Doing More for Less -- Cache-Aware Parallel Contraction Hierarchies Preprocessing", "abstract": "Contraction Hierarchies is a successful speedup-technique to Dijkstra's seminal shortest path algorithm that has a convenient trade-off between preprocessing and query times. We investigate a shared-memory parallel implementation that uses $O(n+m)$ space for storing the graph and O(1) space for each core during preprocessing. The presented data structures and algorithms consequently exploits cache locality and thus exhibit competitive preprocessing times. The presented implementation is especially suitable for preprocessing graphs of planet-wide scale in practice. Also, our experiments show that optimal data structures in the PRAM model can be beaten in practice by exploiting memory cache hierarchies.", "venue": "ArXiv", "authors": ["Dennis  Luxen", "Dennis  Schieferdecker"], "year": 2012, "n_citations": 2}
{"id": 124025, "s2_id": "8d079a4cd9f7886cf44cd8bba7a8c62bd4cf92d4", "title": "Theoretical Performance Analysis of Vehicular Broadcast Communications at Intersection and Their Optimization", "abstract": "Cooperative vehicle safety (CVS) systems are a key application of intelligent transportation systems using vehicle-to-vehicle (V2V) communications because they include many applications, such as cooperative collision warning. In CVS systems, vehicles periodically broadcast their information, e.g., position and speed, so that vehicles can track the positions of other vehicles. In this paper, we propose an optimization method for the broadcast rate in V2V broadcast communications at an intersection on the basis of theoretical analysis. We consider a model in which locations of vehicles are modeled separately as queuing and running segments and derive key performance metrics of V2V broadcast communications via a stochastic geometry approach. Since these theoretical expressions are mathematically intractable, we develop closed-form approximate formulae for them. Using them, we optimize the broadcast rate such that the mean number of successful receivers per unit time is maximized. Because of the closed form approximation, the optimal rate can be used as a guideline for a real-time control-method. We evaluated our method through numerical examples and demonstrated the effectiveness of our method.", "venue": "2019 31st International Teletraffic Congress (ITC 31)", "authors": ["Tatsuaki  Kimura", "Hiroshi  Saito"], "year": 2019, "n_citations": 4}
{"id": 129763, "s2_id": "a4e5aaf7943c0959f69a1bfb8daf14d1689cd32a", "title": "Visualizing the world's largest turbulence simulation", "abstract": "In this exploratory submission we present the visualization of the largest interstellar turbulence simulations ever performed, unravelling key astrophysical processes concerning the formation of stars and the relative role of magnetic fields. The simulations, including pure hydrodynamical (HD) and magneto-hydrodynamical (MHD) runs, up to a size of $10048^3$ grid elements, were produced on the supercomputers of the Leibniz Supercomputing Centre and visualized using the hybrid parallel (MPI+TBB) ray-tracing engine OSPRay associated with VisIt. Besides revealing features of turbulence with an unprecedented resolution, the visualizations brilliantly showcase the stretching-and-folding mechanisms through which astrophysical processes such as supernova explosions drive turbulence and amplify the magnetic field in the interstellar gas, and how the first structures, the seeds of newborn stars are shaped by this process.", "venue": "Parallel Comput.", "authors": ["Salvatore  Cielo", "Luigi  Iapichino", "Johannes  G\u00fcnther", "Christoph  Federrath", "Elisabeth  Mayer", "Markus  Wiedemann"], "year": 2021, "n_citations": 3}
{"id": 132563, "s2_id": "420446d679e3c221da2848a3a1b87f910eff5b62", "title": "Global communications in multiprocessor simulations of flames", "abstract": "In this paper we investigate performance of global communications in a particular parallel code. The code simulates dynamics of expansion of premixed spherical flames using an asymptotic model of Sivashinsky type and a spectral numerical algorithm. As a result, the code heavily relies on global all-to-all interprocessor communications implementing transposition of the distributed data array in which numerical solution to the problem is stored. This global data interdependence makes interprocessor connectivity of the HPC system as important as the floating-point power of the processors of which the system is built. Our experiments show that efficient numerical simulation of this particular model, with global data interdependence, on modern HPC systems is possible. Prospects of performance of more sophisticated models of flame dynamics are analysed as well.", "venue": "ArXiv", "authors": ["Vladimir  Karlin"], "year": 2009, "n_citations": 0}
{"id": 133765, "s2_id": "ef7ea56aec27d794c354d6467f9c5647cc9113e2", "title": "Computational Optimal Transport for 5G Massive C-RAN Device Association", "abstract": "The massive scale of future wireless networks will cause computational bottlenecks in performance optimization. In this paper, we study the problem of connecting mobile traffic to Cloud RAN (C-RAN) stations. To balance station load, we steer the traffic by designing device association rules. The baseline association rule connects each device to the station with the strongest signal, which does not account for interference or traffic hot spots, and leads to load imbalances and performance deterioration. Instead, we can formulate an optimization problem to decide centrally the best association rule at each time instance. However, in practice this optimization has such high dimensions, that even linear programming solvers fail to solve. To address the challenge of massive connectivity, we propose an approach based on the theory of optimal transport, which studies the economical transfer of probability between two distributions. Our proposed methodology can further inspire scalable algorithms for massive optimization problems in wireless networks.", "venue": "2018 IEEE Global Communications Conference (GLOBECOM)", "authors": ["Georgios S. Paschos", "Nikolaos  Liakopoulos", "M\u00e9rouane  Debbah", "Tong  Wen"], "year": 2018, "n_citations": 3}
{"id": 134273, "s2_id": "af52cbea77f6d9a09fa42ddee5193d914b0f186e", "title": "Cubic metric reduction for repetitive CAZAC sequences in frequency domain", "abstract": "To meet the requirements of Occupied Channel Bandwidth (OCB) of unlicensed spectrum, in NR-based Access to Unlicensed Spectrum (NR-U) of 5G New Radio (NR) system, the channels of PRACH and PUCCH have to employ some frequency domain sequence repetition schemes. These repetition schemes cause serious Cubic Metric (CM) problems for these channels, although these two types of channels are composed of Constant Amplitude Zero Auto-correlation (CAZAC) sequences. Considering the properties of CAZAC sequences, which are used for PRACH and PUCCH (refer to PUCCH format 0 and format 1) in 5G NR system, in this paper, we propose some new schemes of CM reduction for these two channels taking into account the design principles to ensure the sequence performance of the autocorrelation and cross-correlation. Then the recommended CM reduction schemes are evaluated and the optimized parameters are further provided considering both CM performance and the complexity.", "venue": "China Communications", "authors": ["Yajun  Zhao", "Juan  Liu", "Saijin  Xie"], "year": 2021, "n_citations": 2}
{"id": 134489, "s2_id": "907e33b6d6fb693fe3c794ad1ba933ce552529d5", "title": "Optimizing Redundancy Levels in Master-Worker Compute Clusters for Straggler Mitigation", "abstract": "Runtime variability in computing systems causes some tasks to straggle and take much longer than expected to complete. These straggler tasks are known to significantly slowdown distributed computation. Job execution with speculative execution of redundant tasks has been the most widely deployed technique for mitigating the impact of stragglers, and many recent theoretical papers have studied the advantages and disadvantages of using redundancy under various system and service models. However, no clear guidelines could yet be found on when, for which jobs, and how much redundancy should be employed in Master-Worker compute clusters, which is the most widely adopted architecture in modern compute systems. We are concerned with finding a strategy for scheduling jobs with redundancy that works well in practice. This is a complex optimization problem, which we address in stages. We first use Reinforcement Learning (RL) techniques to learn good scheduling principles from realistic experience. Building on these principles, we derive a simple scheduling policy and present an approximate analysis of its performance. Specifically, we derive expressions to decide when and which jobs should be scheduled with how much redundancy. We show that policy that we devise in this way performs as good as the more complex policies that are derived by RL. Finally, we extend our approximate analysis to the case when system employs the other widely deployed remedy for stragglers, which is relaunching straggler tasks after waiting some time. We show that scheduling with redundancy significantly outperforms straggler relaunch policy when the offered load on the system is low or moderate, and performs slightly worse when the offered load is very high.", "venue": "ArXiv", "authors": ["Mehmet Fatih Aktas", "Emina  Soljanin"], "year": 2019, "n_citations": 3}
{"id": 135637, "s2_id": "6c0712b6eecb41e3791bed2883a3f083e0e602c6", "title": "A Generic Library for Stencil Computations", "abstract": "In this era of diverse and heterogeneous computer architectures, the programmability issues, such as productivity and portable efficiency, are crucial to software development and algorithm design. One way to approach the problem is to step away from traditional sequential programming languages and move toward domain specific programming environments to balance between expressivity and efficiency. In order to demonstrate this principle, we developed a domain specific C++ generic library for stencil computations, like PDE solvers. The library features high level constructs to specify computation and allows the development of parallel stencil computations with very limited effort. The high abstraction constructs (like do_all and do_reduce) make the program shorter and cleaner with increased contextual information for better performance exploitation. The results show good performance from Windows multicores, to HPC clusters and machines with accelerators, like GPUs.", "venue": "ArXiv", "authors": ["Mauro  Bianco", "Ugo  Varetto"], "year": 2012, "n_citations": 10}
{"id": 136702, "s2_id": "2b9666ae57763b8676b57f482a52f2dcb19dfcf7", "title": "User Mode Memory Page Management: An old idea applied anew to the memory wall problem", "abstract": "It is often said that one of the biggest limitations on computer performance is memory bandwidth (i.e.\"the memory wall problem\"). In this position paper, I argue that if historical trends in computing evolution (where growth in available capacity is exponential and reduction in its access latencies is linear) continue as they have, then this view is wrong - in fact we ought to be concentrating on reducing whole system memory access latencies wherever possible, and by \"whole system\" I mean that we ought to look at how software can be unnecessarily wasteful with memory bandwidth due to legacy design decisions. To this end I conduct a feasibility study to determine whether we ought to virtualise the MMU for each application process such that it has direct access to its own MMU page tables and the memory allocated to a process is managed exclusively by the process and not the kernel. I find under typical conditions that nearly scale invariant performance to memory allocation size is possible such that hundreds of megabytes of memory can be allocated, relocated, swapped and deallocated in almost the same time as kilobytes (e.g. allocating 8Mb is 10x quicker under this experimental allocator than a conventional allocator, and resizing a 128Kb block to 256Kb block is 4.5x faster). I find that first time page access latencies are improved tenfold; moreover, because the kernel page fault handler is never called, the lack of cache pollution improves whole application memory access latencies increasing performance by up to 2x. Finally, I try binary patching existing applications to use the experimental allocation technique, finding almost universal performance improvements without having to recompile these applications to make better use of the new facilities.", "venue": "ArXiv", "authors": ["Niall  Douglas"], "year": 2011, "n_citations": 1}
{"id": 139979, "s2_id": "98f85d3c96f907116944dbfeb1a6630a4a2fd81d", "title": "The Evaluation of Rating Systems in Team-based Battle Royale Games", "abstract": "Online competitive games have become a mainstream entertainment platform. To create a fair and exciting experience, these games use rating systems to match players with similar skills. While there has been an increasing amount of research on improving the performance of these systems, less attention has been paid to how their performance is evaluated. In this paper, we explore the utility of several metrics for evaluating three popular rating systems on a real-world dataset of over 25,000 team battle royale matches. Our results suggest considerable differences in their evaluation patterns. Some metrics were highly impacted by the inclusion of new players. Many could not capture the real differences between certain groups of players. Among all metrics studied, normalized discounted cumulative gain (NDCG) demonstrated more reliable performance and more flexibility. It alleviated most of the challenges faced by the other metrics while adding the freedom to adjust the focus of the evaluations on different groups of players.", "venue": "ArXiv", "authors": ["Arman  Dehpanah", "Muheeb Faizan Ghori", "Jonathan  Gemmell", "Bamshad  Mobasher"], "year": 2021, "n_citations": 2}
{"id": 141068, "s2_id": "c6e9cbeae18687b3451e14b919aeadf29e148c03", "title": "On the sojourn time of a batch in the M[X]/M/1 Processor Sharing Queue", "abstract": "In this paper, we analyze the sojourn of an entire batch in a processor sharing $M^{[X]}/M/1$ processor queue, where geometrically distributed batches arrive according to a Poisson process and jobs require exponential service times. By conditioning on the number of jobs in the systems and the number of jobs in a tagged batch, we establish recurrence relations between conditional sojourn times, which subsequently allow us to derive a partial differential equation for an associated bivariate generating function. This equation involves an unknown generating function, whose coefficients can be computed by solving an infinite lower triangular linear system. Once this unknown function is determined, we compute the Laplace transform and the mean value of the sojourn time of a batch in the system.", "venue": "ArXiv", "authors": ["Fabrice  Guillemin", "Alain  Simonian", "Ridha  Nasri", "Veronica Quintuna Rodriguez"], "year": 2020, "n_citations": 1}
{"id": 144249, "s2_id": "8b8753316ea4173819c091103f54da925c6164e2", "title": "Design And Develop Network Storage Virtualization By Using GNS3", "abstract": "Virtualization is an emerging and optimistic prospect in the IT industry. Its impact has a footprint widely in digital infrastructure. Many innovativeness sectors utilized the concept of virtualization to reduce the cost of frameworks. In this paper, we have designed and developed storage virtualization for physical functional solutions. It is an auspicious type of virtualization that is accessible, secure, scalable, and manageable. In the paper, we have proposed the pool storage method used the RAID-Z file system with the ZFS model which provides the duplication of site approach, compression blueprint, adequate backup methods, expansion in error-correcting techniques, and tested procedure on the real-time network location. Therefore, this study provides useful guidelines to design and develop optimized storage virtualization.", "venue": "ArXiv", "authors": ["Abdul Ahad Abro", "Ufaque  Shaikh"], "year": 2020, "n_citations": 0}
{"id": 145439, "s2_id": "5623e090981227fcada75845785c601fd6c06fd8", "title": "On the random access performance of Cell Broadband Engine with graph analysis application", "abstract": "The Cell Broad Engine (BE) Processor has unique memory access architecture besides its powerful computing engines. Many computing-intensive applications have been ported to Cell/BE successfully. But memory-intensive applications are rarely investigated except for several micro benchmarks. Since Cell/BE has powerful software visible DMA engine, this paper studies on whether Cell/BE is suit for applica- tions with large amount of random memory accesses. Two benchmarks, GUPS and SSCA#2, are used. The latter is a rather complex one that in representative of real world graph analysis applications. We find both benchmarks have good performance on Cell/BE based IBM QS20/22. Com- pared with 2 conventional multi-processor systems with the same core/thread number, GUPS is about 40-80% fast and SSCA#2 about 17-30% fast. The dynamic load balanc- ing and software pipeline for optimizing SSCA#2 are intro- duced. Based on the experiment, the potential of Cell/BE for random access is analyzed in detail as well as its limita- tions of memory controller, atomic engine and TLB manage- ment.Our research shows although more programming effort are needed, Cell/BE has the potencial for irregular memory access applications.", "venue": "ArXiv", "authors": ["Mingyu  Chen", "David A. Bader", "Seunghwa  Kang"], "year": 2011, "n_citations": 0}
{"id": 146522, "s2_id": "b8e3310c807504476d4739539e236270e9037bbf", "title": "A Survey on Network Tomography With Network Coding", "abstract": "The overhead of internal network monitoring motivates techniques of network tomography. Network coding (NC) presents a new opportunity for network tomography as NC introduces topology-dependent correlation that can be further exploited in topology estimation. Compared with traditional methods, network tomography with NC has many advantages, such as the improvement of tomography accuracy and the reduction of complexity in choosing monitoring paths. In this paper, we first introduce the problem of tomography with NC and then propose the taxonomy criteria to classify various methods. We also present existing solutions and future trend. We expect that our comprehensive review on network tomography with NC can serve as a good reference for researchers and practitioners working in the area.", "venue": "IEEE Communications Surveys & Tutorials", "authors": ["Peng  Qin", "Bin  Dai", "Benxiong  Huang", "Guan  Xu", "Kui  Wu"], "year": 2014, "n_citations": 20}
{"id": 146770, "s2_id": "1e572580ab12b52c144a20463aef643b542dd020", "title": "Allocation and Admission Policies for Service Streams", "abstract": "A service provisioning system is examined, where a number of servers are used to offer different types of services to paying customers. A customer is charged for the execution of a stream of jobs; the number of jobs in the stream and the rate of their submission is specified. On the other hand, the provider promises a certain quality of service (QoS), measured by the average waiting time of the jobs in the stream. A penalty is paid if the agreed QoS requirement is not met. The objective is to maximize the total average revenue per unit time. Dynamic policies for making server allocation and stream admission decisions are introduced and evaluated. The results of several simulations are described.", "venue": "2008 IEEE International Symposium on Modeling, Analysis and Simulation of Computers and Telecommunication Systems", "authors": ["Michele  Mazzucco", "Isi  Mitrani", "Mike  Fisher", "Paul  McKee"], "year": 2008, "n_citations": 11}
{"id": 149535, "s2_id": "e198afe101a604dbe871e5f56acdea1136443d40", "title": "Automatic Throughput and Critical Path Analysis of x86 and ARM Assembly Kernels", "abstract": "Useful models of loop kernel runtimes on out-of-order architectures require an analysis of the in-core performance behavior of instructions and their dependencies. While an instruction throughput prediction sets a lower bound to the kernel runtime, the critical path defines an upper bound. Such predictions are an essential part of analytic (i.e., white-box) performance models like the Roofline and Execution-Cache-Memory (ECM) models. They enable a better understanding of the performance-relevant interactions between hardware architecture and loop code. The Open Source Architecture Code Analyzer (OSACA) is a static analysis tool for predicting the execution time of sequential loops. It previously supported only x86 (Intel and AMD) architectures and simple, optimistic full-throughput execution. We have heavily extended OSACA to support ARM instructions and critical path prediction including the detection of loop-carried dependencies, which turns it into a versatile cross-architecture modeling tool. We show runtime predictions for code on Intel Cascade Lake, AMD Zen, and Marvell ThunderX2 micro-architectures based on machine models from available documentation and semi-automatic benchmarking. The predictions are compared with actual measurements.", "venue": "2019 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)", "authors": ["Jan  Laukemann", "Julian  Hammer", "Georg  Hager", "Gerhard  Wellein"], "year": 2019, "n_citations": 8}
{"id": 151736, "s2_id": "fed105d5a5fb16884d598e9b45a44ea5ae59fbbf", "title": "Incentivizing Sharing in Realtime D2D Streaming Networks: A Mean Field Game Perspective", "abstract": "We consider the problem of streaming live content to a cluster of co-located wireless devices that have both an expensive unicast base-station-to-device (B2D) interface, as well as an inexpensive broadcast device-to-device (D2D) interface, which can be used simultaneously. Our setting is a streaming system that uses a block-by-block random linear coding approach to achieve a target percentage of on-time deliveries with minimal B2D usage. Our goal is to design an incentive framework that would promote such cooperation across devices, while ensuring good quality of service. Based on the ideas drawn from truth-telling auctions, we design a mechanism that achieves this goal via appropriate transfers (monetary payments or rebates) in a setting with a large number of devices, and with peer arrivals and departures. Here, we show that a mean field game can be used to accurately approximate our system. Furthermore, the complexity of calculating the best responses under this regime is low. We implement the proposed system on an Android testbed, and illustrate its efficient performance using real world experiments.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Jian  Li", "Rajarshi  Bhattacharyya", "Suman  Paul", "Srinivas  Shakkottai", "Vijay  Subramanian"], "year": 2017, "n_citations": 2}
{"id": 157657, "s2_id": "c84a592178bb732ec25f2604f8a2533b673a52f5", "title": "On the Capacity Requirement for Arbitrary End-to-End Deadline and Reliability Guarantees in Multi-hop Networks", "abstract": "It has been shown that it is impossible to achieve both stringent end-to-end deadline and reliability guarantees in a large network without having complete information of all future packet arrivals. In order to maintain desirable performance in the presence of uncertainty of future packet arrivals, common practice is to add redundancy by increasing link capacities. This paper studies the amount of capacity needed to provide stringent performance guarantees and propose a low-complexity online algorithm. Without adding redundancy, we further propose a low-complexity order-optimal online policy for the network.", "venue": "SIGMETRICS 2017", "authors": ["Han  Deng", "I-Hong  Hou"], "year": 2017, "n_citations": 2}
{"id": 158154, "s2_id": "d982db5d813d82237ad4ad8afb1a690771728aa9", "title": "Practical Interpolation for Spectrum Cartography through Local Path Loss Modeling", "abstract": "A fundamental building block for supporting better utilization of radio spectrum involves predicting the impact that an emitter will have at different geographic locations. To this end, fixed sensors can be deployed to spatially sample the RF environment over an area of interest, with interpolation methods used to infer received power at locations between sensors. This paper describes a radio map interpolation method that exploits the known properties of most path loss models, with the aim of minimizing the RMS errors in predicted dB-power. We show that the results come very close to those for ideal Simple Kriging. Moreover, the method is simpler in terms of real-time computation by the network and it requires no knowledge of the spatial correlation of shadow fading. Our analysis of the method is general, but we exemplify it for a specific network geometry, comprising a grid-like pattern of sensors. We also provide comparisons to other widely used interpolation methods.", "venue": "ArXiv", "authors": ["Shweta  Sagari", "Larry J. Greenstein", "Wade  Trappe"], "year": 2016, "n_citations": 0}
{"id": 160257, "s2_id": "91f40d97180bbce7413f0016121d2a70928b2902", "title": "A General Formula for the Stationary Distribution of the Age of Information and Its Application to Single-Server Queues", "abstract": "This paper considers the stationary distribution of the age of information (AoI) in information update systems. We first derive a general formula for the stationary distribution of the AoI, which holds for a wide class of information update systems. The formula indicates that the stationary distribution of the AoI is given in terms of the stationary distributions of the system delay and the peak AoI. To demonstrate its applicability and usefulness, we analyze the AoI in single-server queues with four different service disciplines: first-come first-served (FCFS), preemptive last-come first-served (LCFS), and two variants of non-preemptive LCFS service disciplines. For the FCFS and the preemptive LCFS service disciplines, the GI/GI/1, M/GI/1, and GI/M/1 queues are considered, and for the non-preemptive LCFS service disciplines, the M/GI/1 and GI/M/1 queues are considered. With these results, we further show comparison results for the mean AoI\u2019s in the M/GI/1 and GI/M/1 queues under those service disciplines.", "venue": "IEEE Transactions on Information Theory", "authors": ["Yoshiaki  Inoue", "Hiroyuki  Masuyama", "Tetsuya  Takine", "Toshiyuki  Tanaka"], "year": 2019, "n_citations": 97}
{"id": 160335, "s2_id": "cc6f3feb59d513cfbe6f9c78208279eecff36e60", "title": "Towards Faster Reasoners By Using Transparent Huge Pages", "abstract": "Various state-of-the-art automated reasoning (AR) tools are widely used as backend tools in research of knowledge representation and reasoning as well as in industrial applications. In testing and verification, those tools often run continuously or nightly. In this work, we present an approach to reduce the runtime of AR tools by 10% on average and up to 20% for long running tasks. Our improvement addresses the high memory usage that comes with the data structures used in AR tools, which are based on conflict driven no-good learning. We establish a general way to enable faster memory access by using the memory cache line of modern hardware more effectively. Therefore, we extend the standard C library (glibc) by dynamically allowing to use a memory management feature called huge pages. Huge pages allow to reduce the overhead that is required to translate memory addresses between the virtual memory of the operating system and the physical memory of the hardware. In that way, we can reduce runtime, costs, and energy consumption of AR tools and applications with similar memory access patterns simply by linking the tool against this new glibc library when compiling it. In every day industrial applications this easily allows to be more eco-friendly in computation. To back up the claimed speed-up, we present experimental results for tools that are commonly used in the AR community, including the domains ASP, BMC, MaxSAT, SAT, and SMT.", "venue": "CP", "authors": ["Johannes K. Fichte", "Norbert  Manthey", "Julian  Stecklina", "Andr'e  Schidler"], "year": 2020, "n_citations": 4}
{"id": 163455, "s2_id": "74658c09f7b5d62e768ad699d86cc1963c46effa", "title": "Accelerating Training and Inference of Graph Neural Networks with Fast Sampling and Pipelining", "abstract": "Improving the training and inference performance of graph neural networks (GNNs) is faced with a challenge uncommon in general neural networks: creating mini-batches requires a lot of computation and data movement due to the exponential growth of multi-hop graph neighborhoods along network layers. Such a unique challenge gives rise to a diverse set of system design choices. We argue in favor of performing mini-batch training with neighborhood sampling in a distributed multi-GPU environment, under which we identify major performance bottlenecks hitherto under-explored by developers: mini-batch preparation and transfer. We present a sequence of improvements to mitigate these bottlenecks, including a performance-engineered neighborhood sampler, a shared-memory parallelization strategy, and the pipelining of batch transfer with GPU computation. We also conduct an empirical analysis that supports the use of sampling for inference, showing that test accuracies are not materially compromised. Such an observation unifies training and inference, simplifying model implementation. We report comprehensive experimental results with several benchmark data sets and GNN architectures, including a demonstration that, for the ogbn-papers100M data set, our system SALIENT achieves a speedup of 3\u00d7 over a standard PyTorch-Geometric implementation with a single GPU and a further 8\u00d7 parallel speedup with 16 GPUs. Therein, training a 3-layer GraphSAGE model with sampling fanout (15, 10, 5) takes 2.0 seconds per epoch and inference with fanout (20, 20, 20) takes 2.4 seconds, attaining test accuracy 64.58%.", "venue": "ArXiv", "authors": ["Tim  Kaler", "Nickolas  Stathas", "Anne  Ouyang", "Alexandros-Stavros  Iliopoulos", "Tao B. Schardl", "Charles E. Leiserson", "Jie  Chen"], "year": 2021, "n_citations": 0}
{"id": 164569, "s2_id": "6379c3d878e8e0b8636161412d1ee2af3716a437", "title": "Insensitivity of the mean-field Limit of Loss Systems Under Power-of-d Routing", "abstract": "In this paper, we study large multi-server loss models under power-of-$d$ routing scheme when service time distributions are general with finite mean. Previous works have addressed the exponential service time case when the number of servers goes to infinity giving rise to a mean field model. The fixed point of limiting mean field equations (MFE) was shown to be insensitive to the service time distribution through simulation. Showing insensitivity to general service time distributions has remained an open problem. Obtaining the MFE in this case poses a challenge due to the resulting Markov description of the system being in positive orthant as opposed to a finite chain in the exponential case. In this paper, we first obtain the MFE and then show that the MFE has a unique fixed point that coincides with the fixed point in the exponential case thus establishing insensitivity. The approach is via a measure-valued Markov process representation and the martingale problem to establish the mean-field limit. The techniques can be applied to other queueing models.", "venue": "ArXiv", "authors": ["Thirupathaiah  Vasantam", "Arpan  Mukhopadhyay", "Ravi  Mazumdar"], "year": 2017, "n_citations": 8}
{"id": 164605, "s2_id": "a837e37b7efe60ef2353361362c0074af80f6ded", "title": "Impact of Different Spreading Codes Using FEC on DWT Based MC-CDMA System", "abstract": "The effect of different spreading codes in DWT based MC-CDMA wireless communication system is investigated. In this paper, we present the Bit Error Rate (BER) performance of different spreading codes (Walsh-Hadamard code, Orthogonal gold code and Golay complementary sequences) using Forward Error Correction (FEC) of the proposed system. The data is analyzed and is compared among different spreading codes in both coded and uncoded cases. It is found via computer simulation that the performance of the proposed coded system is much better than that of the uncoded system irrespective of the spreading codes and all the spreading codes show approximately similar nature for both coded and uncoded in all modulation schemes.", "venue": "ArXiv", "authors": ["Saleh  Masum", "M. Hasnat Kabir", "Md. Matiqul Islam", "Rifat Ara Shams", "Shaikh Enayet Ullah"], "year": 2012, "n_citations": 2}
{"id": 175462, "s2_id": "65a65e84a5e6d3f75bd5f9439597b7292c20774a", "title": "Static vs accumulating priorities in healthcare queues under heavy loads", "abstract": "Amid unprecedented times caused by COVID-19, healthcare systems all over the world are strained to the limits of, or even beyond, capacity. A similar event is experienced by some healthcare systems regularly, due to for instance seasonal spikes in the number of patients. We model this as a queueing system in heavy traffic (where the arrival rate is approaching the service rate from below) or in overload (where the arrival rate exceeds the service rate). In both cases we assume that customers (patients) may have different priorities and we consider two popular service disciplines: static priorities and accumulating priorities. It has been shown that the latter allows for patients of all classes to be seen in a timely manner as long as the system is stable. We demonstrate however that if accumulating priorities are used in the heavy traffic or overload regime, then all patients, including those with the highest priority, will experience very long waiting times. If on the other hand static priorities are applied, then one can ensure that the highest-priority patients will be seen in a timely manner even in overloaded systems.", "venue": "ArXiv", "authors": ["Binyamin  Oz", "Seva  Shneer", "Ilze  Ziedins"], "year": 2020, "n_citations": 1}
{"id": 175614, "s2_id": "19e935dbd5578cb42fdbbd1890e4c7d96d5c3055", "title": "Going through Rough Times: from Non-Equilibrium Surface Growth to Algorithmic Scalability", "abstract": "Efficient and faithful parallel simulation of large asynchronous systems is a challenging computational problem. It requires using the concept of local simulated times and a synchronization scheme. We study the scalability of massively parallel algorithms for discrete-event simulations which employ conservative synchronization to enforce causality. We do this by looking at the simulated time horizon as a complex evolving system, and we identify its universal characteristics. We find that the time horizon for the conservative parallel discrete-event simulation scheme exhibits Kardar-Parisi-Zhang-like kinetic roughening. This implies that the algorithm is asymptotically scalable in the sense that the average progress rate of the simulation approaches a non-zero constant. It also implies, however, that there are diverging memory requirements associated with such schemes.", "venue": "ArXiv", "authors": ["G.  Korniss", "M. A. Novotny", "Per Arne Rikvold", "H.  Guclu", "Zolt\u00e1n  Toroczkai"], "year": 2001, "n_citations": 6}
{"id": 178905, "s2_id": "741dd127ac7319f5fc1518ffcaffcf3d1cb1493c", "title": "Dynamic Weighted Fairness with Minimal Disruptions", "abstract": "In this paper, we consider the following dynamic fair allocation problem: Given a sequence of job arrivals and departures, the goal is to maintain an approximately fair allocation of the resource against a target fair allocation policy, while minimizing the total number of disruptions, which is the number of times the allocation of any job is changed. We consider a rich class of fair allocation policies that significantly generalize those considered in previous work.", "venue": "SIGMETRICS", "authors": ["Sungjin  Im", "Benjamin  Moseley", "Kamesh  Munagala", "Kirk  Pruhs"], "year": 2020, "n_citations": 0}
{"id": 185464, "s2_id": "8fd883f166084bf53946789400c1c27c4edde2f1", "title": "Bound-based power optimization for multi-hop heterogeneous wireless industrial networks under statistical delay constraints", "abstract": "The noticeably increased deployment of wireless networks for battery-limited industrial applications in recent years highlights the need for tractable performance analysis methodologies as well as efficient QoS-aware transmit power management schemes. In this work, we seek to combine several important aspects of such networks, i.e., multi-hop connectivity, channel heterogeneity and the queuing effect, in order to address these needs. We design delay-bound-based algorithms for transmit power minimization and network lifetime maximization of multi-hop heterogeneous wireless networks using our previously developed stochastic network calculus approach for performance analysis of a cascade of buffered wireless fading channels. Our analysis shows an overall transmit power saving of up to 95% compared to a fixed power allocation scheme when using a service model in terms of the Shannon capacity limit. For a more realistic set-up, we evaluate the performance of the suggested algorithm in a WirelessHART network, which is a widely used communication standard for process automation and other industrial applications. We find that link heterogeneity can significantly reduce network lifetime when no efficient power management is applied. Moreover, we show, using extensive simulation study, that the proposed bound-based power allocation performs reasonably well compared to the real optimum, especially in the case of WirelessHART networks.", "venue": "Comput. Networks", "authors": ["Neda  Petreska", "Hussein  Al-Zubaidy", "Rudi  Knorr", "James  Gross"], "year": 2019, "n_citations": 8}
{"id": 185839, "s2_id": "24b5a87935a435182f902702676dbe8411a2ee82", "title": "Transfer learning for performance modeling of configurable systems: An exploratory analysis", "abstract": "Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space.", "venue": "2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)", "authors": ["Pooyan  Jamshidi", "Norbert  Siegmund", "Miguel  Velez", "Christian  K\u00e4stner", "Akshay  Patel", "Yuvraj  Agarwal"], "year": 2017, "n_citations": 66}
{"id": 187583, "s2_id": "841c7dd2bed66e9466f729a73ff3ff6a410eeced", "title": "A Generalized Coupon Collector Problem", "abstract": "This paper presents an analysis of a generalized version of the coupon collector problem, in which the collector receives d coupons each run and chooses the least-collected coupon so far. In the asymptotic case when the number of coupons n goes to infinity, we show that, on average, (nlogn) / d + (n / d)(m \u2212 1)log logn + O(mn) runs are needed to collect m sets of coupons. An exact algorithm is also developed for any finite case to compute the exact mean number of runs. Numerical examples are provided to verify our theoretical predictions.", "venue": "Journal of Applied Probability", "authors": ["Weiyu  Xu", "Ao  Tang"], "year": 2011, "n_citations": 12}
{"id": 190997, "s2_id": "160e800ee28b1d5aec88ef3883f6d0e4f58b9354", "title": "Mobility Management Framework", "abstract": "This paper investigates mobility management strategies from the point of view of their need of signalling and processing resources on the backbone network and load on the air interface. A method is proposed to model the serving network and mobile node mobility in order to be able to compare the different types of mobility management algorithms. To obtain a good description of the network we calculate descriptive parameters from given topologies. Most mobility approaches derived from existing protocols are analyzed and their performances are numerically compared in various network and mobility scenarios. We developed a mobility management framework that is able to give general designing guidelines for the next generation mobility managements on given network, technology and mobility properties. With our model an operator can design the network and tune the parameters to obtain the optimal implementation of course revising existing systems is also possible. We present a vertical handover decision method as a special application of our model framework.", "venue": "ArXiv", "authors": ["P\u00e9ter  F\u00fcl\u00f6p", "Benedek  Kov\u00e1cs", "S\u00e1ndor  Imre"], "year": 2008, "n_citations": 1}
{"id": 191134, "s2_id": "7fe5736bdf607ec40cfa50d5adc046203f94a6a1", "title": "Julia implementation of the Dynamic Distributed Dimensional Data Model", "abstract": "Julia is a new language for writing data analysis programs that are easy to implement and run at high performance. Similarly, the Dynamic Distributed Dimensional Data Model (D4M) aims to clarify data analysis operations while retaining strong performance. D4M accomplishes these goals through a composable, unified data model on associative arrays. In this work, we present an implementation of D4M in Julia and describe how it enables and facilitates data analysis. Several experiments showcase scalable performance in our new Julia version as compared to the original Matlab implementation.", "venue": "2016 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Alexander  Chen", "Alan  Edelman", "Jeremy  Kepner", "Vijay  Gadepally", "Dylan  Hutchison"], "year": 2016, "n_citations": 7}
{"id": 191921, "s2_id": "dcdc650e088a62fbc50c5acfd1ad3667be945e0e", "title": "Optimal union-find in Constraint Handling Rules", "abstract": "Constraint Handling Rules (CHR) is a committed-choice rule-based language that was originally intended for writing constraint solvers. In this paper we show that it is also possible to write the classic union-find algorithm and variants in CHR. The programs neither compromise in declarativeness nor efficiency. We study the time complexity of our programs: they match the almost-linear complexity of the best known imperative implementations. This fact is illustrated with experimental results.", "venue": "Theory and Practice of Logic Programming", "authors": ["Tom  Schrijvers", "Thom W. Fr\u00fchwirth"], "year": 2006, "n_citations": 60}
{"id": 194405, "s2_id": "750c09e27d1fa87996af0d1d5d5e3f1d62976343", "title": "AIBench: An Agile Domain-specific Benchmarking Methodology and an AI Benchmark Suite", "abstract": "Domain-specific software and hardware co-design is encouraging as it is much easier to achieve efficiency for fewer tasks. Agile domain-specific benchmarking speeds up the process as it provides not only relevant design inputs but also relevant metrics, and tools. Unfortunately, modern workloads like Big data, AI, and Internet services dwarf the traditional one in terms of code size, deployment scale, and execution path, and hence raise serious benchmarking challenges. \nThis paper proposes an agile domain-specific benchmarking methodology. Together with seventeen industry partners, we identify ten important end-to-end application scenarios, among which sixteen representative AI tasks are distilled as the AI component benchmarks. We propose the permutations of essential AI and non-AI component benchmarks as end-to-end benchmarks. An end-to-end benchmark is a distillation of the essential attributes of an industry-scale application. We design and implement a highly extensible, configurable, and flexible benchmark framework, on the basis of which, we propose the guideline for building end-to-end benchmarks, and present the first end-to-end Internet service AI benchmark. \nThe preliminary evaluation shows the value of our benchmark suite---AIBench against MLPerf and TailBench for hardware and software designers, micro-architectural researchers, and code developers. The specifications, source code, testbed, and results are publicly available from the web site \\url{this http URL}.", "venue": "ArXiv", "authors": ["Wanling  Gao", "Fei  Tang", "Jianfeng  Zhan", "Chuanxin  Lan", "Chunjie  Luo", "Lei  Wang", "Jiahui  Dai", "Zheng  Cao", "Xiongwang  Xiong", "Zihan  Jiang", "Tianshu  Hao", "Fanda  Fan", "Xu  Wen", "Fan  Zhang", "Yunyou  Huang", "Jianan  Chen", "Mengjia  Du", "Rui  Ren", "Chen  Zheng", "Daoyi  Zheng", "Haoning  Tang", "Kunlin  Zhan", "Biao  Wang", "Defei  Kong", "Minghe  Yu", "Chongkang  Tan", "Huan  Li", "Xinhui  Tian", "Yatao  Li", "Gang  Lu", "Junchao  Shao", "Zhenyu  Wang", "Xiaoyu  Wang", "Hainan  Ye"], "year": 2020, "n_citations": 0}
{"id": 197953, "s2_id": "7f2f2970135bb3795ce69d8f6d1b9a57d1288b18", "title": "Precise request tracing and performance debugging for multi-tier services of black boxes", "abstract": "As more and more multi-tier services are developed from commercial components or heterogeneous middleware without the source code available, both developers and administrators need a precise request tracing tool to help understand and debug performance problems of large concurrent services of black boxes. Previous work fails to resolve this issue in several ways: they either accept the imprecision of probabilistic correlation methods, or rely on knowledge of protocols to isolate requests in pursuit of tracing accuracy. This paper introduces a tool named PreciseTracer to help debug performance problems of multi-tier services of black boxes. Our contributions are two-fold: first, we propose a precise request tracing algorithm for multi-tier services of black boxes, which only uses ap plication-independent knowledge; secondly, we present a component activity graph abstraction to represent causal paths of requests and facilitate end-to-end performance debugging. The low overhead and tolerance of noise make PreciseTracer a promising tracing tool for using on production systems.", "venue": "2009 IEEE/IFIP International Conference on Dependable Systems & Networks", "authors": ["Zhihong  Zhang", "Jianfeng  Zhan", "Yong  Li", "Lei  Wang", "Dan  Meng", "Bo  Sang"], "year": 2009, "n_citations": 24}
{"id": 201471, "s2_id": "d46df1d04c35ecbdf260724f04ac57b788a103db", "title": "Straggler Mitigation by Delayed Relaunch of Tasks", "abstract": "Redundancy for straggler mitigation, originally in data download and more recently in distributed computing context, has been shown to be effective both in theory and practice. Analysis of systems with redundancy has drawn significant attention and numerous papers have studied pain and gain of redundancy under various service models and assumptions on the straggler characteristics. We here present a cost (pain) vs. latency (gain) analysis of using simple replication or erasure coding for straggler mitigation in executing jobs with many tasks. We quantify the effect of the tail of task execution times and discuss tail heaviness as a decisive parameter for the cost and latency of using redundancy. Specifically, we find that coded redundancy achieves better cost vs. latency tradeoff than simple replication and can yield reduction in both cost and latency under less heavy tailed execution times. We show that delaying redundancy is not effective in reducing cost and that delayed relaunch of stragglers can yield significant reduction in cost and latency. We validate these observations by comparing with the simulations that use empirical distributions extracted from Google cluster data.", "venue": "PERV", "authors": ["Mehmet Fatih Aktas", "Pei  Peng", "Emina  Soljanin"], "year": 2018, "n_citations": 34}
{"id": 205631, "s2_id": "a098de0d103bffca92fb2bde79132a89fca0fafe", "title": "Performance engineering for real and complex tall & skinny matrix multiplication kernels on GPUs", "abstract": "General matrix-matrix multiplications with double-precision real and complex entries (DGEMM and ZGEMM) in vendor-supplied BLAS libraries are best optimized for square matrices but often show bad performance for tall & skinny matrices, which are much taller than wide. NVIDIA\u2019s current CUBLAS implementation delivers only a fraction of the potential performance as indicated by the roofline model in this case. We describe the challenges and key characteristics of an implementation that can achieve close to optimal performance. We further evaluate different strategies of parallelization and thread distribution and devise a flexible, configurable mapping scheme. To ensure flexibility and allow for highly tailored implementations we use code generation combined with autotuning. For a large range of matrix sizes in the domain of interest we achieve at least 2/3 of the roofline performance and often substantially outperform state-of-the art CUBLAS results on an NVIDIA Volta GPGPU.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Dominik  Ernst", "Georg  Hager", "Jonas  Thies", "Gerhard  Wellein"], "year": 2021, "n_citations": 3}
{"id": 205724, "s2_id": "3c0ad8e95c3a672f54f02f4b2b9290b00baba28d", "title": "Near-optimal Detector for SWIPT-enabled Differential DF Relay Networks with SER Analysis", "abstract": "In this paper, we analyze the symbol error rate (SER) performance of the simultaneous wireless information and power transfer (SWIPT) enabled three-node differential decode-and-forward (DDF) relay networks, which adopt the power splitting (PS) protocol at the relay. The use of non-coherent differential modulation eliminates the need for sending training symbols to estimate the instantaneous channel state informations (CSIs) at all network nodes, and therefore improves the power efficiency, as compared with the coherent modulation. However, performance analysis results are not yet available for the state-of-the-art detectors such as the approximate maximum-likelihood detector. Existing works rely on Monte-Carlo simulation to show that there exists an optimal PS ratio that minimizes the overall SER. In this work, we propose a near-optimal detector with linear complexity with respect to the modulation size. We derive an accurate approximate SER expression, based on which the optimal PS ratio can be accurately estimated without requiring any Monte-Carlo simulation.", "venue": "ICC 2020 - 2020 IEEE International Conference on Communications (ICC)", "authors": ["Yuxin  Lu", "Wai Ho Mow"], "year": 2020, "n_citations": 1}
{"id": 207362, "s2_id": "e8406a1d9e9266ac0b5023e94380132900727e88", "title": "Scheduling in a Random Environment: Stability and Asymptotic Optimality", "abstract": "We investigate the scheduling of a common resource between several concurrent users when the feasible transmission rate of each user varies randomly over time. Time is slotted, and users arrive and depart upon service completion. This may model, for example, the flow-level behavior of end-users in a narrowband HDR wireless channel (CDMA 1xEV-DO). As performance criteria, we consider the stability of the system and the mean delay experienced by the users. Given the complexity of the problem, we investigate the fluid-scaled system, which allows to obtain important results and insights for the original system: 1) We characterize for a large class of scheduling policies the stability conditions and identify a set of maximum stable policies, giving in each time-slot preference to users being in their best possible channel condition. We find in particular that many opportunistic scheduling policies like Score-Based, Proportionally Best, or Potential Improvement are stable under the maximum stability conditions, whereas the opportunistic scheduler Relative-Best or the c\u03bc-rule are not. 2) We show that choosing the right tie-breaking rule is crucial for the performance (e.g., average delay) as perceived by a user. We prove that a policy is asymptotically optimal if it is maximum stable and the tie-breaking rule gives priority to the user with the highest departure probability. We will refer to such tie-breaking rule as myopic. 3) We derive the growth rates of the number of users in the system in overload settings under various policies, which give additional insights on the performance. 4) We conclude that simple priority-index policies with the myopic tie-breaking rule are stable and asymptotically optimal. All our findings are validated with extensive numerical experiments.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Urtzi  Ayesta", "Martin  Erausquin", "Matthieu  Jonckheere", "Maaike  Verloop"], "year": 2013, "n_citations": 39}
{"id": 207815, "s2_id": "0297b3865d86aa30a7e77b63d8e6a2d01032c18c", "title": "Algorithms of Two-Level Parallelization for DSMC of Unsteady Flows in Molecular Gasdynamics", "abstract": "The general scheme of two-level parallelization (TLP) for direct simulation Monte Carlo of unsteady gas flows on shared memory multiprocessor computers has been described. The high efficient algorithm of parallel independent runs is used on the first level. The data parallelization is employed for the second one. Two versions of TLP algorithm are elaborated with static and dynamic load balancing. The method of dynamic processor reallocation is used for dynamic load balancing. Two gasdynamic unsteady problems were used to study speedup and efficiency of the algorithms. The conditions of efficient application field for the algorithms have been determined.", "venue": "ArXiv", "authors": ["Alexander V. Bogdanov", "Nickolay Y. Bykov", "Igor A. Grishin", "Gregory O. Khanlarov", "German A. Lukianov", "Vladimir V. Zakharov"], "year": 1999, "n_citations": 1}
{"id": 208767, "s2_id": "443726dc70d2713170f6b23865c2b626aac36b5c", "title": "Faster Radix Sort via Virtual Memory and Write-Combining", "abstract": "Sorting algorithms are the deciding factor for the performance of common operations such as removal of duplicates or database sort-merge joins. This work focuses on 32-bit integer keys, optionally paired with a 32-bit value. We present a fast radix sorting algorithm that builds upon a microarchitecture-aware variant of counting sort. Taking advantage of virtual memory and making use of write-combining yields a per-pass throughput corresponding to at least 88 % of the system's peak memory bandwidth. Our implementation outperforms Intel's recently published radix sort by a factor of 1.5. It also compares favorably to the reported performance of an algorithm for Fermi GPUs when data-transfer overhead is included. These results indicate that scalar, bandwidth-sensitive sorting algorithms remain competitive on current architectures. Various other memory-intensive applications can benefit from the techniques described herein.", "venue": "ArXiv", "authors": ["Jan  Wassenberg", "Peter  Sanders"], "year": 2010, "n_citations": 5}
{"id": 212412, "s2_id": "b9960561fe65c978af9fe58c454fb6e4a8635508", "title": "Bring Your Own Codegen to Deep Learning Compiler", "abstract": "Deep neural networks (DNNs) have been ubiquitously applied in many applications, and accelerators are emerged as an enabler to support the fast and efficient inference tasks of these applications. However, to achieve high model coverage with high performance, each accelerator vendor has to develop a full compiler stack to ingest, optimize, and execute the DNNs. This poses significant challenges in the development and maintenance of the software stack. In addition, the vendors have to contiguously update their hardware and/or software to cope with the rapid evolution of the DNN model architectures and operators. To address these issues, this paper proposes an open source framework that enables users to only concentrate on the development of their proprietary code generation tools by reusing as many as possible components in the existing deep learning compilers. Our framework provides users flexible and easy-to-use interfaces to partition their models into segments that can be executed on \u201cthe best\u201d processors to take advantage of the powerful computation capability of accelerators. Our case study shows that our framework has been deployed in multiple commercial vendors\u2019 compiler stacks with only a few thousand lines of code.", "venue": "ArXiv", "authors": ["Zhi  Chen", "Cody Hao Yu", "Trevor  Morris", "Jorn  Tuyls", "Yi-Hsiang  Lai", "Jared  Roesch", "Elliott  Delaye", "Vin  Sharma", "Yida  Wang"], "year": 2021, "n_citations": 1}
{"id": 218631, "s2_id": "a26979c345a85df7a74a75bb3353087aaa8b82e6", "title": "Memory Centric Characterization and Analysis of SPEC CPU2017 Suite", "abstract": "In this paper, we provide a comprehensive, memory-centric characterization of the SPEC CPU2017 benchmark suite, using a number of mechanisms including dynamic binary instrumentation, measurements on native hardware using hardware performance counters and operating system based tools. We present a number of results including working set sizes, memory capacity consumption and memory bandwidth utilization of various workloads. Our experiments reveal that, on the x86_64 ISA, SPEC CPU2017 workloads execute a significant number of memory related instructions, with approximately 50% of all dynamic instructions requiring memory accesses. We also show that there is a large variation in the memory footprint and bandwidth utilization profiles of the entire suite, with some benchmarks using as much as 16 GB of main memory and up to 2.3 GB/s of memory bandwidth. We perform instruction distribution analysis of the benchmark suite and find that the average instruction count for SPEC CPU2017 workloads is an order of magnitude higher than SPEC CPU2006 ones. In addition, we also find that FP benchmarks of the suite have higher compute requirements: on average, FP workloads execute three times the number of compute operations as compared to INT workloads.", "venue": "ICPE", "authors": ["Sarabjeet  Singh", "Manu  Awasthi"], "year": 2019, "n_citations": 12}
{"id": 221517, "s2_id": "9bb2e48b312828b18265146cf9f8f5776c881882", "title": "A New Parallel Algorithm for Two-Pass Connected Component Labeling", "abstract": "Connected Component Labeling (CCL) is one of the most important step in pattern recognition and image processing. It assigns labels to the pixels such that adjacent pixels sharing the same features are assigned the same label. Typically, CCL requires several passes over the data. We focus on two-pass technique where each pixel is given a provisional label in the first pass whereas an actual label is assigned in the second pass. We present a scalable parallel two-pass CCL algorithm, called PAREMSP, which employs a scan strategy and the best union-find technique called REMSP, which uses REM'S algorithm for storing label equivalence information of pixels in a 2-D image. In the first pass, we divide the image among threads and each thread runs the scan phase along with REMSP simultaneously. In the second phase, we assign the final labels to the pixels. As REMSP is easily parallelizable, we use the parallel version of REMSP for merging the pixels on the boundary. Our experiments show the scalability of PAREMSP achieving speedups up to 20.1 using 24 cores on shared memory architecture using OpenMP for an image of size 465.20 MB. We find that our proposed parallel algorithm achieves linear scaling for a large resolution fixed problem size as the number of processing elements are increased. Additionally, the parallel algorithm does not make use of any hardware specific routines, and thus is highly portable.", "venue": "2014 IEEE International Parallel & Distributed Processing Symposium Workshops", "authors": ["Siddharth  Gupta", "Diana  Palsetia", "Md. Mostofa Ali Patwary", "Ankit  Agrawal", "Alok N. Choudhary"], "year": 2014, "n_citations": 24}
{"id": 222400, "s2_id": "a4c88f0afcc5ba68b3860f8507d50649642158b1", "title": "Streaming graph challenge: Stochastic block partition", "abstract": "An important objective for analyzing real-world graphs is to achieve scalable performance on large, streaming graphs. A challenging and relevant example is the graph partition problem. As a combinatorial problem, graph partition is NP-hard, but existing relaxation methods provide reasonable approximate solutions that can be scaled for large graphs. Competitive benchmarks and challenges have proven to be an effective means to advance state-of-the-art performance and foster community collaboration. This paper describes a graph partition challenge with a baseline partition algorithm of sub-quadratic complexity. The algorithm employs rigorous Bayesian inferential methods based on a statistical model that captures characteristics of the real-world graphs. This strong foundation enables the algorithm to address limitations of well-known graph partition approaches such as modularity maximization. This paper describes various aspects of the challenge including: (1) the data sets and streaming graph generator, (2) the baseline partition algorithm with pseudocode, (3) an argument for the correctness of parallelizing the Bayesian inference, (4) different parallel computation strategies such as node-based parallelism and matrix-based parallelism, (5) evaluation metrics for partition correctness and computational requirements, (6) preliminary timing of a Python-based demonstration code and the open source C++ code, and (7) considerations for partitioning the graph in streaming fashion. Data sets and source code for the algorithm as well as metrics, with detailed documentation are available at GraphChallenge.org.", "venue": "2017 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Edward K. Kao", "Vijay  Gadepally", "Michael B. Hurley", "Michael  Jones", "Jeremy  Kepner", "Sanjeev  Mohindra", "Paul  Monticciolo", "Albert  Reuther", "Siddharth  Samsi", "William  Song", "Diane  Staheli", "Steven Thomas Smith"], "year": 2017, "n_citations": 49}
{"id": 226318, "s2_id": "1fdbd0c5ef0b0b54c840e180a4730548d100e70f", "title": "Mapping Stencils on Coarse-grained Reconfigurable Spatial Architecture", "abstract": "Stencils represent a class of computational patterns where an output grid point depends on a fixed shape of neighboring points in an input grid. Stencil computations are prevalent in scientific applications engaging a significant portion of supercomputing resources. Therefore, it has been always important to optimize stencil programs for the best performance. A rich body of research has focused on optimizing stencil computations on almost all parallel architectures. Stencil applications have regular dependency patterns, inherent pipeline-parallelism, and plenty of data reuse. This makes these applications a perfect match for a coarse-grained reconfigurable spatial architecture (CGRA). A CGRA consists of many simple, small processing elements (PEs) connected with an on-chip network. Each PE can be configured to execute part of a stencil computation and all PEs run in parallel; the network can also be configured so that data loaded can be passed from a PE to a neighbor PE directly and thus reused by many PEs without register spilling and memory traffic. How to efficiently map a stencil computation to a CGRA is the key to performance. In this paper, we show a few unique and generalizable ways of mapping one- and multidimensional stencil computations to a CGRA, fully exploiting the data reuse opportunities and parallelism. Our simulation experiments demonstrate that these mappings are efficient and enable the CGRA to outperform state-of-the-art GPUs.", "venue": "ArXiv", "authors": ["Jesmin Jahan Tithi", "Fabrizio  Petrini", "Hongbo  Rong", "Andrei  Valentin", "Carl  Ebeling"], "year": 2020, "n_citations": 0}
{"id": 236710, "s2_id": "d86b92937f4c577a47989372be773114b45e2185", "title": "On the Rate of Convergence of Mean-Field Models: Stein's Method Meets the Perturbation Theory", "abstract": "This paper studies the rate of convergence of a family of continuous-time Markov chains (CTMC) to a mean-field model. When the mean-field model is a finite-dimensional dynamical system with a unique equilibrium point, an analysis based on Stein's method and the perturbation theory shows that under some mild conditions, the stationary distributions of CTMCs converge (in the mean-square sense) to the equilibrium point of the mean-field model if the mean-field model is globally asymptotically stable and locally exponentially stable. In particular, the mean square difference between the $M$th CTMC in the steady state and the equilibrium point of the mean-field system is $O(1/M),$ where $M$ is the size of the $M$th CTMC. This approach based on Stein's method provides a new framework for studying the convergence of CTMCs to their mean-field limit by mainly looking into the stability of the mean-field model, which is a deterministic system and is often easier to analyze than the CTMCs. More importantly, this approach quantifies the rate of convergence, which reveals the approximation error of using mean-field models for approximating finite-size systems.", "venue": "ArXiv", "authors": ["Lei  Ying"], "year": 2015, "n_citations": 3}
{"id": 241216, "s2_id": "6b3f02e53de886259568402cca2a897aabb52e25", "title": "Asynchronous Bounded Expected Delay Networks", "abstract": "The commonly used asynchronous bounded delay (ABD) network models assume a fixed bound on message delay. We propose a probabilistic network model, called asynchronous bounded expected delay (ABE) model. Instead of a strict bound, the ABE model requires only a bound on the expected message delay. While the conditions of ABD networks restrict the set of possible executions, in ABE networks all asynchronous executions are possible, but executions with extremely long delays are less probable. In contrast to ABD networks, ABE networks cannot be synchronised efficiently. At the example of an election algorithm, we show that the minimal assumptions of ABE networks are sufficient for the development of efficient algorithms. For anonymous, unidirectional ABE rings of known size N we devise a probabilistic leader election algorithm having average message and time complexity O(N).", "venue": "ArXiv", "authors": ["Rena  Bakhshi", "J\u00f6rg  Endrullis", "Wan  Fokkink", "Jun  Pang"], "year": 2010, "n_citations": 3}
{"id": 242793, "s2_id": "709f2dbfc5ee48f65ca835ef66ec9e3dc47746b1", "title": "Flattening the Curve: Insights From Queueing Theory", "abstract": "The worldwide outbreak of the coronavirus was first identified in 2019 in Wuhan, China. Since then, the disease has spread worldwide. As it currently spreading in the United States, policy makers, public health officials and citizens are racing to understand the impact of this virus on the United States healthcare system. They fear that the rapid influx of patients will overwhelm the healthcare system leading to unnecessary fatalities. Most countries and states in America have introduced mitigation strategies, such as social distancing, to decrease the rate of newly infected people, i.e. flattening the this http URL this paper, we analyze the time evolution of the number of people hospitalized due to the coronavirus using the methods of queueing theory. Given that the rate of new infections varies over time as the pandemic evolves, we model the number of coronavirus patients as a dynamical system based on the theory of infinite server queues with non-stationary Poisson arrival rates. With this model we are able to quantify how flattening the curve affects the peak demand for hospital resources. This allows us to characterize how aggressively society must flatten the curve in order to avoid overwhelming the capacity of healthcare system. We also demonstrate how flattening the curve impacts the elapsed time between the peak rate of hospitalizations and the time of the peak demand for the hospital resources. Finally, we present empirical evidence from China, South Korea, Italy and the United States that supports the insights from the model.", "venue": "ArXiv", "authors": ["Sergio  Palomo", "Jamol  Pender", "William  Massey", "Robert C. Hampshire"], "year": 2020, "n_citations": 7}
{"id": 244180, "s2_id": "08fff0c50fe3cd1cd213e68a36f0c253548f5e46", "title": "Internet Speed Measurement: Current Challenges and Future Recommendations", "abstract": "Government organizations, regulators, consumers, Internet service providers, and application providers alike all have an interest in measuring user Internet \"speed\". Access speeds have increased by an order of magnitude in past years, with gigabit speeds available to tens of millions of homes. Approaches must evolve to accurately reflect the changing user experience and network speeds. This paper offers historical and technical background on current speed testing methods, highlights their limitations as access network speeds continue to increase, and offers recommendations for the next generation of Internet \"speed\" measurement.", "venue": "ArXiv", "authors": ["Nick  Feamster", "Jason  Livingood"], "year": 2019, "n_citations": 7}
{"id": 244805, "s2_id": "103904808ba4ebac6cd9fa26db04fc3f487f0b4c", "title": "A Multilevel Approach for the Performance Analysis of Parallel Algorithms", "abstract": "We provide a multilevel approach for analysing performances of parallel algorithms. The main outcome of such approach is that the algorithm is described by using a set of operators which are related to each other according to the problem decomposition. Decomposition level determines the granularity of the algorithm. A set of block matrices (decomposition and execution) highlights fundamental characteristics of the algorithm, such as inherent parallelism and sources of overheads.", "venue": "ArXiv", "authors": ["Luisa  D'Amore", "Valeria  Mele", "Diego  Romano", "Giuliano  Laccetti"], "year": 2019, "n_citations": 0}
{"id": 245473, "s2_id": "7a7840d708e1b71785e6fc33935ec31e5cad4c34", "title": "A Parallel Time-Integrator for Solving the Linearized Shallow Water Equations on the Rotating Sphere", "abstract": "With the stagnation of processor core performance, further reductions in the time-to-solution for geophysical fluid problems are becoming increasingly difficult with standard time integrators. Parallel-in-time exposes and exploits additional parallelism in the time dimension which is inherently sequential in traditional methods. The rational approximation of exponential integrators (REXI) method allows taking arbitrarily long time steps based on a sum over a number of decoupled complex PDEs that can be solved independently massively parallel. Hence REXI is assumed to be well suited for modern massively parallel super computers which are currently trending. To date the study and development of the REXI approach has been limited to linearized problems on the periodic 2D plane. This work extends the REXI time stepping method to the linear shallow-water equations (SWE) on the rotating sphere, thus moving the method one step closer to solving fully nonlinear fluid problems of geophysical interest on the sphere. The rotating sphere poses particular challenges for finding an efficient solver due to the zonal dependence of the Coriolis term. Here we present an efficient REXI solver based on spherical harmonics, showing the results of: a geostrophic balance test, a comparison with alternative time stepping methods, an analysis of dispersion relations, indicating superior properties of REXI, and finally a performance comparison on Cheyenne supercomputer. Our results indicate that REXI is not only able to take larger time steps, but that REXI can also be used to gain higher accuracy and significantly reduced time-to-solution compared to currently existing time stepping methods.", "venue": "Numer. Linear Algebra Appl.", "authors": ["Martin  Schreiber", "Richard  Loft"], "year": 2019, "n_citations": 8}
{"id": 248908, "s2_id": "dd7f5bf9af96fc35798eececc9738a85d59bceba", "title": "Paging with dynamic memory capacity", "abstract": "We study a generalization of the classic paging problem that allows the amount of available memory to vary over time - capturing a fundamental property of many modern computing realities, from cloud computing to multi-core and energy-optimized processors. It turns out that good performance in the \"classic\" case provides no performance guarantees when memory capacity fluctuates: roughly speaking, moving from static to dynamic capacity can mean the difference between optimality within a factor 2 in space and time, and suboptimality by an arbitrarily large factor. More precisely, adopting the competitive analysis framework, we show that some online paging algorithms, despite having an optimal (h,k)-competitive ratio when capacity remains constant, are not (3,k)-competitive for any arbitrarily large k in the presence of minimal capacity fluctuations. In this light it is surprising that several classic paging algorithms perform remarkably well even if memory capacity changes adversarially - even without taking those changes into explicit account! In particular, we prove that LFD still achieves the minimum number of faults, and that several classic online algorithms such as LRU have a \"dynamic\" (h,k)-competitive ratio that is the best one can achieve without knowledge of future page requests, even if one had perfect knowledge of future capacity fluctuations (an exact characterization of this ratio shows it is almost, albeit not quite, equal to the \"classic\" ratio k/(k-h+1)). In other words, with careful management, knowing/predicting future memory resources appears far less crucial to performance than knowing/predicting future data accesses.", "venue": "STACS", "authors": ["Enoch  Peserico"], "year": 2019, "n_citations": 10}
{"id": 256883, "s2_id": "bcbce872b54bbcb60957455099d7aa3abca3ca6a", "title": "Exact solutions for the two- and all-terminal reliabilities of the Brecht-Colbourn ladder and the generalized fan", "abstract": "The two- and all-terminal reliabilities of the Brecht-Colbourn ladder and the generalized fan have been calculated exactly for arbitrary size as well as arbitrary individual edge and node reliabilities, using transfer matrices of dimension four at most. While the all-terminal reliabilities of these graphs are identical, the special case of identical edge ($p$) and node ($\\rho$) reliabilities shows that their two-terminal reliabilities are quite distinct, as demonstrated by their generating functions and the locations of the zeros of the reliability polynomials, which undergo structural transitions at $\\rho = \\displaystyle {1/2}$.", "venue": "ArXiv", "authors": ["Christian  Tanguy"], "year": 2007, "n_citations": 8}
{"id": 260226, "s2_id": "810fdbe71d62dcf13e8968ea5b1b0838c2aadd68", "title": "Resource Management in Edge and Fog Computing using FogBus2 Framework", "abstract": "The rapid advancements of hardware, software, and communication technologies enable the Internet of Things (IoT) to offer a wide variety of intelligent solutions in every single aspect of our lives. Therefore, IoT-enabled systems such as smart healthcare, transportation, agriculture, and entertainment, just to mention a few, have been attracting ever-increasing attention in academia and industry. IoT applications generate a massive amount of data which requires processing and storage, while IoT devices often lack sufficient processing and storage resources. Cloud computing offers infrastructure, platform, and software services for IoT-enabled systems, through which IoT applications can process, store, and analyze their generated data in surrogate Cloud Servers (CSs) [1, 2]. There are different Cloud Service Providers (CSPs) with a wide variety of services, where each CSP provides a particular set of services such as computing, database, and data analysis in an optimized way. Hence, no CSP can satisfy the full functional requirements of different IoT applications in an optimized manner [3]. As a result, each IoT application can be particularly serviced by a specific CSP or simultaneously by different CSPs, which is often called hybrid cloud computing [3]. Although hybrid cloud computing platform provides IoT devices with unlimited and diverse computing and storage resources, CSs are residing multi-hops away from IoT devices, which incurs high propagation and queuing latency. Thus, CSs cannot solely provide the best possible services for latency-critical and real-time IoT applications (e.g., intelligent transportation, smart healthcare, emergency, and real-time control systems) [4, 5]. Besides, forwarding the huge amount of data generated by distributed IoT devices to CSs for processing and storage may overload the CSs [6]. To overcome these issues, edge and fog computing has emerged as a novel distributed computing paradigm. In edge and fog computing environments, the geographically distributed heterogeneous Edge Servers (ESs) (e.g., access points, smartphones, Raspberry-Pis), situated in the vicinity of IoT devices, can be used for processing and storage of IoT", "venue": "ArXiv", "authors": ["Mohammad  Goudarzi", "Qifan  Deng", "Rajkumar  Buyya"], "year": 2021, "n_citations": 4}
{"id": 260364, "s2_id": "36b999fcff6f79158da42892e19d7773ef15edce", "title": "Optimization of FASTEST-3D for Modern Multicore Systems", "abstract": "FASTEST-3D is an MPI-parallel finite-volume flow solver based on block-structured meshes that has been developed at the University of Erlangen-Nuremberg since the early 1990s. It can be used to solve the laminar or turbulent incompressible Navier-Stokes equations. Up to now its scalability was strongly limited by a rather rigid communication infrastructure, which led to a dominance of MPI time already at small process counts. \nThis paper describes several optimizations to increase the performance, scalability, and flexibility of FASTEST-3D. First, a node-level performance analysis is carried out in order to pinpoint the main bottlenecks and identify sweet spots for energy-efficient execution. In addition, a single-precision version of the solver for the linear equation system arising from the discretization of the governing equations is devised, which significantly increases the single-core performance. Then the communication mechanisms in FASTEST-3D are analyzed and a new communication strategy based on non-blocking calls is implemented. Performance results with the revised version show significantly increased single-node performance and considerably improved communication patterns along with much better parallel scalability. In this context we discuss the concept of \"acceptable parallel efficiency\" and how it influences the real gain of the optimizations. Scaling measurements are carried out on a modern petascale system. The obtained improvements are of major importance for the use of FASTEST-3D on current high-performance computer clusters and will help to perform simulations with much higher spatial and temporal resolution to tackle turbulent flow in technical applications.", "venue": "ArXiv", "authors": ["Christoph  Scheit", "Georg  Hager", "Jan  Treibig", "Stefan  Becker", "Gerhard  Wellein"], "year": 2013, "n_citations": 3}
{"id": 261751, "s2_id": "8e3a210fdcb3544cce46126e1288faf5b92147b9", "title": "Performance engineering for the Lattice Boltzmann method on GPGPUs: Architectural requirements and performance results", "abstract": "Abstract GPUs offer several times the floating point performance and memory bandwidth of current standard two socket CPU compute nodes, e.g. NVIDIA C2070 vs. Intel Xeon Westmere X5650. The lattice Boltzmann method (LBM) has been established as a flow solver in recent years and was one of the first flow solvers to be successfully ported to GPUs with a performance benefit. We demonstrate advanced optimization strategies for a D3Q19 lattice Boltzmann based incompressible flow solver for GPGPUs and CPUs. Since the implemented algorithm is limited by memory bandwidth, we concentrate on improving memory access. Basic data layout issues for optimal data access are explained and discussed. Furthermore, the algorithmic steps are rearranged to improve scattered access of the GPU memory. The importance of occupancy is discussed as well as optimization strategies to improve overall concurrency. We obtain a well-optimized GPU kernel, which is integrated into a larger framework that can handle single phase fluid flow simulations as well as particle-laden flows. Our 3D LBM GPU implementation reaches up to 650 MLUPS in single precision and 290 MLUPS in double precision on an NVIDIA Tesla C2070 as well as an AMD 6970.", "venue": "ArXiv", "authors": ["Johannes  Habich", "Christian  Feichtinger", "Harald  K\u00f6stler", "Georg  Hager", "Gerhard  Wellein"], "year": 2011, "n_citations": 58}
{"id": 263752, "s2_id": "e023a0d37775ac9387b8a39a4a448951bb6dde48", "title": "Minimizing Flow Completion Times using Adaptive Routing over Inter-Datacenter Wide Area Networks", "abstract": "I Throughput-oriented flows deliver large volumes of data. I Less sensitive to propagation and initial routing latency. I Inter-datacenter networks managed by one organization allow for flexible application of custom routing techniques. I Focus on single path routing and aim to minimize completion times and bandwidth usage. I Adaptive flow routing according to network and flow properties. I Fast heuristic schemes use a cost (distance) metric and select the minimum cost (shortest) path.", "venue": "ArXiv", "authors": ["Mohammad  Noormohammadpour", "Cauligi S. Raghavendra"], "year": 2018, "n_citations": 1}
{"id": 264145, "s2_id": "6be26d81d1be70f5a6b41b4ef523e4886df7326a", "title": "How does Docker affect energy consumption? Evaluating workloads in and out of Docker containers", "abstract": "Context: Virtual machines provide isolation of services at the cost of hypervisors and more resource usage. This spurred the growth of systems like Docker that enable single hosts to isolate several applications, similar to VMs, within a low-overhead abstraction called containers. \nMotivation: Although containers tout low overhead performance, do they still have low energy consumption? \nMethodology: This work statistically compares ($t$-test, Wilcoxon) the energy consumption of three application workloads in Docker and on bare-metal Linux. \nResults: In all cases, there was a statistically significant ($t$-test and Wilcoxon $p < 0.05$) increase in energy consumption when running tests in Docker, mostly due to the performance of I/O system calls.", "venue": "J. Syst. Softw.", "authors": ["Eddie Antonio Santos", "Carson  McLean", "Christopher  Solinas", "Abram  Hindle"], "year": 2018, "n_citations": 16}
{"id": 271950, "s2_id": "5b1c0152bbb12ece2a8817c727e33e6d5c503065", "title": "Speeding up distributed machine learning using codes", "abstract": "Distributed machine learning algorithms that are widely run on modern large-scale computing platforms face several types of randomness, uncertainty and system \u201cnoise.\u201d These include stragglers1, system failures, maintenance outages, and communication bottlenecks. In this work, we view distributed machine learning algorithms through a coding-theoretic lens, and show how codes can equip them with robustness against this system noise. Motivated by their importance and universality, we focus on two of the most basic building blocks of distributed learning algorithms: data shuffling and matrix multiplication. In data shuffling, we use codes to reduce communication bottlenecks: when a constant fraction of the data can be cached at each worker node, and n is the number of workers, coded shuffling reduces the communication cost by up to a factor \u0398(n) over uncoded shuffling. For matrix multiplication, we use codes to alleviate the effects of stragglers, also known as the straggler problem. We show that if the number of workers is n, and the runtime of each subtask has an exponential tail, the optimal coded matrix multiplication is \u0398(log n) times faster than the uncoded matrix multiplication or the optimal task replication scheme.", "venue": "ISIT", "authors": ["Kangwook  Lee", "Maximilian  Lam", "Ramtin  Pedarsani", "Dimitris S. Papailiopoulos", "Kannan  Ramchandran"], "year": 2016, "n_citations": 125}
{"id": 275515, "s2_id": "66e62b3976bdddbd754c576a09ea99b911a0370e", "title": "Simulating Nonlinear Neutrino Oscillations on Next-Generation Many-Core Architectures", "abstract": "In this work an astrophysical simulation code, XFLAT, is developed to study neutrino oscillations in supernovae. XFLAT is designed to utilize multiple levels of parallelism through MPI, OpenMP, and SIMD instructions (vectorization). It can run on both the CPU and the Xeon Phi co-processor, the latter of which is based on the Intel Many Integrated Core Architecture (MIC). The performance of XFLAT on configurations and scenarios has been analyzed. In addition, the impact of I/O and the multi-node configuration on the Xeon Phi-equipped heterogeneous supercomputers such as Stampede at the Texas Advanced Computing Center (TACC) was investigated.", "venue": "ArXiv", "authors": ["Vahid  Noormofidi"], "year": 2019, "n_citations": 1}
{"id": 277344, "s2_id": "d35baede07cdec0aa196f29387448787a4026dab", "title": "SPARK00: A Benchmark Package for the Compiler Evaluation of Irregular/Sparse Codes", "abstract": "We propose a set of benchmarks that specifically targets a major cause of performance degradation in high performance computing platforms: irregular access patterns. These benchmarks are meant to be used to asses the performance of optimizing compilers on codes with a varying degree of irregular access. The irregularity caused by the use of pointers and indirection arrays are a major challenge for optimizing compilers. Codes containing such patterns are notoriously hard to optimize but they have a huge impact on the performance of modern architectures, which are under-utilized when encountering irregular memory accesses. In this paper, a set of benchmarks is described that explicitly measures the performance of kernels containing a variety of different access patterns found in real world applications. By offering a varying degree of complexity, we provide a platform for measuring the effectiveness of transformations. The difference in complexity stems from a difference in traversal patterns, the use of multiple indirections and control flow statements. The kernels used cover a variety of different access patterns, namely pointer traversals, indirection arrays, dynamic loop bounds and run-time dependent if-conditions. The kernels are small enough to be fully understood which makes this benchmark set very suitable for the evaluation of restructuring transformations.", "venue": "ArXiv", "authors": ["Harmen L. A. van der Spek", "Erwin M. Bakker", "Harry A. G. Wijshoff"], "year": 2008, "n_citations": 9}
{"id": 278318, "s2_id": "72c30c7efef1556d7902cb8c2f424b49890e797c", "title": "The abstract Cauchy problem for the non-stationary bulk queue M(t)|M[k, B]|1", "abstract": "We derived state probability equations describing the queue M(t)|M[k, B]|1 and formulated as an abstract Cauchy problem to investigate by means of the semi-group theory of bounded linear operators in functional analysis. For the abstract Cauchy problem of this queue, we determined the eigenfunctions of maximal operator and showed some properties of the Dirichlet operator.", "venue": "ArXiv", "authors": ["Yong Chol Chon"], "year": 2013, "n_citations": 0}
{"id": 282773, "s2_id": "ce769e50d21047f2634e99927137c9c485c055bb", "title": "Performance evaluation of the random replacement policy for networks of caches", "abstract": "Caching is a key component for Content Distribution Networks and new Information-Centric Network architectures. In this paper, we address performance issues of caching networks running the RND replacement policy. We first prove that when the popularity distribution follows a general power-law with decay exponent \u03b1 > 1, the miss probability is asymptotic to O( C1-\u03b1) for large cache size C. We further evaluate network of caches under RND policy for homogeneous tree networks and extend the analysis to tandem cache networks where caches employ either LRU or RND policies.", "venue": "SIGMETRICS '12", "authors": ["Massimo  Gallo", "Bruno  Kauffmann", "Luca  Muscariello", "Alain  Simonian", "Christian  Tanguy"], "year": 2012, "n_citations": 66}
{"id": 283266, "s2_id": "8abfc6c1d66e7f79cb73f4a77af26f823cd22fea", "title": "Optimal Multiserver Scheduling with Unknown Job Sizes in Heavy Traffic", "abstract": "We consider scheduling to minimize mean response time of the M/G/k queue with unknown job sizes. In the singleserver k = 1 case, the optimal policy is the Gittins policy, but it is not known whether Gittins or any other policy is optimal in the multiserver case. Exactly analyzing the M/G/k under any scheduling policy is intractable, and Gittins is a particularly complicated policy that is hard to analyze even in the single-server case. In this work we introduce monotonic Gittins (M-Gittins), a new variation of the Gittins policy, and show that it minimizes mean response time in the heavy-traffic M/G/k for a wide class of finite-variance job size distributions. We also show that the monotonic shortest expected remaining processing time (M-SERPT) policy, which is simpler than M-Gittins, is a 2-approximation for mean response time in the heavy traffic M/G/k under similar conditions. These results constitute the most general optimality results to date for the M/G/k with unknown job sizes. Our techniques build upon work by Grosof et al. [6], who study simple policies, such as SRPT, in the M/G/k; Bansal et al. [2], Kamphorst and Zwart [7], and Lin et al. [9], who analyze mean response time scaling of simple policies in the heavy-traffic M/G/1; and Aalto et al. [1] and Scully et al. [11, 13], who characterize and analyze the Gittins policy in the M/G/1.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Ziv  Scully", "Isaac  Grosof", "Mor  Harchol-Balter"], "year": 2020, "n_citations": 2}
{"id": 283489, "s2_id": "7b16908a8a6b3c9535d3f746c31750a37fe16a92", "title": "Slotted ALOHA Overlay on LoRaWAN - A Distributed Synchronization Approach", "abstract": "LoRaWAN is one of the most promising standards for IoT applications. Nevertheless, the high density of end-devices expected for each gateway, the absence of an effective synchronization scheme between gateway and end-devices, challenge the scalability of these networks. In this article, we propose to regulate the communication of LoRaWAN networks using a Slotted-ALOHA instead of the classic ALOHA approach used by LoRa. The implementation is an overlay on top of the standard LoRaWAN; thus no modification in pre-existing LoRaWAN firmware and libraries is necessary. Our method is based on a novel distributed synchronization service that is suitable for low-cost IoT end-nodes. S-ALOHA supported by our synchronization service significantly improves the performance of traditional LoRaWAN networks regarding packet loss rate and network throughput.", "venue": "2018 IEEE 16th International Conference on Embedded and Ubiquitous Computing (EUC)", "authors": ["Tommaso  Polonelli", "Davide  Brunelli", "Luca  Benini"], "year": 2018, "n_citations": 27}
{"id": 289234, "s2_id": "b277f6cb55d59b4458debb94603030f4922a1962", "title": "Empirical analysis and statistical modeling of attack processes based on honeypots", "abstract": "Honeypots are more and more used to collect data on malicious activities on the Internet and to better understand the strategies and techniques used by attackers to compromise target systems. Analysis and modeling methodologies are needed to support the characterization of attack processes based on the data collected from the honeypots. This paper presents some empirical analyses based on the data collected from the Leurre.com honeypot platforms deployed on the Internet and presents some preliminary modeling studies aimed at fulfilling such objectives.", "venue": "ArXiv", "authors": ["Mohamed  Ka\u00e2niche", "Yves  Deswarte", "Eric  Alata", "Marc  Dacier", "Vincent  Nicomette"], "year": 2007, "n_citations": 54}
{"id": 290307, "s2_id": "f8e475674bdd872bc22d8073f25272b168a81ed3", "title": "Heterogeneous Parallelization and Acceleration of Molecular Dynamics Simulations in GROMACS", "abstract": "The introduction of accelerator devices such as graphics processing units (GPUs) has had profound impact on molecular dynamics simulations and has enabled order-of-magnitude performance advances using commodity hardware. To fully reap these benefits, it has been necessary to reformulate some of the most fundamental algorithms, including the Verlet list, pair searching, and cutoffs. Here, we present the heterogeneous parallelization and acceleration design of molecular dynamics implemented in the GROMACS codebase over the last decade. The setup involves a general cluster-based approach to pair lists and non-bonded pair interactions that utilizes both GPU and central processing unit (CPU) single instruction, multiple data acceleration efficiently, including the ability to load-balance tasks between CPUs and GPUs. The algorithm work efficiency is tuned for each type of hardware, and to use accelerators more efficiently, we introduce dual pair lists with rolling pruning updates. Combined with new direct GPU-GPU communication and GPU integration, this enables excellent performance from single GPU simulations through strong scaling across multiple GPUs and efficient multi-node parallelization.", "venue": "The Journal of chemical physics", "authors": ["Szil'ard  P'all", "Artem  Zhmurov", "Paul  Bauer", "Mark  Abraham", "Magnus  Lundborg", "Alan  Gray", "Berk  Hess", "Erik  Lindahl"], "year": 2020, "n_citations": 33}
{"id": 290891, "s2_id": "29b22000e78603888d4fbb78a48f3673e25c6a31", "title": "NGEMM: Optimizing GEMM for Deep Learning via Compiler-based Techniques", "abstract": "Quantization has emerged to be an effective way to significantly boost the performance of deep neural networks (DNNs) by utilizing low-bit computations. Despite having lower numerical precision, quantized DNNs are able to reduce both memory bandwidth and computation cycles with little losses of accuracy. Integer GEMM (General Matrix Multiplication) is critical to running quantized DNN models efficiently, as GEMM operations often dominate the computations in these models. Various approaches have been developed by leveraging techniques such as vectorization and memory layout to improve the performance of integer GEMM. However, these existing approaches are not fast enough in certain scenarios. We developed NGEMM, a compiler-based GEMM implementation for accelerating lower-precision training and inference. NGEMM has better use of the vector units by avoiding unnecessary vector computation that is introduced during tree reduction. We compared NGEMM's performance with the state-of-art BLAS libraries such as MKL. Our experimental results showed that NGEMM outperformed MKL non-pack and pack version by an average of 1.86x and 1.16x, respectively. We have applied NGEMM to a number of production services in Microsoft.", "venue": "ArXiv", "authors": ["Wenlei  Bao", "Li-Wen  Chang", "Yang  Chen", "Ke  Deng", "Amit  Agarwal", "Emad  Barsoum", "Abe  Taha"], "year": 2019, "n_citations": 1}
{"id": 296960, "s2_id": "ae59d10c14e89ee327e0993054264fbb1db82d53", "title": "A JIT Compiler for Neural Network Inference", "abstract": "This paper describes a C++ library that compiles neural network models at runtime into machine code that performs inference. This approach in general promises to achieve the best performance possible since it is able to integrate statically known properties of the network directly into the code. In our experiments on the NAO V6 platform, it outperforms existing implementations significantly on small networks, while being inferior on large networks. The library was already part of the B-Human code release 2018, but has been extended since and is now available as a standalone version that can be integrated into any C++14 code base.", "venue": "RoboCup", "authors": ["Felix  Thielke", "Arne  Hasselbring"], "year": 2019, "n_citations": 6}
{"id": 299858, "s2_id": "2a5c650aed21ec0d99d94ff1d2bbff434cff0a6c", "title": "Performance Provisioning and Energy Efficiency in Cloud and Distributed Computing Systems", "abstract": "In recent years, the issue of energy consumption in high performance computing (HPC) systems has attracted a great deal of attention. In response to this, many energy-aware algorithms have been developed in different layers of HPC systems, including the hardware layer, service layer and system layer. These algorithms are of two types: first, algorithms which directly try to improve the energy by tweaking frequency operation or scheduling algorithms; and second, algorithms which focus on improving the performance of the system, with the assumption that efficient running of a system may indirectly save more energy. \nIn this thesis, we develop algorithms in both layers. First, we introduce three algorithms to directly improve the energy of scheduled tasks at the hardware level by using Dynamic Voltage Frequency Scaling (DVFS). Second, we propose two algorithms for modelling and resource provisioning of MapReduce applications (a well-known parametric distributed framework currently used by Google, Yahoo, Facebook and LinkedIn) based on its configuration parameters. Certainly, estimating the performance (e.g., execution time or CPU clock ticks) of a MapReduce application can be later used for smart scheduling of such applications in clouds or clusters. \nTo evaluate the algorithms, we have conducted extensive simulation and real experiments on a 5-node physical cluster with up to 25 virtual nodes, using both synthetic and real world applications. Also, the proposed new algorithms are compared with existing algorithms by experimentation, and the experimental results reveal new information on the performance of these algorithms, as well as on the properties of MapReduce and DVFS. In the end, three open problems are revealed by the experimental observations, and their importance is explained.", "venue": "ArXiv", "authors": ["Nikzad Babaii Rizvandi"], "year": 2014, "n_citations": 1}
{"id": 302246, "s2_id": "533283b6d811d329f0793e09e37ec07037435b55", "title": "Cost of Virtual Machine Live Migration in Clouds: A Performance Evaluation", "abstract": "Virtualization has become commonplace in modern data centers, often referred as \"computing clouds\". The capability of virtual machine live migration brings benefits such as improved performance, manageability and fault tolerance, while allowing workload movement with a short service downtime. However, service levels of applications are likely to be negatively affected during a live migration. For this reason, a better understanding of its effects on system performance is desirable. In this paper, we evaluate the effects of live migration of virtual machines on the performance of applications running inside Xen VMs. Results show that, in most cases, migration overhead is acceptable but cannot be disregarded, especially in systems where availability and responsiveness are governed by strict Service Level Agreements. Despite that, there is a high potential for live migration applicability in data centers serving modern Internet applications. Our results are based on a workload covering the domain of multi-tier Web 2.0 applications.", "venue": "CloudCom", "authors": ["William  Voorsluys", "James  Broberg", "Srikumar  Venugopal", "Rajkumar  Buyya"], "year": 2009, "n_citations": 587}
{"id": 302396, "s2_id": "7e92efec706cbbf6e84c3c99360aea3fae3a28dd", "title": "Novel Binary-Addition Tree Algorithm (BAT) for Binary-State Network Reliability Problem", "abstract": "Network structures and models have been widely adopted, e.g., for Internet of Things, wireless sensor networks, smart grids, transportation networks, communication networks, social networks, and computer grid systems. Network reliability is an effective and popular technique to estimate the probability that the network is still functioning. Networks composed of binary-state (e.g., working or failed) components (arcs and/or nodes) are called binary-state networks. The binary-state network is the fundamental type of network; thus, there is always a need for a more efficient algorithm to calculate the network reliability. Thus, a novel binary-addition tree (BAT) algorithm that employs binary addition for finding all the possible state vectors and the path-based layered-search algorithm for filtering out all the connected vectors is proposed for calculating the binary-state network reliability. According to the time complexity and numerical examples, the efficiency of the proposed BAT is higher than those of traditional algorithms for solving the binary-state network reliability problem.", "venue": "Reliab. Eng. Syst. Saf.", "authors": ["Wei-Chang  Yeh"], "year": 2021, "n_citations": 15}
{"id": 303004, "s2_id": "db53afb1fc2b6522a7848cbaa47b1acfa0e28152", "title": "Covert queueing problem with a Markovian statistic", "abstract": "Based on the covert communication framework, we consider a covert queueing problem that has a Markovian statistic. Willie jobs arrive according to a Poisson process and require service from server Bob. Bob does not have a queue for jobs to wait and hence when the server is busy, arriving Willie jobs are lost. Willie and Bob enter a contract under which Bob should only serve Willie jobs. As part of the usage statistic, for a sequence of N consecutive jobs that arrived, Bob informs Willie whether each job was served or lost (this is the Markovian statistic). Bob is assumed to be violating the contract and admitting non-Willie (Nillie) jobs according to a Poisson process. For such a setting, we identify the hypothesis testing to be performed (given the Markovian data) by Willie to detect the presence or absence of Nillie jobs. We also characterize the upper bound on arrival rate of Nillie jobs such that the error in the hypothesis testing of Willie is arbitrarily large, ensuring covertness in admitting Nillie jobs.", "venue": "2021 IEEE Information Theory Workshop (ITW)", "authors": ["Arti  Yardi", "Tejas  Bodas"], "year": 2021, "n_citations": 0}
{"id": 304669, "s2_id": "352abf8abfccb3dbaceb94166dd537b17717be03", "title": "Computation of gray-level co-occurrence matrix based on CUDA and its optimization", "abstract": "As in various fields like scientific research and industrial application, the computation time optimization is becoming a task that is of increasing importance because of its highly parallel architecture. The graphics processing unit is regarded as a powerful engine for application programs that demand fairly high computation capabilities. Based on this, an algorithm was introduced in this paper to optimize the method used to compute the gray-level co-occurrence matrix (GLCM) of an image, and strategies (e.g., \"copying\", \"image partitioning\", etc.) were proposed to optimize the parallel algorithm. Results indicate that without losing the computational accuracy, the speed-up ratio of the GLCM computation of images with different resolutions by GPU by the use of CUDA was 50 times faster than that of the GLCM computation by CPU, which manifested significantly improved performance.", "venue": "ArXiv", "authors": ["Huichao  Hong", "Lixin  Zheng", "Shuwan  Pan"], "year": 2017, "n_citations": 1}
{"id": 306015, "s2_id": "6f7712ad33bff378d473df541bc979b1b7abcea3", "title": "Modeling and Predicting DNS Server Load", "abstract": "The DNS relies on caching to ensure high scalability and good performance. In optimizing caching, TTL adjustment provides a means of balancing between query load and TTL-dependent performances such as data consistency, load balancing, migration time, etc. To gain the desired balance, TTL adjustment depends on predictions of query loads under alternative TTLs. This paper proposes a model of DNS server load, which employs the uniform aggregate caching model to simplify the complexity of modeling clients' requests and their caching. A method of predicting DNS server load is developed using that model. The prediction method is solely based on the unilateral measurements or observations at authoritative servers. Without reliance on lots of multi-point measurements nor distributed measuring facilities, the method is best suited for DNS authoritative operators. The proposed model and prediction method are validated through extensive simulations. Finally, global sensibility analysis is conducted to evaluate the impacts of measurement uncertainties or errors on the predictions.", "venue": "ArXiv", "authors": ["Zheng  Wang"], "year": 2016, "n_citations": 1}
{"id": 311093, "s2_id": "b6cf6d2799359b6af5a8d0648c58c25ef1aa8e32", "title": "A Data-Assisted Reliability Model for Carrier-Assisted Cold Data Storage Systems", "abstract": "Abstract Cold data storage systems are used to allow long term digital preservation for institutions\u2019 archive. The common functionality among cold and warm/hot data storage is that the data is stored on some physical medium for read-back at a later time. However in cold storage, write and read operations are not necessarily done in the same exact geographical location. Hence, a third party assistance is typically utilized to bring together the medium and the drive. On the other hand, the reliability modeling of such a decomposed system poses few challenges that do not necessarily exist in other warm/hot storage alternatives such as fault detection and absence of the carrier, all totaling up to the data unavailability issues. In this paper, we propose a generalized non-homogenous Markov model that encompasses the aging of the carriers in order to address the requirements of today\u2019s cold data storage systems in which the data is encoded and spread across multiple nodes for the long-term data retention. We have derived useful lower/upper bounds on the overall system availability. Furthermore, the collected field data is used to estimate parameters of a Weibull distribution to accurately predict the lifetime of the carriers in an example scale-out setting.", "venue": "Reliab. Eng. Syst. Saf.", "authors": ["Suayb S. Arslan", "James  Peng", "Turguy  Goker"], "year": 2020, "n_citations": 2}
{"id": 313289, "s2_id": "aab184681450e023fd234618f0879e58febbc0ea", "title": "Ansor : Generating High-Performance Tensor Programs for Deep Learning", "abstract": "High-performance tensor programs are crucial to guarantee efficient execution of deep learning models. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously difficult. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering efforts in developing platform-specific optimization code or fall short in finding high-performance programs due to restricted search space and ineffective exploration strategy. \nWe present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores much more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing state-of-the-art approaches. Besides, Ansor utilizes a scheduler to simultaneously optimize multiple subgraphs in a set of deep neural networks. Our evaluation shows that Ansor improves the execution performance of deep neural networks on the Intel CPU, ARM CPU, and NVIDIA GPU by up to $3.8\\times$, $2.6\\times$, and $1.7 \\times$, respectively.", "venue": "OSDI", "authors": ["Lianmin  Zheng", "Chengfan  Jia", "Minmin  Sun", "Zhao  Wu", "Cody Hao Yu", "Ameer  Haj-Ali", "Yida  Wang", "Jun  Yang", "Danyang  Zhuo", "Koushik  Sen", "Joseph  Gonzalez", "Ion  Stoica"], "year": 2020, "n_citations": 45}
{"id": 314478, "s2_id": "b42acda0a523243ec72dac7131136edad501ac4a", "title": "Evaluation of DVFS techniques on modern HPC processors and accelerators for energy\u2010aware applications", "abstract": "Energy efficiency is becoming increasingly important for computing systems, in particular for large scale High Performance Computing (HPC) facilities. In this work, we evaluate, from a user perspective, the use of Dynamic Voltage and Frequency Scaling techniques, assisted by the power and energy monitoring capabilities of modern processors to tune applications for energy efficiency. We run selected kernels and a full HPC application on 2 high\u2010end processors widely used in the HPC context, namely, an NVIDIA K80 GPU and an Intel Haswell CPU. We evaluate the available trade\u2010offs between energy\u2010to\u2010solution and time\u2010to\u2010solution, attempting a function\u2010by\u2010function frequency tuning. We finally estimate the benefits obtainable running the full code on an HPC multi\u2010GPU node, with respect to default clock frequency governors. We instrument our code to accurately monitor power consumption and execution time without the need of any additional hardware, and we enable it to change CPUs and GPUs clock frequencies while running. We analyze our results on the different architectures using a simple energy\u2010performance model and derive a number of energy saving strategies, which can be easily adopted on recent high\u2010end HPC systems for generic applications.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Enrico  Calore", "Alessandro  Gabbana", "Sebastiano Fabio Schifano", "Raffaele  Tripiccione"], "year": 2017, "n_citations": 28}
{"id": 316215, "s2_id": "11cc8dfc28404338f23b24e2d727965bef5f04ed", "title": "How to Schedule Near-Optimally under Real-World Constraints", "abstract": "Scheduling is a critical part of practical computer systems, and scheduling has also been extensively studied from a theoretical perspective. Unfortunately, there is a gap between theory and practice, as the optimal scheduling policies presented by theory can be difficult or impossible to perfectly implement in practice. In this work, we use recent breakthroughs in queueing theory to begin to bridge this gap. We show how to translate theoretically optimal policies\u2014which provably minimize mean response time (a.k.a. latency)\u2014into near-optimal policies that are easily implemented in practical settings. Specifically, we handle the following real-world constraints:", "venue": "ArXiv", "authors": ["Ziv  Scully", "Mor  Harchol-Balter"], "year": 2021, "n_citations": 0}
{"id": 318507, "s2_id": "3b358e028df90cf5b09a1bac303a594846397240", "title": "A New Interpretation of Amdahl's Law and Geometric Scalability", "abstract": "The multiprocessor effect refers to the loss of computing cycles due to processing overhead. Amdahl\u2019s law and the Multiprocessing Factor (MPF) are two scaling models used in industry and academia for estimating multiprocessor capacity in the presence of this multiprocessor effect. Both models express different laws of diminishing returns. Amdahl\u2019s law identifies diminishing processor capacity with a fixed degree of serialization in the workload, while the MPF model treats it as a constant geometric ratio. The utility of both models for performance evaluation stems from the presence of a single parameter that can be determined easily from a small set of benchmark measurements. This utility, however, is marred by a dilemma. The two models produce different results, especially for large processor configurations that are so important for today\u2019s applications. The question naturally arises: Which of these two models is the correct one to use? Ignoring this question merely reduces capacity prediction to arbitrary curve-fitting. Removing the dilemma requires a dynamical interpretation of these scaling models. We present a physical interpretation based on queueing theory and show that Amdahl\u2019s law corresponds to synchronous queueing in a bus model while the MPF model belongs to a Coxian server model. The latter exhibits unphysical effects such as sublinear response times hence, we caution against its use for large multiprocessor configurations.", "venue": "ArXiv", "authors": ["Neil J. Gunther"], "year": 2002, "n_citations": 13}
{"id": 319956, "s2_id": "6d6809d45bbfba2d5cd086bda6cc2efb34cb79ff", "title": "An abstract Monte-Carlo method for the analysis of probabilistic programs", "abstract": "We introduce a new method, combination of random testing and abstract interpretation, for the analysis of programs featuring both probabilistic and non-probabilistic nondeterminism. After introducing \"ordinary\" testing, we show how to combine testing and abstract interpretation and give formulas linking the precision of the results to the number of iterations. We then discuss complexity and optimization issues and end with some experimental results.", "venue": "POPL '01", "authors": ["David  Monniaux"], "year": 2001, "n_citations": 66}
{"id": 321325, "s2_id": "7e8470ee43274be3c7e296327baad1b518f07a57", "title": "Performability Aspects of the Atlas Vo; Using Lmbench Suite", "abstract": "The ATLAS Virtual Organization is grid's largest Virtual Organization which is currently in full production stage. Hereby a case is being made that a user working within that VO is going to face a wide spectrum of different systems, whose heterogeneity is enough to count as \"orders of magnitude\" according to a number of metrics; including integer/float operations, memory throughput (STREAM) and communication latencies. Furthermore, the spread of performance does not appear to follow any known distribution pattern, which is demonstrated in graphs produced during May 2007 measurements. It is implied that the current practice where either \"all-WNs-are-equal\" or, the alternative of SPEC-based rating used by LCG/EGEE is an oversimplification which is inappropriate and expensive from an operational point of view, therefore new techniques are needed for optimal grid resources allocation.", "venue": "ArXiv", "authors": ["Fotis  Georgatos", "John  Kouvakis", "John  Kouretis"], "year": 2008, "n_citations": 1}
{"id": 331365, "s2_id": "e4648e1737cbe32060f35475dbd02523cff2397c", "title": "Evaluating the Strength of Genomic Privacy Metrics", "abstract": "The genome is a unique identifier for human individuals. The genome also contains highly sensitive information, creating a high potential for misuse of genomic data (for example, genetic discrimination). In this article, we investigate how genomic privacy can be measured in scenarios where an adversary aims to infer a person\u2019s genomic markers by constructing probability distributions on the values of genetic variations. We measured the strength of privacy metrics by requiring that metrics are monotonic with increasing adversary strength and uncovered serious problems with several existing metrics currently used to measure genomic privacy. We provide suggestions on metric selection, interpretation, and visualization and illustrate the work flow using case studies for three real-world diseases.", "venue": "ACM Trans. Priv. Secur.", "authors": ["Isabel  Wagner"], "year": 2017, "n_citations": 22}
{"id": 331603, "s2_id": "468f08c6e68c23b79562a0e99399c81ee089b12a", "title": "Impact of Traffic Characteristics on Request Aggregation in an NDN Router", "abstract": "The paper revisits the performance evaluation of caching in a Named Data Networking (NDN) router where the content store (CS) is supplemented by a pending interest table (PIT). The PIT aggregates requests for a given content that arrive within the download delay and thus brings an additional reduction in upstream bandwidth usage beyond that due to CS hits. We extend prior work on caching with non-zero download delay (non-ZDD) by proposing a novel mathematical framework that is more easily applicable to general traffic models and by considering alternative cache insertion policies. Specifically we evaluate the use of an LRU filter to improve CS hit rate performance in this non-ZDD context. We also consider the impact of time locality in demand due to finite content lifetimes. The models are used to quantify the impact of the PIT on upstream bandwidth reduction, demonstrating notably that this is significant only for relatively small content catalogues or high average request rate per content. We further explore how the effectiveness of the filter with finite content lifetimes depends on catalogue size and traffic intensity.", "venue": "ArXiv", "authors": ["Mahdieh  Ahmadi", "James  Roberts", "Emilio  Leonardi", "Ali  Movaghar"], "year": 2019, "n_citations": 2}
{"id": 334728, "s2_id": "1cb881b2de40cb014670c99f9f22d55cc9558782", "title": "Program Execution on Reconfigurable Multicore Architectures", "abstract": "Based on the two observations that diverse applications perform better on different multicore architectures, and that different phases of an application may have vastly different resource requirements, Pal et al. proposed a novel reconfigurable hardware approach for executing multithreaded programs. Instead of mapping a concurrent program to a fixed architecture, the architecture adaptively reconfigures itself to meet the application's concurrency and communication requirements, yielding significant improvements in performance. Based on our earlier abstract operational framework for multicore execution with hierarchical memory structures, we describe execution of multithreaded programs on reconfigurable architectures that support a variety of clustered configurations. Such reconfiguration may not preserve the semantics of programs due to the possible introduction of race conditions arising from concurrent accesses to shared memory by threads running on the different cores. We present an intuitive partial ordering notion on the cluster configurations, and show that the semantics of multithreaded programs is always preserved for reconfigurations \"upward\" in that ordering, whereas semantics preservation for arbitrary reconfigurations can be guaranteed for well-synchronised programs. We further show that a simple approximate notion of efficiency of execution on the different configurations can be obtained using the notion of amortised bisimulations, and extend it to dynamic reconfiguration.", "venue": "PLACES", "authors": ["Sanjiva  Prasad"], "year": 2016, "n_citations": 0}
{"id": 336117, "s2_id": "cb2ec62e5c06a75c8f635f146f96dd7d6c7413cf", "title": "Characterization of the burst stabilization protocol for the RR/RR CICQ switch", "abstract": "Input buffered switches with virtual output queueing (VOQ) can be unstable when presented with unbalanced loads. Existing scheduling algorithms, including iSLIP for input queued (IQ) switches and round robin (RR) for combined input and crossbar queued (CICQ) switches, exhibit instability for some schedulable loads. We investigate the use of a queue length threshold and bursting mechanism to achieve stability without requiring internal speed-up. An analytical model is developed to prove that the burst stabilization protocol achieves stability and to predict the minimum burst value needed as a function of offered load. The analytical model is shown to have very good agreement with simulation results. These results show the advantage of the RR/RR CICQ switch as a contender for the next generation of high-speed switches.", "venue": "28th Annual IEEE International Conference on Local Computer Networks, 2003. LCN '03. Proceedings.", "authors": ["Neil J. Gunther", "Kenneth J. Christensen", "Kenji  Yoshigoe"], "year": 2003, "n_citations": 6}
{"id": 339052, "s2_id": "3d33b18013dd78726242e5c9af357a0fd752f871", "title": "FLCD: A Flexible Low Complexity Design of Coded Distributed Computing", "abstract": "We propose a flexible low complexity design (FLCD) of coded distributed computing (CDC) with empirical evaluation on Amazon Elastic Compute Cloud (Amazon EC2). CDC can expedite MapReduce like computation by trading increased map computations to reduce communication load and shuffle time. A main novelty of FLCD is to utilize the design freedom in defining map and reduce functions to develop asymptotic homogeneous systems to support varying intermediate values (IV) sizes under a general MapReduce framework. Compared to existing designs with constant IV sizes, FLCD offers greater flexibility in adapting to network parameters and significantly reduces the implementation complexity by requiring fewer input files and shuffle groups. The FLCD scheme is the first proposed low-complexity CDC design that can operate on a network with an arbitrary number of nodes and computation load. We perform empirical evaluations of the FLCD by executing the TeraSort algorithm on an Amazon EC2 cluster. This is the first time that theoretical predictions of the CDC shuffle time are validated by empirical evaluations. The evaluations demonstrate a 2.0 to 4.24x speedup compared to conventional uncoded MapReduce, a 12% to 52% reduction in total time, and a wider range of operating network parameters compared to existing CDC schemes.", "venue": "ArXiv", "authors": ["Nicholas  Woolsey", "Xingyue  Wang", "Rong-Rong  Chen", "Mingyue  Ji"], "year": 2020, "n_citations": 0}
{"id": 342617, "s2_id": "28f27f3b17294a1ec50cee645522d2c66c812693", "title": "Seagull: An Infrastructure for Load Prediction and Optimized Resource Allocation", "abstract": "Microsoft Azure is dedicated to guarantee high quality of service to its customers, in particular, during periods of high customer activity, while controlling cost. We employ a Data Science (DS) driven solution to predict user load and leverage these predictions to optimize resource allocation. To this end, we built the Seagull infrastructure that processes per-server telemetry, validates the data, trains and deploys ML models. The models are used to predict customer load per server (24h into the future), and optimize service operations. Seagull continually re-evaluates accuracy of predictions, fallback to previously known good models and triggers alerts as appropriate. We deployed this infrastructure in production for PostgreSQL and MySQL servers across all Azure regions, and applied it to the problem of scheduling server backups during low-load time. This minimizes interference with user-induced load and improves customer experience.", "venue": "Proc. VLDB Endow.", "authors": ["Olga  Poppe", "Tayo  Amuneke", "Dalitso  Banda", "Aritra  De", "Ari  Green", "Manon  Knoertzer", "Ehi  Nosakhare", "Karthik  Rajendran", "Deepak  Shankargouda", "Meina  Wang", "Alan  Au", "Carlo  Curino", "Qun  Guo", "Alekh  Jindal", "Ajay  Kalhan", "Morgan  Oslake", "Sonia  Parchani", "Vijay  Ramani", "Raj  Sellappan", "Saikat  Sen", "Sheetal  Shrotri", "Soundararajan  Srinivasan", "Ping  Xia", "Shize  Xu", "Alicia  Yang", "Yiwen  Zhu"], "year": 2020, "n_citations": 5}
{"id": 344082, "s2_id": "c166f46510f462bed3dda8a30642a4ae22c70719", "title": "Queuing theoretic analysis of power-performance tradeoff in power-efficient computing", "abstract": "In this paper we study the power-performance relationship of power-efficient computing from a queuing theoretic perspective. We investigate the interplay of several system operations including processing speed, system on/off decisions, and server farm size. We identify that there are oftentimes 'sweet spots' in power-efficient operations: there exist optimal combinations of processing speed and system settings that maximize power efficiency. For the single server case, a widely deployed threshold mechanism is studied. We show that there exist optimal processing speed and threshold value pairs that minimize the power consumption. This holds for the threshold mechanism with job batching. For the multi-server case, it is shown that there exist best processing speed and server farm size combinations.", "venue": "2013 47th Annual Conference on Information Sciences and Systems (CISS)", "authors": ["Yanpei  Liu", "Stark C. Draper", "Nam Sung Kim"], "year": 2013, "n_citations": 6}
{"id": 345358, "s2_id": "78decc7dc5889f5582f217908837f198b721f2e8", "title": "A Taxonomy of Performance Assurance Methodologies and its Application in High Performance Computer Architectures", "abstract": "This paper presents a systematic approach to the complex problem of high confidence performance assurance of high performance architectures based on methods used over several generations of industrial microprocessors. A taxonomy is presented for performance assurance through three key stages of a product life cycle-high level performance, RTL performance, and silicon performance. The proposed taxonomy includes two components-independent performance assurance space for each stage and a correlation performance assurance space between stages. It provides a detailed insight into the performance assurance space in terms of coverage provided taking into account capabilities and limitations of tools and methodologies used at each stage. An application of the taxonomy to cases described in the literature and to high performance Intel architectures is shown. The proposed work should be of interest to manufacturers of high performance microprocessor/chipset architectures and has not been discussed in the literature.", "venue": "ArXiv", "authors": ["Hemant G. Rotithor"], "year": 2013, "n_citations": 2}
{"id": 348588, "s2_id": "4304eba08ae9360afeff7e7953f7a9e8f90082aa", "title": "Reduction Methods on Probabilistic Control-flow Programs for Reliability Analysis", "abstract": "Modern safety-critical systems are heterogeneous, complex, and highly dynamic. They require reliability evaluation methods that go beyond the classical static methods such as fault trees, event trees, or reliability block diagrams. Promising dynamic reliability analysis methods employ probabilistic model checking on various probabilistic state-based models. However, such methods have to tackle the well-known state-space explosion problem. To compete with this problem, reduction methods such as symmetry reduction and partial-order reduction have been successfully applied to probabilistic models by means of discrete Markov chains or Markov decision processes. Such models are usually specified using probabilistic programs provided in guarded command language. In this paper, we propose two automated reduction methods for probabilistic programs that operate on a purely syntactic level: reset value optimization and register allocation optimization. The presented techniques rely on concepts well known from compiler construction such as live range analysis and register allocation through interference graph coloring. Applied on a redundancy system model for an aircraft velocity control loop modeled in SIMULINK, we show effectiveness of our implementation of the reduction methods. We demonstrate that model-size reductions in three orders of magnitude are possible and show that we can achieve significant speedups for a reliability analysis.", "venue": "ArXiv", "authors": ["Clemens  Dubslaff", "Andrey  Morozov", "Christel  Baier", "Klaus  Janschek"], "year": 2020, "n_citations": 5}
{"id": 349054, "s2_id": "c69f2db74cc1b0482a6f5df9421bfe0dedb587bb", "title": "Jointly Optimal Channel Pairing and Power Allocation for Multichannel Multihop Relaying", "abstract": "We study the problem of channel pairing and power allocation in a multichannel multihop relay network to enhance the end-to-end data rate. Both amplify-and-forward (AF) and decode-and-forward (DF) relaying strategies are considered. Given fixed power allocation to the channels, we show that channel pairing over multiple hops can be decomposed into independent pairing problems at each relay, and a sorted-SNR channel pairing strategy is sum-rate optimal, where each relay pairs its incoming and outgoing channels by their SNR order. For the joint optimization of channel pairing and power allocation under both total and individual power constraints, we show that the problem can be decoupled into two subproblems solved separately. This separation principle is established by observing the equivalence between sorting SNRs and sorting channel gains in the jointly optimal solution. It significantly reduces the computational complexity in finding the jointly optimal solution. It follows that the channel pairing problem in joint optimization can be again decomposed into independent pairing problems at each relay based on sorted channel gains. The solution for optimizing power allocation for DF relaying is also provided, as well as an asymptotically optimal solution for AF relaying. Numerical results are provided to demonstrate substantial performance gain of the jointly optimal solution over some suboptimal alternatives. It is also observed that more gain is obtained from optimal channel pairing than optimal power allocation through judiciously exploiting the variation among multiple channels. Impact of the variation of channel gain, the number of channels, and the number of hops on the performance gain is also studied through numerical examples.", "venue": "IEEE Transactions on Signal Processing", "authors": ["Mahdi  Hajiaghayi", "Min  Dong", "Ben  Liang"], "year": 2011, "n_citations": 39}
{"id": 351605, "s2_id": "3755ca2c3674a814e62f09bd8a67f9d7aefa2638", "title": "HPTT: a high-performance tensor transposition C++ library", "abstract": "Recently we presented TTC, a domain-specific compiler for tensor transpositions. Despite the fact that the performance of the generated code is nearly optimal, due to its offline nature, TTC cannot be utilized in all the application codes in which the tensor sizes and the necessary tensor permutations are determined at runtime. To overcome this limitation, we introduce the open-source C++ library High-Performance Tensor Transposition (HPTT). Similar to TTC, HPTT incorporates optimizations such as blocking, multi-threading, and explicit vectorization; furthermore it decomposes any transposition into multiple loops around a so called micro-kernel. This modular design-inspired by BLIS-makes HPTT easy to port to different architectures, by only replacing the hand-vectorized micro-kernel (e.g.,a 4 x 4 transpose). HPTT also offers an optional autotuning framework-guided by performance heuristics-that explores a vast search space of implementations at runtime (similar to FFTW). Across a wide range of different tensor transpositions and architectures (e.g., Intel Ivy Bridge, ARMv7, IBM Power7), HPTT attains a bandwidth comparable to that of SAXPY, and yields remarkable speedups over Eigen's tensor transposition implementation. Most importantly, the integration of HPTT into the Cyclops Tensor Framework (CTF) improves the overall performance of tensor contractions by up to 3.1x.", "venue": "ARRAY@PLDI", "authors": ["Paul  Springer", "Tong  Su", "Paolo  Bientinesi"], "year": 2017, "n_citations": 32}
{"id": 351774, "s2_id": "a392cccea7ee32de1f9f2d37a1bf2d954212d674", "title": "Concurrent Processing Memory", "abstract": "A theoretical memory with limited processing power and internal connectivity at each element is proposed. This memory carries out parallel processing within itself to solve generic array problems. The applicability of this in-memory finest-grain massive SIMD approach is studied in some details. For an array of N items, it reduces the total instruction cycle count of universal operations such as insertion/deletion and match finding to ~ 1, local operations such as filtering and template matching to ~ local operation size, and global operations such as sum, finding global limit and sorting to ~\u221aN instruction cycles. It eliminates most streaming activities for data processing purpose on the system bus. Yet it remains general-purposed, easy to use, pin compatible with conventional memory, and practical for implementation. Keyword: SIMD processors; Parallel Processors; Memory Structures; Performance evaluation of algorithms and systems;", "venue": "ArXiv", "authors": ["Chengpu  Wang", "Zhen  Wang"], "year": 2006, "n_citations": 1}
{"id": 352319, "s2_id": "e8feb5f96f7a5507882408cbf175e00dea53fa6a", "title": "Exploring the Fairness and Resource Distribution in an Apache Mesos Environment", "abstract": "Apache Mesos, a cluster-wide resource manager, is widely deployed in massive scale at several Clouds and Data Centers. Mesos aims to provide high cluster utilization via fine grained resource co-scheduling and resource fairness among multiple users through Dominant Resource Fairness (DRF) based allocation. DRF takes into account different resource types (CPU, Memory, Disk I/O) requested by each application and determines the share of each cluster resource that could be allocated to the applications. Mesos has adopted a two-level scheduling policy: (1) DRF to allocate resources to competing frameworks and (2) task level scheduling by each framework for the resources allocated during the previous step. We have conducted experiments in a local Mesos cluster when used with frameworks such as Apache Aurora, Marathon, and our own framework Scylla, to study resource fairness and cluster utilization. Experimental results show how informed decision regarding second level scheduling policy of frameworks and attributes like offer holding period, offer refusal cycle and task arrival rate can reduce unfair resource distribution. Bin-Packing scheduling policy on Scylla with Marathon can reduce unfair allocation from 38% to 3%. By reducing unused free resources in offers we bring down the unfairness from to 90% to 28%. We also show the effect of task arrival rate to reduce the unfairness from 23% to 7%.", "venue": "2018 IEEE 11th International Conference on Cloud Computing (CLOUD)", "authors": ["Pankaj  Saha", "Angel  Beltre", "Madhusudhan  Govindaraju"], "year": 2018, "n_citations": 10}
{"id": 353404, "s2_id": "d34353ce14fab8a6a16c0f145fa38f1f47525811", "title": "Downlink Resource Allocation in Multiuser Cell-free MIMO Networks with User-centric Clustering", "abstract": "In this paper, we optimize user scheduling, power allocation and beamforming in distributed multiple-input multipleoutput (MIMO) networks implementing user-centric clustering. We study both the coherent and non-coherent transmission modes, formulating a weighted sum rate maximization problem for each; finding the optimal solution to these problems is known to be NP-hard. We use tools from fractional programming, block coordinate descent, and compressive sensing to construct an algorithm that optimizes the beamforming weights and user scheduling and converges in a smooth non-decreasing pattern. Channel state information (CSI) being crucial for optimization, we highlight the importance of employing a low-overhead pilot assignment policy for scheduling problems. In this regard, we use a variant of hierarchical agglomerative clustering, which provides a suboptimal, but feasible, pilot assignment scheme; for our cellfree case, we formulate an area-based pilot reuse factor. Our results show that our scheme provides large gains in the longterm network sum spectral efficiency compared to benchmark schemes such as zero-forcing and conjugate beamforming (with round-robin scheduling) respectively. Furthermore, the results show the superiority of coherent transmission compared to the non-coherent mode under ideal and imperfect CSI for the areabased pilot-reuse factors we consider.", "venue": "ArXiv", "authors": ["Hussein A. Ammar", "Raviraj  Adve", "Shahram  Shahbazpanahi", "Gary  Boudreau", "Kothapalli Venkata Srinivas"], "year": 2021, "n_citations": 2}
{"id": 355989, "s2_id": "0db69919ae66c5cf0f94541049e082feff1040b3", "title": "Catch Me If You Can: Using Power Analysis to Identify HPC Activity", "abstract": "Author(s): Copos, Bogdan; Peisert, Sean | Abstract: Monitoring users on large computing platforms such as high performance computing (HPC) and cloud computing systems is non-trivial. Utilities such as process viewers provide limited insight into what users are running, due to granularity limitation, and other sources of data, such as system call tracing, can impose significant operational overhead. However, despite technical and procedural measures, instances of users abusing valuable HPC resources for personal gains have been documented in the past \\cite{hpcbitmine}, and systems that are open to large numbers of loosely-verified users from around the world are at risk of abuse. In this paper, we show how electrical power consumption data from an HPC platform can be used to identify what programs are executed. The intuition is that during execution, programs exhibit various patterns of CPU and memory activity. These patterns are reflected in the power consumption of the system and can be used to identify programs running. We test our approach on an HPC rack at Lawrence Berkeley National Laboratory using a variety of scientific benchmarks. Among other interesting observations, our results show that by monitoring the power consumption of an HPC rack, it is possible to identify if particular programs are running with precision up to and recall of 95\\% even in noisy scenarios.", "venue": "ArXiv", "authors": ["Bogdan  Copos", "Sean  Peisert"], "year": 2020, "n_citations": 0}
{"id": 361463, "s2_id": "b9af9ed6db40f5e3234e4e700a9f9efe6ab54b62", "title": "Eco: A Hardware-Software Co-Design for In Situ Power Measurement on Low-end IoT Systems", "abstract": "In this paper, we present Eco, a hardware-software co-design enabling generic energy management on IoT nodes. Eco is tailored to devices with limited resources and thus targets most of the upcoming IoT scenarios. The proposed measurement module combines commodity components with common system interfaces to achieve easy, flexible integration with various hardware platforms and the RIOT IoT operating system. We thoroughly evaluate and compare accuracy and overhead. Our findings indicate that our commodity design competes well with highly optimized solutions, while being significantly more versatile. We employ Eco for energy management on RIOT and validate its readiness for deployment in a five-week field trial integrated with energy harvesting.", "venue": "ENSsys@SenSys", "authors": ["Michel  Rottleuthner", "Thomas C. Schmidt", "Matthias  W\u00e4hlisch"], "year": 2019, "n_citations": 4}
{"id": 361763, "s2_id": "5b54c92732a3c4de7d7ac4a96505011713711ba4", "title": "Are Markov Models Effective for Storage Reliability Modelling?", "abstract": "Continuous Time Markov Chains (CTMC) have been used extensively to model reliability of storage systems. While the exponentially distributed sojourn time of Markov models is widely known to be unrealistic (and it is necessary to consider Weibull-type models for components such as disks), recent work has also highlighted some additional infirmities with the CTMC model, such as the ability to handle repair times. Due to the memoryless property of these models, any failure or repair of one component resets the \"clock\" to zero with any partial repair or aging in some other subsystem forgotten. It has therefore been argued that simulation is the only accurate technique available for modelling the reliability of a storage system with multiple components. \nWe show how both the above problematic aspects can be handled when we consider a careful set of approximations in a detailed model of the system. A detailed model has many states, and the transitions between them and the current state captures the \"memory\" of the various components. We model a non-exponential distribution using a sum of exponential distributions, along with the use of a CTMC solver in a probabilistic model checking tool that has support for reducing large state spaces. Furthermore, it is possible to get results close to what is obtained through simulation and at much lower cost.", "venue": "ArXiv", "authors": ["Prasenjit  Karmakar", "K.  Gopinath"], "year": 2015, "n_citations": 6}
{"id": 364895, "s2_id": "46fcb6ac84a830eb0d76ccfa548835e8495e2a72", "title": "Power-Aware Runtime Scheduler for Mixed-Criticality Systems on Multicore Platform", "abstract": "In modern multicore mixed-criticality (MC) systems, a rise in peak power consumption due to parallel execution of tasks with maximum frequency, specially in the overload situation, may lead to thermal issues, which may affect the reliability and timeliness of MC systems. Therefore, managing peak power consumption has become imperative in multicore MC systems. In this regard, we propose an online peak power and thermal management heuristic for multicore MC systems. This heuristic reduces the peak power consumption of the system as much as possible during runtime by exploiting dynamic slack and per-cluster dynamic voltage and frequency scaling (DVFS). Specifically, our approach examines multiple tasks ahead to determine the most appropriate one for slack assignment, that has the most impact on the system peak power and temperature. However, changing the frequency and selecting a proper task for slack assignment and a proper core for task remapping at runtime can be time-consuming and may cause deadline violation which is not admissible for high-criticality tasks. Therefore, we analyze and then optimize our runtime scheduler and evaluate it for various platforms. The proposed approach is experimentally validated on the ODROID-XU3 (DVFS-enabled heterogeneous multicore platform) with various embedded real-time benchmarks. Results show that our heuristic achieves up to 5.25% reduction in system peak power and 20.33% reduction in maximum temperature compared to an existing method while meeting deadline constraints in different criticality modes.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Behnaz  Ranjbar", "Tuan D. A. Nguyen", "Alireza  Ejlali", "Akash  Kumar"], "year": 2021, "n_citations": 3}
{"id": 365552, "s2_id": "808d6145643d7e57d1e3dcd3d166b98849886d9e", "title": "Learning-Based Hybrid Beamforming Design for Full-Duplex Millimeter Wave Systems", "abstract": "Millimeter Wave (mmWave) communications with full-duplex (FD) have the potential of increasing the spectral efficiency, relative to those with half-duplex. However, the residual self-interference (SI) from FD and high pathloss inherent to mmWave signals may degrade the system performance. Meanwhile, hybrid beamforming (HBF) is an efficient technology to enhance the channel gain and mitigate interference with reasonable complexity. However, conventional HBF approaches for FD mmWave systems are based on optimization processes, which are either too complex or strongly rely on the quality of channel state information (CSI). We propose two learning schemes to design HBF for FD mmWave systems, i.e., extreme learning machine based HBF (ELM-HBF) and convolutional neural networks based HBF (CNN-HBF). Specifically, we first propose an alternating direction method of multipliers (ADMM) based algorithm to achieve SI cancellation beamforming, and then use a majorization-minimization (MM) based algorithm for joint transmitting and receiving HBF optimization. To train the learning networks, we simulate noisy channels as input, and select the hybrid beamformers calculated by proposed algorithms as targets. Results show that both learning based schemes can provide more robust HBF performance and achieve at least 22.1% higher spectral efficiency compared to orthogonal matching pursuit (OMP) algorithms. Besides, the online prediction time of proposed learning based schemes is almost 20 times faster than the OMP scheme. Furthermore, the training time of ELM-HBF is about 600 times faster than that of CNN-HBF with 64 transmitting and receiving antennas.", "venue": "IEEE Transactions on Cognitive Communications and Networking", "authors": ["Shaocheng  Huang", "Yu  Ye", "Ming  Xiao"], "year": 2021, "n_citations": 12}
{"id": 366026, "s2_id": "664ba65ed0a28166fbf500e9941c5feb8a3d2dc4", "title": "Statistical Delay Bound for WirelessHART Networks", "abstract": "In this paper we provide a performance analysis framework for wireless industrial networks by deriving a service curve and a bound on the delay violation probability. For this purpose we use the $(\\min, \\times)$~stochastic network calculus as well as a recently presented recursive formula for an end-to-end delay bound of wireless heterogeneous networks. The derived results are mapped to WirelessHART networks used in process automation and validated via simulations. In addition to WirelessHART, our results can be applied to any wireless network whose physical layer conforms the IEEE 802.15.4 standard, while its MAC protocol incorporates channel hopping and TDMA, like e.g. ISA100.11a or TSCH-based networks. The provided delay analysis is especially useful during the network design phase, offering further research potential towards optimal routing and power management in QoS-constrained wireless industrial networks.", "venue": "PE-WASUN@MSWiM", "authors": ["Neda  Petreska", "Hussein  Al-Zubaidy", "Barbara  Staehle", "Rudi  Knorr", "James  Gross"], "year": 2016, "n_citations": 3}
{"id": 366931, "s2_id": "e91a2c09ecd64f9aaa473b7f52ae04776289860e", "title": "ROOT I/O compression algorithms and their performance impact within Run 3", "abstract": "The LHCs Run3 will push the envelope on data-intensive workflows and, since at the lowest level this data is managed using the ROOT software framework, preparations for managing this data are starting already. At the beginning of LHC Run 1, all ROOT data was compressed with the ZLIB algorithm; since then, ROOT has added support for additional algorithms such as LZMA and LZ4, each with unique strengths. This work must continue as industry introduces new techniques - ROOT can benefit saving disk space or reducing the I/O and bandwidth for online and offline needs of experiments by introducing better compression algorithms. In addition to alternate algorithms, we have been exploring alternate techniques to improve parallelism and apply pre-conditioners to the serialized data. \nWe have performed a survey of the performance of the new compression techniques. Our survey includes various use cases of data compression of ROOT files provided by different LHC experiments. We also provide insight into solutions applied to resolve bottlenecks in compression algorithms, resulting in improved ROOT performance.", "venue": "ArXiv", "authors": ["Oksana  Shadura", "Brian Paul Bockelman"], "year": 2019, "n_citations": 3}
{"id": 374372, "s2_id": "50e7cf0c5ca578d40f1cc440ada58b99eeb6eb16", "title": "Experiences with Some Benchmarks for Deductive Databases and Implementations of Bottom-Up Evaluation", "abstract": "OpenRuleBench is a large benchmark suite for rule engines, which includes deductive databases. We previously proposed a translation of Datalog to C++ based on a method that \"pushes\" derived tuples immediately to places where they are used. In this paper, we report performance results of various implementation variants of this method compared to XSB, YAP and DLV. We study only a fraction of the OpenRuleBench problems, but we give a quite detailed analysis of each such task and the factors which influence performance. The results not only show the potential of our method and implementation approach, but could be valuable for anybody implementing systems which should be able to execute tasks of the discussed types.", "venue": "WLP / WFLP", "authors": ["Stefan  Brass", "Heike  Stephan"], "year": 2017, "n_citations": 3}
{"id": 376919, "s2_id": "10d91849c40428f5027aec6f0c2eb6fb8590b3ec", "title": "REMR: A Reliability Evaluation Method for Dynamic Edge Computing Network under Time Constraints", "abstract": "While the concept of Artificial Intelligent Internet of Things (AIoT) is booming, computation and/or communicationintensive tasks accompanied by several sub-tasks are slowly moving from centralized deployment to edge-side deployment. The idea of edge computing also makes intelligent services sink locally. But in actual scenarios like dynamic edge computing networks (DECN), due to fluctuations in available computing resources of intermediate servers and changes in bandwidth during data transmission, service reliability becomes difficult to guarantee. Coupled with changes in the amount of data in a service, the above three problems all make the existing reliability evaluation methods no longer accurate. To study the effect of distributed service deployment strategies under such a background, this paper proposes a reliability evaluation method (REMR) based on lower boundary rule under time constraint to study the degree of the rationality of a service deployment plan combined with DECN. In this scenario, time delay is the main concern which would be affected by three quantitative factors: data packet storing and sending time, data transmission time and the calculation time of executing sub-tasks on the node devices, specially while the last two are in dynamic scenarios. In actual calculation, based on the idea of the minimal paths, the solution set would to be found that can meet requirements in the current deployment. Then the reliability of the service supported by the solution sets would be found out based on the principle of inclusion-exclusion combined with the distribution of available data transmission bandwidth and the distribution of node available computing resources. Besides a illustrative example was provided, to verify the calculated reliability of the designed service deployment plan, the NS3 is utilized along with Google cluster data set for simulation. The results gained from NS3 proved the accuracy and applicability of the proposed reliability evaluation method.", "venue": "ArXiv", "authors": ["Liang  Chen", "Jianpeng  Qi", "Xiao  Su", "Rui  Wang"], "year": 2021, "n_citations": 0}
{"id": 377885, "s2_id": "63775daaeefc2f07b5035107d9e8161d9557a9ff", "title": "Performance Assessment of WhatsApp and IMO on Android Operating System (Lollipop and KitKat) during VoIP calls using 3G or WiFi", "abstract": "This paper assesses the performance of mobile messaging and VoIP connections. We investigate the CPU usage of WhatsApp and IMO under different scenarios. This analysis also enabled a comparison of the performance of these applications on two Android operating system (OS) versions: KitKat or Lollipop. Two models of smartphones were considered, viz. Galaxy Note 4 and Galaxy S4. The applications behavior was statistically investigated for both sending and receiving VoIP calls. Connections have been examined over 3G and WiFi. The handset model plays a decisive role in CPU usage of the application. t-tests showed that IMO has a better performance that WhatsApp whatever be the Android at a significance level 1%, on Galaxy Note 4. In contrast, WhatsApp requires less CPU than IMO on Galaxy S4 whatever be the OS and access (3G/WiFi). Galaxy Note 4 using WiFi always outperformed S4 in terms of processing efficiency.", "venue": "ArXiv", "authors": ["R. C. de Oliveira", "H\u00e9lio Magalh\u00e3es de Oliveira", "R. A. Ramalho", "L. P. S. Viana"], "year": 2016, "n_citations": 1}
{"id": 378141, "s2_id": "cd97d4e2e45fe7150697b6efc932588c8c37340a", "title": "Supercharge me: Boost Router Convergence with SDN", "abstract": "By enabling logically-centralized and direct control of the forwarding behavior of a network, Software-Defined Networking (SDN) holds great promise in terms of improving network management, performance, and costs. Realizing this vision is challenging though as SDN proposals to date require substantial and expensive changes to the existing network architecture before the benefits can be realized. As a result, the number of SDN deployments has been rather limited in scope. To kickstart a wide-scale SDN deployment, there is a need for low-risk, high return solutions that solve a timely problem. As one possible solution, we show how we can significantly improve the performance of legacy IP routers, i.e. \"supercharge\" them, by combining them with SDN-enabled devices. In this abstract, we supercharge one particular aspect of the router performance: its convergence time after a link or a node failure.", "venue": "SIGCOMM 2015", "authors": ["Michael Alan Chang", "Thomas  Holterbach", "Markus  Happe", "Laurent  Vanbever"], "year": 2015, "n_citations": 6}
{"id": 378743, "s2_id": "6b67a076f7fa69c3f04dc335f36dd647cd0048bd", "title": "A parallel sparse tensor benchmark suite on CPUs and GPUs", "abstract": "Tensor computations present significant performance challenges that impact a wide spectrum of applications. Efforts on improving the performance of tensor computations include exploring data layout, execution scheduling, and parallelism in common tensor kernels. This work presents a benchmark suite for arbitrary-order sparse tensor kernels using state-of-the-art tensor formats: coordinate (COO) and hierarchical coordinate (HiCOO). It demonstrates a set of reference tensor kernel implementations and some observations on Intel CPUs and NVIDIA GPUs. The full paper can be referred to at http://arxiv.org/abs/2001.00660.", "venue": "PPoPP", "authors": ["Jiajia  Li", "Mahesh  Lakshminarasimhan", "Xiaolong  Wu", "Ang  Li", "Catherine  Olschanowsky", "Kevin  Barker"], "year": 2020, "n_citations": 0}
{"id": 384133, "s2_id": "12770c61ec005c7e4bbdb1f7f304e6288b42fdf1", "title": "Large-Scale Benchmarks for the Job Shop Scheduling Problem", "abstract": "The aim of this report is to present and provide access to two novel benchmarks for the Job Shop Scheduling Problem (JSSP). The JSSP is one of the most studied scheduling problem and as such, there are a large number of benchmarks available in literature (e.g. [10, 13, 1, 12, 2]). However, a common shortcoming of classic problem instances is the limited number of jobs and operations in comparison with real industrial scenarios. The industrial field is one of the domains that had more impact in the development of scheduling theory [11, 4, 3], to the point that even the terminology adopted by scholars derives from its semantic field. In fact, terms often used in the scheduling domain are strictly coupled with industrial concepts, like machines to indicate resources and jobs to indicate tasks. Following this terminology, the factory layout (i.e. the number of machines and their functionality) is called shop, and when a job is composed by sub-tasks ordered in a specific sequence, they are called operations. Operations are linked to machines with the concept of operation type. In fact, every operation has a type and each machine can process operations of certain types, and not others. Despite this strong link, scholars have begun to study the scheduling problem in a more abstract and \u201cpure\u201d form. This allowed researchers to concentrate on the aspects that are at the core of the problem complexity (e.g. [8]), but made more and more difficult to apply the academic results on real-life scenarios [7]. One of the aspects where this discrepancy is more visible is the size of the scheduling problems.In fact, one of the few benchmark targeted to industrial scenarios is the Taillard benchmark, from 1992 [13]. He published a benchmark simulating the size of real industrial data, with the largest JSSP instances reaching 100 jobs to be scheduled on 20 machines. After his work, however, little effort was put into maintaining the parallelism between real industrial problems and JSSP instances. Nowadays it is not uncommon to reach scheduling problems as big as 1000 jobs on 1000 machines for a rather short planning horizon (e.g. a week); however, in JSSP there are no benchmarks of such size. Some recent studies, like Zhai et al. in 2014 [16], address the problem, but are still reluctant to test instances beyond 100 jobs on 50 machines. Others, like one", "venue": "ArXiv", "authors": ["Giacomo Da Col", "Erich  Teppan"], "year": 2021, "n_citations": 0}
{"id": 387671, "s2_id": "14035b77f15d8a692a96bfbf797ef6a0e137ad45", "title": "Blockchain Goes Green? An Analysis of Blockchain on Low-Power Nodes", "abstract": "Motivated by the massive energy usage of blockchain, on the one hand, and by significant performance improvements in low-power, wimpy systems, on the other hand, we perform an in-depth time-energy analysis of blockchain systems on low-power nodes in comparison to high-performance nodes. We use three low-power systems to represent a wide range of the performance-power spectrum, while covering both x86/64 and ARM architectures. We show that low-end wimpy nodes are struggling to run full-fledged blockchains mainly due to their small and low-bandwidth memory. On the other hand, wimpy systems with balanced performance-to-power ratio achieve reasonable performance while saving significant amounts of energy. For example, Jetson TX2 nodes achieve around 80% and 30% of the throughput of Parity and Hyperledger, respectively, while using 18x and 23x less energy compared to traditional brawny servers with Intel Xeon CPU.", "venue": "ArXiv", "authors": ["Dumitrel  Loghin", "Gang  Chen", "Tien Tuan Anh Dinh", "Beng Chin Ooi", "Yong Meng Teo"], "year": 2019, "n_citations": 5}
{"id": 388459, "s2_id": "71fa157e3e8ff221679e5664c37f382485ca1f92", "title": "Dockless Bike-Sharing Systems with Unusable Bikes: Removing, Repair and Redistribution under Batch Policies", "abstract": "This paper discusses a large-scale dockless bike-sharing system (DBSS) with unusable bikes, which can be removed, repaired, redistributed and reused under two batch policies: One for removing the unusable bikes from each parking region to a maintenance shop, and the other for redistributing the repaired bikes from the maintenance shop to some suitable parking regions. For such a bike-sharing system, this paper proposes and develops a new computational method by applying the RG-factorizations of block-structured Markov processes in the closed queueing networks. Different from previous works in the literature of queueing networks, a key contribution of our computational method is to set up a new nonlinear matrix equation to determine the relative arrival rates, and to show that the nonlinearity comes from two different groups of processes: The failure and removing processes; and the repair and redistributing processes. Once the relative arrival rate is introduced to each node, these nodes are isolated from each other, so that the Markov processes of all the nodes are independent of each other, thus the Markov system of each node is described as an elegant block-structured Markov process whose stationary probabilities can be easily computed by the RG-factorizations. Based on this, this paper can establish a more general product-form solution of the closed queueing network, and provides performance analysis of the DBSS through a comprehensive discussion for the bikes' failure, removing, repair, redistributing and reuse processes under two batch policies. We hope that our method opens a new avenue to quantitative evaluation of more general DBSSs with unusable bikes.", "venue": "ArXiv", "authors": ["Rui-Na  Fan", "Quan-Lin  Li", "Xiaole  Wu", "Zhe George Zhang"], "year": 2019, "n_citations": 0}
{"id": 394782, "s2_id": "c5aacfe5f135d9abaf936dd41645ffb0b0c17f12", "title": "LLAMA: The Low Level Abstraction For Memory Access", "abstract": "The performance gap between CPU and memory widens continuously. Choosing the best memory layout for each hardware architecture is increasingly important as more and more programs become memory bound. For portable codes that run across heterogeneous hardware architectures, the choice of the memory layout for data structures is ideally decoupled from the rest of a program. This can be accomplished via a zero-runtime-overhead abstraction layer, underneath which memory layouts can be freely exchanged. We present the Low-Level Abstraction of Memory Access (LLAMA), a C++ library that provides such a data structure abstraction layer with example implementations for multidimensional arrays of nested, structured data. LLAMA provides fully C++ compliant methods for defining and switching custom memory layouts for user-defined data types. The library is extensible with third-party allocators. Providing two close-to-life examples, we show that the LLAMA-generated AoS (Array of Structs) and SoA (Struct of Arrays) layouts produce identical code with the same performance characteristics as manually written data structures. Integrations into the SPEC CPU\u00ae lbm benchmark and the particle-in-cell simulation PIConGPU demonstrate LLAMA\u2019s abilities in real-world applications. LLAMA\u2019s layout-aware copy routines can significantly speed up transfer and reshuffling of data between layouts compared with naive element-wise copying. LLAMA provides a novel tool for the development of high-performance C++ applications in a heterogeneous environment. 1 ar X iv :2 10 6. 04 28 4v 2 [ cs .P F] 2 4 N ov 2 02 1", "venue": "ArXiv", "authors": ["Bernhard Manfred Gruber", "Guilherme  Amadio", "Jakob  Blomer", "Alexander  Matthes", "Ren'e  Widera", "Michael  Bussmann"], "year": 2021, "n_citations": 0}
{"id": 395623, "s2_id": "0ea51e4a5dad5efedf63127957775bbbed766b30", "title": "Fork-join and redundancy systems with heavy-tailed job sizes", "abstract": "We investigate the tail asymptotics of the response time distribution for the cancel-on-start (c.o.s.) and cancel-on-completion (c.o.c.) variants of redundancyd scheduling and the fork-join model with heavy-tailed job sizes. We present bounds, which only differ in the pre-factor, for the tail probability of the response time in the case of the first-come first-served (FCFS) discipline. For the c.o.s. variant we restrict ourselves to redundancy-d scheduling, which is a special case of the fork-join model. In particular, for regularly varying job sizes with tail index \u2212\u03bd the tail index of the response time for the c.o.s. variant of redundancyd equals \u2212min{dcap(\u03bd \u2212 1), \u03bd}, where dcap = min{d,N \u2212 k}, N is the number of servers and k is the integer part of the load. This result indicates that for dcap < \u03bd \u03bd\u22121 the waiting time component is dominant, whereas for dcap > \u03bd \u03bd\u22121 the job size component is dominant. Thus, having d = \u2308min{ \u03bd \u03bd\u22121 , N \u2212 k}\u2309 replicas is sufficient to achieve the optimal asymptotic tail behavior of the response time. For the c.o.c. variant of the fork-join(nF, nJ) model the tail index of the response time, under some assumptions on the load, equals 1\u2212 \u03bd and 1\u2212 (nF+1\u2212nJ)\u03bd, for identical and i.i.d. replicas, respectively; here the waiting time component is always dominant.", "venue": "ArXiv", "authors": ["Youri  Raaijmakers", "Sem  Borst", "Onno  Boxma"], "year": 2021, "n_citations": 1}
{"id": 396296, "s2_id": "1a4aac89f4e16354b88c649dfba5412aed5a3e7b", "title": "Learning Algorithms for Minimizing Queue Length Regret", "abstract": "We consider a system consisting of a single transmitter/receiver pair and <italic>N</italic> channels over which they may communicate. Packets randomly arrive to the transmitter\u2019s queue and wait to be successfully sent to the receiver. The transmitter may attempt a frame transmission on one channel at a time, where each frame includes a packet if one is in the queue. For each channel, an attempted transmission is successful with an unknown probability. The transmitter\u2019s objective is to quickly identify the best channel to minimize the number of packets in the queue over <italic>T</italic> time slots. To analyze system performance, we introduce queue length regret, which is the expected difference between the total queue length of a learning policy and a controller that knows the rates, a priori. One approach to designing a transmission policy would be to apply algorithms from the literature that solve the closely-related stochastic multi-armed bandit problem. These policies would focus on maximizing the number of successful frame transmissions over time. However, we show that these methods have <inline-formula> <tex-math notation=\"LaTeX\">$\\Omega (\\log {{T}})$ </tex-math></inline-formula> queue length regret. On the other hand, we show that there exists a set of queue-length based policies that can obtain order optimal <inline-formula> <tex-math notation=\"LaTeX\">${O}(1)$ </tex-math></inline-formula> queue length regret. We use our theoretical analysis to devise heuristic methods that are shown to perform well in simulation.", "venue": "IEEE Transactions on Information Theory", "authors": ["Thomas  Stahlbuhk", "Brooke  Shrader", "Eytan  Modiano"], "year": 2021, "n_citations": 1}
{"id": 396335, "s2_id": "010263f1009cb621b33b3f1f844d7e7476884a44", "title": "PROFIT: A Novel Training Method for sub-4-bit MobileNet Models", "abstract": "4-bit and lower precision mobile models are required due to the ever-increasing demand for better energy efficiency in mobile devices. In this work, we report that the activation instability induced by weight quantization (AIWQ) is the key obstacle to sub-4-bit quantization of mobile networks. To alleviate the AIWQ problem, we propose a novel training method called PROgressive-Freezing Iterative Training (PROFIT), which attempts to freeze layers whose weights are affected by the instability problem stronger than the other layers. We also propose a differentiable and unified quantization method (DuQ) and a negative padding idea to support asymmetric activation functions such as h-swish. We evaluate the proposed methods by quantizing MobileNet-v1, v2, and v3 on ImageNet and report that 4-bit quantization offers comparable (within 1.48 % top-1 accuracy) accuracy to full precision baseline. In the ablation study of the 3-bit quantization of MobileNet-v3, our proposed method outperforms the state-of-the-art method by a large margin, 12.86 % of top-1 accuracy.", "venue": "ECCV", "authors": ["Eunhyeok  Park", "Sungjoo  Yoo"], "year": 2020, "n_citations": 16}
{"id": 400421, "s2_id": "7f3e7fe1efc0b74a4364c11a07668251f4875724", "title": "A survey of sparse matrix-vector multiplication performance on large matrices", "abstract": "We contribute a third-party survey of sparse matrix-vector (SpMV) product performance on industrial-strength, large matrices using: (1) The SpMV implementations in Intel MKL, the Trilinos project (Tpetra subpackage), the CUSPARSE library, and the CUSP library, each running on modern architectures. (2) NVIDIA GPUs and Intel multi-core CPUs (supported by each software package). (3) The CSR, BSR, COO, HYB, and ELL matrix formats (supported by each software package).", "venue": "ArXiv", "authors": ["Max  Grossman", "Christopher  Thiele", "Mauricio  Araya-Polo", "Florian  Frank", "Faruk O. Alpak", "Vivek  Sarkar"], "year": 2016, "n_citations": 21}
{"id": 400522, "s2_id": "ef3535577e2d459a2dcaad5ebb56b11a8022e5d8", "title": "Fundamental delay bounds in peer-to-peer chunk-based real-time streaming systems", "abstract": "This paper addresses the following foundational question: what is the maximum theoretical delay performance achievable by an overlay peer-to-peer streaming system where the streamed content is subdivided into chunks? As shown in this paper, when posed for chunk-based systems, and as a consequence of the store-and-forward way in which chunks are delivered across the network, this question has a fundamentally different answer with respect to the case of systems where the streamed content is distributed through one or more flows (sub-streams). To circumvent the complexity emerging when directly dealing with delay, we express performance in term of a convenient metric, called \u201cstream diffusion metric\u201d. We show that it is directly related to the end-to-end minimum delay achievable in a P2P streaming network. In a homogeneous scenario, we derive a performance bound for such metric, and we show how this bound relates to two fundamental parameters: the upload bandwidth available at each node, and the number of neighbors a node may deliver chunks to. In this bound, k-step Fibonacci sequences do emerge, and appear to set the fundamental laws that characterize the optimal operation of chunk-based systems.", "venue": "2009 21st International Teletraffic Congress", "authors": ["Giuseppe  Bianchi", "Nicola  Blefari-Melazzi", "Lorenzo  Bracciale", "Francesca Lo Piccolo", "Stefano  Salsano"], "year": 2009, "n_citations": 10}
{"id": 401330, "s2_id": "eefbae9b17d2fc69c28d4c3720dad7070972768f", "title": "ReLeaSER: A Reinforcement Learning Strategy for Optimizing Utilization Of Ephemeral Cloud Resources", "abstract": "Cloud data center capacities are over-provisioned to handle demand peaks and hardware failures which leads to low resources' utilization. One way to improve resource utilization and thus reduce the total cost of ownership is to offer unused resources (referred to as ephemeral resources) at a lower price. However, reselling resources needs to meet the expectations of its customers in terms of Quality of Service. The goal is so to maximize the amount of reclaimed resources while avoiding SLA penalties. To achieve that, cloud providers have to estimate their future utilization to provide availability guarantees. The prediction should consider a safety margin for resources to react to unpredictable workloads. The challenge is to find the safety margin that provides the best trade-off between the amount of resources to reclaim and the risk of SLA violations. Most state-of-the-art solutions consider a fixed safety margin for all types of metrics (e.g., CPU, RAM). However, a unique fixed margin does not consider various workloads variations over time which may lead to SLA violations or/and poor utilization. In order to tackle these challenges, we propose ReLeaSER, a Reinforcement Learning strategy for optimizing the ephemeral resources' utilization in the cloud. ReLeaSER dynamically tunes the safety margin at the host-level for each resource metric. The strategy learns from past prediction errors (that caused SLA violations). Our solution reduces significantly the SLA violation penalties on average by $\\mathbf{2.7}\\times$ and up to $\\mathbf{3.4}\\times$. It also improves considerably the CPs' potential savings by 27.6% on average and up to 43.6%.", "venue": "2020 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)", "authors": ["Mohamed  Handaoui", "Jean-Emile  Dartois", "Jalil  Boukhobza", "Olivier  Barais", "Laurent  d'Orazio"], "year": 2020, "n_citations": 0}
{"id": 402853, "s2_id": "c2aeed292e067cc01f0fcbf3fcb90506c3b84e7b", "title": "COCOA: Cold Start Aware Capacity Planning for Function-as-a-Service Platforms", "abstract": "Function-as-a-Service (FaaS) has become increasingly popular in the software industry due to the implied cost-savings in event-driven workloads and its synergy with DevOps. To size an on-premise FaaS platform, it is important to estimate the required CPU and memory capacity to serve the expected loads. Given the service-level agreements, it is however challenging to take the cold start issue into account during the sizing process. We have investigated the similarity of this problem with the hit rate improvement problem in Time to Live (TTL) caches and concluded that solutions for TTL cache, although potentially applicable, lead to over-provisioning in FaaS. Thus, we propose a novel approach, COCOA, to solve this issue. COCOA uses a queueing-based approach to assess the effect of cold starts on FaaS response times. It also considers different memory consumption values depending on whether the function is idle or in execution. Using an event-driven FaaS simulator, FaasSim, that we have developed, we show that COCOA can reduce overprovisioning by over 70% under some of the workloads we have considered, while satisfying the service-level agreements.", "venue": "2020 28th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)", "authors": ["Alim Ul Gias", "Giuliano  Casale"], "year": 2020, "n_citations": 6}
{"id": 404555, "s2_id": "ab0afae695025583602944d129bbd71b25e6994a", "title": "Time-Based Roofline for Deep Learning Performance Analysis", "abstract": "Deep learning applications based on neural networks are generating considerable interest in various fields due to their high accuracy. Such an application is usually very compute-intensive thus requires a long run time. Researchers and engineers are actively exploring new solutions to this issue from both hardware and software/algorithm sides. However, little previous work has focused on providing a practical methodology to characterize deep learning performance bottlenecks and potentially guide the following optimization efforts. In this paper, we introduce an extension of the Roofline model and use it to analyze two representative computation kernels in deep learning, 2D convolution and long short-term memory, on NVIDIA GPUs. This new time-based Roofline model incorporates both compute/bandwidth complexity and run time in its formulae to demonstrate performance issues that cannot be reflected by the classic Roofline. Factors such as arithmetic intensity, data transfer, kernel launch overhead, and the Tensor Core usage will be examined by varying different parameters such as batch size and feature size, etc. This work helped form a more systematic way to understand the performance issue of deep learning applications. Last but not least, this generic performance model can be applied to a wide category of applications besides deep learning as well.", "venue": "2020 IEEE/ACM Fourth Workshop on Deep Learning on Supercomputers (DLS)", "authors": ["Yunsong  Wang", "Charlene  Yang", "Steven  Farrell", "Yan  Zhang", "Thorsten  Kurth", "Samuel  Williams"], "year": 2020, "n_citations": 3}
{"id": 404694, "s2_id": "79db23f865415dc8a7521e1225160f9be038cf9f", "title": "Effect of Bitcoin fee on transaction-confirmation process", "abstract": "In Bitcoin system, transactions are prioritized according to transaction fees. Transactions without fees are given low priority and likely to wait for confirmation. Because the demand of micro payment in Bitcoin is expected to increase due to low remittance cost, it is important to quantitatively investigate how transactions with small fees of Bitcoin affect the transaction-confirmation time. In this paper, we analyze the transaction-confirmation time by queueing theory. We model the transaction-confirmation process of Bitcoin as a priority queueing system with batch service, deriving the mean transaction-confirmation time. Numerical examples show how the demand of transactions with low fees affects the transaction-confirmation time. We also consider the effect of the maximum block size on the transaction-confirmation time.", "venue": "Journal of Industrial & Management Optimization", "authors": ["Shoji  Kasahara", "Jun  Kawahara"], "year": 2019, "n_citations": 53}
{"id": 407683, "s2_id": "8735e166b9452d53f447856fc2e9a2b69d9c2856", "title": "A Delay Analysis of Maximal Matching Switching with Speedup", "abstract": "In this paper we analyze the average queue backlog in a combined input-output queued switch using a maximal size matching scheduling algorithm. We compare this average backlog to the average backlog achieved by an optimal switch. We model the cell arrival process as independent and identically distributed between time slots and uniformly distributed among input and output ports. For switches with many input and output ports, the backlog associated with maximal size matching with speedup 3 is no more than 10/3 times the backlog associated with an optimal switch. Moreover, this performance ratio rapidly approaches 2 as speedup increases.", "venue": "ArXiv", "authors": ["Randy  Cogill", "Sanjay  Lall"], "year": 2006, "n_citations": 0}
{"id": 409965, "s2_id": "8a4598344feb1cfd0e340e57ff373bb2f9760da5", "title": "Daleel: Simplifying cloud instance selection using machine learning", "abstract": "Decision making in cloud environments is quite challenging due to the diversity in service offerings and pricing models, especially considering that the cloud market is an incredibly fast moving one. In addition, there are no hard and fast rules; each customer has a specific set of constraints (e.g. budget) and application requirements (e.g. minimum computational resources). Machine learning can help address some of the complicated decisions by carrying out customer-specific analytics to determine the most suitable instance type(s) and the most opportune time for starting or migrating instances. We employ machine learning techniques to develop an adaptive deployment policy, providing an optimal match between the customer demands and the available cloud service offerings. We provide an experimental study based on extensive set of job executions over a major public cloud infrastructure.", "venue": "NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium", "authors": ["Faiza  Samreen", "Yehia  El-khatib", "Matthew  Rowe", "Gordon S. Blair"], "year": 2016, "n_citations": 45}
{"id": 410389, "s2_id": "39a7e3330bf510380464aa0a1432e456fd2f6aa8", "title": "Modified Erlang loss system for cognitive wireless networks", "abstract": "This paper considers a modified Erlang loss system for cognitive wireless networks and related applications. A primary user has preemptive priority over secondary users and the primary customer is lost if upon arrival all the channels are used by other primary users. Secondary users cognitively use idle channels and they can wait at an infinite buffer in cases idle channels are not available upon arrival or they are interrupted by primary users. We obtain explicit stability condition for the cases where arrival processes of primary users and secondary users follow Poisson processes and their service times follow two distinct arbitrary distributions. The stability condition is insensitive to the service time distributions and implies the maximal throughout of secondary users. For a special case of exponential service time distributions, we analyze in depth to show the effect of parameters on the delay performance and the mean number of interruptions of secondary users. Our simulations for distributions rather than exponential reveal that the mean number of terminations for secondary users is less sensitive to the service time distribution of primary users.", "venue": "ArXiv", "authors": ["Evsey V. Morozov", "S. S. Rogozin", "H. Q. Nguyen", "Tuan  Phung-Duc"], "year": 2021, "n_citations": 5}
{"id": 415808, "s2_id": "fcb74a27ae8bf54ecce9aed765fa009103309254", "title": "Data motifs: a lens towards fully understanding big data and AI workloads", "abstract": "The complexity and diversity of big data and AI workloads make understanding them difficult and challenging. This paper proposes a new approachto modelling and characterizing big data and AI workloads. We consider each big data and AI workload as a pipeline of one or more classes of units of computation performed on different initial or intermediate data inputs. Each class of unit of computation captures the common requirements while being reasonably divorced from individual implementations, and hence we call it a data motif. For the first time, among a wide variety of big data and AI workloads, we identify eight data motifs that take up most of the run time of those workloads, including Matrix, Sampling, Logic, Transform, Set, Graph, Sort and Statistic. We implement the eight data motifs on different software stacks as the micro benchmarks of an open-source big data and AI benchmark suite --- BigDataBench 4.0 (publicly available from http://prof.ict.ac.cn/BigDataBench), and perform comprehensive characterization of those data motifs from perspective of data sizes, types, sources, and patterns as a lens towards fully understanding big data and AI workloads. We believe the eight data motifs are promising abstractions and tools for not only big data and AI benchmarking, but also domain-specific hardware and software co-design.", "venue": "PACT", "authors": ["Wanling  Gao", "Lei  Wang", "Jianfeng  Zhan", "Chunjie  Luo", "Daoyi  Zheng", "Zhen  Jia", "Biwei  Xie", "Chen  Zheng", "Qiang  Yang", "Haibin  Wang"], "year": 2018, "n_citations": 27}
{"id": 416022, "s2_id": "aa8db60eaf3849dd21b9a58aa60f30ee68c93f2f", "title": "COCO: The Large Scale Black-Box Optimization Benchmarking (bbob-largescale) Test Suite", "abstract": "The bbob-largescale test suite, containing 24 single-objective functions in continuous domain, extends the well-known single-objective noiseless bbob test suite, which has been used since 2009 in the BBOB workshop series, to large dimension. The core idea is to make the rotational transformations R, Q in search space that appear in the bbob test suite computationally cheaper while retaining some desired properties. This documentation presents an approach that replaces a full rotational transformation with a combination of a block-diagonal matrix and two permutation matrices in order to construct test functions whose computational and memory costs scale linearly in the dimension of the problem.", "venue": "ArXiv", "authors": ["Ouassim Ait ElHara", "Konstantinos  Varelas", "Duc Manh Nguyen", "Tea  Tusar", "Dimo  Brockhoff", "Nikolaus  Hansen", "Anne  Auger"], "year": 2019, "n_citations": 9}
{"id": 425995, "s2_id": "ed86adc77a70bc65465e2d370a4df70d85f0afc2", "title": "Benchmarking a New Paradigm: An Experimental Analysis of a Real Processing-in-Memory Architecture", "abstract": "Many modern workloads, such as neural networks, databases, and graph processing, are fundamentally memory-bound. For such workloads, the data movement between main memory and CPU cores imposes a significant overhead in terms of both latency and energy. Amajor reason is that this communication happens through a narrow bus with high latency and limited bandwidth, and the low data reuse in memory-bound workloads is insufficient to amortize the cost of main memory access. Fundamentally addressing this data movement bottleneck requires a paradigm where the memory system assumes an active role in computing by integrating processing capabilities. This paradigm is known as processing-in-memory (PIM). Recent research explores different forms of PIM architectures, motivated by the emergence of new 3D-stacked memory technologies that integrate memory with a logic layer where processing elements can be easily placed. Past works evaluate these architectures in simulation or, at best, with simplified hardware prototypes. In contrast, the UPMEM company has designed and manufactured the first publicly-available real-world PIM architecture. The UPMEM PIM architecture combines traditional DRAMmemory arrays with general-purpose in-order cores, called DRAM Processing Units (DPUs), integrated in the same chip. This paper provides the first comprehensive analysis of the first publicly-available real-world PIM architecture. We make two key contributions. First, we conduct an experimental characterization of the UPMEM-based PIM system using microbenchmarks to assess various architecture limits such as compute throughput and memory bandwidth, yielding new insights. Second, we present PrIM (Processing-In-Memory benchmarks), a benchmark suite of 16 workloads from different application domains (e.g., dense/sparse linear algebra, databases, data analytics, graph processing, neural networks, bioinformatics, image processing), which we identify as memory-bound. We evaluate the performance and scaling characteristics of PrIM benchmarks on the UPMEM PIM architecture, and compare their performance and energy consumption to their state-of-the-art CPU and GPU counterparts. Our extensive evaluation conducted on two real UPMEM-based PIM systems with 640 and 2,556 DPUs provides new insights about suitability of different workloads to the PIM system, programming recommendations for software designers, and suggestions and hints for hardware and architecture designers of future PIM systems.", "venue": "ArXiv", "authors": ["Juan  G'omez-Luna", "Izzat El Hajj", "Ivan  Fernandez", "Christina  Giannoula", "Geraldo F. Oliveira", "Onur  Mutlu"], "year": 2021, "n_citations": 8}
{"id": 429569, "s2_id": "6ff18e3334f97ea16d1fb7d7df182b3cd04c1d21", "title": "ReaDmE: Read-Rate Based Dynamic Execution Scheduling for Intermittent RF-Powered Devices", "abstract": "This paper presents a method for remotely and dynamically determining the execution schedule of long-running tasks on intermittently powered devices such as computational RFID. Our objective is to prevent brown-out events caused by sudden power-loss due to the intermittent nature of the powering channel. We formulate, validate and demonstrate that the read-rate measured from an RFID reader (number of successful interrogations per second) can provide an adequate means of estimating the powering channel condition for passively powered CRFID devices. This method is attractive because it can be implemented without imposing an added burden on the device or requiring additional hardware. We further propose ReaDmE, a dynamic execution scheduling scheme to mitigate brownout events to support long-run execution of complex tasks, such as cryptographic algorithms, on CRFID. Experimental results demonstrate that the ReaDmE method can improve CRFID\u2019s long-run execution success rate by 20% at the critical operational range or reduce time overhead by up to 23% compared to previous execution scheduling methods.", "venue": "2021 IEEE International Conference on RFID (RFID)", "authors": ["Yang  Su", "Damith C. Ranasinghe"], "year": 2021, "n_citations": 0}
{"id": 431287, "s2_id": "ec40c306a8d220bc566186f0b7f329e41f9e7f60", "title": "Analysis of Stochastic Service Guarantees in Communication Networks: A Basic Calculus", "abstract": "A basic calculus is presented for stochastic service guarantee analysis in communication networks. Central to the calculus are two definitions, maximum-(virtual)-backlog-centric (m.b.c) stochastic arrival curve and stochastic service curve, which respectively generalize arrival curve and service curve in the deterministic network calculus framework. With m.b.c stochastic arrival curve and stochastic service curve, various basic results are derived under the (min, +) algebra for the general case analysis, which are crucial to the development of stochastic network calculus. These results include (i) superposition of flows, (ii) concatenation of servers, (iii) output characterization, (iv) per-flow service under aggregation, and (v) stochastic backlog and delay guarantees. In addition, to perform independent case analysis, stochastic strict server is defined, which uses an ideal service process and an impairment process to characterize a server. The concept of stochastic strict server not only allows us to improve the basic results (i) -- (v) under the independent case, but also provides a convenient way to find the stochastic service curve of a serve. Moreover, an approach is introduced to find the m.b.c stochastic arrival curve of a flow and the stochastic service curve of a server.", "venue": "ArXiv", "authors": ["Yuming  Jiang"], "year": 2005, "n_citations": 10}
{"id": 435906, "s2_id": "aa0c0aa667de832921df88ef91b9407020590d52", "title": "Efficient Instrumentation for Performance Profiling", "abstract": "Performance profiling consists of tracing a software system during execution and then analyzing the obtained traces. However, traces themselves affect the performance of the system distorting its execution. Therefore, there is a need to minimize the effect of the tracing on the underlying system's performance. To achieve this, the trace set needs to be optimized according to the performance profiling problem being solved. Our position is that such minimization can be achieved only by adding the software trace design and implementation to the overall software development process. In such a process, the performance analyst supplies the knowledge of performance measurement requirements, while the software developer supplies the knowledge of the software. Both of these are needed for an optimal trace placement.", "venue": "ArXiv", "authors": ["Edu  Metz", "Raimondas  Lencevicius"], "year": 2003, "n_citations": 14}
{"id": 436561, "s2_id": "7685c791e42d2c16cdbad92b540d52681a6259e5", "title": "A moment-matching metric for latent variable generative models", "abstract": "It is di cult to assess the quality of a fitted model when facing unsupervised learning problems. Latent variable models, such as variational autoencoders and Gaussian mixture models, are often trained with likelihood-based approaches. In the scope of Goodhart\u2019s law, when a metric becomes a target it ceases to be a good metric and therefore we should not use likelihood to assess the quality of the fit of these models. The solution we propose is a new metric for model comparison or regularization that relies on moments. The key idea to study the di erence between the data moments and the model moments using a matrix norm, such as the Frobenius norm. We show how to use this new metric for model comparison and then for regularization. We show that our proposed metric is faster to compute and has a smaller variance than the commonly used procedure of drawing samples from the fitted distribution. We conclude this article with a proof of concept for both applications and we discuss future work.", "venue": "ArXiv", "authors": ["C'edric  Beaulac"], "year": 2021, "n_citations": 0}
{"id": 439042, "s2_id": "be210ef642a49f09ccc89ef91657c06b5e1d383e", "title": "Efficient Spherical Harmonic Transforms aimed at pseudo-spectral numerical simulations", "abstract": "In this paper, we report on very efficient algorithms for the spherical harmonic transform (SHT). Explicitly vectorized variations of the algorithm based on the Gauss-Legendre quadrature are discussed and implemented in the SHTns library which includes scalar and vector transforms. The main breakthrough is to achieve very efficient on-the-fly computations of the Legendre associated functions, even for very high resolutions, by taking advantage of the specific properties of the SHT and the advanced capabilities of current and future computers. This allows us to simultaneously and significantly reduce memory usage and computation time of the SHT. We measure the performance and accuracy of our algorithms. Even though the complexity of the algorithms implemented in SHTns are in $O(N^3)$ (where N is the maximum harmonic degree of the transform), they perform much better than any third party implementation, including lower complexity algorithms, even for truncations as high as N=1023. SHTns is available at this https URL as open source software.", "venue": "ArXiv", "authors": ["Nathana\u00ebl  Schaeffer"], "year": 2012, "n_citations": 157}
{"id": 440981, "s2_id": "9300a0a09a2c1ab0e1be9a0564d08cf1393139bb", "title": "Performance study of LTE and mmWave in vehicle-to-network communications", "abstract": "A key enabler for the emerging autonomous and cooperative driving services is high-throughput and reliable Vehicle-to-Network (V2N) communication. In this respect, the millimeter wave (mmWave) frequencies hold great promises because of the large available bandwidth which may provide the required link capacity. However, this potential is hindered by the challenging propagation characteristics of high-frequency channels and the dynamic topology of the vehicular scenarios, which affect the reliability of the connection. Moreover, mmWave transmissions typically leverage beamforming gain to compensate for the increased path loss experienced at high frequencies. This, however, requires fine alignment of the transmitting and receiving beams, which may be difficult in vehicular scenarios. Those limitations may undermine the performance of V2N communications and pose new challenges for proper vehicular communication design. In this paper, we study by simulation the practical feasibility of some mmWave-aware strategies to support V2N, in comparison to the traditional LTE connectivity below 6 GHz. The results show that the orchestration among different radios represents a viable solution to enable both high-capacity and robust V2N communications.", "venue": "2018 17th Annual Mediterranean Ad Hoc Networking Workshop (Med-Hoc-Net)", "authors": ["Marco  Giordani", "Andrea  Zanella", "Takamasa  Higuchi", "Onur  Altintas", "Michele  Zorzi"], "year": 2018, "n_citations": 12}
{"id": 442200, "s2_id": "5f48d5a5598223355792713fece4786058cf47a0", "title": "The Effect of TCP Variants on the Coexistence of MMORPG and Best-Effort Traffic", "abstract": "Massive Multiplayer Online Role Playing Games (MMORPGs) have been reported as real-time applications using TCP. This surprising fact makes it interesting to study their behavior when sharing a link with other TCP flows. In this paper the coexistence of an MMORPG flow and an FTP background application is studied. A network scenario which fits an access network is simulated with NS2, using synthetic traffic of a game, generated according to a statistical model. The FTP background application uses different TCP variants. The results show that TCP Vegas is able to maintain a constant rate while competing with the traffic of the game, since it prevents packet loss and high queuing delays by avoiding the increase of the sending window size. In contrast, TCP SACK and TCP New Reno tend to indefinitely increase their window size, thus causing packet loss and adding undesired delays to the game traffic. Finally, small buffers have been proved to be more convenient when dealing with real-time applications, since big ones add higher delays.", "venue": "2012 21st International Conference on Computer Communications and Networks (ICCCN)", "authors": ["Jose  Saldana", "Mirko  Suznjevic", "Luis  Sequeira", "Juli\u00e1n  Fern\u00e1ndez-Navajas", "Maja  Matijasevic", "Jos\u00e9  Ru\u00edz"], "year": 2012, "n_citations": 4}
{"id": 442243, "s2_id": "a2b1eee0acb4df450657c586cbd0467bcd161767", "title": "Parallel Data Distribution Management on Shared-memory Multiprocessors", "abstract": "The problem of identifying intersections between two sets of d-dimensional axis-parallel rectangles appears frequently in the context of agent-based simulation studies. For this reason, the High Level Architecture (HLA) specification\u2014a standard framework for interoperability among simulators\u2014includes a Data Distribution Management (DDM) service whose responsibility is to report all intersections between a set of subscription and update regions. The algorithms at the core of the DDM service are CPU-intensive, and could greatly benefit from the large computing power of modern multi-core processors. In this article, we propose two parallel solutions to the DDM problem that can operate effectively on shared-memory multiprocessors. The first solution is based on a data structure (the interval tree) that allows concurrent computation of intersections between subscription and update regions. The second solution is based on a novel parallel extension of the Sort Based Matching algorithm, whose sequential version is considered among the most efficient solutions to the DDM problem. Extensive experimental evaluation of the proposed algorithms confirm their effectiveness on taking advantage of multiple execution units in a shared-memory architecture.", "venue": "ACM Trans. Model. Comput. Simul.", "authors": ["Moreno  Marzolla", "Gabriele  D'Angelo"], "year": 2020, "n_citations": 2}
{"id": 442679, "s2_id": "9ae17f8fb98ac18c86c48b4d7774b658b066527c", "title": "A static analyzer for large safety-critical software", "abstract": "We show that abstract interpretation-based static program analysis can be made efficient and precise enough to formally verify a class of properties for a family of large programs with few or no false alarms. This is achieved by refinement of a general purpose static analyzer and later adaptation to particular programs of the family by the end-user through parametrization. This is applied to the proof of soundness of data manipulation operations at the machine level for periodic synchronous safety critical embedded software.The main novelties are the design principle of static analyzers by refinement and adaptation through parametrization (Sect. 3 and 7), the symbolic manipulation of expressions to improve the precision of abstract transfer functions (Sect. 6.3), the octagon (Sect. 6.2.2), ellipsoid (Sect. 6.2.3), and decision tree (Sect. 6.2.4) abstract domains, all with sound handling of rounding errors in oating point computations, widening strategies (with thresholds: Sect. 7.1.2, delayed: Sect. 7.1.3) and the automatic determination of the parameters (parametrized packing: Sect. 7.2).", "venue": "PLDI '03", "authors": ["Bruno  Blanchet", "Patrick  Cousot", "Radhia  Cousot", "J\u00e9r\u00f4me  Feret", "Laurent  Mauborgne", "Antoine  Min\u00e9", "David  Monniaux", "Xavier  Rival"], "year": 2003, "n_citations": 705}
{"id": 445928, "s2_id": "0fd55165cb46e8c26b294dfd9b06314481c8506d", "title": "Comparison Between IPv4 to IPv6 Transition Techniques", "abstract": "The IPv4 addresses exhaustion demands a protocol transition from IPv4 to IPv6. The original transition technique, the dual stack, is not widely deployed yet and it demanded the creation of new transition techniques to extend the transition period. This work makes an experimental comparison of techniques that use dual stack with a limited IPv4 address. This limited address might be a RFC 1918 address with a NAT at the Internet Service Provider (ISP) gateway, also known as Carrier Grade NAT (CGN), or an Address Plus Port (A+P) shared IPv4 address. The chosen techniques also consider an IPv6 only ISP network. The transport of the IPv4 packets through the IPv6 only networks may use IPv4 packets encapsulated on IPv6 packets or a double translation, by making one IPv4 to IPv6 translation to enter the IPv6 only network and one IPv6 to IPv4 translation to return to the IPv4 network. The chosen techniques were DS-Lite, 464XLAT, MAP-E and MAP-T. The first part of the test is to check some of the most common usages of the Internet by a home user and the impacts of the transition techniques on the user experience. The second part is a measured comparison considering bandwidth, jitter and latency introduced by the techniques and processor usage on the network equipment.", "venue": "ArXiv", "authors": ["Edwin  Cordeiro", "Rodrigo  Carnier", "Wagner L. Zucchi"], "year": 2016, "n_citations": 1}
{"id": 447233, "s2_id": "72f629bfad2e49eb8cb75e0b746a5afcd703f2ab", "title": "BenchIP: Benchmarking Intelligence Processors", "abstract": "The increasing attention on deep learning has tremendously spurred the design of intelligence processing hardware. The variety of emerging intelligence processors requires standard benchmarks for fair comparison and system optimization (in both software and hardware). However, existing benchmarks are unsuitable for benchmarking intelligence processors due to their non-diversity and nonrepresentativeness. Also, the lack of a standard benchmarking methodology further exacerbates this problem. In this paper, we propose BenchIP, a benchmark suite and benchmarking methodology for intelligence processors. The benchmark suite in BenchIP consists of two sets of benchmarks: microbenchmarks and macrobenchmarks. The microbenchmarks consist of single-layer networks. They are mainly designed for bottleneck analysis and system optimization. The macrobenchmarks contain state-of-the-art industrial networks, so as to offer a realistic comparison of different platforms. We also propose a standard benchmarking methodology built upon an industrial software stack and evaluation metrics that comprehensively reflect various characteristics of the evaluated intelligence processors. BenchIP is utilized for evaluating various hardware platforms, including CPUs, GPUs, and accelerators. BenchIP will be open-sourced soon.", "venue": "Journal of Computer Science and Technology", "authors": ["Jin-Hua  Tao", "Zi-Dong  Du", "Qi  Guo", "Hui-Ying  Lan", "Lei  Zhang", "Sheng-Yuan  Zhou", "Ling-Jie  Xu", "Cong  Liu", "Hai-Feng  Liu", "Shan  Tang", "Allen  Rush", "Willian  Chen", "Shao-Li  Liu", "Yun-Ji  Chen", "Tian-Shi  Chen"], "year": 2018, "n_citations": 25}
{"id": 447892, "s2_id": "dc2069fd3fdcb36e0017246159b3bba09971687f", "title": "Deep learning at 15PF: supervised and semi-supervised classification for scientific data", "abstract": "This paper presents the first, 15-PetaFLOP Deep Learning system for solving scientific pattern classification problems on contemporary HPC architectures. We develop supervised convolutional architectures for discriminating signals in high-energy physics data as well as semi-supervised architectures for localizing and classifying extreme weather in climate data. Our Intelcaffe-based implementation obtains ~2TFLOP/s on a single Cori Phase-II Xeon-Phi node. We use a hybrid strategy employing synchronous node-groups, while using asynchronous communication across groups. We use this strategy to scale training of a single model to ~9600 Xeon-Phi nodes; obtaining peak performance of 11.73-15.07 PFLOP/s and sustained performance of 11.41-13.27 PFLOP/s. At scale, our HEP architecture produces state-of-the-art classification accuracy on a dataset with 10M images, exceeding that achieved by selections on high-level physics-motivated features. Our semi-supervised architecture successfully extracts weather patterns in a 15TB climate dataset. Our results demonstrate that Deep Learning can be optimized and scaled effectively on many-core, HPC systems.", "venue": "SC", "authors": ["Thorsten  Kurth", "Jian  Zhang", "Nadathur  Satish", "Ioannis  Mitliagkas", "Evan  Racah", "Mostofa Ali Patwary", "Tareq  Malas", "Narayanan  Sundaram", "Wahid  Bhimji", "Mikhail  Smorkalov", "Jack  Deslippe", "Mikhail  Shiryaev", "Srinivas  Sridharan", "Prabhat", "Pradeep  Dubey"], "year": 2017, "n_citations": 71}
{"id": 450745, "s2_id": "c6431225e2eae890f29c13158cc6ba2bf1cc96e8", "title": "Chimbuko: A Workflow-Level Scalable Performance Trace Analysis Tool", "abstract": "Due to the sheer volume of data it is typically impractical to analyze the detailed performance of an HPC application running at-scale. While conventional small-scale benchmarking and scaling studies are often sufficient for simple applications, many modern workflow-based applications couple multiple elements with competing resource demands and complex inter-communication patterns for which performance cannot easily be studied in isolation and at small scale. This work discusses Chimbuko, a performance analysis framework that provides real-time, in situ anomaly detection. By focusing specifically on performance anomalies and their origin (aka provenance), data volumes are dramatically reduced without losing necessary details. To the best of our knowledge, Chimbuko is the first online, distributed, and scalable workflow-level performance trace analysis framework. We demonstrate the tool\u2019s usefulness on Oak Ridge National Laboratory\u2019s Summit system.", "venue": "ISAV@SC", "authors": ["Sungsoo  Ha", "Wonyong  Jeong", "Gyorgy  Matyasfalvi", "Cong  Xie", "Kevin  Huck", "Jong Youl Choi", "Abid  Malik", "Li  Tang", "Hubertus Van Dam", "Line  Pouchard", "Wei  Xu", "Shinjae  Yoo", "Nicholas  D'Imperio", "Kerstin Kleese Van Dam"], "year": 2020, "n_citations": 1}
{"id": 456745, "s2_id": "80338991128f599da08f8b27224bd2cd4b92daed", "title": "Communication-Efficient Jaccard similarity for High-Performance Distributed Genome Comparisons", "abstract": "The Jaccard similarity index is an important measure of the overlap of two sets, widely used in machine learning, computational genomics, information retrieval, and many other areas. We design and implement SimilarityAtScale, the first communication-efficient distributed algorithm for computing the Jaccard similarity among pairs of large datasets. Our algorithm provides an efficient encoding of this problem into a multiplication of sparse matrices. Both the encoding and sparse matrix product are performed in a way that minimizes data movement in terms of communication and synchronization costs. We apply our algorithm to obtain similarity among all pairs of a set of large samples of genomes. This task is a key part of modern metagenomics analysis and an evergrowing need due to the increasing availability of high-throughput DNA sequencing data. The resulting scheme is the first to enable accurate Jaccard distance derivations for massive datasets, using large-scale distributed-memory systems. We package our routines in a tool, called GenomeAtScale, that combines the proposed algorithm with tools for processing input sequences. Our evaluation on real data illustrates that one can use GenomeAtScale to effectively employ tens of thousands of processors to reach new frontiers in large-scale genomic and metagenomic analysis. While GenomeAtScale can be used to foster DNA research, the more general underlying SimilarityAtScale algorithm may be used for high-performance distributed similarity computations in other data analytics application domains.", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Maciej  Besta", "Raghavendra  Kanakagiri", "Harun  Mustafa", "Mikhail  Karasikov", "Gunnar  R\u00e4tsch", "Torsten  Hoefler", "Edgar  Solomonik"], "year": 2020, "n_citations": 23}
{"id": 459714, "s2_id": "f403b5ecbe463baddd88f03801c34ba6fe77674b", "title": "Optimising the Performance of Convolutional Neural Networks across Computing Systems using Transfer Learning", "abstract": "The choice of convolutional routines (primitives) to implement neural networks has a tremendous impact on their inference performance (execution speed) on a given hardware platform. To optimise a neural network by primitive selection, the optimal primitive is identified for each layer of the network. This process requires a lengthy profiling stage, iterating over all the available primitives for each layer configuration, to measure their execution time on the target platform. Because each primitive exploits the hardware in different ways, new profiling is needed to obtain the best performance when moving to another platform. In this work, we propose to replace this prohibitively expensive profiling stage with a machine learning based approach of performance modeling. Our approach speeds up the optimisation time drastically. After training, our performance model can estimate the performance of convolutional primitives in any layer configuration. The time to optimise the execution of large neural networks via primitive selection is reduced from hours to just seconds. Our performance model is easily transferable to other target platforms. We demonstrate this by training a performance model on an Intel platform and performing transfer learning to AMD and ARM processor devices with minimal profiled samples.", "venue": "ArXiv", "authors": ["Rik  Mulder", "Valentin  Radu", "Christophe  Dubach"], "year": 2020, "n_citations": 1}
{"id": 461154, "s2_id": "16b450c679199a46a6ee94109b4a9c0a79f7868f", "title": "Performance analysis of the Globus Toolkit Monitoring and Discovery Service, MDS2", "abstract": "Monitoring and information services form a key component of a distributed system, or grid. A quantitative study of such services can aid in understanding the performance limitations, advise in the deployment of the monitoring system, and help evaluate future development work. To this end, we examined the performance of the Globus Toolkit/spl reg/ Monitoring and Discovery Service (MDS2) by instrumenting its main services using NetLogger. Our study shows a strong advantage to caching or prefetching the data, as well as the need to have primary components at well-connected sites.", "venue": "IEEE International Conference on Performance, Computing, and Communications, 2004", "authors": ["Xuehai  Zhang", "Jennifer M. Schopf"], "year": 2004, "n_citations": 93}
{"id": 461766, "s2_id": "084ee4e2cd5ecf25a63474ff1674621b47bb221a", "title": "A refined and asymptotic analysis of optimal stopping problems of Bruss and Weber", "abstract": "The classical secretary problem has been generalized over the years into several directions. In this paper we confine our interest to those generalizations which have to do with the more general problem of stopping on a last observation of a specific kind. We follow Dendievel, (where a bibliography can be found) who studies several types of such problems, mainly initiated by Bruss and Weber. Whether in discrete time or continuous time, whether all parameters are known or must be sequentially estimated, we shall call such problems simply \"Bruss-Weber problems\". Our contribution in the present paper is a refined analysis of several problems in this class and a study of the asymptotic behaviour of solutions. \nThe problems we consider center around the following model. Let $X_1,X_2,\\ldots,X_n$ be a sequence of independent random variables which can take three values: $\\{+1,-1,0\\}.$ Let $p:=\u00b6(X_i=1), p':=\u00b6(X_i=-1), \\qt:=\u00b6(X_i=0), p\\geq p'$, where $p+p'+\\qt=1$. The goal is to maximize the probability of stopping on a value $+1$ or $-1$ appearing for the last time in the sequence. Following a suggestion by Bruss, we have also analyzed an x-strategy with incomplete information: the cases $p$ known, $n$ unknown, then $n$ known, $p$ unknown and finally $n,p$ unknown are considered. We also present simulations of the corresponding complete selection algorithm.", "venue": "ArXiv", "authors": ["Guy  Louchard"], "year": 2017, "n_citations": 2}
{"id": 463515, "s2_id": "781e0e81834119c135091c8bdfcd1966c10b09ab", "title": "SIMD compression and the intersection of sorted integers", "abstract": "Sorted lists of integers are commonly used in inverted indexes and database systems. They are often compressed in memory. We can use the single\u2010instruction, multiple data (SIMD) instructions available in common processors to boost the speed of integer compression schemes. Our S4\u2010BP128\u2010D4 scheme uses as little as 0.7\u2009CPU cycles per decoded 32\u2010bit integer while still providing state\u2010of\u2010the\u2010art compression. However, if the subsequent processing of the integers is slow, the effort spent on optimizing decompression speed can be wasted. To show that it does not have to be so, we (1) vectorize and optimize the intersection of posting lists; (2) introduce the SIMD GALLOPING algorithm. We exploit the fact that one SIMD instruction can compare four pairs of 32\u2010bit integers at once. We experiment with two Text REtrieval Conference (TREC) text collections, GOV2 and ClueWeb09 (category B), using logs from the TREC million\u2010query track. We show that using only the SIMD instructions ubiquitous in all modern CPUs, our techniques for conjunctive queries can double the speed of a state\u2010of\u2010the\u2010art approach. Copyright \u00a9 2015 John Wiley & Sons, Ltd.", "venue": "Softw. Pract. Exp.", "authors": ["Daniel  Lemire", "Leonid  Boytsov", "Nathan  Kurz"], "year": 2016, "n_citations": 73}
{"id": 470125, "s2_id": "f89d406191c8755e49c8b5fced65a5ee15b4e5ce", "title": "Blaze: Simplified High Performance Cluster Computing", "abstract": "MapReduce and its variants have significantly simplified and accelerated the process of developing parallel programs. However, most MapReduce implementations focus on data-intensive tasks while many real-world tasks are compute intensive and their data can fit distributedly into the memory. For these tasks, the speed of MapReduce programs can be much slower than those hand-optimized ones. We present Blaze, a C++ library that makes it easy to develop high performance parallel programs for such compute intensive tasks. At the core of Blaze is a highly-optimized in-memory MapReduce function, which has three main improvements over conventional MapReduce implementations: eager reduction, fast serialization, and special treatment for a small fixed key range. We also offer additional conveniences that make developing parallel programs similar to developing serial programs. These improvements make Blaze an easy-to-use cluster computing library that approaches the speed of hand-optimized parallel code. We apply Blaze to some common data mining tasks, including word frequency count, PageRank, k-means, expectation maximization (Gaussian mixture model), and k-nearest neighbors. Blaze outperforms Apache Spark by more than 10 times on average for these tasks, and the speed of Blaze scales almost linearly with the number of nodes. In addition, Blaze uses only the MapReduce function and 3 utility functions in its implementation while Spark uses almost 30 different parallel primitives in its official implementation.", "venue": "ArXiv", "authors": ["Junhao  Li", "Hang  Zhang"], "year": 2019, "n_citations": 1}
{"id": 470583, "s2_id": "076d21d54a9122357b541dada58a4e7c54662663", "title": "Elements for response-time statistics in ERP transaction systems", "abstract": "The aim of this work is to provide some insight into the response-time statistics of enterprise resource planning systems. We propose a simple mean-field model for the response-time distribution in such systems. This model yields a log-normal distribution of response-times. We present data from performance measurements to support the result. The data show that the response-time distribution of a given transaction in a given system is generically a log-normal distribution or, in some situations, a sum of two or more log-normal distributions. Deviations of the log-normal form can often be traced back to performance problems in the system. Consequences for the interpretation of response-time data and for service level agreements are discussed.", "venue": "Perform. Evaluation", "authors": ["Andreas  Mielke"], "year": 2006, "n_citations": 13}
{"id": 471079, "s2_id": "98cb23a34a59e730f10c55c64802f5798d0c68d6", "title": "On the Scalability of Data Reduction Techniques in Current and Upcoming HPC Systems from an Application Perspective", "abstract": "We implement and benchmark parallel I/O methods for the fully-manycore driven particle-in-cell code PIConGPU. Identifying throughput and overall I/O size as a major challenge for applications on today's and future HPC systems, we present a scaling law characterizing performance bottlenecks in state-of-the-art approaches for data reduction. Consequently, we propose, implement and verify multi-threaded data-transformations for the I/O library ADIOS as a feasible way to trade underutilized host-side compute potential on heterogeneous systems for reduced I/O latency.", "venue": "ISC Workshops", "authors": ["Axel  Huebl", "Ren\u00e9  Widera", "Felix  Schmitt", "Alexander  Matthes", "Norbert  Podhorszki", "Jong Youl Choi", "Scott  Klasky", "Michael  Bussmann"], "year": 2017, "n_citations": 6}
{"id": 472237, "s2_id": "2bd65524726adb74f28b8006c1d9c840bc76cea6", "title": "High throughput virtual screening with data level parallelism in multi-core processors", "abstract": "Improving the throughput of molecular docking, a computationally intensive phase of the virtual screening process, is a highly sought area of research since it has a significant weight in the drug designing process. With such improvements, the world might find cures for incurable diseases like HIV disease and Cancer sooner. Our approach presented in this paper is to utilize a multi-core environment to introduce Data Level Parallelism (DLP) to the Autodock Vina software, which is a widely used for molecular docking software. Autodock Vina already exploits Instruction Level Parallelism (ILP) in multi-core environments and therefore optimized for such environments. However, with the results we have obtained, it can be clearly seen that our approach has enhanced the throughput of the already optimized software by more than six times. This will dramatically reduce the time consumed for the lead identification phase in drug designing along with the shift in the processor technology from multi-core to many-core of the current era. Therefore, we believe that the contribution of this project will effectively make it possible to expand the number of small molecules docked against a drug target and improving the chances to design drugs for incurable diseases.", "venue": "2012 IEEE 6th International Conference on Information and Automation for Sustainability", "authors": ["Upul  Senanayake", "Rahal  Prabuddha", "Roshan G. Ragel"], "year": 2012, "n_citations": 0}
{"id": 473992, "s2_id": "8997d22e33c1ec5fc3fc0c365d0634ffd6d4f05a", "title": "Asymptotically Optimal Load Balancing Topologies", "abstract": "We consider a system of N ~servers inter-connected by some underlying graph topology~G N . Tasks with unit-mean exponential processing times arrive at the various servers as independent Poisson processes of rate lambda. Each incoming task is irrevocably assigned to whichever server has the smallest number of tasks among the one where it appears and its neighbors in G N . The above model arises in the context of load balancing in large-scale cloud networks and data centers, and has been extensively investigated in the case G N is a clique. Since the servers are exchangeable in that case, mean-field limits apply, and in particular it has been proved that for any lambda < 1, the fraction of servers with two or more tasks vanishes in the limit as N -> \u0131nfty. For an arbitrary graph G N , mean-field techniques break down, complicating the analysis, and the queue length process tends to be worse than for a clique. Accordingly, a graph G N is said to be N -optimal or \u221eN-optimal when the queue length process on G N is equivalent to that on a clique on an N -scale or \u221eN-scale, respectively. We prove that if G N is an Erdos-R\u00e9nyi random graph with average degree d(N), then with high probability it is N -optimal and \u221eN-optimal if d(N) -> \u0131nfty$ and d(N) / (\u221eN \u0142og(N)) -> \u0131nfty as N -> \u0131nfty, respectively. This demonstrates that optimality can be maintained at N -scale and \u221eN-scale while reducing the number of connections by nearly a factor N and \u221eN/ \u0142og(N) compared to a clique, provided the topology is suitably random. It is further shown that if G N contains \u0398(N) bounded-degree nodes, then it cannot be N -optimal. In addition, we establish that an arbitrary graph G N is N -optimal when its minimum degree is N - o(N), and may not be N -optimal even when its minimum degree is c N + o(N) for any 0 < c < 1/2. Simulation experiments are conducted for various scenarios to corroborate the asymptotic results.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Debankur  Mukherjee", "Sem C. Borst", "Johan van Leeuwaarden"], "year": 2018, "n_citations": 3}
{"id": 474445, "s2_id": "557c4d273e89a752f9171bd3a153af436348b7b3", "title": "The Design and Implementation of a Scalable DL Benchmarking Platform", "abstract": "The current Deep Learning (DL) landscape is fast-paced and is rife with non-uniform models, hardware/software (HW/SW) stacks, but lacks a DL benchmarking platform to facilitate evaluation and comparison of DL innovations, be it models, frameworks, libraries, or hardware. Due to the lack of a benchmarking platform, the current practice of evaluating the benefits of proposed DL innovations is both arduous and error-prone - stifling the adoption of the innovations. \nIn this work, we first identify $10$ design features which are desirable within a DL benchmarking platform. These features include: performing the evaluation in a consistent, reproducible, and scalable manner, being framework and hardware agnostic, supporting real-world benchmarking workloads, providing in-depth model execution inspection across the HW/SW stack levels, etc. We then propose MLModelScope, a DL benchmarking platform design that realizes the $10$ objectives. MLModelScope proposes a specification to define DL model evaluations and techniques to provision the evaluation workflow using the user-specified HW/SW stack. MLModelScope defines abstractions for frameworks and supports board range of DL models and evaluation scenarios. We implement MLModelScope as an open-source project with support for all major frameworks and hardware architectures. Through MLModelScope's evaluation and automated analysis workflows, we performed case-study analyses of $37$ models across $4$ systems and show how model, hardware, and framework selection affects model accuracy and performance under different benchmarking scenarios. We further demonstrated how MLModelScope's tracing capability gives a holistic view of model execution and helps pinpoint bottlenecks.", "venue": "ArXiv", "authors": ["Cheng  Li", "Abdul  Dakkak", "Jinjun  Xiong", "Wen-mei  Hwu"], "year": 2019, "n_citations": 2}
{"id": 477284, "s2_id": "a4ea21c500ec0e2fd4c0d73d8a0af736270e0167", "title": "Energy Efficient Spectrum Sensing and Handoff Strategies in Cognitive Radio Networks", "abstract": "The limited spectrum resources and dramatic growth of high data rate communications have motivated opportunistic spectrum access using the promising concept of cognitive radio networks. Although this concept has emerged primarily to enhance spectrum utilization, the importance of energy consumption poses new challenges, because energy efficiency and communication performance can be at odds. In this paper, the existing approaches to energy efficiency spectrum sensing and handoff are classified. The tradeoff between energy consumption and throughput is established as function of the numerous design parameters of cognitive radio networks, both in the case of local and of cooperative spectrum sensing. It is argued that a number of important aspects still needs to be researched, such as fairness, dynamic behavior, reactive and proactive schemes for energy efficiency.", "venue": "ArXiv", "authors": ["Hossein Shokri Ghadikolaei", "Ioannis  Glaropoulos", "Viktoria  Fodor", "Carlo  Fischione", "Konstantinos D. Dimou"], "year": 2013, "n_citations": 6}
{"id": 478508, "s2_id": "b120a10310645df329c691b782dea9ceb7dfe786", "title": "Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time", "abstract": "User experience in modern content discovery applications critically depends on high-quality personalized recommendations. However, building systems that provide such recommendations presents a major challenge due to a massive pool of items, a large number of users, and requirements for recommendations to be responsive to user actions and generated on demand in real-time. Here we present Pixie, a scalable graph-based real-time recommender system that we developed and deployed at Pinterest. Given a set of user-specific pins as a query, Pixie selects in real-time from billions of possible pins those that are most related to the query. To generate recommendations, we develop Pixie Random Walk algorithm that utilizes the Pinterest object graph of 3 billion nodes and 17 billion edges. Experiments show that recommendations provided by Pixie lead up to 50% higher user engagement when compared to the previous Hadoop-based production system. Furthermore, we develop a graph pruning strategy at that leads to an additional 58% improvement in recommendations. Last, we discuss system aspects of Pixie, where a single server executes 1,200 recommendation requests per second with 60 millisecond latency. Today, systems backed by Pixie contribute to more than 80% of all user engagement on Pinterest.", "venue": "WWW", "authors": ["Chantat  Eksombatchai", "Pranav  Jindal", "Jerry Zitao Liu", "Yuchen  Liu", "Rahul  Sharma", "Charles  Sugnet", "Mark  Ulrich", "Jure  Leskovec"], "year": 2018, "n_citations": 101}
{"id": 479021, "s2_id": "8c0f0c3c62f683dc8694087518d669919089430a", "title": "Analysis of parallel I/O use on the UK national supercomputing service, ARCHER using Cray LASSi and EPCC SAFE", "abstract": "In this paper, we describe how we have used a combination of the LASSi tool (developed by Cray) and the SAFE software (developed by EPCC) to collect and analyse Lustre I/O performance data for all jobs running on the UK national supercomputing service, ARCHER; and to provide reports on I/O usage for users in our standard reporting framework. We also present results from analysis of parallel I/O use on ARCHER and analysis on the potential impact of different applications on file system performance using metrics we have derived from the LASSi data. We show that the performance data from LASSi reveals how the same application can stress different components of the file system depending on how it is run, and how the LASSi risk metrics allow us to identify use cases that could potentially cause issues for global I/O performance and work with users to improve their I/O use. We use the IO-500 benchmark to help us understand how LASSi risk metrics correspond to observed performance on the ARCHER file systems. We also use LASSi data imported into SAFE to identify I/O use patterns associated with different research areas, understand how the research workflow gives rise to the observed patterns and project how this will affect I/O requirements in the future. Finally, we provide an overview of likely future directions for the continuation of this work.", "venue": "ArXiv", "authors": ["Andrew  Turner", "Dominic  Sloan-Murphy", "Karthee  Sivalingam", "Harvey  Richardson", "Julian M. Kunkel"], "year": 2019, "n_citations": 3}
{"id": 479462, "s2_id": "cfb3bad2b0a4183b299be0d4fc2b4df9f9cc1e02", "title": "Continuous evaluation of the performance of cloud infrastructure for scientific applications", "abstract": "Cloud computing recently developed into a viable alternative to on-premises systems for executing high-performance computing (HPC) applications. With the emergence of new vendors and hardware options, there is now a growing need to continuously evaluate the performance of the infrastructure with respect to the most commonly-used simulation workflows. We present an online ecosystem and the corresponding tools aimed at providing a collaborative and repeatable way to assess the performance of the underlying hardware for multiple real-world application-specific benchmark cases. The ecosystem allows for the benchmark results to be stored and shared online in a centrally accessible database in order to facilitate their comparison, traceability, and curation. We include the current up-to-date example results for multiple cloud vendors and explain how to contribute new results and benchmark cases.", "venue": "ArXiv", "authors": ["Mohammad  Mohammadi", "Timur  Bazhirov"], "year": 2018, "n_citations": 1}
{"id": 481235, "s2_id": "d007cd61be867258489360c907191fb94194d14e", "title": "Me Love (SYN-)Cookies: SYN Flood Mitigation in Programmable Data Planes", "abstract": "The SYN flood attack is a common attack strategy on the Internet, which tries to overload services with requests leading to a Denial-of-Service (DoS). Highly asymmetric costs for connection setup - putting the main burden on the attackee - make SYN flooding an efficient and popular DoS attack strategy. Abusing the widely used TCP as an attack vector complicates the detection of malicious traffic and its prevention utilizing naive connection blocking strategies. Modern programmable data plane devices are capable of handling traffic in the 10 Gbit/s range without overloading. We discuss how we can harness their performance to defend entire networks against SYN flood attacks. Therefore, we analyze different defense strategies, SYN authentication and SYN cookie, and discuss implementation difficulties when ported to different target data planes: software, network processors, and FPGAs. We provide prototype implementations and performance figures for all three platforms. Further, we fully disclose the artifacts leading to the experiments described in this work.", "venue": "ArXiv", "authors": ["Dominik  Scholz", "Sebastian  Gallenm\u00fcller", "Henning  Stubbe", "Bassam  Jaber", "Minoo  Rouhi", "Georg  Carle"], "year": 2020, "n_citations": 6}
{"id": 486727, "s2_id": "f47289e7c9932daaaa369c9fcd34d5f006437486", "title": "A queueing network approach to the analysis and control of mobility-on-demand systems", "abstract": "This paper presents a queueing network approach to the analysis and control of mobility-on-demand (MoD) systems for urban personal transportation. A MoD system consists of a fleet of vehicles providing one-way car sharing service and a team of drivers to rebalance such vehicles. The drivers then rebalance themselves by driving select customers similar to a taxi service. We model the MoD system as two coupled closed Jackson networks with passenger loss. We show that the system can be approximately balanced by solving two decoupled linear programs and exactly balanced through nonlinear optimization. The rebalancing techniques are applied to a system sizing example using taxi data in three neighborhoods of Manhattan, which suggests that the optimal vehicle-to-driver ratio in a MoD system is between 3 and 5. Lastly, we formulate a real-time closed-loop rebalancing policy for drivers and demonstrate its stability (in terms of customer wait times) for typical system loads.", "venue": "2015 American Control Conference (ACC)", "authors": ["Rick  Zhang", "Marco  Pavone"], "year": 2015, "n_citations": 25}
{"id": 488266, "s2_id": "243d7b91d4f079c329c57249c61057d2c00c2d64", "title": "Simple Near-Optimal Scheduling for the M/G/1", "abstract": "We consider the problem of preemptively scheduling jobs to minimize mean response time of an M/G/1 queue. When the scheduler knows each job's size, the shortest remaining processing time (SRPT) policy is optimal. Unfortunately, in many settings we do not have access to each job's size. Instead, we know only the job size distribution. In this setting, the Gittins policy is known to minimize mean response time, but its complex priority structure can be computationally intractable. A much simpler alternative to Gittins is the shortest expected remaining processing time (SERPT) policy. While SERPT is a natural extension of SRPT to unknown job sizes, it is unknown how close SERPT is to optimal.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Ziv  Scully", "Mor  Harchol-Balter", "Alan  Scheller-Wolf"], "year": 2019, "n_citations": 5}
{"id": 495180, "s2_id": "a33d3b52dec594a51ba3bbb6de7908291219e172", "title": "Matrix multiplication and universal scalability of the time on the Intel Scalable processors", "abstract": "Matrix multiplication is one of the core operations in many areas of scientific computing. We present the results of the experiments with the matrix multiplication of the big size comparable with the big size of the onboard memory, which is 1.5 terabyte in our case. We run experiments on the computing board with two sockets and with two Intel Xeon Platinum 8164 processors, each with 26 cores and with multi-threading. The most interesting result of our study is the observation of the perfect scalability law of the matrix multiplication, and of the universality of this law.", "venue": "Journal of Physics: Conference Series", "authors": ["Alexander  Russkov", "Lev  Shchur"], "year": 2019, "n_citations": 1}
{"id": 496507, "s2_id": "b06d28a1b7d37639eb8ecb2ea876db83789e0228", "title": "Secured Message Transmission in Mobile AD HOC Networks through Identification and Removal of Byzantine Failures", "abstract": "The emerging need for mobile ad hoc networks and secured data transmission phase is of crucial importance depending upon the environments like military. In this paper, a new way to improve the reliability of message transmission is presented. In the open collaborative MANET environment, any node can maliciously or selfishly disrupt and deny communication of other nodes. Dynamic changing topology makes it hard to determine the adversary nodes that affect the communication in MANET. An SMT protocol provides a way to secure message transmission by dispersing the message among several paths with minimal redundancy. The multiple routes selected are known as APS -Active Path Set. This paper describes a technique for fault discovery process to identify Byzantine failures which include nodes that drop, modify, or mis-route packets in an attempt to disrupt the routing service. An adaptive probing technique detects a malicious link through binary search and according to the nodes behavior, these links are avoided in the active path by multiplicatively increasing their weights. The proposed scheme provides secure communication even with increased number of adversaries.", "venue": "ArXiv", "authors": ["V.  Anitha", "J.  Akilandeswari"], "year": 2011, "n_citations": 4}
{"id": 497294, "s2_id": "59e47c92ccd09d9a9daa89b17ef72b033d8d109a", "title": "Memory Requirement Reduction of Deep Neural Networks Using Low-bit Quantization of Parameters", "abstract": "Effective employment of deep neural networks (DNNs) in mobile devices and embedded systems is hampered by requirements for memory and computational power. This paper presents a non-uniform quantization approach which allows for dynamic quantization of DNN parameters for different layers and within the same layer. A virtual bit shift (VBS) scheme is also proposed to improve the accuracy of the proposed scheme. Our method reduces the memory requirements, preserving the performance of the network. The performance of our method is validated in a speech enhancement application, where a fully connected DNN is used to predict the clean speech spectrum from the input noisy speech spectrum. A DNN is optimized and its memory footprint and performance are evaluated using the short-time objective intelligibility, STOI, metric. The application of the low-bit quantization allows a 50% reduction of the DNN memory footprint while the STOI performance drops only by 2.7%.", "venue": "ArXiv", "authors": ["Niccol'o  Nicodemo", "Gaurav  Naithani", "Konstantinos  Drossos", "Tuomas  Virtanen", "Roberto  Saletti"], "year": 2019, "n_citations": 0}
{"id": 498858, "s2_id": "59700a0375badd01ecf9719ef1d77b87b999ffab", "title": "Coded Distributed Tracking", "abstract": "We consider the problem of tracking the state of a process that evolves over time in a distributed setting, with multiple observers each observing parts of the state, which is a fundamental information processing problem with a wide range of applications. We propose a cloud-assisted scheme where the tracking is performed over the cloud. In particular, to provide timely and accurate updates, and alleviate the straggler problem of cloud computing, we propose a coded distributed computing approach where coded observations are distributed over multiple workers. The proposed scheme is based on a coded version of the Kalman filter that operates on data encoded with an erasure correcting code, such that the state can be estimated from partial updates computed by a subset of the workers. We apply the proposed scheme to the problem of tracking multiple vehicles. We show that replication achieves significantly higher accuracy than the corresponding uncoded scheme. The use of maximum distance separable (MDS) codes further improves accuracy for larger update intervals. In both cases, the proposed scheme approaches the accuracy of an ideal centralized scheme when the update interval is large enough. Finally, we observe a trade-off between age-of- information and estimation accuracy for MDS codes.", "venue": "2019 IEEE Global Communications Conference (GLOBECOM)", "authors": ["Albin  Severinson", "Eirik  Rosnes", "Alexandre Graell i Amat"], "year": 2019, "n_citations": 0}
{"id": 499493, "s2_id": "6b29c578f6561cf1cc938bc45c0a6e0578df51f2", "title": "uIP Support for the Network Simulation Cradle", "abstract": "This paper introduces the ongoing integration of Contiki's uIP stack into the OMNeT++ port of the Network Simulation Cradle (NSC). The NSC utilizes code from real world stack implementations and allows for an accurate simulation and comparison of different TCP/IP stacks. uIP(v6) provides resource-constrained devices with an RFC-compliant TCP/IP stack and promotes the use of IPv6 in the vastly growing field of Internet of Things scenarios. This work-in-progress report discusses our motivation to integrate uIP into the NSC, our chosen approach and possible use cases for the simulation of uIP in OMNeT++.", "venue": "ArXiv", "authors": ["Michael  Kirsche", "Roman  Kremmer"], "year": 2015, "n_citations": 0}
{"id": 500332, "s2_id": "5a8b2a3daf4d223aaf76bb9dc78cee5033ad145f", "title": "Profile-Based Resource Allocation for Virtualized Network Functions", "abstract": "The virtualization of compute and network resources enables an unseen flexibility for deploying network services. A wide spectrum of emerging technologies allows an ever-growing range of orchestration possibilities in cloud-based environments. But in this context it remains challenging to rhyme dynamic cloud configurations with deterministic performance. The service operator must somehow map the performance specification in the Service Level Agreement (SLA) to an adequate resource allocation in the virtualized infrastructure. We propose the use of a VNF profile to alleviate this process. This is illustrated by profiling the performance of four example network functions (a virtual router, switch, firewall and cache server) under varying workloads and resource configurations. We then compare several methods to derive a model from the profiled datasets. We select the most accurate method to further train a model which predicts the services\u2019 performance, in function of incoming workload and allocated resources. Our presented method can offer the service operator a recommended resource allocation for the targeted service, in function of the targeted performance and maximum workload specified in the SLA. This helps to deploy the softwarized service with an optimal amount of resources to meet the SLA requirements, thereby avoiding unnecessary scaling steps.", "venue": "IEEE Transactions on Network and Service Management", "authors": ["Steven  Van Rossem", "Wouter  Tavernier", "Didier  Colle", "Mario  Pickavet", "Piet  Demeester"], "year": 2019, "n_citations": 9}
{"id": 501568, "s2_id": "14e1f20764d6733499e0a07062de42433522bddf", "title": "Algorithm runtime prediction: Methods & evaluation", "abstract": "Perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm's runtime as a function of problem-specific instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic configuration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and -- perhaps most importantly -- a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisfiability (SAT), travelling salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of SAT, MIP, and TSP instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously.", "venue": "Artif. Intell.", "authors": ["Frank  Hutter", "Lin  Xu", "Holger H. Hoos", "Kevin  Leyton-Brown"], "year": 2014, "n_citations": 317}
{"id": 504972, "s2_id": "5bf7fdff9c69a03591e1e7c51e20b3be17968f26", "title": "User Mode Memory Page Allocation: A Silver Bullet For Memory Allocation?", "abstract": "This paper proposes a novel solution: the elimination of paged virtual memory and partial outsourcing of memory page allocation and manipulation from the operating system kernel into the individual process' user space - a user mode page allocator - which allows an application to have direct, bare metal access to the page mappings used by the hardware Memory Management Unit (MMU) for its part of the overall address space. A user mode page allocator based emulation of the mmap() abstraction layer of dlmalloc is then benchmarked against the traditional kernel mode implemented mmap() in a series of synthetic Monte-Carlo and real world application settings. Given the superb synthetic and positive real world results from the profiling conducted, this paper proposes that with proper operating system and API support one could gain a further order higher performance again while keeping allocator performance invariant to the amount of memory being allocated or freed i.e. a 100x performance improvement or more in some common use cases. It is rare that through a simple and easy to implement API and operating system structure change one can gain a Silver Bullet with the potential for a second one.", "venue": "ArXiv", "authors": ["Niall  Douglas"], "year": 2011, "n_citations": 4}
{"id": 505584, "s2_id": "139d41603624185c2552e235d2bf815f1d8046eb", "title": "Bit Efficient Quantization for Deep Neural Networks", "abstract": "Quantization for deep neural networks have afforded models for edge devices that use less on-board memory and enable efficient low-power inference. In this paper, we present a comparison of model-parameter driven quantization approaches that can achieve as low as 3-bit precision without affecting accuracy. The post-training quantization approaches are data-free, and the resulting weight values are closely tied to the dataset distribution on which the model has converged to optimality. We show quantization results for a number of state-of-art deep neural networks (DNN) using large dataset like ImageNet. To better analyze quantization results, we describe the overall range and local sparsity of values afforded through various quantization schemes. We show the methods to lower bit-precision beyond quantization limits with object class clustering.", "venue": "2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)", "authors": ["Prateeth  Nayak", "David  Zhang", "Sek  Chai"], "year": 2019, "n_citations": 23}
{"id": 507842, "s2_id": "ad85b1db9c046ea8e48c803ff479f7b05b0e2ef2", "title": "A Full-stack Accelerator Search Technique for Vision Applications", "abstract": "The rapidly-changing ML model landscape presents a unique opportunity for building hardware accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. Although FAST can be used on any number and type of deep learning workload, in this paper we focus on optimizing for a single or small set of vision models, resulting in significantly faster and more powerefficient designs relative to a general purpose ML accelerator. When evaluated on EfficientNet [45], ResNet50v2 [18] and OCR [36] inference performance relative to a TPUv3 [22], designs generated by FAST optimized for single workloads can improve Perf/TDP (peak power) by over 6x in the best case and 4x on average. On a limited workload subset, FAST improves Perf/TDP 2.85x on average, with a reduction to 2.35x for a single design optimized over the set of workloads. In addition, we demonstrate a potential 1.8x speedup opportunity for TPU-v3 with improved scheduling.", "venue": "ArXiv", "authors": ["Dan  Zhang", "Safeen  Huda", "Ebrahim  Songhori", "Quoc  Le", "Anna  Goldie", "Azalia  Mirhoseini"], "year": 2021, "n_citations": 1}
{"id": 508214, "s2_id": "13fbc3199fa547ed709f34b60e58fd183fd0ec41", "title": "LEGOStore: A Linearizable Geo-Distributed Store Combining Replication and Erasure Coding", "abstract": "We design and implement LEGOStore, an erasure coding (EC) based linearizable data store over geo-distributed public cloud data centers (DCs). For such a data store, the confluence of the following factors opens up opportunities for EC to be latency-competitive with replication: (a) the necessity of communicating with remote DCs to tolerate entire DC failures and implement linearizability; and (b) the emergence of DCs near most large population centers. LEGOStore employs an optimization framework that, for a given object, carefully chooses among replication and EC, as well as among various DC placements to minimize overall costs. To handle workload dynamism, LEGOStore employs a novel agile reconfiguration protocol. Our evaluation using a LEGOStore prototype spanning 9 Google Cloud Platform DCs demonstrates the efficacy of our ideas. We observe cost savings ranging from moderate (5-20%) to significant (60%) over baselines representing the state of the art while meeting tail latency SLOs. Our reconfiguration protocol is able to transition key placements in 3 to 4 inter-DC RTTs (< 1s in our experiments), allowing for agile adaptation to dynamic conditions .", "venue": "ArXiv", "authors": ["Hamidreza  Zare", "Viveck R. Cadambe", "Bhuvan  Urgaonkar", "Chetan  Sharma", "Praneet  Soni", "Nader  Alfares", "Arif  Merchant"], "year": 2021, "n_citations": 0}
{"id": 514190, "s2_id": "b79ab72c0b66b1277d21679c1a4c12a12af7c5f9", "title": "On Time-Sensitive Revenue Management and Energy Scheduling in Green Data Centers", "abstract": "In this paper, we design an analytically and experimentally better online energy and job scheduling algorithm with the objective of maximizing net profit for a service provider in green data centers. We first study the previously known algorithms and conclude that these online algorithms have provable poor performance against their worst-case scenarios. To guarantee an online algorithm's performance in hindsight, we design a randomized algorithm to schedule energy and jobs in the data centers and prove the algorithm's expected competitive ratio in various settings. Our algorithm is theoretical-sound and it outperforms the previously known algorithms in many settings using both real traces and simulated data. An optimal offline algorithm is also implemented as an empirical benchmark.", "venue": "ArXiv", "authors": ["Huangxin  Wang", "Jean X. Zhang", "Fei  Li"], "year": 2014, "n_citations": 1}
{"id": 515036, "s2_id": "112621f0487065ba910c94dfe756a5ba74a2c5f1", "title": "Asymptotic regime analysis of NOMA uplink networks under QoS delay Constraints", "abstract": "In the fifth generation and beyond (B5G) technologies, delay constrains emerge as a topic of particular interest for ultra reliable low latency communications (e.g., enhanced reality, haptic communications). In this report, we study the performance of a two user uplink non orthogonal multiple access (NOMA) network under quality of service (QoS) delay constraints, captured through each user delay exponents in their effective capacity (EC). We propose novel closed form expressions for the EC of the NOMA users and validate them through Monte Carlo simulations. Interestingly, our study shows that in the high signal to noise ratio (SNR) region, the strong NOMA user has a limited EC no matter how large the transmit SNR is, under the same delay constraint as the weak user. We show that for the weak user OMA achieves higher EC than NOMA at small values of the transmit SNR and that NOMA become more beneficial at high values of the transmit SNR. For the strong user, we show that NOMA achieves a higher EC than OMA at small values of the transmit SNR and that at high values of the transmit SNR OMA becomes more beneficial. By introducing user pairing when more than two NOMA users are present, we show that NOMA with user pairing outperforms OMA in term of the total link layer EC. Finally, we find the set of pairs which gives the highest total link-layer in the uplink for NOMA with multiple user-pairs.", "venue": "ArXiv", "authors": ["Mouktar  Bello"], "year": 2020, "n_citations": 1}
{"id": 515495, "s2_id": "115bb88c2757acffb31b50890b62169b2bbd7317", "title": "On the Analysis of Spatially Constrained Power of Two Choice Policies", "abstract": "We consider a class of power of two choice based assignment policies for allocating users to servers, where both users and servers are located on a two-dimensional Euclidean plane. In this framework, we investigate the inherent tradeoff between the communication cost, and load balancing performance of different allocation policies. To this end, we first design and evaluate a Spatial Power of two (sPOT) policy in which each user is allocated to the least loaded server among its two geographically nearest servers sequentially. When servers are placed on a two-dimensional square grid, sPOT maps to the classical Power of two (POT) policy on the Delaunay graph associated with the Voronoi tessellation of the set of servers. We show that the associated Delaunay graph is 4-regular and provide expressions for asymptotic maximum load using results from the literature. For uniform placement of servers, we map sPOT to a classical balls and bins allocation policy with bins corresponding to the Voronoi regions associated with the second order Voronoi diagram of the set of servers. We provide expressions for the lower bound on the asymptotic expected maximum load on the servers and prove that sPOT does not achieve POT load balancing benefits. However, experimental results suggest the efficacy of sPOT with respect to expected communication cost. Finally, we propose two non-uniform server sampling based POT policies that achieve the best of both the performance metrics. Experimental results validate the effectiveness of our proposed policies.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Nitish K. Panigrahy", "Prithwish  Basu", "Don  Towsley", "Ananthram  Swami", "Kin K. Leung"], "year": 2020, "n_citations": 1}
{"id": 516243, "s2_id": "0a89b4a34c86cb65689f5e79a88fd4bc7c8f63c5", "title": "Execution-Cache-Memory Performance Model: Introduction and Validation", "abstract": "This report serves two purposes: To introduce and validate the Execution-Cache-Memory (ECM) performance model and to provide a thorough analysis of current Intel processor architectures with a special emphasis on Intel Xeon Haswell-EP. The ECM model is a simple analytical performance model which focuses on basic architectural resources. The architectural analysis and model predictions are showcased and validated using a set of elementary microbenchmarks.", "venue": "ArXiv", "authors": ["Johannes  Hofmann", "Jan  Eitzinger", "Dietmar  Fey"], "year": 2015, "n_citations": 17}
{"id": 516700, "s2_id": "1ce45c81c701d90035e71ae659badef3e68f5e52", "title": "Comparative Code Structure Analysis using Deep Learning for Performance Prediction", "abstract": "Performance analysis has always been an afterthought during the application development process, focusing on application correctness first. The learning curve of the existing static and dynamic analysis tools are steep, which requires understanding low-level details to interpret the findings for actionable optimizations. Additionally, application performance is a function of a number of unknowns stemming from the application-, runtime-, and interactions between the OS and underlying hardware, making it difficult to model using any deep learning technique, especially without a large labeled dataset. In this paper, we address both of these problems by presenting a large corpus of a labeled dataset for the community and take a comparative analysis approach to mitigate all unknowns except their source code differences between different correct implementations of the same problem. We put the power of deep learning to the test for automatically extracting information from the hierarchical structure of abstract syntax trees to represent source code. This paper aims to assess the feasibility of using purely static information (e.g., abstract syntax tree or AST) of applications to predict performance change based on the change in code structure. This research will enable performance-aware application development since every version of the application will continue to contribute to the corpora, which will enhance the performance of the model. We evaluate several deep learning-based representation learning techniques for source code. Our results show that tree-based Long Short-Term Memory (LSTM) models can leverage source code's hierarchical structure to discover latent representations. Specifically, LSTM-based predictive models built using a single problem and a combination of multiple problems can correctly predict if a source code will perform better or worse up to 84% and 73% of the time, respectively.", "venue": "2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Nathan  Pinnow", "Tarek  Ramadan", "Tanzima Z. Islam", "Chase  Phelps", "Jayaraman J. Thiagarajan"], "year": 2021, "n_citations": 0}
{"id": 516848, "s2_id": "759f11cf9857c78c13d399035c5d482be4835f0e", "title": "Fast Histograms using Adaptive CUDA Streams", "abstract": "Histograms are widely used in medical imaging, network intrusion detection, packet analysis and other stream- based high throughput applications. However, while porting such software stacks to the GPU, the computation of the histogram is a typical bottleneck primarily due to the large impact on kernel speed by atomic operations. In this work, we propose a stream-based model implemented in CUDA, using a new adaptive kernel that can be optimized based on latency hidden CPU compute. We also explore the tradeoffs of using the new kernel vis-a-vis the stock NVIDIA SDK kernel, and discuss an intelligent kernel switching method for the stream based on a degeneracy criterion that is adaptively computed from the input stream.", "venue": "ArXiv", "authors": ["Sisir  Koppaka", "Dheevatsa  Mudigere", "Srihari  Narasimhan", "Babu  Narayanan"], "year": 2010, "n_citations": 1}
{"id": 516936, "s2_id": "cda80e7147990f81444d830348f4d8dd3238a88a", "title": "gearshifft - The FFT Benchmark Suite for Heterogeneous Platforms", "abstract": "Fast Fourier Transforms (FFTs) are exploited in a wide variety of fields ranging from computer science to natural sciences and engineering. With the rising data production bandwidths of modern FFT applications, judging best which algorithmic tool to apply, can be vital to any scientific endeavor. As tailored FFT implementations exist for an ever increasing variety of high performance computer hardware, choosing the best performing FFT implementation has strong implications for future hardware purchase decisions, for resources FFTs consume and for possibly decisive financial and time savings ahead of the competition. This paper therefor presents gearshifft, which is an open-source and vendor agnostic benchmark suite to process a wide variety of problem sizes and types with state-of-the-art FFT implementations (fftw, clFFT and cuFFT). gearshifft provides a reproducible, unbiased and fair comparison on a wide variety of hardware to explore which FFT variant is best for a given problem size.", "venue": "ISC", "authors": ["Peter  Steinbach", "Matthias  Werner"], "year": 2017, "n_citations": 14}
{"id": 520949, "s2_id": "89f1b64d90a6bc218dd8483830b291810699e61a", "title": "Performance Heuristics for GR(1) Synthesis and Related Algorithms", "abstract": "Reactive synthesis for the GR(1) fragment of LTL has been implemented and studied in many works. In this workshop paper we present and evaluate a list of heuristics to potentially reduce running times for GR(1) synthesis and related algorithms. The list includes early detection of fixed-points and unrealizability, fixed-point recycling, and heuristics for unrealizable core computations. We evaluate the presented heuristics on SYNTECH15, a total of 78 specifications of 6 autonomous Lego robots, written by 3rd year undergraduate computer science students in a project class we have taught, as well as on several benchmarks from the literature. The evaluation investigates not only the potential of the suggested heuristics to improve computation times, but also the difference between existing benchmarks and the robot's specifications in terms of the effectiveness of the heuristics.", "venue": "SYNT@CAV", "authors": ["Elizabeth  Firman", "Shahar  Maoz", "Jan Oliver Ringert"], "year": 2017, "n_citations": 9}
{"id": 526284, "s2_id": "ccc9e7fed4d33539049bc33f22f9307691d17711", "title": "Online Collection and Forecasting of Resource Utilization in Large-Scale Distributed Systems", "abstract": "Large-scale distributed computing systems often contain thousands of distributed nodes (machines). Monitoring the conditions of these nodes is important for system management purposes, which, however, can be extremely resource demanding as this requires collecting local measurements of each individual node and constantly sending those measurements to a central controller. Meanwhile, it is often useful to forecast the future system conditions for various purposes such as resource planning/allocation and anomaly detection, but it is usually too resource-consuming to have one forecasting model running for each node, which may also neglect correlations in observed metrics across different nodes. In this paper, we propose a mechanism for collecting and forecasting the resource utilization of machines in a distributed computing system in a scalable manner. We present an algorithm that allows each local node to decide when to transmit its most recent measurement to the central node, so that the transmission frequency is kept below a given constraint value. Based on the measurements received from local nodes, the central node summarizes the received data into a small number of clusters. Since the cluster partitioning can change over time, we also present a method to capture the evolution of clusters and their centroids. As an effective way to reduce the amount of computation, time-series forecasting models are trained on the time-varying centroids of each cluster, to forecast the future resource utilizations of a group of local nodes. The effectiveness of our proposed approach is confirmed by extensive experiments using multiple real-world datasets.", "venue": "2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)", "authors": ["Tiffany  Tuor", "Shiqiang  Wang", "Kin K. Leung", "Bong Jun Ko"], "year": 2019, "n_citations": 3}
{"id": 527709, "s2_id": "3de242901c0dc5c796d7071b44e6ce4fb4efc319", "title": "Pliant: Leveraging Approximation to Improve Datacenter Resource Efficiency", "abstract": "Cloud multi-tenancy is typically constrained to a single interactive service colocated with one or more batch, low-priority services, whose performance can be sacrificed when deemed necessary. Approximate computing applications offer the opportunity to enable tighter colocation among multiple applications whose performance is important. We present Pliant, a lightweight cloud runtime that leverages the ability of approximate computing applications to tolerate some loss in their output quality to boost the utilization of shared servers. During periods of high resource contention, Pliant employs incremental and interference-aware approximation to reduce contention in shared resources, and prevent QoS violations for co-scheduled interactive, latency-critical services. We evaluate Pliant across different interactive and approximate computing applications, and show that it preserves QoS for all co-scheduled workloads, while incurring a 2.1\\% loss in output quality, on average.", "venue": "2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Neeraj  Kulkarni", "Feng  Qi", "Christina  Delimitrou"], "year": 2019, "n_citations": 8}
{"id": 529802, "s2_id": "6bb32be11e41032698df6aed7e0b30bd92ba036c", "title": "Non-asymptotic delay bounds for (k, l) fork-join systems and multi-stage fork-join networks", "abstract": "Parallel systems have received increasing attention with numerous recent applications such as fork-join systems, load-balancing, and l-out-of-k redundancy. Common to these systems is a join or resequencing stage, where tasks that have finished service may have to wait for the completion of other tasks so that they leave the system in a predefined order. These synchronization constraints make the analysis of parallel systems challenging and few explicit results are known. In this work, we model parallel systems using a max-plus approach that enables us to derive statistical bounds of waiting and sojourn times. Taking advantage of max-plus system theory, we also show end-to-end delay bounds for multi-stage fork-join networks. We contribute solutions for basic G|G|1 fork-join systems, parallel systems with load-balancing, as well as general (k, l) fork-join systems with redundancy. Our results provide insights into the respective advantages of l-out-of-k redundancy vs. load-balancing.", "venue": "IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications", "authors": ["Markus  Fidler", "Yuming  Jiang"], "year": 2016, "n_citations": 23}
{"id": 531526, "s2_id": "6f7ce6b05ba0e0c980ae4b57ce3f2d2f7c3e02c6", "title": "A Measurement Theory of Locality", "abstract": "Locality is a fundamental principle used extensively in program and system optimization. It can be measured in many ways. This paper formalizes the metrics of locality into a measurement theory. The new theory includes the precise definition of locality metrics based on access frequency, reuse time, reuse distance, working set, footprint, and the cache miss ratio. It gives the formal relation between these definitions and the proofs of equivalence or non-equivalence. It provides the theoretical justification for four successful locality models in operating systems, programming languages, and computer architectures which were developed empirically.", "venue": "ArXiv", "authors": ["Liang  Yuan", "Chen  Ding", "Peter J. Denning", "Yunquan  Zhang"], "year": 2018, "n_citations": 1}
{"id": 533555, "s2_id": "25731d2c88b610984f2d130da50659903584772e", "title": "Characterization of the Gittins index for sequential multistage jobs", "abstract": "The optimal scheduling problem in single-server queueing systems is a classic problem in queueing theory. The Gittins index policy is known to be the optimal preemptive nonanticipating policy (both for the open version of the problem with Poisson arrivals and the closed version without arrivals) minimizing the expected holding costs. While the Gittins index is thoroughly characterized for ordinary jobs whose state is described by the attained service, it is not at all the case with jobs that have more complex structure. Recently, a class of such jobs, the multistage jobs, were introduced, and it was shown that the computation of Gittins index of a multistage job reduces into separable computations for the individual stages. The characterization is, however, indirect in the sense that it relies on the recursion for an auxiliary function (so called SJP function) and not for the Gittins index itself. In this paper, we answer the natural question: Is it possible to compute the Gittins index for a multistage job more directly by recursively combining the Gittins indexes of its individual stages? According to our results, it seems to be possible, at least, for sequential multistage jobs that have a fixed (deterministic) sequence of stages. We prove this for sequential two-stage jobs that have monotonous hazard rates in both stages, but our numerical experiments give an indication that the result could possibly be generalized to any sequential multistage jobs. Our approach, in this paper, is based on the Whittle index originally developed in the context of restless bandits.", "venue": "ArXiv", "authors": ["Samuli  Aalto"], "year": 2021, "n_citations": 0}
{"id": 534253, "s2_id": "b03f6c2ce3efd209427859d2c277309b9f5e51af", "title": "R3-DLA (Reduce, Reuse, Recycle): A More Efficient Approach to Decoupled Look-Ahead Architectures", "abstract": "Modern societies have developed insatiable demands for more computation capabilities. Exploiting implicit parallelism to provide automatic performance improvement remains a central goal in engineering future general-purpose computing systems. One approach is to use a separate thread context to perform continuous look-ahead to improve the data and instruction supply to the main pipeline. Such a decoupled look-ahead (DLA) architecture can be quite effective in accelerating a broad range of applications in a relatively straightforward implementation. It also has broad design flexibility as the look-ahead agent need not be concerned with correctness constraints. In this paper, we explore a number of optimizations that make the look-ahead agent more efficient and yet extract more utility from it. With these optimizations, a DLA architecture can achieve an average speedup of 1.4 over a state-of-the-art microarchitecture for a broad set of benchmark suites, making it a powerful tool to enhance single-thread performance.", "venue": "2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Sushant  Kondguli", "Michael  Huang"], "year": 2019, "n_citations": 6}
{"id": 534911, "s2_id": "c49bbaf1a8a2f4ecefe496591197707aecbd12f8", "title": "Ready for Rain? A View from SPEC Research on the Future of Cloud Metrics", "abstract": "George Kousiouris School of Electrical and Computer Engineering National Technical University of Athens Athens, Greece gkousiou@mail.ntua.gr Athanasia Evangelinou School of Electrical and Computer Engineering National Technical University of Athens Athens, Greece aevang@mail.ntua.gr Alexandru Iosup Faculty of Engineering, Mathematics and Computer Science Delft University of Technology Delft, Netherlands A.Iosup@tudelft.nl", "venue": "ArXiv", "authors": ["Nikolas Roman Herbst", "Rouven  Krebs", "Giorgos  Oikonomou", "George  Kousiouris", "Athanasia  Evangelinou", "Alexandru  Iosup", "Samuel  Kounev"], "year": 2016, "n_citations": 35}
{"id": 538172, "s2_id": "fc1f2331d370449827310ab0dd5f73102be62ceb", "title": "Statistical Analysis of Link Scheduling on Long Paths", "abstract": "We study how the choice of packet scheduling algorithms influences end-to-end performance on long network paths. Taking a network calculus approach, we consider both deterministic and statistical performance metrics. A key enabling contribution for our analysis is a significantly sharpened method for computing a statistical bound for the service given to a flow by the network as a whole. For a suitably parsimonious traffic model we develop closed-form expressions for end-to-end delays, backlog, and output burstiness. The deterministic versions of our bounds yield optimal bounds on end-to-end backlog and output burstiness for some schedulers, and are highly accurate for end-to-end delay bounds.", "venue": "ArXiv", "authors": ["Yashar  Ghiassi-Farrokhfal", "J\u00f6rg  Liebeherr", "Almut  Burchard"], "year": 2011, "n_citations": 1}
{"id": 552886, "s2_id": "f819c2bdc9b86cc77accc8c6edf3e093c546133c", "title": "An Efficient Routing Protocol in Mobile Ad-hoc Networks by using Artificial Immune System", "abstract": "Characteristics of the mobile ad-hoc networks such as nodes high mobility and limited energy are regarded as the routing challenges in these networks. OLSR protocol is one of the routing protocols in mobile ad hoc network that selects the shortest route between source and destination through Dijkstra's algorithm. However, OLSR suffers from a major problem. It does not consider parameters such as nodes\u2019 energy level and links length in its route processing. This paper employs the artificial immune system (AIS) to enhance efficiency of OLSR routing protocol. The proposed algorithm, called AIS-OLSR, considers hop count, remaining energy in the intermediate nodes, and distance among node, which is realized by negative selection and ClonalG algorithms of AIS. Widespread packet - level simulation in ns-2 environment, shows that AIS-OLSR outperforms OLSR and EA-OLSR in terms of packet delivery ratio, throughput, end-end delay and lifetime.", "venue": "ArXiv", "authors": ["Fatemeh  Sarkohaki", "Reza  Fotohi", "Vahab  Ashrafian"], "year": 2020, "n_citations": 36}
{"id": 556124, "s2_id": "3b61021465fa9c474deac63a8f8376ce70f7d6c0", "title": "Modeling Corruption in Eventually-Consistent Graph Databases", "abstract": "We present a model and analysis of an eventually consistent graph database where loosely cooperating servers accept concurrent updates to a partitioned, distributed graph. The model is high-fidelity and preserves design choices from contemporary graph database management systems. To explore the problem space, we use two common graph topologies as data models for realistic experimentation. The analysis reveals, even assuming completely fault-free hardware and bug-free software, that if it is possible for updates to interfere with one-another, corruption will occur and spread significantly through the graph within the production database lifetime. Using our model, database designers and operators can compute the rate of corruption for their systems and determine whether they are sufficiently dependable for their intended use.", "venue": "ArXiv", "authors": ["Jim  Webber", "Paul D. Ezhilchelvan", "Isi  Mitrani"], "year": 2019, "n_citations": 1}
{"id": 562366, "s2_id": "87859bb8b5f84b31e2c49a8fab1222e7c3f08125", "title": "False Data Injection Attacks in Internet of Things and Deep Learning enabled Predictive Analytics", "abstract": "Industry 4.0 is the latest industrial revolution primarily merging automation with advanced manufacturing to reduce direct human effort and resources. Predictive maintenance (PdM) is an industry 4.0 solution, which facilitates predicting faults in a component or a system powered by state-of-the-art machine learning (ML) algorithms and the Internet-of-Things (IoT) sensors. However, IoT sensors and deep learning (DL) algorithms, both are known for their vulnerabilities to cyber-attacks. In the context of PdM systems, such attacks can have catastrophic consequences as they are hard to detect due to the nature of the attack. To date, the majority of the published literature focuses on the accuracy of DL enabled PdM systems and often ignores the effect of such attacks. In this paper, we demonstrate the effect of IoT sensor attacks on a PdM system. At first, we use three state-of-the-art DL algorithms, specifically, Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Convolutional Neural Network (CNN) for predicting the Remaining Useful Life (RUL) of a turbofan engine using NASA's C-MAPSS dataset. The obtained results show that the GRU-based PdM model outperforms some of the recent literature on RUL prediction using the C-MAPSS dataset. Afterward, we model two different types of false data injection attacks (FDIA) on turbofan engine sensor data and evaluate their impact on CNN, LSTM, and GRU-based PdM systems. The obtained results demonstrate that FDI attacks on even a few IoT sensors can strongly defect the RUL prediction. However, the GRU-based PdM model performs better in terms of accuracy and resiliency. Lastly, we perform a study on the GRU-based PdM model using four different GRU networks with different sequence lengths. Our experiments reveal an interesting relationship between the accuracy, resiliency and sequence length for the GRU-based PdM models.", "venue": "ArXiv", "authors": ["Gautam Raj Mode", "Prasad  Calyam", "Khaza Anuarul Hoque"], "year": 2019, "n_citations": 6}
{"id": 565921, "s2_id": "e08c01ac27670ad4d1c1cc74a53cf8d46648e386", "title": "Algorithms of evaluation of the waiting time and the modelling of the terminal activity", "abstract": "This paper approaches the application of the waiting model with Poisson inputs and priorities in the port activity. The arrival of ships in the maritime terminal is numerically modelled, and specific parameters for the distribution functions of service and of inputs are determined, in order to establish the waiting time of ships in the seaport and a stationary process. The modelling is based on waiting times and on the traffic coefficient.", "venue": "ArXiv", "authors": ["Gh.  Miscoi", "A.  Costea", "R. I. Ticu", "C.  Pomazan"], "year": 2019, "n_citations": 0}
{"id": 567104, "s2_id": "e3d8049990c0d2768a0dcd7b4285e52e9b856525", "title": "\"The Tail Wags the Dog\": A Study of Anomaly Detection in Commercial Application Performance", "abstract": "The IT industry needs systems management models that leverage available application information to detect quality of service, scalability and health of service. Ideally this technique would be common for varying application types with different n-tier architectures under normal production conditions of varying load, user session traffic, transaction type, transaction mix, and hosting environment. This paper shows that a whole of service measurement paradigm utilizing a black box M/M/1 queuing model and auto regression curve fitting of the associated CDF are an accurate model to characterize system performance signatures. This modeling method is used to detect application slow down events. The method did not rely on customizations specific to the n-tier architecture of the systems being analyzed and so the performance anomaly detection technique was shown to be platform and configuration agnostic.", "venue": "2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems", "authors": ["Richard  Gow", "Srikumar  Venugopal", "Pradeep Kumar Ray"], "year": 2013, "n_citations": 4}
{"id": 567163, "s2_id": "04b020e55d211b8c441c8b15d2c8711292ea5aba", "title": "A GPU accelerated discontinuous Galerkin incompressible flow solver", "abstract": "We present a GPU-accelerated version of a high-order discontinuous Galerkin discretization of the unsteady incompressible Navier-Stokes equations. The equations are discretized in time using a semi-implicit scheme with explicit treatment of the nonlinear term and implicit treatment of the split Stokes operators. The pressure system is solved with a conjugate gradient method together with a fully GPU-accelerated multigrid preconditioner which is designed to minimize memory requirements and to increase overall performance. A semi-Lagrangian subcycling advection algorithm is used to shift the computational load per timestep away from the pressure Poisson solve by allowing larger timestep sizes in exchange for an increased number of advection steps. Numerical results confirm we achieve the design order accuracy in time and space. We optimize the performance of the most time-consuming kernels by tuning the fine-grain parallelism, memory utilization, and maximizing bandwidth. To assess overall performance we present an empirically calibrated roofline performance model for a target GPU to explain the achieved efficiency. We demonstrate that, in the most cases, the kernels used in the solver are close to their empirically predicted roofline performance.", "venue": "J. Comput. Phys.", "authors": ["Ali  Karakus", "Noel  Chalmers", "Kasia  Swirydowicz", "Timothy  Warburton"], "year": 2019, "n_citations": 9}
{"id": 568140, "s2_id": "1a820668ba5a31dd8bfbd7ec2febbb03d194c199", "title": "BrainSlug: Transparent Acceleration of Deep Learning Through Depth-First Parallelism", "abstract": "Neural network frameworks such as PyTorch and TensorFlow are the workhorses of numerous machine learning applications ranging from object recognition to machine translation. While these frameworks are versatile and straightforward to use, the training of and inference in deep neural networks is resource (energy, compute, and memory) intensive. In contrast to recent works focusing on algorithmic enhancements, we introduce BrainSlug, a framework that transparently accelerates neural network workloads by changing the default layer-by-layer processing to a depth-first approach, reducing the amount of data required by the computations and thus improving the performance of the available hardware caches. BrainSlug achieves performance improvements of up to 41.1% on CPUs and 35.7% on GPUs. These optimizations come at zero cost to the user as they do not require hardware changes and only need tiny adjustments to the software.", "venue": "ArXiv", "authors": ["Nicolas  Weber", "Florian  Schmidt", "Mathias  Niepert", "Felipe  Huici"], "year": 2018, "n_citations": 7}
{"id": 576966, "s2_id": "e75def8bb1bd9485d101a892b2b71b517cd75eea", "title": "De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks", "abstract": "Machine learning techniques have been widely applied to various applications. However, they are potentially vulnerable to data poisoning attacks, where sophisticated attackers can disrupt the learning procedure by injecting a fraction of malicious samples into the training dataset. Existing defense techniques against poisoning attacks are largely attack-specific: they are designed for one specific type of attacks but do not work for other types, mainly due to the distinct principles they follow. Yet few general defense strategies have been developed. In this paper, we propose De-Pois, an attack-agnostic defense against poisoning attacks. The key idea of De-Pois is to train a mimic model the purpose of which is to imitate the behavior of the target model trained by clean samples. We take advantage of Generative Adversarial Networks (GANs) to facilitate informative training data augmentation as well as the mimic model construction. By comparing the prediction differences between the mimic model and the target model, De-Pois is thus able to distinguish the poisoned samples from clean ones, without explicit knowledge of any ML algorithms or types of poisoning attacks. We implement four types of poisoning attacks and evaluate De-Pois with five typical defense methods on different realistic datasets. The results demonstrate that De-Pois is effective and efficient for detecting poisoned data against all the four types of poisoning attacks, with both the accuracy and F1-score over 0.9 on average.", "venue": "IEEE Transactions on Information Forensics and Security", "authors": ["Jian  Chen", "Xuxin  Zhang", "Rui  Zhang", "Chen  Wang", "Ling  Liu"], "year": 2021, "n_citations": 3}
{"id": 581930, "s2_id": "9cc142f5b191e94160b02bf001ddc5f8ea58db4f", "title": "Study and Analysis of MAC/IPAD Lab Configuration", "abstract": "This paper is about three virtualization modes: VMware, Parallels, and Boot Camping. The trade off of their testing is the hardware requirements. The main question is, among the three, which is the most suitable? The answer actually varies from user to user. It depends on the user needs. Moreover, it is necessary to consider its performance, graphics, efficiency and reliability, and interoperability, and that is our major scope. In order to take the final decision in choosing one of the modes it is important to run some tests, which costs a lot in terms of money, complexity, and time consumption. Therefore, in order to overcome this trade off, most of the research has been done through online benchmarking and my own anticipation. The final solution was extracted after comparing all previously mentioned above and after rigorous testing made which will be introduced later in this document.", "venue": "ArXiv", "authors": ["Ayman  Noor"], "year": 2017, "n_citations": 0}
{"id": 582924, "s2_id": "4141f1d5900523e9ab62f6b06c8c856a49bee322", "title": "Performance and energy footprint assessment of FPGAs and GPUs on HPC systems using Astrophysics application", "abstract": "New challenges in Astronomy and Astrophysics (AA) are urging the need for a large number of exceptionally computationally intensive simulations. \"Exascale\" (and beyond) computational facilities are mandatory to address the size of theoretical problems and data coming from the new generation of observational facilities in AA. Currently, the High Performance Computing (HPC) sector is undergoing a profound phase of innovation, in which the primary challenge to the achievement of the \"Exascale\" is the power-consumption. The goal of this work is to give some insights about performance and energy footprint of contemporary architectures for a real astrophysical application in an HPC context. We use a state-of-the-art N-body application that we re-engineered and optimized to exploit the heterogeneous underlying hardware fully. We quantitatively evaluate the impact of computation on energy consumption when running on four different platforms. Two of them represent the current HPC systems (Intel-based and equipped with NVIDIA GPUs), one is a micro-cluster based on ARM-MPSoC, and one is a \"prototype towards Exascale\" equipped with ARM-MPSoCs tightly coupled with FPGAs. We investigate the behavior of the different devices where the high-end GPUs excel in terms of time-to-solution while MPSoC-FPGA systems outperform GPUs in power consumption. Our experience reveals that considering FPGAs for computationally intensive application seems very promising, as their performance is improving to meet the requirements of scientific applications. This work can be a reference for future platforms development for astrophysics applications where computationally intensive calculations are required.", "venue": "Comput.", "authors": ["David  Goz", "Georgios  Ieronymakis", "Vassilis  Papaefstathiou", "Nikolaos  Dimou", "Sara  Bertocco", "Giuliano  Taffoni", "Francesco  Simula", "Antonio  Ragagnin", "Luca  Tornatore", "Igor  Coretti"], "year": 2020, "n_citations": 1}
{"id": 585038, "s2_id": "93f6fc8c9c7e179f8228bf0be65d29f1a34d4ff4", "title": "Effective Handling of Urgent Jobs - Speed Up Scheduling for Computing Applications", "abstract": "A queue is required when a service provider is not able to handle jobs arriving over the time. In a highly flexible and dynamic environment, some jobs might demand for faster execution at run-time especially when the resources are limited and the jobs are competing for acquiring resources. A user might demand for speed up (reduced wait time) for some of the jobs present in the queue at run time. In such cases, it is required to accelerate (directly sending the job to the server) urgent jobs (requesting for speed up) ahead of other jobs present in the queue for an earlier completion of urgent jobs. Under the assumption of no additional resources, such acceleration of jobs would result in slowing down of other jobs present in the queue. In this paper, we formulate the problem of Speed Up Scheduling without acquiring any additional resources for the scheduling of on-line speed up requests posed by a user at run-time and present algorithms for the same. We apply the idea of Speed Up Scheduling to two different domains -Web Scheduling and CPU Scheduling. We demonstrate our results with a simulation based model using trace driven workload and synthetic datasets to show the usefulness of Speed Up scheduling. Speed Up provides a new way of addressing urgent jobs, provides a different evaluation criteria for comparing scheduling algorithms and has practical applications.", "venue": "ArXiv", "authors": ["Yash  Gupta", "Kamalakar  Karlapalem"], "year": 2015, "n_citations": 0}
{"id": 588851, "s2_id": "f5ff04244ba5eef27961bebe00b9ef1b8a90858b", "title": "A Blind Time-Reversal Detector in the Presence of Channel Correlation", "abstract": "A blind target detector using the time reversal transmission is proposed in the presence of channel correlation. We calculate the exact moments of the test statistics involved. The derived moments are used to construct an accurate approximative Likelihood Ratio Test (LRT) based on multivariate Edgeworth expansion. Performance gain over an existing detector is observed in scenarios with channel correlation and relatively strong target signal.", "venue": "IEEE Signal Processing Letters", "authors": ["Zhong  Zheng", "Lu  Wei", "Jyri  H\u00e4m\u00e4l\u00e4inen", "Olav  Tirkkonen"], "year": 2013, "n_citations": 6}
{"id": 599925, "s2_id": "ebb1d2e751579980b2258d5ad79280358566c281", "title": "Power-Aware Wireless File Downloading: A Lyapunov Indexing Approach to a Constrained Restless Bandit Problem", "abstract": "This paper treats power-aware throughput maximization in a multiuser file downloading system. Each user can receive a new file only after its previous file is finished. The file state processes for each user act as coupled Markov chains that form a generalized restless bandit system. First, an optimal algorithm is derived for the case of one user. The algorithm maximizes throughput subject to an average power constraint. Next, the one-user algorithm is extended to a low-complexity heuristic for the multiuser problem. The heuristic uses a simple online index policy. In a special case with no power-constraint, the multiuser heuristic is shown to be throughput-optimal. Simulations are used to demonstrate effectiveness of the heuristic in the general case. For simple cases where the optimal solution can be computed offline, the heuristic is shown to be near-optimal for a wide range of parameters.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Xiaohan  Wei", "Michael J. Neely"], "year": 2016, "n_citations": 5}
{"id": 600287, "s2_id": "674734373df72bd28c2823709ad8618d69e627b2", "title": "HTTPI Based Web Service Security over SOAP", "abstract": "Now a days, a new family of web applications open applications, are emerging (e.g., Social Networking, News and Blogging). Generally, these open applications are non-confidential. The security needs of these applications are only client/server authentication and data integrity. For securing these open applications, effectively and efficiently, HTTPI, a new transport protocol is proposed, which ensures the entire security requirements of open applications. Benefit of using the HTTPI is that it is economical in use, well-suited for cache proxies, like HTTP is, and provides security against many Internet attacks (Server Impersonation and Message Modification) like HTTPS does. In terms of performance HTTPI is very close to the HTTP, but much better than HTTPS. A Web service is a method of communication between two ends over the Internet. These web services are developed over XML and HTTP. Today, most of the open applications use web services for most of their operations. For securing these web services, security design based on HTTPI is proposed. Our work involves securing the web services over SOAP, based on the HTTPI. This secure web service might be applicable for open applications, where authentication and integrity is needed, but no confidentiality required. In our paper, we introduce a web service security model based on HTTPI protocol over SOAP and develop a preliminary implementation of this model. We also analyze the performance of our approach through an experiment and show that our proposed approach provides higher throughput, lower average response time and lower response size than HTTPS based web service security approach.", "venue": "ArXiv", "authors": ["Pankaj  Choudhary", "Rajendra  Aaseri", "Nirmal  Roberts"], "year": 2013, "n_citations": 6}
{"id": 601997, "s2_id": "a7a63c3d9d70a3c71fb34626d698abbaa02ce327", "title": "An Empirical Evaluation of Cost-based Federated SPARQL Query Processing Engines", "abstract": "Finding a good query plan is key to the optimization of query runtime. This holds in particular for cost-based federation engines, which make use of cardinality estimations to achieve this goal. A number of studies compare SPARQL federation engines across different performance metrics, including query runtime, result set completeness and correctness, number of sources selected and number of requests sent. Albeit informative, these metrics are generic and unable to quantify and evaluate the accuracy of the cardinality estimators of cost-based federation engines. To thoroughly evaluate cost-based federation engines, the effect of estimated cardinality errors on the overall query runtime performance must be measured. In this paper, we address this challenge by presenting novel evaluation metrics targeted at a fine-grained benchmarking of cost-based federated SPARQL query engines. We evaluate five cost-based federated SPARQL query engines using existing as well as novel evaluation metrics by using LargeRDFBench queries. Our results provide a detailed analysis of the experimental outcomes that reveal novel insights, useful for the development of future cost-based federated SPARQL query processing engines.", "venue": "Semantic Web", "authors": ["Umair  Qudus", "Muhammad  Saleem", "Axel-Cyrille Ngonga Ngomo", "Young-koo  Lee"], "year": 2021, "n_citations": 1}
{"id": 603893, "s2_id": "e7458bceb93b1bc4e2fbda10b387b2753adb42f4", "title": "High-performance Physics Simulations Using Multi-core CPUs and GPGPUs in a Volunteer Computing Context", "abstract": "This paper presents two conceptually simple methods for parallelizing a Parallel Tempering Monte Carlo simulation in a distributed volunteer computing context, where computers belonging to the general public are used. The first method uses conventional multi-threading. The second method uses CUDA, a graphics card computing system. Parallel Tempering is described, and challenges such as parallel random number generation and mapping of Monte Carlo chains to different threads are explained. While conventional multi-threading on central processing units is well-established, GPGPU programming techniques and technologies are still developing and present several challenges, such as the effective use of a relatively large number of threads. Having multiple chains in Parallel Tempering allows parallelization in a manner that is similar to the serial algorithm. Volunteer computing introduces important constraints to high performance computing, and we show that both versions of the application are able to adapt themselves to the varying and unpredictable computing resources of volunteers\u2019 computers, while leaving the machines responsive enough to use. We present experiments to show the scalable performance of these two approaches, and indicate that the efficiency of the methods increases with bigger problem sizes.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Kamran  Karimi", "Neil G. Dickson", "Firas  Hamze"], "year": 2011, "n_citations": 21}
{"id": 607254, "s2_id": "1c62800b54df77df743674724c67f15fee57f1b0", "title": "Personal Data Access Control Through Distributed Authorization", "abstract": "This paper presents an architecture of a Personal Information Management System, in which individuals can define the access to their personal data by means of smart contracts. These smart contracts, running on the Ethereum blockchain, implement access control lists and grant immutability, traceability and verifiability of the references to personal data, which is stored itself in a (possibly distributed) file system. A distributed authorization mechanism is devised, where trust from multiple network nodes is necessary to grant the access to the data. To this aim, two possible alternatives are described: a Secret Sharing scheme and Threshold Proxy Re-Encryption scheme. The performance of these alternatives is experimentally compared in terms of execution time. Threshold Proxy Re- Encryption appears to be faster in different scenarios, in particular when increasing message size, number of nodes and the threshold value, i.e. number of nodes needed to grant the data disclosure.", "venue": "2020 IEEE 19th International Symposium on Network Computing and Applications (NCA)", "authors": ["Mirko  Zichichi", "Stefano  Ferretti", "Gabriele  D'Angelo", "V\u00edctor  Rodr\u00edguez-Doncel"], "year": 2020, "n_citations": 2}
{"id": 610051, "s2_id": "cc1c3577dc0160fbd43d124dd48de25c02ba2c59", "title": "Performance of spatial Multi-LRU caching under traffic with temporal locality", "abstract": "In this work a novel family of decentralised caching policies for wireless networks is introduced, referred to as spatial multi-LRU. These improve cache-hit probability by exploiting multi-coverage. Two variations are proposed, the multi-LRU-One and -All, which differ in the number of replicas inserted in the covering edge-caches. The evaluation is done under spatial traffic that exhibits temporal locality, with varying content catalogue and dependent demands. The performance metric is hit probability and the policies are compared to (1) the single-LRU and (2) an upper bound for all centralised policies with periodic popularity updates. Numerical results show the multi-LRU policies outperform both comparison policies. The reason is their passive adaptability to popularity changes. Between the -One and -All variation, which one is preferable strongly depends on the available storage space and on traffic characteristics. The performance also depends on the popularity shape.", "venue": "2016 9th International Symposium on Turbo Codes and Iterative Information Processing (ISTC)", "authors": ["Apostolos  Avranas", "Anastasios  Giovanidis"], "year": 2016, "n_citations": 1}
{"id": 612211, "s2_id": "5eb97805262b94a1d5902563da086c28c91a79d9", "title": "On the stability of flow-aware CSMA", "abstract": "We consider a wireless network where each flow (instead of each link) runs its own CSMA (Carrier Sense Multiple Access) algorithm. Specifically, each flow attempts to access the radio channel after some random time and transmits a packet if the channel is sensed idle. We prove that, unlike the standard CSMA algorithm, this simple distributed access scheme is optimal in the sense that the network is stable for all traffic intensities in the capacity region of the network.", "venue": "Perform. Evaluation", "authors": ["Thomas  Bonald", "Mathieu  Feuillet"], "year": 2010, "n_citations": 23}
{"id": 614482, "s2_id": "b88e82b415131777605ea930fa4bf1da6dc3eba9", "title": "RCD: Rapid Close to Deadline Scheduling for datacenter networks", "abstract": "Datacenter-based Cloud Computing services provide a flexible, scalable and yet economical infrastructure to host online services such as multimedia streaming, email and bulk storage. Many such services perform geo-replication to provide necessary quality of service and reliability to users resulting in frequent large inter-datacenter transfers. In order to meet tenant service level agreements (SLAs), these transfers have to be completed prior to a deadline. In addition, WAN resources are quite scarce and costly, meaning they should be fully utilized. Several recently proposed schemes, such as B4 [1], TEMPUS [2], and SWAN [3] have focused on improving the utilization of inter-datacenter transfers through centralized scheduling, however, they fail to provide a mechanism to guarantee that admitted requests meet their deadlines. Also, in a recent study, authors propose Amoeba [4], a system that allows tenants to define deadlines and guarantees that the specified deadlines are met, however, to admit new traffic, the proposed system has to modify the allocation of already admitted transfers. In this paper, we propose Rapid Close to Deadline Scheduling (RCD), a close to deadline traffic allocation technique that is fast and efficient. Through simulations, we show that RCD is up to 15 times faster than Amoeba, provides high link utilization along with deadline guarantees, and is able to make quick decisions on whether a new request can be fully satisfied before its deadline.", "venue": "2016 World Automation Congress (WAC)", "authors": ["Mohammad  Noormohammadpour", "Cauligi S. Raghavendra", "Sriram  Rao", "Azad M. Madni"], "year": 2016, "n_citations": 10}
{"id": 620108, "s2_id": "b40f6f4c4d5f6bab88e391c0275d501e74081a56", "title": "End-to-End Delay Modeling in Buffer-Limited MANETs: A General Theoretical Framework", "abstract": "This paper focuses on a class of important two-hop relay mobile ad hoc networks (MANETs) with limited-buffer constraint and any mobility model that leads to the uniform distribution of the locations of nodes in steady state, and develops a general theoretical framework for the end-to-end (E2E) delay modeling there. We first combine the theories of fixed-point (FP), quasi-birth-and-death process, and embedded Markov chain to model the limiting distribution of the occupancy states of a relay buffer, and then apply the absorbing Markov chain theory to characterize the packet delivery process, such that a complete theoretical framework is developed for the E2E delay analysis. With the help of this framework, we derive a general and exact expression for the E2E delay based on the modeling of both packet queuing delay and delivery delay. To demonstrate the application of our framework, case studies are further provided under two network scenarios with different MAC protocols to show how the E2E delay can be analytically determined for a given network scenario. Finally, we present extensive simulation and numerical results to illustrate the efficiency of our delay analysis as well as the impacts of network parameters on delay performance.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Jia  Liu", "Min  Sheng", "Yang  Xu", "Jiandong  Li", "Xiaohong  Jiang"], "year": 2016, "n_citations": 28}
{"id": 626724, "s2_id": "66591a6e543a02b27fcae921f48d45582be04bf7", "title": "On the Correlation of Geographic and Network Proximity at Internet Edges and Its Implications for Mobile Unicast and Multicast Routing", "abstract": "Significant effort has been invested recently to accelerate handover operations in a next generation mobile Internet. Corresponding works for developing efficient mobile multicast management are emergent. Both problems simultaneously expose routing complexity between subsequent points of attachment as a characteristic parameter for handover performance in access networks. As continuous mobility handovers necessarily occur between access routers located in geographic vicinity, this paper investigates on the hypothesis that geographically adjacent edge networks attain a reduced network distances as compared to arbitrary Internet nodes. We therefore evaluate and analyze edge distance distributions in various regions for clustered IP ranges on their geographic location such as a city. We use traceroute to collect packet forwarding path and round-trip-time of each intermediate node to scan-wise derive an upper bound of the node distances. Results of different scanning origins are compared to obtain the best estimation of network distance of each pair. Our results are compared with corresponding analysis of CAIDA Skitter data, overall leading to fairly stable, reproducible edge distance distributions. As a first conclusion on expected impact on handover performance measures, our results indicate a general optimum for handover anticipation time in 802.11 networks of 25 ms.", "venue": "Sixth International Conference on Networking (ICN'07)", "authors": ["Thomas C. Schmidt", "Matthias  W\u00e4hlisch", "Ying  Zhang"], "year": 2007, "n_citations": 4}
{"id": 630619, "s2_id": "a908b2f369d96d0da1db33b6c952eb74f0eda6d7", "title": "Optimizing Memory-Access Patterns for Deep Learning Accelerators", "abstract": "Deep learning (DL) workloads are moving towards accelerators for faster processing and lower cost. Modern DL accelerators are good at handling the large-scale multiply-accumulate operations that dominate DL workloads; however, it is challenging to make full use of the compute power of an accelerator since the data must be properly staged in a software-managed scratchpad memory. Failing to do so can result in significant performance loss. This paper proposes a systematic approach which leverages the polyhedral model to analyze all operators of a DL model together to minimize the number of memory accesses. Experiments show that our approach can substantially reduce the impact of memory accesses required by common neural-network models on a homegrown AWS machine-learning inference chip named Inferentia, which is available through Amazon EC2 Inf1 instances.", "venue": "ArXiv", "authors": ["Hongbin  Zheng", "Sejong  Oh", "Huiqing  Wang", "Preston  Briggs", "Jiading  Gai", "Animesh  Jain", "Yizhi  Liu", "Rich  Heaton", "Randy  Huang", "Yida  Wang"], "year": 2020, "n_citations": 2}
{"id": 631258, "s2_id": "b3f7c63087bde8d3ca8c608cefa0c848a78c3102", "title": "Challenges in Quantitative Abstractions for Collective Adaptive Systems", "abstract": "Like with most large-scale systems, the evaluation of quantitative properties of collective adaptive systems is an important issue that crosscuts all its development stages, from design (in the case of engineered systems) to runtime monitoring and control. Unfortunately it is a difficult problem to tackle in general, due to the typically high computational cost involved in the analysis. This calls for the development of appropriate quantitative abstraction techniques that preserve most of the system's dynamical behaviour using a more compact representation. This paper focuses on models based on ordinary differential equations and reviews recent results where abstraction is achieved by aggregation of variables, reflecting on the shortcomings in the state of the art and setting out challenges for future research.", "venue": "FORECAST@STAF", "authors": ["Mirco  Tribastone"], "year": 2016, "n_citations": 1}
{"id": 633623, "s2_id": "3a1066ef7b8ce186cfe9e11ff64369f7b79660e6", "title": "Efficient Parallel Simulations of Asynchronous Cellular Arrays", "abstract": "A definition for a class of asynchronous cellular arrays is proposed. An example of such asynchrony would be independent Poisson arrivals of cell iterations. The Ising model in the continuous time formulation of Glauber falls into this class. Also proposed are efficient parallel algorithms for simulating these asynchronous cellular arrays. In the algorithms, one or several cells are assigned to a processing element (PE), local times for different PEs can be different. Although the standard serial algorithm by Metropolis, Rosenbluth, Rosenbluth, Teller, and Teller can simulate such arrays, it is usually believed to be without an efficient parallel counterpart. However, the proposed parallel algorithms contradict this belief proving to be both efficient and able to perform the same task as the standard algorithm. The results of experiments with the new algorithms are encouraging: the speed-up is greater than 16 using 25 PEs on a shared memory MIMD bus computer, and greater than 1900 using 2**14 PEs on a SIMD computer. The algorithm by Bortz, Kalos, and Lebowitz can be incorporated in the proposed parallel algorithms, further contributing to speed-up. [In this paper I invented the update-cites-of-local-time-minima parallel simulation scheme. Now the scheme is becoming popular. Many misprints of the original 1987 Complex Systems publication are corrected here.-B.L.]", "venue": "Complex Syst.", "authors": ["Boris D. Lubachevsky"], "year": 1987, "n_citations": 55}
{"id": 633760, "s2_id": "4692960b0423e297fe9404d0315a899dbcac7357", "title": "Data science and Machine learning in the Clouds: A Perspective for the Future", "abstract": "As we are fast approaching the beginning of a paradigm shift in the field of science, Data driven science (the so called fourth science paradigm) is going to be the driving force in research and innovation. From medicine to biodiversity and astronomy to geology, all these terms are somehow going to be affected by this paradigm shift. The huge amount of data to be processed under this new paradigm will be a major concern in the future and one will strongly require cloud based services in all the aspects of these computations (from storage to compute and other services). Another aspect will be energy consumption and performance of prediction jobs and tasks within such a scientific paradigm which will change the way one sees computation. Data science has heavily impacted or rather triggered the emergence of Machine Learning, Signal/Image/Video processing related algorithms, Artificial intelligence, Robotics, health informatics, geoinformatics, and many more such areas of interest. Hence, we envisage an era where Data science can deliver its promises with the help of the existing cloud based platforms and services with the addition of new services. In this article, we discuss about data driven science and Machine learning and how they are going to be linked through cloud based services in future. It also discusses the rise of paradigms like approximate computing, quantum computing and many more in recent times and their applicability in big data processing, data science, analytics, prediction and machine learning in the cloud environments.", "venue": "ArXiv", "authors": ["Hrishav Bakul Barua"], "year": 2021, "n_citations": 0}
{"id": 636420, "s2_id": "bf6ab1c5177163c7ba2c744e4f8c036465ee74ae", "title": "Clustering case statements for indirect branch predictors", "abstract": "We present an O(nlogn) algorithm to compile a switch statement into jump tables. To generate jump tables that can be efficiently predicted by current hardware branch predictors, we added an upper bound on the number of entries for each table. This modification of the previously best known algorithm reduces the complexity from O(n^2) to O(nlogn).", "venue": "ArXiv", "authors": ["Evandro  Menezes", "Sebastian  Pop", "Aditya  Kumar"], "year": 2019, "n_citations": 0}
{"id": 636825, "s2_id": "0f592c18b1382e71164b7c6120f896a6bc871273", "title": "Derandomized Load Balancing using Random Walks on Expander Graphs", "abstract": "In a computing center with a huge amount of machines, when a job arrives, a dispatcher need to decide which machine to route this job to based on limited information. A classical method, called the power-of-$d$ choices algorithm is to pick $d$ servers independently at random and dispatch the job to the least loaded server among the $d$ servers. In this paper, we analyze a low-randomness variant of this dispatching scheme, where $d$ queues are sampled through $d$ independent non-backtracking random walks on a $k$-regular graph $G$. Under certain assumptions of the graph $G$ we show that under this scheme, the dynamics of the queuing system converges to the same deterministic ordinary differential equation (ODE) for the power-of-$d$ choices scheme. We also show that the system is stable under the proposed scheme, and the stationary distribution of the system converges to the fixed point of the ODE.", "venue": "ArXiv", "authors": ["Dengwang  Tang", "Vijay G. Subramanian"], "year": 2019, "n_citations": 2}
{"id": 637654, "s2_id": "8b16ff348a190bb49ce47e92b85b16fd7dee996e", "title": "IPA in the loop: Control design for throughput regulation in computer processors", "abstract": "A new technique for performance regulation in event-driven systems, recently proposed by the authors, consists of an adaptive-gain integral control. The gain is adjusted in the control loop by a real-time estimation of the derivative of the plant-function with respect to the control input. This estimation is carried out by Infinitesimal Perturbation Analysis (IPA). The main motivation comes from applications to throughput regulation in computer processors, where to-date, the testing of the proposed control technique has been assessed by simulation. The purpose of this paper is to report on its implementation on Intel's Haswell microprocessor, and compare its performance to that obtained from cycle-level, full system simulation environment. The intrinsic contribution of the paper to the Workshop on Discrete Event Systems is in describing the process of taking an IPA-based design and simulation to a concrete implementation, thereby providing a bridge between theory and applications.", "venue": "2016 13th International Workshop on Discrete Event Systems (WODES)", "authors": ["Xinwei  Chen", "Yorai  Wardi", "Sudhakar  Yalamanchili"], "year": 2016, "n_citations": 6}
{"id": 639709, "s2_id": "cc4f97572620e1780241fe1086012f2e223fccde", "title": "Criticality of large delay tolerant networks via directed continuum percolation in space-time", "abstract": "We study delay tolerant networking (DTN) and in particular, its capacity to store, carry and forward messages to their final destination(s). We approach this broad question in the framework of percolation theory. To this end, we assume an elementary mobility model, where nodes arrive to an infinite plane according to a Poisson point process, move a certain distance \u2113, and then depart. In this setting, we characterize the mean density of nodes required to support DTN style networking. Under the given assumptions, we show that DTN communication is feasible when the mean node degree \u03bd is greater than 4 \u00b7 \u03b7<sub>c</sub>(\u03b3), where parameter \u03b3= \u2113/d is the ratio of the distance \u2113 to the transmission range d, and \u03b7<sub>c</sub>(\u03b3) is the critical reduced number density of tilted cylinders in a directed continuum percolation model. By means of Monte Carlo simulations, we give numerical values for \u03b7<sub>c</sub>(\u03b3). The asymptotic behavior of \u03b7<sub>c</sub>(\u03b3) when \u03b3 tends to \u221e is also derived from a fluid flow analysis.", "venue": "2013 Proceedings IEEE INFOCOM", "authors": ["Esa  Hyyti\u00e4", "J\u00f6rg  Ott"], "year": 2013, "n_citations": 8}
{"id": 645005, "s2_id": "321c47c50c39d68cb271eec107553b54eb8c6890", "title": "Mixed gated/exhaustive service in a polling model with\u00a0priorities", "abstract": "In this paper we consider a single-server polling system with switch-over times. We introduce a new service discipline, mixed gated/exhaustive service, that can be used for queues with two types of customers: high and low priority customers. At the beginning of a visit of the server to such a queue, a gate is set behind all customers. High priority customers receive priority in the sense that they are always served before any low priority customers. But high priority customers have a second advantage over low priority customers. Low priority customers are served according to the gated service discipline, i.e. only customers standing in front of the gate are served during this visit. In contrast, high priority customers arriving during the visit period of the queue are allowed to pass the gate and all low priority customers before the gate.We study the cycle time distribution, the waiting time distributions for each customer type, the joint queue length distribution of all priority classes at all queues at polling epochs, and the steady-state marginal queue length distributions for each customer type. Through numerical examples we illustrate that the mixed gated/exhaustive service discipline can significantly decrease waiting times of high priority jobs. In many cases there is a minimal negative impact on the waiting times of low priority customers but, remarkably, it turns out that in polling systems with larger switch-over times there can be even a positive impact on the waiting times of low priority customers.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Marko A. A. Boon", "Ivo J. B. F. Adan"], "year": 2009, "n_citations": 20}
{"id": 650272, "s2_id": "88f887767121ae36c41e1cdc04afe56d6dcb1fb2", "title": "Universal Routing in Multi-hop Radio Networks", "abstract": "In this article we introduce a new model to study stability in multi-hop wireless networks in the framework of adversarial queueing. In such a model, a routing protocol consists of three components: a transmission policy, a scheduling policy to select the packet to transmit form a set of packets parked at a node, and a hearing control mechanism to coordinate transmissions with scheduling. For such a setting, we propose a definition of universal stability that takes into account not only the scheduling policies (as in the standard wireline adversarial model), but also the transmission policies. First, we show that any scheduling policy that is unstable in the classical wireline adversarial model remains unstable in the multi-hop radio network model, even in scenarios free of inter- ferences. Then, we show that both SIS and LIS (two well-known universally stable scheduling policies in the wireline adversarial model) remain stable in the multi-hop radio network model, provided a proactive hearing control is used. In contrast, such scheduling policies turn out to be unstable when using a reactive hearing control. However, the scheduling policy LIS can be enforced to be universally stable provided ties are resolved in a permanent manner. Such a situation doesn't hold in the case of SIS, which remains unstable regardless of how ties are resolved. Furthermore, for some transmission policies which we call regular, we also show that all scheduling policies that are universally stable when using proactive hearing control (which include SIS and LIS) remain universally stable when using reactive hearing control.", "venue": "ArXiv", "authors": ["Bogdan S. Chlebus", "Vicent  Cholvi", "Dariusz R. Kowalski"], "year": 2016, "n_citations": 6}
{"id": 655363, "s2_id": "3f1ec193d060f83e49dd1e7bfcf4a5ec8a4f1b1a", "title": "Hybrid MPI-OpenMP Paradigm on SMP Clusters: MPEG-2 Encoder and N-Body Simulation", "abstract": "Clusters of SMP nodes provide support for a wide diversity of parallel programming paradigms. Combining both shared memory and message passing parallelizations within the same application, the hybrid MPI-OpenMP paradigm is an emerging trend for parallel programming to fully exploit distributed shared-memory architecture. In this paper, we improve the performance of MPEG-2 encoder and n-body simulation by employing the hybrid MPI-OpenMP programming paradigm on SMP clusters. The hierarchical image data structure of the MPEG bit-stream is eminently suitable for the hybrid model to achieve multiple levels of parallelism: MPI for parallelism at the group of pictures level across SMP nodes and OpenMP for parallelism within pictures at the slice level within each SMP node. Similarly, the work load of the force calculation which accounts for upwards of 90% of the cycles in typical computations in the n-body simulation is shared among OpenMP threads after ORB domain decomposition among MPI processes. Besides, loop scheduling of OpenMP threads is adopted with appropriate chunk size to provide better load balance of work, leading to enhanced performance. With the n-body simulation, experimental results demonstrate that the hybrid MPI-OpenMP program outperforms the corresponding pure MPI program by average factors of 1.52 on a 4-way cluster and 1.21 on a 2-way cluster. Likewise, the hybrid model offers a performance improvement of 18% compared to the MPI model for the MPEG-2 encoder.", "venue": "ArXiv", "authors": ["Truong Vinh Truong Duy", "Katsuhiro  Yamazaki", "Kosai  Ikegami", "Shigeru  Oyanagi"], "year": 2012, "n_citations": 9}
{"id": 657456, "s2_id": "684fb36e5786f6e187cd4f82877e0cd69e2c4227", "title": "CPU and GPU Accelerated Fully Homomorphic Encryption", "abstract": "Fully Homomorphic Encryption (FHE) is one of the most promising technologies for privacy protection as it allows an arbitrary number of function computations over encrypted data. However, the computational cost of these FHE systems limits their widespread applications. In this paper, our objective is to improve the performance of FHE schemes by designing efficient parallel frameworks. In particular, we choose Torus Fully Homomorphic Encryption (TFHE) [1] as it offers exact results for an infinite number of boolean gate (e.g., AND, XOR) evaluations. We first extend the gate operations to algebraic circuits such as addition, multiplication, and their vector and matrix equivalents. Secondly, we consider the multi-core CPUs to improve the efficiency of both the gate and the arithmetic operations. Finally, we port the TFHE to the Graphics Processing Units (GPU) and device novel optimizations for boolean and arithmetic circuits employing the multitude of cores. We also experimentally analyze both the CPU and GPU parallel frameworks for different numeric representations (16 to 32-bit). Our GPU implementation outperforms the existing technique [1], and it achieves a speedup of $ 20\\times$ for any 32-bit boolean operation and $ 14.5\\times$ for multiplications.", "venue": "2020 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)", "authors": ["Toufique  Morshed", "Md Momin Al Aziz", "Noman  Mohammed"], "year": 2020, "n_citations": 1}
{"id": 657510, "s2_id": "76d79c158590010f02fde1d8f84adb981100332a", "title": "Dynamic Weighted Fairness with Minimal Disruptions", "abstract": "In this paper, we consider the following dynamic fair allocation problem: Given a sequence of job arrivals and departures, the goal is to maintain an approximately fair allocation of the resource against a target fair allocation policy, while minimizing the total number of disruptions, which is the number of times the allocation of any job is changed. We consider a rich class of fair allocation policies that significantly generalize those considered in previous work. We first consider the models where jobs only arrive, or jobs only depart. We present tight upper and lower bounds for the number of disruptions required to maintain a constant approximate fair allocation every time step. In particular, for the canonical case where jobs have weights and the resource allocation is proportional to the job's weight, we show that maintaining a constant approximate fair allocation requires \u0398(log* n) disruptions per job, almost matching the bounds in prior work for the unit weight case. For the more general setting where the allocation policy only decreases the allocation to a job when new jobs arrive, we show that maintaining a constant approximate fair allocation requires \u0398(log n) disruptions per job. We then consider the model where jobs can both arrive and depart. We first show strong lower bounds on the number of disruptions required to maintain constant approximate fairness for arbitrary instances. In contrast we then show that there there is an algorithm that can maintain constant approximate fairness with O(1) expected disruptions per job if the weights of the jobs are independent of the jobs arrival and departure order. We finally show how our results can be extended to the setting with multiple resources.", "venue": "Abstracts of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems", "authors": ["Sungjin  Im", "Benjamin  Moseley", "Kamesh  Munagala", "Kirk  Pruhs"], "year": 2020, "n_citations": 0}
{"id": 663963, "s2_id": "eebfb4255337e110513c4f7cf72329f52e7cb25f", "title": "On Time Synchronization Issues in Time-Sensitive Networks with Regulators and Nonideal Clocks", "abstract": "Flow reshaping is used in time-sensitive networks (as in the context of IEEE TSN and IETF Detnet) in order to reduce burstiness inside the network and to support the computation of guaranteed latency bounds. This is performed using per-flow regulators (such as the Token Bucket Filter) or interleaved regulators (as with IEEE TSN Asynchronous Traffic Shaping, ATS). The former use one FIFO queue per flow, whereas the latter use one FIFO queue per input port. Both types of regulators are beneficial as they cancel the increase of burstiness due to multiplexing inside the network. It was demonstrated, by using network calculus, that they do not increase the worst-case latency. However, the properties of regulators were established assuming that time is perfect in all network nodes. In reality, nodes use local, imperfect clocks. Time-sensitive networks exist in two flavours: (1) in non-synchronized networks, local clocks run independently at every node and their deviations are not controlled and (2) in synchronized networks, the deviations of local clocks are kept within very small bounds using for example a synchronization protocol (such as PTP) or a satellite based geo-positioning system (such as GPS). We revisit the properties of regulators in both cases. In non-synchronized networks, we show that ignoring the timing inaccuracies can lead to network instability due to unbounded delay in per-flow or interleaved regulators. We propose and analyze two methods (rate and burst cascade, and asynchronous dual arrival-curve method) for avoiding this problem. In synchronized networks, we show that there is no instability with per-flow regulators but, surprisingly, interleaved regulators can lead to instability. To establish these results, we develop a new framework that captures industrial requirements on clocks in both non-synchronized and synchronized networks, and we develop a toolbox that extends network calculus to account for clock imperfections.", "venue": "Abstracts of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems", "authors": ["Ludovic  Thomas", "Jean-Yves  Le Boudec"], "year": 2020, "n_citations": 0}
{"id": 665868, "s2_id": "980c90c47ff31e09df2d2d30eb806e9a246ac10a", "title": "T-RACKs: A Faster Recovery Mechanism for TCP in Data Center Networks", "abstract": "Cloud interactive data-driven applications generate swarms of small TCP flows that compete for the small switch buffer space in data-center. Such applications require a small flow completion time (FCT) to be effective. Unfortunately, TCP is myopic with respect to the composite nature of application data. In addition it tends to artificially inflate the FCT of individual flows by several orders of magnitude, because of its Internet-centric design, that fixes the retransmission timeout (RTO) to be at least hundreds of milliseconds. To better understand this problem, in this paper, we use empirical measurements in a small data center testbed to study, at a microscopic level, the effects of various types of packet losses on TCP\u2019s performance. In particular, we single out packet losses that impact the tail end of small flows, as well as bursty losses that span a significant fraction of small TCP congestion windows, and show a non-negligible effect of such losses on the FCT. Based on this, we propose the so-called, timely-retransmitted ACKs (or T-RACKs), a simple loss recovery mechanism that conceals the drawbacks of the long RTO even in the presence of heavy packet losses. Interestingly enough, T-RACKS achieves this transparently to TCP itself as it does not require any change to TCP in the tenant\u2019s virtual machine (VM) or container. T-RACKs can be implemented as a software shim layer in the hypervisor between the VMs and the server\u2019s NIC or in hardware as a networking function in a SmartNIC. Simulation and real testbed results show remarkable performance improvements.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Ahmed M. Abdelmoniem", "Brahim  Bensaou"], "year": 2021, "n_citations": 1}
{"id": 665923, "s2_id": "874fe1489616063c8f2a10d209ee80058f8da62e", "title": "Optimizing the performance of Lattice Gauge Theory simulations with Streaming SIMD extensions", "abstract": "Two factors, which affect simulation quality are the amount of computing power and implementation. The Streaming SIMD (single instruction multiple data) extensions (SSE) present a technique for influencing both by exploiting the processor's parallel functionalism. In this paper, we show how SSE improves performance of lattice gauge theory simulations. We identified two significant trends through an analysis of data from various runs. The speed-ups were higher for single precision than double precision floating point numbers. Notably, though the use of SSE significantly improved simulation time, it did not deliver the theoretical maximum. There are a number of reasons for this: architectural constraints imposed by the FSB speed, the spatial and temporal patterns of data retrieval, ratio of computational to non-computational instructions, and the need to interleave miscellaneous instructions with computational instructions. We present a model for analyzing the SSE performance, which could help factor in the bottlenecks or weaknesses in the implementation, the computing architecture, and the mapping of software to the computing substrate while evaluating the improvement in efficiency. The model or framework would be useful in evaluating the use of other computational frameworks, and in predicting the benefits that can be derived from future hardware or architectural improvements.", "venue": "ArXiv", "authors": ["Shyam  Srinivasan"], "year": 2013, "n_citations": 0}
{"id": 669701, "s2_id": "27fa0d108e54ca1188245c91752332a20749326e", "title": "Dimensioning of PA for massive MIMO system with load adaptive number of antennas", "abstract": "This paper takes into consideration the non-ideal efficiency characteristics of realistic power amplifiers (PAs) along with the daily traffic profile in order to investigate the impact of PA dimensioning on the energy efficiency (EE) of load adaptive massive MIMO system. A multicellular system has been considered where each base station (BS) is equipped with a large number of antennas to serve many single antenna users. For a given number of users in a cell, the optimum number of active antennas maximizing EE has been derived where total BS downlink power is assumed to be fixed. Under the same assumption, the PAs have been dimensioned in a way that maximizes network EE not only for a single time snapshot but over twenty four hours of operation while considering dynamic efficiency characteristics of the PAs. In order to incorporate this daily load profile, each BS has been modeled as an M/G/m/m state dependent queue under the assumption that the network is dimensioned to serve a maximum number of users at a time corresponding to 100% cell traffic load. This load adaptive system along with the optimized PA dimensioning achieves 30% higher energy efficiency compared to a base line system where the BSs always run with a fixed number of active antennas which are most energy efficient while serving 100% traffic load.", "venue": "2014 IEEE Globecom Workshops (GC Wkshps)", "authors": ["M. M. Aftab Hossain", "Riku  J\u00e4ntti", "Cicek  Cavdar"], "year": 2014, "n_citations": 6}
{"id": 670007, "s2_id": "bb42883b0b696be257e35201d0e049dc855d6a9e", "title": "Delay-Optimal Policies in Partial Fork-Join Systems with Redundancy and Random Slowdowns", "abstract": "We consider a large distributed service system consisting of n homogeneous servers with infinite capacity FIFO queues. Jobs arrive as a Poisson process of rate \u03bb n/kn (for some positive constant \u03bb and integer kn). Each incoming job consists of kn identical tasks that can be executed in parallel, and that can be encoded into at least kn \"replicas\" of the same size (by introducing redundancy) so that the job is considered to be completed when any kn replicas associated with it finish their service. Moreover, we assume that servers can experience random slowdowns in their processing rate so that the service time of a replica is the product of its size and a random slowdown. First, we assume that the server slowdowns are shifted exponential and independent of the replica sizes. In this setting we show that the delay of a typical job is asymptotically minimized (as n\\\u2192\\\u221e) when the number of replicas per task is a constant that only depends on the arrival rate \u03bb, and on the expected slowdown of servers. Second, we introduce a new model for the server slowdowns in which larger tasks experience less variable slowdowns than smaller tasks. In this setting we show that, under the class of policies where all replicas start their service at the same time, the delay of a typical job is asymptotically minimized (as n\\\u2192\\\u221e) when the number of replicas per task is made to depend on the actual size of the tasks being replicated, with smaller tasks being replicated more than larger tasks.", "venue": "Abstracts of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems", "authors": ["Martin  Zubeldia"], "year": 2020, "n_citations": 3}
{"id": 671020, "s2_id": "6921f982a26fbc498f723703039590fd7e3239cd", "title": "Solaris System Resource Manager: All I Ever Wanted Was My Unfair Advantage (And Why You Can't Have It!)", "abstract": "Traditional UNIX time-share schedulers attempt to be fair to all users by employing a round-robin style algorithm for allocating CPU time. Unfortunately, a loophole exists whereby the scheduler can be biased in favor of a greedy user running many short CPU-time processes. This loophole is not a defect but an intrinsic property of the round-robin scheduler that ensures responsiveness to the short CPU demands associated with multiple interactive users. A new generation of UNIX system resource management software constrains the scheduler to be equitable to all users regardless of the number of processes each may be running. This \"fair-share\" scheduling draws on the concept of pro rating resource \"shares\" across users and groups and then dynamically adjusting CPU usage to meet those share proportions. The simple notion of statically allocating these shares, however, belies the potential consequences for performance as measured by user response time and service level targets. We demonstrate this point by modeling several simple share allocation scenarios and analyzing the corresponding performance effects. A brief comparison of commercial system resource management implementations from HP, IBM, and SUN is also given.", "venue": "ArXiv", "authors": ["Neil J. Gunther"], "year": 2000, "n_citations": 0}
{"id": 672349, "s2_id": "948467007d5389f2a60c59a302dd3bbbccc71083", "title": "Performance optimizations for scalable CFD applications on hybrid CPU+MIC heterogeneous computing system with millions of cores", "abstract": "Abstract For computational fluid dynamics (CFD) applications with a large number of grid points/cells, parallel computing is a common efficient strategy to reduce the computational time. How to achieve the best performance in the modern supercomputer system, especially with heterogeneous computing resources such as hybrid CPU+GPU, or a CPU + Intel Xeon Phi (MIC) co-processors, is still a great challenge.An in-house parallel CFD code capable of simulating three dimensional structured grid applications is developed and tested in this study. Several methods of parallelization, performance optimization and code tuning both in the CPU-only homogeneous system and in the heterogeneous system are proposed based on identifying potential parallelism of applications, balancing the work load among all kinds of computing devices, tuning the multi-thread code toward better performance in intra-machine node with hundreds of CPU/MIC cores, and optimizing the communication among inter-nodes, inter-cores, and between CPUs and MICs.Some benchmark cases from model and/or industrial CFD applications are tested on the Tianhe-1A and Tianhe-2 supercomputer to evaluate the performance. Among these CFD cases, the maximum number of grid cells reached 780 billion. The tuned solver successfully scales up to half of the entire Tianhe-2 supercomputer system with over 1.376 million of heterogeneous cores. The test results and performance analysis are discussed in detail.", "venue": "Computers & Fluids", "authors": ["Yong-Xian  Wang", "Lilun  Zhang", "Wei  Liu", "Xinghua  Cheng", "Yu  Zhuang", "Anthony T. Chronopoulos"], "year": 2018, "n_citations": 13}
{"id": 672695, "s2_id": "a76dbb923f3698dc51a4ac4322ffd67b4f56093a", "title": "Performance analysis of random linear network coding in two-source single-relay networks", "abstract": "This paper considers the multiple-access relay channel in a setting where two source nodes transmit packets to a destination node, both directly and via a relay node, over packet erasure channels. Intra-session network coding is used at the source nodes and inter-session network coding is employed at the relay node to combine the recovered source packets of both source nodes. In this work, we investigate the performance of the network-coded system in terms of the probability that the destination node will successfully recover the source packets of the two source nodes. We build our analysis on fundamental probability expressions for random matrices over finite fields and we derive upper bounds on the system performance for the case of systematic and non-systematic network coding. Simulation results show that the upper bounds are very tight and accurately predict the decoding probability at the destination node. Our analysis also exposes the clear benefits of systematic network coding at the source nodes compared to non-systematic transmission.", "venue": "2015 IEEE International Conference on Communication Workshop (ICCW)", "authors": ["Amjad Saeed Khan", "Ioannis  Chatzigeorgiou"], "year": 2015, "n_citations": 12}
{"id": 678397, "s2_id": "3112e58d4e37cdf098bc5313a9ebaf1af9f89c4d", "title": "Throughput capacity of two-hop relay MANETs under finite buffers", "abstract": "Since the seminal work of Grossglauser and Tse [1], the two-hop relay algorithm and its variants have been attractive for mobile ad hoc networks (MANETs) due to their simplicity and efficiency. However, most literature assumed an infinite buffer size for each node, which is obviously not applicable to a realistic MANET. In this paper, we focus on the exact throughput capacity study of two-hop relay MANETs under the practical finite relay buffer scenario. The arrival process and departure process of the relay queue are fully characterized, and an ergodic Markov chain-based framework is also provided. With this framework, we obtain the limiting distribution of the relay queue and derive the throughput capacity under any relay buffer size. Extensive simulation results are provided to validate our theoretical framework and explore the relationship among the throughput capacity, the relay buffer size and the number of nodes.", "venue": "2014 IEEE 25th Annual International Symposium on Personal, Indoor, and Mobile Radio Communication (PIMRC)", "authors": ["Jia  Liu", "Min  Sheng", "Yang  Xu", "Hongguang  Sun", "Xijun  Wang", "Xiaohong  Jiang"], "year": 2014, "n_citations": 2}
{"id": 682231, "s2_id": "d8df456f790381f4ddb388be24a546625bd75ee2", "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks", "abstract": "The recent Natural Language Processing techniques have been refreshing the stateof-the-art performance at an incredible speed. Training huge language models is therefore an imperative demand in both industry and academy. However, the huge models impose challenges to both hardware and software. Graphical processing units (GPUs) are iterated frequently to meet the exploding demand, and a variety of ASICs like TPUs are spawned. However, there is still a tension between the fast growth of the extremely huge models and fact that Moore\u2019s law is approaching the end. To this end, many model parallelism techniques are proposed to distribute the model parameters to multiple devices, so as to alleviate the tension on both memory and computation. Our work is the first to introduce a 3-dimensional model parallelism for expediting huge language models. By reaching a perfect load balance, our approach presents smaller memory and communication cost than existing state-of-the-art 1-D and 2-D model parallelism. Our experiments on 64 TACC\u2019s V100 GPUs show that our 3-D parallelism outperforms the 1-D and 2-D parallelism with 2.32X and 1.57X speedup, respectively.", "venue": "ArXiv", "authors": ["Zhengda  Bian", "Qifan  Xu", "Boxiang  Wang", "Yang  You"], "year": 2021, "n_citations": 2}
{"id": 683752, "s2_id": "423d3345144058841aeca552d9e4b26be9be3739", "title": "Fast GPGPU Data Rearrangement Kernels using CUDA", "abstract": "Many high performance-computing algorithms are bandwidth limited, hence the need for optimal data rearrangement kernels as well as their easy integration into the rest of the application. In this work, we have built a CUDA library of fast kernels for a set of data rearrangement operations. In particular, we have built generic kernels for rearranging m dimensional data into n dimensions, including Permute, Reorder, Interlace/De-interlace, etc. We have also built kernels for generic Stencil computations on a two-dimensional data using templates and functors that allow application developers to rapidly build customized high performance kernels. All the kernels built achieve or surpass best-known performance in terms of bandwidth utilization.", "venue": "ArXiv", "authors": ["Michael  Bader", "Hans-Joachim  Bungartz", "Dheevatsa  Mudigere", "Srihari  Narasimhan", "Babu  Narayanan"], "year": 2010, "n_citations": 13}
{"id": 691095, "s2_id": "c496a3929fa6eb635f4665dbfdd53a48fac9b600", "title": "Non-Asymptotic Performance Analysis of Size-Based Routing Policies", "abstract": "We investigate the performance of two size-based routing policies: the Size Interval Task Assignment (SITA) and Task Assignment based on Guessing Size (TAGS). We consider a system with two servers and Bounded Pareto distributed job sizes with tail parameter 1 where the difference between the size of the largest and the smallest job is finite. We show that the ratio between the mean waiting time of TAGS over the mean waiting time of SITA is unbounded when the largest job size is large and the arrival rate times the largest job size is less than one. We provide numerical experiments that show that our theoretical findings extend to Bounded Pareto distributed job sizes with tail parameter different to 1.", "venue": "2020 28th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)", "authors": ["Eitan  Bachmat", "Josu  Doncel"], "year": 2020, "n_citations": 1}
{"id": 692328, "s2_id": "ec5d645769b771be5c66c89294b147bf16bc7a11", "title": "PPT-SASMM: Scalable Analytical Shared Memory Model: Predicting the Performance of Multicore Caches from a Single-Threaded Execution Trace", "abstract": "Performance modeling of parallel applications on multicore processors remains a challenge in computational co-design due to multicore processors\u2019 complex design. Multicores include complex private and shared memory hierarchies. We present a Scalable Analytical Shared Memory Model (SASMM). SASMM can predict the performance of parallel applications running on a multicore. SASMM uses a probabilistic and computationally-efficient method to predict the reuse distance profiles of caches in multicores. SASMM relies on a stochastic, static basic block-level analysis of reuse profiles. The profiles are calculated from the memory traces of applications that run sequentially rather than using multi-threaded traces. The experiments show that our model can predict private L1 cache hit rates with 2.12% and shared L2 cache hit rates with about 1.50% error rate.", "venue": "MEMSYS", "authors": ["Atanu  Barai", "Gopinath  Chennupati", "Nandakishore  Santhi", "Abdel-Hameed  Badawy", "Yehia  Arafa", "Stephan  Eidenbenz"], "year": 2020, "n_citations": 4}
{"id": 694560, "s2_id": "ad5cd25b229bf551e53126c5afa6475258e695a5", "title": "Improving the Performance of WLANs by Reducing Unnecessary Active Scans", "abstract": "We consider the problem of excessive and unnecessary active scans in heavily utilized WLANs during which low rate probe requests and responses are broadcast. These management frames severely impact the goodput. Our analysis of two production WLANs reveals that lesser number of non-overlapping channels in $2.4$ GHz makes it more prone to the effects of increased probe frames than $5$ GHz. We find that not only up to $90$% of probe responses carry redundant information but the probe traffic can be as high as $60$\\% of the management traffic. Furthermore, active scanning severely impacts real-time applications at a client as it increases the latency by $91$ times. \nWe present a detailed analysis of the impact of active scans on an individual client and the whole network. We discuss three ways to control the probe traffic in production WLANs -- access point configurations, network planning, and client modification. Our proposals for access point configuration are in line with current WLAN deployments, better network planning is device agnostic in nature, and client modification reduces the average number of probe requests per client by up to $50$% without hampering the ongoing WiFi connection.", "venue": "ArXiv", "authors": ["Dheryta  Jaisinghani", "Vinayak S. Naik", "Sanjit K. Kaul", "Rajesh  Balan", "Sumit  Roy"], "year": 2018, "n_citations": 3}
{"id": 701303, "s2_id": "f5574f5c3ffda6249cf9eefb452333d876296fd3", "title": "Performance Considerations for Gigabyte per Second Transcontinental Disk-to-Disk File Transfers", "abstract": "Moving data from CERN to Pasadena at a gigabyte per second using the next generation Internet requires good networking and good disk IO. Ten Gbps Ethernet and OC192 links are in place, so now it is simply a matter of programming. This report describes our preliminary work and measurements in configuring the disk subsystem for this effort. Using 24 SATA disks at each endpoint we are able to locally read and write an NTFS volume is striped across 24 disks at 1.2 GBps. A 32-disk stripe delivers 1.7 GBps. Experiments on higher performance and higher-capacity systems deliver up to 3.5 GBps.", "venue": "ArXiv", "authors": ["Peter  Kukol", "Jim  Gray"], "year": 2005, "n_citations": 1}
{"id": 703074, "s2_id": "8637ffe6c628e956cb70c517aee24e59ccb5b939", "title": "Efficient Resource Sharing Through GPU Virtualization on Accelerated High Performance Computing Systems", "abstract": "The High Performance Computing (HPC) field is witnessing a widespread adoption of Graphics Processing Units (GPUs) as co-processors for conventional homogeneous clusters. The adoption of prevalent Single- Program Multiple-Data (SPMD) programming paradigm for GPU-based parallel processing brings in the challenge of resource underutilization, with the asymmetrical processor/co-processor distribution. In other words, under SPMD, balanced CPU/GPU distribution is required to ensure full resource utilization. In this paper, we propose a GPU resource virtualization approach to allow underutilized microprocessors to effi- ciently share the GPUs. We propose an efficient GPU sharing scenario achieved through GPU virtualization and analyze the performance potentials through execution models. We further present the implementation details of the virtualization infrastructure, followed by the experimental analyses. The results demonstrate considerable performance gains with GPU virtualization. Furthermore, the proposed solution enables full utilization of asymmetrical resources, through efficient GPU sharing among microprocessors, while incurring low overhead due to the added virtualization layer.", "venue": "ArXiv", "authors": ["Teng  Li", "Vikram K. Narayana", "Tarek A. El-Ghazawi"], "year": 2015, "n_citations": 0}
{"id": 704241, "s2_id": "c8c4a9d7e70a6cf910eea02942ea493b08b51972", "title": "On Performance Modeling for MANETs Under General Limited Buffer Constraint", "abstract": "Understanding the real achievable performance of mobile ad hoc networks (MANETs) under practical network constraints is of great importance for their applications in future highly heterogeneous wireless network environments. This paper explores, for the first time, the performance modeling for MANETs under a general limited buffer constraint, where each network node maintains a limited source buffer of size $B_s$  to store its locally generated packets and also a limited shared relay buffer of size  $B_r$ to store relay packets for other nodes. Based on the Queuing theory and birth-death chain theory, we first develop a general theoretical framework to fully depict the source/relay buffer occupancy process in such a MANET, which applies to any distributed MAC protocol and any mobility model that leads to the uniform distribution of nodes\u2019 locations in steady state. With the help of this framework, we then derive the exact expressions of several key network performance metrics, including achievable throughput, throughput capacity, and expected end-to-end delay. We further conduct case studies under two network scenarios and provide the corresponding theoretical/simulation results to demonstrate the application as well as the efficiency of our theoretical framework. Finally, we present extensive numerical results to illustrate the impacts of buffer constraint on the performance of a buffer-limited MANET.", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Jia  Liu", "Yang  Xu", "Yulong  Shen", "Xiaohong  Jiang", "Tarik  Taleb"], "year": 2017, "n_citations": 13}
{"id": 717946, "s2_id": "7f81a1c543e07f892fe10d00e1781eace1592f67", "title": "Page Table Management for Heterogeneous Memory Systems", "abstract": "Modern enterprise servers are increasingly embracing tiered memory systems with a combination of low latency DRAMs and large capacity but high latency non-volatile main memories (NVMMs) such as Intel\u2019s Optane DC PMM. Prior works have focused on efficient placement and migration of data on a tiered memory system, but have not studied the optimal placement of page tables. Explicit and efficient placement of page tables is crucial for large memory footprint applications with high TLB miss rates because they incur dramatically higher page walk latency when page table pages are placed in NVMM. We show that (i) page table pages can end up on NVMM even when enough DRAM memory is available and (ii) page table pages that spill over to NVMM due to DRAM memory pressure are not migrated back later when memory is available in DRAM. We study the performance impact of page table placement in a tiered memory system and propose an efficient and transparent page table management technique that (i) applies different placement policies for data and page table pages, (ii) introduces a differentiating policy for page table pages by placing a small but critical part of the page table in DRAM, and (iii) dynamically and judiciously manages the rest of the page table by transparently migrating the page table pages between DRAM and NVMM. Our implementation on a real system equipped with Intel\u2019s Optane NVMM running Linux reduces the page table walk cycles by 12% and total cycles by 20% on an average. This improves the runtime by 20% on an average for a set of synthetic and real-world large memory footprint applications when compared with various default Linux kernel techniques.", "venue": "ArXiv", "authors": ["Sandeep  Kumar", "Aravinda  Prasad", "Smruti R. Sarangi", "Sreenivas  Subramoney"], "year": 2021, "n_citations": 0}
{"id": 720362, "s2_id": "846535fb609a98b57edfa0d617af3d1037235efd", "title": "Cloud Benchmarking for Performance", "abstract": "How can applications be deployed on the cloud to achieve maximum performance? This question has become significant and challenging with the availability of a wide variety of Virtual Machines (VMs) with different performance capabilities in the cloud. The above question is addressed by proposing a six step benchmarking methodology in which a user provides a set of four weights that indicate how important each of the following groups: memory, processor, computation and storage are to the application that needs to be executed on the cloud. The weights along with cloud benchmarking data are used to generate a ranking of VMs that can maximise performance of the application. The rankings are validated through an empirical analysis using two case study applications, the first is a financial risk application and the second is a molecular dynamics simulation, which are both representative of workloads that can benefit from execution on the cloud. Both case studies validate the feasibility of the methodology and highlight that maximum performance can be achieved on the cloud by selecting the top ranked VMs produced by the methodology.", "venue": "2014 IEEE 6th International Conference on Cloud Computing Technology and Science", "authors": ["Blesson  Varghese", "Ozgur  Akgun", "Ian  Miguel", "Long  Thai", "Adam  Barker"], "year": 2014, "n_citations": 30}
{"id": 721679, "s2_id": "10839aa0efe777a6c90faa0571419c8243bb0e84", "title": "AI Matrix: A Deep Learning Benchmark for Alibaba Data Centers", "abstract": "Alibaba has China's largest e-commerce platform. To support its diverse businesses, Alibaba has its own large-scale data centers providing the computing foundation for a wide variety of software applications. Among these applications, deep learning (DL) has been playing an important role in delivering services like image recognition, objection detection, text recognition, recommendation, and language processing. To build more efficient data centers that deliver higher performance for these DL applications, it is important to understand their computational needs and use that information to guide the design of future computing infrastructure. An effective way to achieve this is through benchmarks that can fully represent Alibaba's DL applications.", "venue": "ArXiv", "authors": ["Wei  Zhang", "Wei  Wei", "Lingjie  Xu", "Lingling  Jin", "Cheng  Li"], "year": 2019, "n_citations": 8}
{"id": 723379, "s2_id": "ab03872d11c0606175b7d7a7d86f9ffa4980597e", "title": "Low-rank Tensor Decomposition for Compression of Convolutional Neural Networks Using Funnel Regularization", "abstract": "Tensor decomposition is one of the fundamental technique for model compression of deep convolution neural networks owing to its ability to reveal the latent relations among complex structures. However, most existing methods compress the networks layer by layer, which cannot provide a satisfactory solution to achieve global optimization. In this paper, we proposed a model reduction method to compress the pre-trained networks using low-rank tensor decomposition of the convolution layers. Our method is based on the optimization techniques to select the proper ranks of decomposed network layers. A new regularization method, called funnel function, is proposed to suppress the unimportant factors during the compression, so the proper ranks can be revealed much easier. The experimental results show that our algorithm can reduce more model parameters than other tensor compression methods. For ResNet18 with ImageNet2012, our reduced model can reach more than 2 times speed up in terms of GMAC with merely 0.7% Top-1 accuracy drop, which outperforms most existing methods in both metrics.", "venue": "ArXiv", "authors": ["Bo-Shiuan  Chu", "Che-Rung  Lee"], "year": 2021, "n_citations": 0}
{"id": 724909, "s2_id": "afe2c79a95c710f6539269c8ca17c63911f1c4d6", "title": "Performance Tuning and Scaling Enterprise Blockchain Applications", "abstract": "Blockchain scalability can be complicated and costly. As enterprises begin to adopt blockchain technology to solve business problems, there are valid concerns if blockchain applications can support the transactional demands of production systems. In fact, the multiple distributed components and protocols that underlie blockchain applications makes performance optimization a non-trivial task. Blockchain performance optimization and scalability require a methodology to reduce complexity and cost. Furthermore, existing performance results often lack the requirements, load, and infrastructure of a production application. In this paper, we first develop a methodical approach to performance tuning enterprise blockchain applications to increase performance and transaction capacity. The methodology is applied to an enterprise blockchain-based application (leveraging Hyperledger Fabric) for performance tuning and optimization with the goal of bridging the gap between laboratory and production deployed system performance. We then present extensive results and analysis of our performance testing for on-premise and cloud deployments, in which we were able to scale the application from 30 to 3000 TPS without forking the Hyperledger Fabric source code and maintaining a reasonable infrastructure footprint. We also provide blockchain application and platform recommendations for performance improvement.", "venue": "ArXiv", "authors": ["Grant  Chung", "Luc  Desrosiers", "Manav  Gupta", "Andrew  Sutton", "Kaushik  Venkatadri", "Ontak  Wong", "Goran  Zugic"], "year": 2019, "n_citations": 9}
{"id": 725089, "s2_id": "e07100ad7bc165f4b758f78dce1d3b6bb38dcfa3", "title": "Forward Correction and Fountain Codes in Delay-Tolerant Networks", "abstract": "Delay-tolerant ad hoc networks leverage the mobility of relay nodes to compensate for lack of permanent connectivity and thus enable communication between nodes that are out of range of each other. To decrease delivery delay, the information to be delivered is replicated in the network. Our objective in this paper is to study a class of replication mechanisms that include coding in order to improve the probability of successful delivery within a given time limit. We propose an analytical approach that allows to quantify tradeoffs between resources and performance measures (energy and delay). We study the effect of coding on the performance of the network while optimizing parameters that govern routing. Our results, based on fluid approximations, are compared to simulations that validate the model.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Eitan  Altman", "Francesco De Pellegrini"], "year": 2011, "n_citations": 79}
{"id": 730455, "s2_id": "a730ff66f109c462c5c808e9193799d68000aac0", "title": "On the Power-of-d-choices with Least Loaded Server Selection", "abstract": "Motivated by distributed schedulers that combine the power-of-d-choices with late binding and systems that use replication with cancellation-on-start, we study the performance of the LL(d) policy which assigns a job to a server that currently has the least workload among d randomly selected servers in large-scale homogeneous clusters.\n We consider general job size distributions and propose a partial integro-differential equation to describe the evolution of the system. This equation relies on the earlier proven ansatz for LL(d) which asserts that the workload distribution of any finite set of queues becomes independent of one another as the number of servers tends to infinity. Based on this equation we propose a fixed point iteration for the limiting workload distribution and study its convergence.", "venue": "SIGMETRICS", "authors": ["Tim  Hellemans", "Benny Van Houdt"], "year": 2018, "n_citations": 3}
{"id": 731933, "s2_id": "70792164858afe0f590a40bb4d3e97c6155ab6a5", "title": "Pre-Defined Sparsity for Low-Complexity Convolutional Neural Networks", "abstract": "The high energy cost of processing deep convolutional neural networks impedes their ubiquitous deployment in energy-constrained platforms such as embedded systems and IoT devices. This article introduces convolutional layers with pre-defined sparse 2D kernels that have support sets that repeat periodically within and across filters. Due to the efficient storage of our periodic sparse kernels, the parameter savings can translate into considerable improvements in energy efficiency due to reduced DRAM accesses, thus promising significant improvements in the trade-off between energy consumption and accuracy for both training and inference. To evaluate this approach, we performed experiments with two widely accepted datasets, CIFAR-10 and Tiny ImageNet in sparse variants of the ResNet18 and VGG16 architectures. Compared to baseline models, our proposed sparse variants require up to <inline-formula><tex-math notation=\"LaTeX\">$\\mathord {\\sim }82\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>\u223c</mml:mo><mml:mn>82</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"kundu-ieq1-2972520.gif\"/></alternatives></inline-formula> fewer model parameters with <inline-formula><tex-math notation=\"LaTeX\">$5.6\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>5</mml:mn><mml:mo>.</mml:mo><mml:mn>6</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"kundu-ieq2-2972520.gif\"/></alternatives></inline-formula> fewer FLOPs with negligible loss in accuracy for ResNet18 on CIFAR-10. For VGG16 trained on Tiny ImageNet, our approach requires <inline-formula><tex-math notation=\"LaTeX\">$5.8 \\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>5</mml:mn><mml:mo>.</mml:mo><mml:mn>8</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"kundu-ieq3-2972520.gif\"/></alternatives></inline-formula> fewer FLOPs and up to <inline-formula><tex-math notation=\"LaTeX\">$\\mathord {\\sim }83.3\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>\u223c</mml:mo><mml:mn>83</mml:mn><mml:mo>.</mml:mo><mml:mn>3</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"kundu-ieq4-2972520.gif\"/></alternatives></inline-formula> fewer model parameters with a drop in top-5 (top-1) accuracy of only 1.2% (<inline-formula><tex-math notation=\"LaTeX\">$\\mathord {\\sim }2.1\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>\u223c</mml:mo><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mn>1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"kundu-ieq5-2972520.gif\"/></alternatives></inline-formula>). We also compared the performance of our proposed architectures with that of ShuffleNet and MobileNetV2. Using similar hyperparameters and FLOPs, our ResNet18 variants yield an average accuracy improvement of <inline-formula><tex-math notation=\"LaTeX\">$\\mathord {\\sim }2.8\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>\u223c</mml:mo><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mn>8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"kundu-ieq6-2972520.gif\"/></alternatives></inline-formula>.", "venue": "IEEE Transactions on Computers", "authors": ["Souvik  Kundu", "Mahdi  Nazemi", "Massoud  Pedram", "Keith M. Chugg", "Peter A. Beerel"], "year": 2020, "n_citations": 10}
{"id": 732701, "s2_id": "7fb44f61555c1b8505314ac94f86bce4f905a7ed", "title": "RIOT OS Paves the Way for Implementation of High-performance MAC Protocols", "abstract": "Implementing new, high-performance MAC protocols requires real-time features, to be able to synchronize correctly between different unrelated devices. Such features are highly desirable for operating wireless sensor networks (WSN) that are designed to be part of the Internet of Things (IoT). Unfortunately, the operating systems commonly used in this domain cannot provide such features. On the other hand, \"bare-metal\" development sacrifices portability, as well as the mul-titasking abilities needed to develop the rich applications that are useful in the domain of the Internet of Things. We describe in this paper how we helped solving these issues by contributing to the development of a port of RIOT OS on the MSP430 microcontroller, an architecture widely used in IoT-enabled motes. RIOT OS offers rich and advanced real-time features, especially the simultaneous use of as many hardware timers as the underlying platform (microcontroller) can offer. We then demonstrate the effectiveness of these features by presenting a new implementation, on RIOT OS, of S-CoSenS, an efficient MAC protocol that uses very low processing power and energy.", "venue": "SENSORNETS", "authors": ["Kevin  Roussel", "Yeqiong  Song", "Olivier  Zendra"], "year": 2015, "n_citations": 8}
{"id": 733925, "s2_id": "9f35c25377f322d975527bfe070c535c1284e659", "title": "Modeling Impact of Human Errors on the Data Unavailability and Data Loss of Storage Systems", "abstract": "Data storage systems (DSSs) and their availability play a crucial role in contemporary datacenters. Despite using mechanisms such as automatic failover in datacenters, the role of human agents and consequently their destructive errors is inevitable. Due to very large number of disk drives used in exascale datacenters and their high failure rates, the disk subsystem in storage systems has become a major source of <italic>data unavailability</italic> (DU) and <italic>data loss</italic> (DL) initiated by human errors. In this paper, we investigate the effect of <italic>incorrect disk replacement service</italic> (IDRS) on the availability and reliability of DSSs. To this end, we analyze the consequences of IDRS in a disk array, and conduct Monte\u00a0Carlo simulations to evaluate DU and DL during mission time. The proposed modeling framework can cope with different storage array configurations and <italic> data object survivability</italic>, representing the effect of system-level redundancies such as remote backups and mirrors. In the proposed framework, the model parameters are obtained from industrial and scientific reports alongside field data, which have been extracted from a datacenter operating with 70 storage racks. The results show that ignoring the impact of IDRS leads to unavailability underestimation by up to three orders of magnitude. Moreover, our study suggests that by considering the effect of human errors, the conventional beliefs about the dependability of different <italic>redundant array of independent disks</italic> (RAID) mechanisms should be revised. The results show that <inline-formula><tex-math notation=\"LaTeX\">$\\text{RAID}1$</tex-math></inline-formula> can result in lower availability compared to <inline-formula><tex-math notation=\"LaTeX\">$\\text{RAID}5$</tex-math></inline-formula> in the presence of human errors. The results also show that employing automatic fail-over policy (using hot spare disks) can reduce the drastic impacts of human errors by two orders of magnitude.", "venue": "IEEE Transactions on Reliability", "authors": ["Mostafa  Kishani", "Hossein  Asadi"], "year": 2018, "n_citations": 10}
{"id": 735933, "s2_id": "dc83b076b684da92c8f1c7e23c2f4fdeb84b4303", "title": "Distributed Semantic Web Data Management in HBase and MySQL Cluster", "abstract": "Various computing and data resources on the Web are being enhanced with machine-interpretable semantic descriptions to facilitate better search, discovery and integration. This interconnected metadata constitutes the Semantic Web, whose volume can potentially grow the scale of the Web. Efficient management of Semantic Web data, expressed using the W3C's Resource Description Framework (RDF), is crucial for supporting new data-intensive, semantics-enabled applications. In this work, we study and compare two approaches to distributed RDF data management based on emerging cloud computing technologies and traditional relational database clustering technologies. In particular, we design distributed RDF data storage and querying schemes for HBase and MySQL Cluster and conduct an empirical comparison of these approaches on a cluster of commodity machines using datasets and queries from the Third Provenance Challenge and Lehigh University Benchmark. Our study reveals interesting patterns in query evaluation, shows that our algorithms are promising, and suggests that cloud computing has a great potential for scalable Semantic Web data management.", "venue": "2011 IEEE 4th International Conference on Cloud Computing", "authors": ["Craig  Franke", "Samuel  Morin", "Artem  Chebotko", "John  Abraham", "Pearl  Brazier"], "year": 2011, "n_citations": 55}
{"id": 740304, "s2_id": "cbe171db8890a4574d9b87cb178fd37c726acf26", "title": "Estimate The Efficiency Of Multiprocessor\u2019s Cash Memory Work Algorithms", "abstract": "Many computer systems for calculating the proper organization of memory are among the most critical issues. Using a tier cache memory (along with branching prediction) is an effective means of increasing modern multi-core processors' performance. Designing high-performance processors is a complex task and requires preliminary verification and analysis of the model level, usually used in analytical and simulation modeling. The refinement of extreme programming is an unfortunate challenge. Few experts disagree with the synthesis of access points. This article demonstrates that Internet QoS and 16-bit architectures are always incompatible, but it's the same situation for write-back caches. The solution to this problem can be implemented by analyzing simulation models of different complexity in combination with the analytical evaluation of individual algorithms. This work is devoted to designing a multi-parameter simulation model of a multi-process for evaluating the performance of cache memory algorithms and the optimality of the structure. Optimization of the structures and algorithms of the cache memory allows you to accelerate the interaction of the memory process and improve the performance of the entire system.", "venue": "2021 IEEE International Conference on Smart Information Systems and Technologies (SIST)", "authors": ["Mohamed A. Hamada", "Abdelrahman  Abdallah"], "year": 2021, "n_citations": 0}
{"id": 742319, "s2_id": "654f13983650b67b23132d99a5ea513e39d498c9", "title": "ECM modeling and performance tuning of SpMV and Lattice QCD on A64FX", "abstract": "The A64FX CPU is arguably the most powerful Arm-based processor design to date. Although it is a traditional cache-based multicore processor, its peak performance and memory bandwidth rival accelerator devices. A good understanding of its performance features is of paramount importance for developers who wish to leverage its full potential. We present an architectural analysis of the A64FX used in the Fujitsu FX1000 supercomputer at a level of detail that allows for the construction of Execution-Cache-Memory (ECM) performance models for steady-state loops. In the process we identify architectural peculiarities that point to viable generic optimization strategies. After validating the model using simple streaming loops we apply the insight gained to sparse matrix-vector multiplication (SpMV) and the domain wall (DW) kernel from quantum chromodynamics (QCD). For SpMV we show why the CRS matrix storage format is not a good practical choice on this architecture and how the SELL-Cformat can achieve bandwidth saturation. For the DW kernel we provide a cache-reuse analysis and show how an appropriate choice of data layout for complex arrays can realize memory-bandwidth saturation in this case as well. A comparison with state-of-the-art high-end Intel Cascade Lake AP and Nvidia V100 systems puts the capabilities of the A64FX into perspective. We also explore the potential for power optimizations using the tuning knobs provided by the Fugaku system, achieving energy savings of about 31% for SpMV and 18% for DW.", "venue": "ArXiv", "authors": ["Christie  Alappat", "Nils  Meyer", "Jan  Laukemann", "Thomas  Gruber", "Georg  Hager", "Gerhard  Wellein", "Tilo  Wettig"], "year": 2021, "n_citations": 1}
{"id": 746346, "s2_id": "a2c72eefe6087837bfa8730d305a957947ba9f61", "title": "Enhanced cluster computing performance through proportional fairness", "abstract": "The performance of cluster computing depends on how concurrent jobs share multiple data center resource types like CPU, RAM and disk storage. Recent research has discussed efficiency and fairness requirements and identified a number of desirable scheduling objectives including so-called dominant resource fairness (DRF). We argue here that proportional fairness (PF), long recognized as a desirable objective in sharing network bandwidth between ongoing flows, is preferable to DRF. The superiority of PF is manifest under the realistic modelling assumption that the population of jobs in progress is a stochastic process. In random traffic the strategy-proof property of DRF proves unimportant while PF is shown by analysis and simulation to offer a significantly better efficiency-fairness tradeoff.", "venue": "Perform. Evaluation", "authors": ["Thomas  Bonald", "James W. Roberts"], "year": 2014, "n_citations": 17}
{"id": 748775, "s2_id": "2765e305d9ad7c6dfa3584c62e798258c86d5448", "title": "Repeated Recursion Unfolding for Super-Linear Speedup within Bounds", "abstract": "Repeated recursion unfolding is a new approach that repeatedly unfolds a recursion with itself and simplifies it while keeping all unfolded rules. Each unfolding doubles the number of recursive steps covered. This reduces the number of recursive rule applications to its logarithm at the expense of introducing a logarithmic number of unfolded rules to the program. Efficiency crucially depends on the amount of simplification inside the unfolded rules. We prove a super-linear speedup theorem in the best case, i.e. speedup by more than a constant factor. Our optimization can lower the time complexity class of a program. In this paper, the super-linear speedup is within bounds: it holds up to an arbitrary but chosen upper bound on the number of recursive steps. We also report on the first results with a prototype implementation of repeated recursion unfolding. A simple program transformation completely removes recursion up to the chosen bound. The actual runtime improvement quickly reaches several orders of magnitude.", "venue": "ArXiv", "authors": ["Thom  Fruehwirth"], "year": 2020, "n_citations": 0}
{"id": 748835, "s2_id": "da28165f30e1e8ad91521c2a99dc59681176162f", "title": "Accelerating SLIDE Deep Learning on Modern CPUs: Vectorization, Quantizations, Memory Optimizations, and More", "abstract": "Deep learning implementations on CPUs (Central Processing Units) are gaining more traction. Enhanced AI capabilities on commodity x86 architectures are commercially appealing due to the reuse of existing hardware and virtualization ease. A notable work in this direction is the SLIDE system. SLIDE is a C++ implementation of a sparse hash table based back-propagation, which was shown to be significantly faster than GPUs in training hundreds of million parameter neural models. In this paper, we argue that SLIDE\u2019s current implementation is sub-optimal and does not exploit several opportunities available in modern CPUs. In particular, we show how SLIDE\u2019s computations allow for a unique possibility of vectorization via AVX (Advanced Vector Extensions)-512. Furthermore, we highlight opportunities for different kinds of memory optimization and quantizations. Combining all of them, we obtain up to 7x speedup in the computations on the same hardware. Our experiments are focused on large (hundreds of millions of parameters) recommendation and NLP models. Our work highlights several novel perspectives and opportunities for implementing randomized algorithms for deep learning on modern CPUs. We provide the code and benchmark scripts at https://github.com/RUSH-LAB/SLIDE", "venue": "ArXiv", "authors": ["Shabnam  Daghaghi", "Nicholas  Meisburger", "Mengnan  Zhao", "Yong  Wu", "Sameh  Gobriel", "Charlie  Tai", "Anshumali  Shrivastava"], "year": 2021, "n_citations": 9}
{"id": 749711, "s2_id": "af5a53168c56de79bdf2216019c06e356acc1e1d", "title": "Seismic Imaging: An Overview and Parallel Implementation of Poststack Depth Migration", "abstract": "Seismic migration is the core step of seismic data processing which is important for oil exploration. Poststack depth migration in frequency-space (f-x) domain is one of commonly used algorithms. The wave-equation solution can be approximated as FIR filtering process to extrapolate the raw data and extract the subsurface image. Because of its computational complexity, its parallel implementation is encouraged. For calculating the next depth level, previous depth level is required. So, this part cannot be parallelized because of data dependence. But at each depth level there is plenty of roam for parallelism and can be parallelized. In case of CUDA programming, each thread calculate a single pixel on the next depth plan. After calculating the next depth plan, we can calculate the depth row by summing over all the frequencies and calculating all the depth rows results in the final migrated image. The poststack depth migration is implemented in CUDA and its performance is evaluated with the sequential code with different problem sizes.", "venue": "ArXiv", "authors": ["Ahmad  Shawahna", "Syed Abdul Salam", "Mayez  Al-Mouhamed"], "year": 2019, "n_citations": 0}
{"id": 750408, "s2_id": "bf66da1dee62b7e3245712bce3a98eaf312569a8", "title": "A unified analysis approach for hardware and software implementations", "abstract": "Modern computing systems are hybrid in nature and employ various processing technologies that range from specific-to general-purpose processors. In co-design environments, specific-purpose processors, also known as hardware, work to support software implementations under general-purpose systems to create high-performance computers. Algorithms and computationally intensive tasks are partitioned among the different processing subsystems to achieve desirable degrees of parallel processing and performance characteristics. In this paper, a unified statistical performance analysis formulation is presented. The proposed statistical formulation combines the heterogeneous characteristics of both hardware and software implementations to provide grounds for thorough evaluations. The formulation includes the development of performance profiles, key indicators, and the composition of a master indicator based-on heterogeneous measurements. The investigation includes a case-study that targets a set of simple cryptographic algorithms. The two main targeted high performance computing devices are multi-core processors for software implementations and high-end Field Programmable Gate Arrays for hardware implementations.", "venue": "2016 IEEE 59th International Midwest Symposium on Circuits and Systems (MWSCAS)", "authors": ["Issam W. Damaj"], "year": 2016, "n_citations": 4}
{"id": 760644, "s2_id": "f5dcdf616c3af51210631d39981de7c131682887", "title": "Benchmarking TPU, GPU, and CPU Platforms for Deep Learning", "abstract": "Training deep learning models is compute-intensive and there is an industry-wide trend towards hardware specialization to improve performance. To systematically benchmark deep learning platforms, we introduce ParaDnn, a parameterized benchmark suite for deep learning that generates end-to-end models for fully connected (FC), convolutional (CNN), and recurrent (RNN) neural networks. Along with six real-world models, we benchmark Google's Cloud TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep dive into TPU architecture, reveal its bottlenecks, and highlight valuable lessons learned for future specialized system design. We also provide a thorough comparison of the platforms and find that each has unique strengths for some types of models. Finally, we quantify the rapid performance improvements that specialized software stacks provide for the TPU and GPU platforms.", "venue": "ArXiv", "authors": ["Yu  Wang", "Gu-Yeon  Wei", "David  Brooks"], "year": 2019, "n_citations": 96}
{"id": 762097, "s2_id": "95224698f075e6b128844438c5c4a2ab478161d6", "title": "Machines are benchmarked by code, not algorithms", "abstract": "This article highlights how small modifications to either the source code of a benchmark program or the compilation options may impact its behavior on a specific machine. It argues that for evaluating machines, benchmark providers and users be careful to ensure reproducibility of results based on the machine code actually running on the hardware and not just source code. The article uses color to grayscale conversion of digital images as a running example.", "venue": "ArXiv", "authors": ["Raphael 'kena' Poss"], "year": 2013, "n_citations": 0}
{"id": 762655, "s2_id": "d6c4e44cae0d47bdcb494a32b5e1c0960bfe4bca", "title": "Analytic Modeling of Idle Waves in Parallel Programs: Communication, Cluster Topology, and Noise Impact", "abstract": "Most distributed-memory bulk-synchronous parallel programs in HPC assume that compute resources are available continuously and homogeneously across the allocated set of compute nodes. However, long one-off delays on individual processes can cause global disturbances, so-called idle waves, by rippling through the system. This process is mainly governed by the communication topology of the underlying parallel code. This paper makes significant contributions to the understanding of idle wave dynamics. We study the propagation mechanisms of idle waves across the ranks of MPI-parallel programs. We present a validated analytic model for their propagation velocity with respect to communication parameters and topology, with a special emphasis on sparse communication patterns. We study the interaction of idle waves with MPI collectives and show that, depending on the implementation, a collective may be transparent to the wave. Finally we analyze two mechanisms of idle wave decay: topological decay, which is rooted in differences in communication characteristics among parts of the system, and noise-induced decay, which is caused by system or application noise. We show that noise-induced decay is largely independent of noise characteristics but depends only on the overall noise power. An analytic expression for idle wave decay rate with respect to noise power is derived. For model validation we use microbenchmarks and stencil algorithms on three different supercomputing platforms.", "venue": "ISC", "authors": ["Ayesha  Afzal", "Georg  Hager", "Gerhard  Wellein"], "year": 2021, "n_citations": 0}
{"id": 763580, "s2_id": "83e8eddaa61ef11229d07133eaa326500b1de717", "title": "Memory and Parallelism Analysis Using a Platform-Independent Approach", "abstract": "Emerging computing architectures such as near-memory computing (NMC) promise improved performance for applications by reducing the data movement between CPU and memory. However, detecting such applications is not a trivial task. In this ongoing work, we extend the state-of-the-art platform-independent software analysis tool with NMC related metrics such as memory entropy, spatial locality, data-level, and basic-block-level parallelism. These metrics help to identify the applications more suitable for NMC architectures.", "venue": "SCOPES", "authors": ["Stefano  Corda", "Gagandeep  Singh", "Ahsan Javed Awan", "Roel  Jordans", "Henk  Corporaal"], "year": 2019, "n_citations": 5}
{"id": 765427, "s2_id": "2ec05af24c786375bfcb48cf72c1471faf28c74c", "title": "A Survey of Stability Results for Redundancy Systems", "abstract": "Redundancy mechanisms consist in sending several copies of a same job to a subset of servers. It constitutes one of the most promising ways to exploit diversity in multi-servers applications. However, its pros and cons are still not sufficiently understood in the context of realistic models with generic statistical properties of service-times distributions and correlation structures of copies. We aim at giving a survey of recent results concerning the stability - arguably the first benchmark of performance - of systems with cancel-on-completion redundancy. We also point out open questions and conjectures.", "venue": "Modern Trends in Controlled Stochastic Processes:", "authors": ["Elene  Anton", "Urtzi  Ayesta", "Matthieu  Jonckheere", "Maaike  Verloop"], "year": 2021, "n_citations": 1}
{"id": 770310, "s2_id": "bf333fdb7ce2c12ec5cd23e250e03f64235d2670", "title": "Seeing Through Black Boxes : Tracking Transactions through Queues under Monitoring Resource Constraints", "abstract": "The problem of optimal allocation of monitoring resources for tracking transactions progressing through a distributed system, modeled as a queueing network, is considered. Two forms of monitoring information are considered, viz., locally unique transaction identifiers, and arrival and departure timestamps of transactions at each processing queue. The timestamps are assumed to be available at all the queues but in the absence of identifiers, only enable imprecise tracking since parallel processing can result in out-of-order departures. On the other hand, identifiers enable precise tracking but are not available without proper instrumentation. Given an instrumentation budget, only a subset of queues can be selected for the production of identifiers, while the remaining queues have to resort to imprecise tracking using timestamps. The goal is then to optimally allocate the instrumentation budget to maximize the overall tracking accuracy. The challenge is that the optimal allocation strategy depends on accuracies of timestamp-based tracking at different queues, which has complex dependencies on the arrival and service processes, and the queueing discipline. We propose two simple heuristics for allocation by predicting the order of timestamp-based tracking accuracies of different queues. We derive sufficient conditions for these heuristics to achieve optimality through the notion of the stochastic comparison of queues. Simulations show that our heuristics are close to optimality, even when the parameters deviate from these conditions.", "venue": "Perform. Evaluation", "authors": ["Anima  Anandkumar", "Ting  He", "Chatschik  Bisdikian", "Dakshi  Agrawal"], "year": 2013, "n_citations": 0}
{"id": 770377, "s2_id": "f9f91023daa62ef83ba80e77419c4fddcea8fc4c", "title": "Language-based Abstractions for Dynamical Systems", "abstract": "Ordinary differential equations (ODEs) are the primary means to modelling dynamical systems in many natural and engineering sciences. The number of equations required to describe a system with high heterogeneity limits our capability of effectively performing analyses. This has motivated a large body of research, across many disciplines, into abstraction techniques that provide smaller ODE systems while preserving the original dynamics in some appropriate sense. In this paper we give an overview of a recently proposed computer-science perspective to this problem, where ODE reduction is recast to finding an appropriate equivalence relation over ODE variables, akin to classical models of computation based on labelled transition systems.", "venue": "QAPL@ETAPS", "authors": ["Andrea  Vandin"], "year": 2017, "n_citations": 1}
{"id": 776669, "s2_id": "61f08526d2417c47a67b5c9657375c71191b4eb2", "title": "FT-BLAS: a high performance BLAS implementation with online fault tolerance", "abstract": "Basic Linear Algebra Subprograms (BLAS) is a core library in scientific computing and machine learning. This paper presents FT-BLAS, a new implementation of BLAS routines that not only tolerates soft errors on the fly, but also provides comparable performance to modern state-of-the-art BLAS libraries on widely-used processors such as Intel Skylake and Cascade Lake. To accommodate the features of BLAS, which contains both memory-bound and computing-bound routines, we propose a hybrid strategy to incorporate fault tolerance into our brand-new BLAS implementation: duplicating computing instructions for memory-bound Level-1 and Level-2 BLAS routines and incorporating an Algorithm-Based Fault Tolerance mechanism for computing-bound Level-3 BLAS routines. Our high performance and low overhead are obtained from delicate assembly-level optimization and a kernel-fusion approach to the computing kernels. Experimental results demonstrate that FT-BLAS offers high reliability and high performance -- faster than Intel MKL, OpenBLAS, and BLIS by up to 3.50%, 22.14% and 21.70%, respectively, for routines spanning all three levels of BLAS we benchmarked, even under hundreds of errors injected per minute.", "venue": "ICS", "authors": ["Yujia  Zhai", "Elisabeth  Giem", "Quan  Fan", "Kai  Zhao", "Jinyang  Liu", "Zizhong  Chen"], "year": 2021, "n_citations": 1}
{"id": 777010, "s2_id": "eb85d0c2903051e43fdae14f507ddb6938447266", "title": "A Resource Intensive Traffic-Aware Scheme for Cluster-based Energy Conservation in Wireless Devices", "abstract": "Wireless traffic that is destined for a certain device in a network, can be exploited in order to minimize the availability and delay trade-offs, and mitigate the Energy consumption. The Energy Conservation (EC) mechanism can be node-centric by considering the traversed nodal traffic in order to prolong the network lifetime. This work describes a quantitative traffic-based approach where a clustered Sleep-Proxy mechanism takes place in order to enable each node to sleep according to the time duration of the active traffic that each node expects and experiences. Sleep-proxies within the clusters are created according to pairwise active-time comparison, where each node expects during the active periods, a requested traffic. For resource availability and recovery purposes, the caching mechanism takes place in case where the node for which the traffic is destined is not available. The proposed scheme uses Role-based nodes which are assigned to manipulate the traffic in a cluster, through the time-oriented backward difference traffic evaluation scheme. Simulation study is carried out for the proposed backward estimation scheme and the effectiveness of the end-to-end EC mechanism taking into account a number of metrics and measures for the effects while incrementing the sleep time duration under the proposed framework. Comparative simulation results show that the proposed scheme could be applied to infrastructure-less systems, providing energy-efficient resource exchange with significant minimization in the power consumption of each device.", "venue": "2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems", "authors": ["Marios C. Charalambous", "Constandinos X. Mavromoustakis", "Muneer O. Bani Yassein"], "year": 2012, "n_citations": 18}
{"id": 779864, "s2_id": "0faba74c87606b1f60740144344a77033aae5975", "title": "Verified instruction-level energy consumption measurement for NVIDIA GPUs", "abstract": "GPUs are prevalent in modern computing systems at all scales. They consume a significant fraction of the energy in these systems. However, vendors do not publish the actual cost of the power/energy overhead of their internal microarchitecture. In this paper, we accurately measure the energy consumption of various PTX instructions found in modern NVIDIA GPUs. We provide an exhaustive comparison of more than 40 instructions for four high-end NVIDIA GPUs from four different generations (Maxwell, Pascal, Volta, and Turing). Furthermore, we show the effect of the CUDA compiler optimizations on the energy consumption of each instruction. We use three different software techniques to read the GPU on-chip power sensors, which use NVIDIA's NVML API and provide an in-depth comparison between these techniques. Additionally, we verified the software measurement techniques against a custom-designed hardware power measurement. The results show that Volta GPUs have the best energy efficiency of all the other generations for the different categories of the instructions. This work should aid in understanding NVIDIA GPUs' microarchitecture. It should also make energy measurements of any GPU kernel both efficient and accurate.", "venue": "CF", "authors": ["Yehia  Arafa", "Ammar  ElWazir", "Abdelrahman  ElKanishy", "Youssef  Aly", "Ayatelrahman  Elsayed", "Abdel-Hameed  Badawy", "Gopinath  Chennupati", "Stephan  Eidenbenz", "Nandakishore  Santhi"], "year": 2020, "n_citations": 7}
{"id": 782004, "s2_id": "7ebb384d8eea5538e4a01037c727a1fae2899b29", "title": "An Analysis of Energy Consumption on ACK plus Rate Packet in Rate Based Transport Protocol", "abstract": "Rate based transport protocol determines the rate of data transmission between the sender and receiver and then sends the data according to that rate. To notify the rate to the sender, the receiver sends ACKplusRate packet based on epoch timer expiry. In this paper, through detailed arguments and simulation it is shown that the transmission of ACKplusRate packet based on epoch timer expiry consumes more energy in network with low mobility. To overcome this problem, a new technique called Dynamic Rate Feedback (DRF) is proposed. DRF sends ACKplusRate whenever there is a change in rate of (plus or minus) 25 percent than the previous rate. Based on ns2 simulation DRF is compared with a reliable transport protocol for ad hoc network (ATP)", "venue": "ArXiv", "authors": ["P.  Ganeshkumar", "K.  Thyagarajah"], "year": 2009, "n_citations": 1}
{"id": 782705, "s2_id": "e3e0ac6d4fb8e214dff933d22f4c5719f999034e", "title": "Optimizing Compiler for Engineering Problems", "abstract": "New information technologies provide a lot of prospects for performance improvement. One of them is \"Dynamic Source Code Generation and Compilation\". This article shows how this way provides high performance for engineering problems.", "venue": "ArXiv", "authors": ["Petr R. Ivankov"], "year": 2008, "n_citations": 0}
{"id": 784250, "s2_id": "16be23ec47b590b9e2845bc1118e047835a2bed5", "title": "Exploring Task Mappings on Heterogeneous MPSoCs using a Bias-Elitist Genetic Algorithm", "abstract": "Exploration of task mappings plays a crucial role in achieving high performance in heterogeneous multi-processor system-on-chip (MPSoC) platforms. The problem of optimally mapping a set of tasks onto a set of given heterogeneous processors for maximal throughput has been known, in general, to be NP-complete. The problem is further exacerbated when multiple applications (i.e., bigger task sets) and the communication between tasks are also considered. Previous research has shown that Genetic Algorithms (GA) typically are a good choice to solve this problem when the solution space is relatively small. However, when the size of the problem space increases, classic genetic algorithms still suffer from the problem of long evolution times. To address this problem, this paper proposes a novel bias-elitist genetic algorithm that is guided by domain-specific heuristics to speed up the evolution process. Experimental results reveal that our proposed algorithm is able to handle large scale task mapping problems and produces high-quality mapping solutions in only a short time period.", "venue": "ArXiv", "authors": ["Wei  Quan", "Andy D. Pimentel"], "year": 2014, "n_citations": 6}
{"id": 784352, "s2_id": "a341623ebc9c9395baa10dc5b2b239df505fb32d", "title": "Performance Optimizations of Recursive Electronic Structure Solvers targeting Multi-Core Architectures (LA-UR-20-26665)", "abstract": "As we rapidly approach the frontiers of ultra large computing resources, software optimization is becoming of paramount interest to scientific application developers interested in efficiently leveraging all available on-Node computing capabilities and thereby improving a requisite science per watt metric. The scientific application of interest here is the Basic Math Library (BML) that provides a singular interface for linear algebra operation frequently used in the Quantum Molecular Dynamics (QMD) community. The provisioning of a singular interface indicates the presence of an abstraction layer which in-turn suggests commonalities in the code-base and therefore any optimization or tuning introduced in the core of code-base has the ability to positively affect the performance of the aforementioned library as a whole. With that in mind, we proceed with this investigation by performing a survey of the entirety of the BML code-base, and extract, in form of micro-kernels, common snippets of code. We introduce several optimization strategies into these micro-kernels including 1.) Strength Reduction 2.) Memory Alignment for large arrays 3.) Non Uniform Memory Access (NUMA) aware allocations to enforce data locality and 4.) appropriate thread affinity and bindings to enhance the overall multi-threaded performance. After introducing these optimizations, we benchmark the micro-kernels and compare the run-time before and after optimization for several target architectures. Finally we use the results as a guide to propagating the optimization strategies into the BML code-base. As a demonstration, herein, we test the efficacy of these optimization strategies by comparing the benchmark and optimized versions of the code.", "venue": "ArXiv", "authors": ["Adetokunbo A. Adedoyin", "Christian F. A. Negre", "Jamaludin  Mohd-Yusof", "Nicolas  Bock", "Daniel  Osei-Kuffuor", "Jean-Luc  Fattebert", "Michael E. Wall", "Anders M. N. Niklasson", "Susan M. Mniszewski"], "year": 2021, "n_citations": 1}
{"id": 786435, "s2_id": "4e76759eac9259fa9b3c8e7f8e5e49027da0ec7d", "title": "QPS-r: A Cost-Effective Crossbar Scheduling Algorithm and Its Stability and Delay Analysis", "abstract": "In an input-queued switch, a crossbar schedule, or a matching between the input ports and the output ports needs to be computed in each switching cycle, or time slot. Designing switching algorithms with very low computational complexity, that lead to high throughput and small delay is a challenging problem. There appears to be a fundamental tradeoff between the computational complexity of the switching algorithm and the resultants throughput and delay. Parallel maximal matching algorithms (adapted for switching) appear to have stricken a sweet spot in this tradeoff, and prior work has shown the following performance guarantees. Using maximal matchings in every time slot results in at least 50% switch throughput and order-optimal (i.e., independent of the switch size N) average delay bounds for various traffic arrival processes. On the other hand, their computational complexity can be as low as $O(log^2N)$ per port/processor, which is much lower than those of the algorithms such as maximum weighted matching which ensures better throughput performance. \nIn this work, we propose QPS-r, a parallel iterative switching algorithm that has the lowest possible computational complexity: O(1) per port. Using Lyapunov stability analysis, we show that the throughput and delay performance is identical to that of maximal matching algorithm. Although QPS-r builds upon an existing technique called Queue-Proportional Sampling (QPS), in this paper, we provide analytical guarantees on its throughput and delay under i.i.d. traffic as well as a Markovian traffic model which can model many realistic traffic patterns. We also demonstrate that QPS-3 (running 3 iterations) has comparable empirical throughput and delay performances as iSLIP (running $log_2 N$ iterations), a refined and optimized representative maximal matching algorithm adapted for switching.", "venue": "ArXiv", "authors": ["Long  Gong", "Jun  Xu", "Liang  Liu", "Siva Theja Maguluri"], "year": 2019, "n_citations": 3}
{"id": 791066, "s2_id": "b7c041cbf3f7529e42838749d51e360cb174db89", "title": "Resource-Allocation Frameworks for Network-Coded Layered Multimedia Multicast Services", "abstract": "The explosive growth of content-on-the-move, such as video streaming to mobile devices, has propelled research on multimedia broadcast and multicast schemes. Multirate transmission strategies have been proposed as a means of delivering layered services to users experiencing different downlink channel conditions. In this paper, we consider point-to-multipoint layered service delivery across a generic cellular system and improve it by applying different random linear network coding approaches. We derive packet error probability expressions and use them as performance metrics in the formulation of resource-allocation frameworks. The aim of these frameworks is both the optimization of the transmission scheme and the minimization of the number of broadcast packets on each downlink channel, while offering service guarantees to a predetermined fraction of users. As a case of study, our proposed frameworks are then adapted to the LTE-A standard and the eMBMS technology. We focus on the delivery of a video service based on the H.264/SVC standard and demonstrate the advantages of layered network coding over multirate transmission. Furthermore, we establish that the choice of both the network coding technique and the resource-allocation method play a critical role on the network footprint, as well as the quality of each received video layer.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Andrea  Tassi", "Ioannis  Chatzigeorgiou", "Dejan  Vukobratovic"], "year": 2015, "n_citations": 69}
{"id": 791422, "s2_id": "46a906895494230b2ddf1c6201b6682219aea6b7", "title": "Statistical end-to-end performance bounds for networks under long memory FBM cross traffic", "abstract": "Fractional Brownian motion (fBm) became known as a useful model for Internet traffic incorporating its self-similar and long-range dependent properties. In this paper we derive end-to-end performance bounds for a through flow in a network of tandem queues under fBm cross traffic. We build on a previously derived sample path envelope for fBm, which possesses a Weibullian decay of overflow probabilities. We employ the sample path envelope and the concept of leftover service curves to model the remaining service after scheduling fBm cross traffic at a system. Using composition results for tandem systems from the stochastic network calculus we derive end-to-end statistical performance bounds for individual flows in networks under fBm cross traffic. We discover that these bounds grow in O(n(log n)1/2\u22122H) for n systems in series where H is the Hurst parameter of the fBm cross traffic. We show numerical results on the impact of the variability and the correlation of fBm traffic on network performance.", "venue": "2010 IEEE 18th International Workshop on Quality of Service (IWQoS)", "authors": ["Amr  Rizk", "Markus  Fidler"], "year": 2010, "n_citations": 10}
{"id": 801138, "s2_id": "9849a9a60d05c79e5cb757ef784982744ebab679", "title": "Pushing the limits for medical image reconstruction on recent standard multicore processors", "abstract": "Volume reconstruction by backprojection is the computational bottleneck in many interventional clinical computed tomography (CT) applications. Today vendors in this field replace special purpose hardware accelerators with standard hardware such as multicore chips and GPGPUs. Medical imaging algorithms are on the verge of employing high-performance computing (HPC) technology, and are therefore an interesting new candidate for optimization. This paper presents low-level optimizations for the backprojection algorithm, guided by a thorough performance analysis on four generations of Intel multicore processors (Harpertown, Westmere, Westmere EX, and Sandy Bridge). We choose the RabbitCT benchmark, a standardized testcase well supported in industry, to ensure transparent and comparable results. Our aim is to provide not only the fastest possible implementation but also compare with performance models and hardware counter data in order to fully understand the results. We separate the influence of algorithmic optimizations, parallelization, SIMD vectorization, and microarchitectural issues and pinpoint problems with current SIMD instruction set extensions on standard CPUs (SSE, AVX). The use of assembly language is mandatory for best performance. Finally, we compare our results to the best GPGPU implementations available for this open competition benchmark.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Jan  Treibig", "Georg  Hager", "Hannes G. Hofmann", "Joachim  Hornegger", "Gerhard  Wellein"], "year": 2013, "n_citations": 34}
{"id": 801977, "s2_id": "2b9034c2c05c7f182f1fa0333f37d28cffbaa64a", "title": "Novel Performance Analysis of Network Coded Communications in Single-Relay Networks", "abstract": "In this paper, we analyze the performance of a single-relay network in which the reliability is provided by means of Random Linear Network Coding (RLNC). We consider a scenario when both source and relay nodes can encode packets. Unlike the traditional approach to relay networks, we introduce a passive relay mode, in which the relay node simply retransmits collected packets in case it cannot decode them. In contrast with the previous studies, we derive a novel theoretical framework for the performance characterization of the considered relay network. We extend our analysis to a more general scenario, in which coding coefficients are generated from non-binary fields. The theoretical results are verified using simulation, for both binary and non-binary fields. It is also shown that the passive relay mode significantly improves the performance compared with the active-only case, offering an up to two-fold gain in terms of the decoding probability. The proposed framework can be used as a building block for the analysis of more complex network topologies.", "venue": "2016 IEEE Global Communications Conference (GLOBECOM)", "authors": ["Evgeny  Tsimbalo", "Andrea  Tassi", "Robert J. Piechocki"], "year": 2016, "n_citations": 5}
{"id": 806392, "s2_id": "7155522183c51d7a9178ee1d5b20351c3220ebd2", "title": "Load balancing in large-scale systems with multiple dispatchers", "abstract": "Load balancing algorithms play a crucial role in delivering robust application performance in data centers and cloud networks. Recently, strong interest has emerged in Join-the-Idle-Queue (JIQ) algorithms, which rely on tokens issued by idle servers in dispatching tasks and outperform power-of-d policies. Specifically, JiQ strategies involve minimal information exchange, and yet achieve zero blocking and wait in the many-server limit. The latter property prevails in a multiple-dispatcher scenario when the loads are strictly equal among dispatchers. For various reasons it is not uncommon however for skewed load patterns to occur. We leverage product-form representations and fluid limits to establish that the blocking and wait then no longer vanish, even for arbitrarily low overall load. Remarkably, it is the least-loaded dispatcher that throttles tokens and leaves idle servers stranded, thus acting as bottleneck. Motivated by the above issues, we introduce two enhancements of the ordinary JIQ scheme where tokens are either distributed non-uniformly or occasionally exchanged among the various dispatchers. We prove that these extensions can achieve zero blocking and wait in the many-server limit, for any subcritical overall load and arbitrarily skewed load profiles. Extensive simulation experiments demonstrate that the asymptotic results are highly accurate, even for moderately sized systems.", "venue": "IEEE INFOCOM 2017 - IEEE Conference on Computer Communications", "authors": ["Mark van der Boor", "Sem C. Borst", "Johan van Leeuwaarden"], "year": 2017, "n_citations": 19}
{"id": 809384, "s2_id": "680daccbd4b662ea8ccba859bd42e4b9df97d27d", "title": "Understanding the Interactions of Workloads and DRAM Types: A Comprehensive Experimental Study", "abstract": "It has become increasingly difficult to understand the complex interaction between modern applications and main memory, composed of DRAM chips. Manufacturers are now selling and proposing many different types of DRAM, with each DRAM type catering to different needs (e.g., high throughput, low power, high memory density). At the same time, the memory access patterns of prevalent and emerging workloads are rapidly diverging, as these applications manipulate larger data sets in very different ways. As a result, the combined DRAM-workload behavior is often difficult to intuitively determine today, which can hinder memory optimizations in both hardware and software. In this work, we identify important families of workloads, as well as prevalent types of DRAM chips, and rigorously analyze the combined DRAM--workload behavior. To this end, we perform a comprehensive experimental study of the interaction between nine different DRAM types and 115 modern applications and multiprogrammed workloads. We draw 12 key observations from our characterization, enabled in part by our development of new metrics that take into account contention between memory requests due to hardware design. Notably, we find that (1) newer DRAM types such as DDR4 and HMC often do not outperform older types such as DDR3, due to higher access latencies and, in the case of HMC, poor exploitation of locality; (2) there is no single DRAM type that can cater to all components of a heterogeneous system (e.g., GDDR5 significantly outperforms other memories for multimedia acceleration, while HMC significantly outperforms other memories for network acceleration); and (3) there is still a strong need to lower DRAM latency, but unfortunately the current design trend of commodity DRAM is toward higher latencies to obtain other benefits. We hope that the trends we identify can drive optimizations in both hardware and software design.", "venue": "ArXiv", "authors": ["Saugata  Ghose", "Tianshi  Li", "Nastaran  Hajinazar", "Damla Senol Cali", "Onur  Mutlu"], "year": 2019, "n_citations": 7}
{"id": 810259, "s2_id": "054ce8220a2bc45b9fec9dd58933609e1e238157", "title": "Modelling Delay Jitter in Voice over IP", "abstract": "It has been suggested in voice over IP that an appropriate choice of the distribution used in modeling the delay jitters, can improve the play-out algorithm. In this paper, we propose a tool using which, one can determine, at a given instance, which distribution model best explains the jitter distribution. This is done using Expectation Maximization, to choose amongst possible distribution models which include, the i.i.d exponential distribution, the gamma distribution etc.", "venue": "ArXiv", "authors": ["R.  Ganesh", "B.  Kaushik", "R.  Sadhu"], "year": 2003, "n_citations": 1}
{"id": 810573, "s2_id": "c72e73bf69d42b923397afbc844a30f37f115b55", "title": "Constructing Performance Models for Dense Linear Algebra Algorithms on Cray XE Systems", "abstract": "Hiding or minimizing the communication cost is key in order to obtain good performance on large-scale systems. While communication overlapping attempts to hide communications cost, 2.5D communication avoiding algorithms improve performance scalability by reducing the volume of data transfers at the cost of extra memory usage. Both approaches can be used together or separately and the best choice depends on the machine, the algorithm and the problem size. Thus, the development of performance models is crucial to determine the best option for each scenario. In this paper, we present a methodology for constructing performance models for parallel numerical routines on Cray XE systems. Our models use portable benchmarks that measure computational cost and network characteristics, as well as performance degradation caused by simultaneous accesses to the network. We validate our methodology by constructing the performance models for the 2D and 2.5D approaches, with and without overlapping, of two matrix multiplication algorithms (Cannon's and SUMMA), triangular solve (TRSM) and Cholesky. We compare the estimations provided by these models with the experimental results using up to 24,576 cores of a Cray XE6 system and predict the performance of the algorithms on larger systems. Results prove that the estimations significantly improve when taking into account network contention.", "venue": "ArXiv", "authors": ["Jorge  Gonz\u00e1lez-Dom\u00ednguez", "Evangelos  Georganas", "Yili  Zheng", "Mar\u00eda J. Mart\u00edn"], "year": 2014, "n_citations": 0}
{"id": 816850, "s2_id": "adc7ee2fad6e5cd3b7f1a3f37d6b6bcc669aeac7", "title": "Mantis: Predicting System Performance through Program Analysis and Modeling", "abstract": "We present Mantis, a new framework that automatically predicts program performance with high accuracy. Mantis integrates techniques from programming language and machine learning for performance modeling, and is a radical departure from traditional approaches. Mantis extracts program features, which are information about program execution runs, through program instrumentation. It uses machine learning techniques to select features relevant to performance and creates prediction models as a function of the selected features. Through program analysis, it then generates compact code slices that compute these feature values for prediction. Our evaluation shows that Mantis can achieve more than 93% accuracy with less than 10% training data set, which is a significant improvement over models that are oblivious to program features. The system generates code slices that are cheap to compute feature values.", "venue": "ArXiv", "authors": ["Byung-Gon  Chun", "Ling  Huang", "Sangmin  Lee", "Petros  Maniatis", "Mayur  Naik"], "year": 2010, "n_citations": 13}
{"id": 817820, "s2_id": "1e3097324ab0d09712f7dc28467faacc65e993e7", "title": "Breaking the Limits of Redundancy Systems Analysis", "abstract": "Redundancy mechanisms such as triple modular redundancy protect safety-critical components by replication and thus improve systems fault tolerance. However, the gained fault tolerance comes along with costs to be invested, e.g., increasing execution time, energy consumption, or packaging size, for which constraints have to be obeyed during system design. This turns the question of finding suitable combinations of components to be protected into a challenging task as the number of possible protection combinations grows exponentially in the number of components. We propose family-based approaches to tackle the combinatorial blowup in redundancy systems modeling and analysis phases. Based on systems designed in SIMULINK we show how to obtain models that include all possible protection combinations and present a tool chain that, given a probabilistic error model, generates discrete Markov chain families. Using symbolic techniques that enable concise family representation and analysis, we show how SIMULINK models of realistic size can be protected and analyzed with a single family-based analysis run while a one-by-one analysis of each protection combination would clearly exceed any realistic time constraints.", "venue": "Proceedings of the 29th European Safety and Reliability Conference (ESREL)", "authors": ["Clemens  Dubslaff", "Kai  Ding", "Andrey  Morozov", "Christel  Baier", "Klaus  Janschek"], "year": 2019, "n_citations": 4}
{"id": 818585, "s2_id": "217a6511321ba4ecea3e82a862f799aa28811d71", "title": "Mean Field Models of Message Throughput in Dynamic Peer-to-Peer Systems", "abstract": "The churn rate of a peer-to-peer system places direct limitations on the rate at which messages can be effectively communicated to a group of peers. These limitations are independent of the topology and message transmission latency. In this paper we consider a peer-to-peer network, based on the Engset model, where peers arrive and depart independently at random. We show how the arrival and departure rates directly limit the capacity for message streams to be broadcast to all other peers, by deriving mean field models that accurately describe the system behavior. Our models cover the unit and more general k buffer cases, i.e. where a peer can buffer at most k messages at any one time, and we give results for both single and multi-source message streams. We define coverage rate as peer-messages per unit time, i.e. the rate at which a number of peers receive messages, and show that the coverage rate is limited by the churn rate and buffer size. Our theory introduces an Instantaneous Message Exchange (IME) model and provides a template for further analysis of more complicated systems. Using the IME model, and assuming random processes, we have obtained very accurate equations of the system dynamics in a variety of interesting cases, that allow us to tune a peer-to-peer system. It remains to be seen if we can maintain this accuracy for general processes and when applying a non-instantaneous model.", "venue": "ArXiv", "authors": ["Aaron  Harwood", "Olga  Ohrimenko"], "year": 2007, "n_citations": 2}
{"id": 818904, "s2_id": "0328fbe7d488c4e31af2973ffe90b36069758700", "title": "Domain-Sharding for Faster HTTP/2 in Lossy Cellular Networks", "abstract": "HTTP/2 (h2) is a new standard for Web communications that already delivers a large share of Web traffic. Unlike HTTP/1, h2 uses only one underlying TCP connection. In a cellular network with high loss and sudden spikes in latency, which the TCP stack might interpret as loss, using a single TCP connection can negatively impact Web performance. In this paper, we perform an extensive analysis of real world cellular network traffic and design a testbed to emulate loss characteristics in cellular networks. We use the emulated cellular network to measure h2 performance in comparison to HTTP/1.1, for webpages synthesized from HTTP Archive repository data. \nOur results show that, in lossy conditions, h2 achieves faster page load times (PLTs) for webpages with small objects. For webpages with large objects, h2 degrades the PLT. We devise a new domain-sharding technique that isolates large and small object downloads on separate connections. Using sharding, we show that under lossy cellular conditions, h2 over multiple connections improves the PLT compared to h2 with one connection and HTTP/1.1 with six connections. Finally, we recommend content providers and content delivery networks to apply h2-aware domain-sharding on webpages currently served over h2 for improved mobile Web performance.", "venue": "ArXiv", "authors": ["Utkarsh  Goel", "Moritz  Steiner", "Mike P. Wittie", "Stephen  Ludin", "Martin  Flack"], "year": 2017, "n_citations": 11}
{"id": 820333, "s2_id": "e53535695bc7ed1801ec9cb1ba0cc7d470e8682c", "title": "GARDENIA: A Domain-specific Benchmark Suite for Next-generation Accelerators", "abstract": "This paper presents the Graph Analytics Repository for Designing Next-generation Accelerators (GARDENIA), a benchmark suite for studying irregular algorithms on massively parallel accelerators. Existing generic benchmarks for accelerators have mainly focused on high performance computing (HPC) applications with limited control and data irregularity, while available graph analytics benchmarks do not apply state-of-the-art algorithms and/or optimization techniques. GARDENIA includes emerging irregular applications in big-data and machine learning domains which mimic massively multithreaded commercial programs running on modern large-scale datacenters. Our characterization shows that GARDENIA exhibits irregular microarchitectural behavior which is quite different from structured workloads and straightforward-implemented graph benchmarks.", "venue": "ArXiv", "authors": ["Zhen  Xu", "Xuhao  Chen", "Jie  Shen", "Yang  Zhang", "Cheng  Chen", "Canqun  Yang"], "year": 2017, "n_citations": 4}
{"id": 820740, "s2_id": "cccc67f31f105abe0a8f1e5b842308ebc52ab471", "title": "On Time Synchronization Issues in Time-Sensitive Networks with Regulators and Nonideal Clocks", "abstract": "Flow reshaping is used in time-sensitive networks (as in the context of IEEE TSN and IETF Detnet) in order to reduce burstiness inside the network and to support the computation of guaranteed latency bounds. This is performed using per-flow regulators (such as the Token Bucket Filter) or interleaved regulators (as with IEEE TSN Asynchronous Traffic Shaping, ATS). Both types of regulators are beneficial as they cancel the increase of burstiness due to multiplexing inside the network. It was demonstrated, by using network calculus, that they do not increase the worst-case latency. However, the properties of regulators were established assuming that time is perfect in all network nodes. In reality, nodes use local, imperfect clocks. Time-sensitive networks exist in two flavours: (1) in non-synchronized networks, local clocks run independently at every node and their deviations are not controlled and (2) in synchronized networks, the deviations of local clocks are kept within very small bounds using for example a synchronization protocol (such as PTP) or a satellite based geo-positioning system (such as GPS). We revisit the properties of regulators in both cases. In non-synchronized networks, we show that ignoring the timing inaccuracies can lead to network instability due to unbounded delay in per-flow or interleaved regulators. We propose and analyze two methods (rate and burst cascade, and asynchronous dual arrival-curve method) for avoiding this problem. In synchronized networks, we show that there is no instability with perflow regulators but, surprisingly, interleaved regulators can lead to instability. To establish these results, we develop a new framework that captures industrial requirements on clocks in both nonsynchronized and synchronized networks, and we develop a toolbox that extends network calculus to account for clock imperfections.", "venue": "SIGMETRICS", "authors": ["Ludovic  Thomas", "Jean-Yves Le Boudec"], "year": 2020, "n_citations": 0}
{"id": 824034, "s2_id": "5d0cc125e4aa1b12d1e1c0ff3c84380de0a6a4c5", "title": "Age of Information for Single Buffer Systems with Vacation Server", "abstract": "In this research, we consider age-related metrics for queueing systems with vacation server. Assuming that there is a single buffer at the queue to receive packets, we consider three variations of this single buffer system, namely Conventional Buffer System (CBS), Buffer Relaxation System (BRS), and Conventional Buffer System with Preemption in Service (CBS-P). We introduce a decomposition approach to derive the closed-form expressions for expected Age of Information (AoI), expected Peak Age of Information (PAoI) as well as the variance of peak age for these systems. We then consider these three systems with non-independent vacations, and use polling system as an example to show that the decomposition approach can be applied to derive closed-form expressions of PAoI for general situation. We explore the conditions under which one of these systems has advantage over the others, and we further perform numerical studies to validate our results and develop insights.", "venue": "IEEE Transactions on Network Science and Engineering", "authors": ["Jin  Xu", "I-Hong  Hou", "Natarajan  Gautam"], "year": 2021, "n_citations": 1}
{"id": 830335, "s2_id": "efab3f11331ae7e64a155c704b2a79fa5761c16b", "title": "A stochastic performance model for pipelined Krylov methods", "abstract": "Pipelined Krylov methods seek to ameliorate the latency due to inner products necessary for projection by overlapping it with the computation associated with sparse matrix\u2010vector multiplication. We clarify a folk theorem that this can only result in a speedup of 2\u00d7 over the naive implementation. Examining many repeated runs, we show that stochastic noise also contributes to the latency, and we model this using an analytical probability distribution. Our analysis shows that speedups greater than 2\u00d7 are possible with these algorithms. Copyright \u00a9 2016 John Wiley & Sons, Ltd.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Hannah  Morgan", "Matthew G. Knepley", "Patrick  Sanan", "L. Ridgway Scott"], "year": 2016, "n_citations": 13}
{"id": 830726, "s2_id": "f1dfa6cda019ad6459874ccd79ce63a800ff6915", "title": "Power and Performance Analysis of Persistent Key-Value Stores", "abstract": "With the current rate of data growth, processing needs are becoming difficult to fulfill due to CPU power and energy limitations. Data serving systems and especially persistent key-value stores have become a substantial part of data processing stacks in the data center, providing access to massive amounts of data for applications and services. Key-value stores exhibit high CPU and I/O overheads because of their constant need to reorganize data on the devices. In this paper, we examine the efficiency of two key-value stores on four servers of different generations and with different CPU architectures. We use RocksDB, a key-value that is deployed widely, e.g. in Facebook, and Kreon, a research key-value store that has been designed to reduce CPU overhead. We evaluate their behavior and overheads on an ARM-based microserver and three different generations of x86 servers. Our findings show that microservers have better power efficiency in the range of 0.68-3.6x with a comparable tail latency.", "venue": "ArXiv", "authors": ["Stella  Mikrou", "Anastasios  Papagiannis", "Giorgos  Saloustros", "Manolis  Marazakis", "Angelos  Bilas"], "year": 2020, "n_citations": 0}
{"id": 832165, "s2_id": "7de8c96ccf390b00f0c46eedd2ffc1b8ba6deb68", "title": "Parallel breadth-first search on distributed memory systems", "abstract": "Data-intensive, graph-based computations are pervasive in several scientific applications, and are known to to be quite challenging to implement on distributed memory systems. In this work, we explore the design space of parallel algorithms for Breadth-First Search (BFS), a key subroutine in several graph algorithms. We present two highly-tuned parallel approaches for BFS on large parallel systems: a level-synchronous strategy that relies on a simple vertex-based partitioning of the graph, and a two-dimensional sparse matrix partitioning-based approach that mitigates parallel communication overhead. For both approaches, we also present hybrid versions with intra-node multithreading. Our novel hybrid two-dimensional algorithm reduces communication times by up to a factor of 3.5, relative to a common vertex based approach. Our experimental study identifies execution regimes in which these approaches will be competitive, and we demonstrate extremely high performance on leading distributed-memory parallel systems. For instance, for a 40,000-core parallel execution on Hopper, an AMD MagnyCours based system, we achieve a BFS performance rate of 17.8 billion edge visits per second on an undirected graph of 4.3 billion vertices and 68.7 billion edges with skewed degree distribution.", "venue": "2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC)", "authors": ["Aydin  Bulu\u00e7", "Kamesh  Madduri"], "year": 2011, "n_citations": 206}
{"id": 832933, "s2_id": "0be28d2602dc83a3ce3d7d582530679769682291", "title": "A large deviation approach to super-critical bootstrap percolation on the random graph Gn, p", "abstract": "We consider the Erd\\\"{o}s--R\\'{e}nyi random graph $G_{n,p}$ and we analyze the simple irreversible epidemic process on the graph, known in the literature as bootstrap percolation. We give a quantitative version of some results by Janson et al. (2012), providing a fine asymptotic analysis of the final size $A_n^*$ of active nodes, under a suitable super-critical regime. More specifically, we establish large deviation principles for the sequence of random variables $\\{\\frac{n- A_n^*}{f(n)}\\}_{n\\geq 1}$ with explicit rate functions and allowing the scaling function $f$ to vary in the widest possible range.", "venue": "Stochastic Processes and their Applications", "authors": ["Giovanni Luca Torrisi", "Michele  Garetto", "Emilio  Leonardi"], "year": 2019, "n_citations": 28}
{"id": 835462, "s2_id": "8ecacf6e3f0777730cc0a42c8974b9029f764807", "title": "Online GANs for Automatic Performance Testing", "abstract": "In this paper we present a novel algorithm for automatic performance testing that uses an online variant of the Generative Adversarial Network (GAN) to optimize the test generation process. The objective of the proposed approach is to generate, for a given test budget, a test suite containing a high number of tests revealing performance defects. This is achieved using a GAN to generate the tests and predict their outcome. This GAN is trained online while generating and executing the tests. The proposed approach does not require a prior training set or model of the system under test. We provide an initial evaluation the algorithm using an example test system, and compare the obtained results with other possible approaches.We consider that the presented algorithm serves as a proof of concept and we hope that it can spark a research discussion on the application of GANs to test generation.", "venue": "2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)", "authors": ["Ivan  Porres", "Hergys  Rexha", "S'ebastien  Lafond"], "year": 2021, "n_citations": 0}
{"id": 836776, "s2_id": "b54ed48ba0ba0c2c1a2d834797f5e623f690b084", "title": "Approximately optimal scheduling of an M/G/1 queue with heavy tails", "abstract": "Distributions with a heavy tail are difficult to estimate. If the design of an optimal scheduling policy is sensitive to the details of heavy tail distributions of the service times, an approximately optimal solution is difficult to obtain. This paper shows that the mean optimal scheduling of an M/G/1 queue with heavy tailed service times does not present this difficulty and that an approximately optimal strategy can be derived by truncating the distributions.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Vijay  Kamble", "Jean C. Walrand"], "year": 2015, "n_citations": 0}
{"id": 842409, "s2_id": "074a8b5214799a609cc1c887043073cda5dcc092", "title": "Performance Reproduction and Prediction of Selected Dynamic Loop Scheduling Experiments", "abstract": "Scientific applications are complex, large, and often exhibit irregular and stochastic behavior. The use of efficient loop scheduling techniques, from static to fully dynamic, in computationally-intensive applications is crucial for improving their performance, often degraded by load imbalance, on highperformance computing (HPC) platforms. A number of dynamic loop scheduling (DLS) techniques have been proposed between the late 1980's and early 2000's, and efficiently used in scientific applications. In most cases, the computing systems on which they have been tested and validated are no longer available. This work is concerned with the minimization of the sources of uncertainty in the implementation of DLS techniques to avoid unnecessary influences on the performance of scientific applications. Therefore, it is important to ensure that the DLS techniques employed in scientific applications today adhere to their original design goals and specifications. The goal of this work is to attain and increase the trust in the implementation of DLS techniques in today's studies. To achieve this goal, the performance of a selection of scheduling experiments from the 1992 original work that introduced factoring is reproduced and predicted via both, simulative and native experimentation. The scientific challenge is the reproduction of the performance of the past experiments with incomplete information, such as the computing system characteristics and the implementation details. The experiments show that the simulation reproduces the performance achieved on the past computing platform and accurately predicts the performance achieved on the present computing platform. The performance reproduction and prediction confirm that the present implementation of the DLS techniques considered both, in simulation and natively, adheres to their original description. The results confirm the hypothesis that reproducing experiments of identical scheduling scenarios on past and modern hardware leads to an entirely different behavior from expected.", "venue": "2018 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Ali  Mohammed", "Ahmed  Eleliemy", "Florina M. Ciorba"], "year": 2018, "n_citations": 5}
{"id": 849517, "s2_id": "d9ff590885f16c142b84d59fd7339651d8326e2d", "title": "Machine Learning for Performance Prediction of Spark Cloud Applications", "abstract": "Big data applications and analytics are employed in many sectors for a variety of goals: improving customers satisfaction, predicting market behavior or improving processes in public health. These applications consist of complex software stacks that are often run on cloud systems. Predicting execution times is important for estimating the cost of cloud services and for effectively managing the underlying resources at runtime. Machine Learning (ML), providing black box solutions to model the relationship between application performance and system configuration without requiring in-detail knowledge of the system, has become a popular way of predicting the performance of big data applications. We investigate the cost-benefits of using supervised ML models for predicting the performance of applications on Spark, one of today's most widely used frameworks for big data analysis. We compare our approach with Ernest (an ML-based technique proposed in the literature by the Spark inventors) on a range of scenarios, application workloads, and cloud system configurations. Our experiments show that Ernest can accurately estimate the performance of very regular applications, but it fails when applications exhibit more irregular patterns and/or when extrapolating on bigger data set sizes. Results show that our models match or exceed Ernest's performance, sometimes enabling us to reduce the prediction error from 126-187% to only 5-19%.", "venue": "2019 IEEE 12th International Conference on Cloud Computing (CLOUD)", "authors": ["Alexandre  Maros", "Fabricio  Murai", "Ana Paula Couto da Silva", "Jussara M. Almeida", "Marco  Lattuada", "Eugenio  Gianniti", "Marjan  Hosseini", "Danilo  Ardagna"], "year": 2019, "n_citations": 8}
{"id": 854537, "s2_id": "f3dd3d8ac5707dbec84739428b010637179f0a22", "title": "Saturation throughput - delay analysis of IEEE 802.11 DCF in fading channel", "abstract": "We analytically analyzed the impact of an error-prone channel over all performance measures in a traffic-saturated IEEE 802.11 WLAN. We calculated station's transmission probability by using the modified Markov chain model of the backoff window size that considers the frame-error rates and maximal allowable number of retransmission attempts. The frame error rate has a significant impact over theoretical throughput, mean frame delay, and discard probability. The peak throughput of a WLAN is insensitive of the maximal number of retransmissions. Discard probabilities are insensitive to the station access method, basic or RTS/CTS.", "venue": "IEEE International Conference on Communications, 2003. ICC '03.", "authors": ["Zoran  Hadzi-Velkov", "Boris  Spasenovski"], "year": 2003, "n_citations": 192}
{"id": 859908, "s2_id": "8838b42fb558d3ece4afba31b3cb21273215c9ce", "title": "Performance portability through machine learning guided kernel selection in SYCL libraries", "abstract": "Automatically tuning parallel compute kernels allows libraries and frameworks to achieve performance on a wide range of hardware, however these techniques are typically focused on finding optimal kernel parameters for particular input sizes and parameters. General purpose compute libraries must be able to cater to all inputs and parameters provided by a user, and so these techniques are of limited use. Additionally, parallel programming frameworks such as SYCL require that the kernels be deployed in a binary format embedded within the library. As such it is impractical to deploy a large number of possible kernel configurations without inflating the library size. \nMachine learning methods can be used to mitigate against both of these problems and provide performance for general purpose routines with a limited number of kernel configurations. We show that unsupervised clustering methods can be used to select a subset of the possible kernels that should be deployed and that simple classification methods can be trained to select from these kernels at runtime to give good performance. As these techniques are fully automated, relying only on benchmark data, the tuning process for new hardware or problems does not require any developer effort or expertise.", "venue": "Parallel Comput.", "authors": ["John  Lawson"], "year": 2021, "n_citations": 0}
{"id": 861807, "s2_id": "478b7374c7be7552644118b7b9282dd16521f684", "title": "Liquid Cloud Storage", "abstract": "A liquid system provides durable object storage based on spreading redundantly generated data across a network of hundreds to thousands of potentially unreliable storage nodes. A liquid system uses a combination of a large code, lazy repair, and flow storage organization. We show that a liquid system can be operated to enable flexible and essentially optimal combinations of storage durability, storage overhead, repair bandwidth usage, and access performance.", "venue": "ACM Trans. Storage", "authors": ["Michael  Luby", "Roberto  Padovani", "Thomas J. Richardson", "Lorenz  Minder", "Pooja  Aggarwal"], "year": 2019, "n_citations": 18}
{"id": 862327, "s2_id": "abcdfd882824d0add9b03511a408bc32bc945fed", "title": "Formal and Informal Methods for Multi-Core Design Space Exploration", "abstract": "We propose a tool-supported methodology for design-space exploration for embedded systems. It provides means to define high-level models of applications and multi-processor architectures and evaluate the performance of different deployment (mapping, scheduling) strategies while taking uncertainty into account. We argue that this extension of the scope of formal verification is important for the viability of the domain.", "venue": "QAPL", "authors": ["Jean-Francois  Kempf", "Olivier  Lebeltel", "Oded  Maler"], "year": 2014, "n_citations": 0}
{"id": 863932, "s2_id": "9a585029b2099428ea15ad792915ad469603363d", "title": "Updatable Queue Protocol Based On TCP For Virtual Reality Environment", "abstract": "The variance in number and types of tasks required to be implemented within Distributed Virtual Environments (DVE) highlighted the needs for commun ication protocols could achieve consistency. In addition, these applications had to handle an incre asing number of participants and deal with the diff icult problem of scalability. Moreover, the real-time req uirements of these applications made the scalabilit y problem more difficult to be solved. In this paper, we had implemented Updatable Queue Abstraction protocol (UQA) on TCP (TCP-UQA) and compared our im plementation with the original TCP, UDP, and Updatable Queue Abstraction based on UDP (UDP-UQA) protocols. Results showed that TCP-UQA was the best in queue management.", "venue": "ArXiv", "authors": ["Ala'a Z. Al-Howaide", "Mohammed I. Khaleel", "Ayad M. Salhieh"], "year": 2011, "n_citations": 2}
{"id": 864930, "s2_id": "ac866b68b90890d1cc67b7f98ddc141a2f77c640", "title": "Analysis of an M/G/1 system for the optimization of the RTG performances in the delivery of containers in Abidjan Terminal", "abstract": "In front of the major challenges to increase its productivity while satisfying its customer, it is today important to establish in advance the operational performances of the RTG Abidjan Terminal. In this article, by using an M/G/1 retrial queue system, we obtained the average number of parked delivery trucks and as well as their waiting time. Finally, we used Matlab to represent them graphically then analyze the RTG performances according to the traffic rate.", "venue": "ArXiv", "authors": ["Bakary  Kone", "Salimata Gueye Diagne", "Dethie  Dione", "Coumba  Diallo"], "year": 2020, "n_citations": 0}
{"id": 866485, "s2_id": "f6ee999393ceacd00fdcc8e30f42002d7eb66c82", "title": "Evaluating the Performance of NVIDIA's A100 Ampere GPU for Sparse Linear Algebra Computations", "abstract": "GPU accelerators have become an important backbone for scientific high performance computing, and the performance advances obtained from adopting new GPU hardware are significant. In this paper we take a first look at NVIDIA's newest server line GPU, the A100 architecture part of the Ampere generation. Specifically, we assess its performance for sparse linear algebra operations that form the backbone of many scientific applications and assess the performance improvements over its predecessor.", "venue": "ArXiv", "authors": ["Yuhsiang Mike Tsai", "Terry  Cojean", "Hartwig  Anzt"], "year": 2020, "n_citations": 4}
{"id": 868113, "s2_id": "969354a199cfe6a91f1dda6beaca6ad3b1a8b417", "title": "Asymptotic Mean Time To Failure and Higher Moments for Large, Recursive Networks", "abstract": "This paper deals with asymptotic expressions of the Mean Time To Failure (MTTF) and higher moments for large, recursive, and non-repairable systems in the context of two-terminal reliability. Our aim is to extend the well-known results of the series and parallel cases. We first consider several exactly solvable configurations of identical components with exponential failure-time distribution functions to illustrate different (logarithmic or power-law) behaviors as the size of the system, indexed by an integer n, increases. The general case is then addressed: it provides a simple interpretation of the origin of the power-law exponent and an efficient asymptotic expression for the total reliability of large, recursive systems. Finally, we assess the influence of the non-exponential character of the component reliability on the n-dependence of the MTTF.", "venue": "ArXiv", "authors": ["Christian  Tanguy"], "year": 2008, "n_citations": 5}
{"id": 869805, "s2_id": "8afb670b4428b8d69ac40c1af65f49c6d1c502de", "title": "On the Performance of Space Shift Keying (SSK) Modulation with Imperfect Channel Knowledge", "abstract": "In this paper, we study the sensitivity and robustness of Space Shift Keying (SSK) modulation to imperfect channel knowledge at the receiver. Unlike the common widespread belief, we show that SSK modulation is more robust to imperfect channel knowledge than other state-of-the-art transmission technologies, and only few training pilots are needed to get reliable enough channel estimates for data detection. More precisely, we focus our attention on the so-called Time-Orthogonal-Signal-Design (TOSD-) SSK modulation scheme, which is an improved version of SSK modulation offering transmit-diversity gains, and provide the following contributions: i) we develop a closed-form analytical framework to compute the Average Bit Error Probability (ABEP) of a mismatched detector for TOSD-SSK modulation, which can be used for arbitrary transmit-antenna, receive-antenna, channel fading, and training pilots; ii) we perform a comparative study of the performance of TOSD-SSK modulation and the Alamouti code under the same imperfect channel knowledge, and show that TOSD-SSK modulation is more robust to channel estimation errors; iii) we point out that only few pilot pulses are required to get performance very close to the perfect channel knowledge lower-bound; and iv) we verify that transmit- and receive-diversity gains of TOSD-SSK modulation are preserved even for a mismatched receiver.", "venue": "2011 IEEE Global Telecommunications Conference - GLOBECOM 2011", "authors": ["Marco Di Renzo", "Dario De Leonardis", "Fabio  Graziosi", "Harald  Haas"], "year": 2011, "n_citations": 9}
{"id": 871443, "s2_id": "f9ae117a1118715d683c98de551972454d24a3ce", "title": "Exact Failure Frequency Calculations for Extended Systems", "abstract": "This paper shows how the steady-state availability and failure frequency can be calculated in a single pass for very large systems, when the availability is expressed as a product of matrices. We apply the general procedure to $k$-out-of-$n$:G and linear consecutive $k$-out-of-$n$:F systems, and to a simple ladder network in which each edge and node may fail. We also give the associated generating functions when the components have identical availabilities and failure rates. For large systems, the failure rate of the whole system is asymptotically proportional to its size. This paves the way to ready-to-use formulae for various architectures, as well as proof that the differential operator approach to failure frequency calculations is very useful and straightforward.", "venue": "ArXiv", "authors": ["Annie  Druault-Vicard", "Christian  Tanguy"], "year": 2006, "n_citations": 5}
{"id": 871711, "s2_id": "593667716e04e3747b791904bc2e7afd500db7be", "title": "Towards a system theoretic approach to wireless network capacity in finite time and space", "abstract": "In asymptotic regimes, both in time and space (network size), the derivation of network capacity results is grossly simplified by brushing aside queueing behavior in nonJackson networks. This simplifying double-limit model, however, lends itself to conservative numerical results in finite regimes. To properly account for queueing behavior beyond a simple calculus based on average rates, we advocate a system theoretic methodology for the capacity problem in finite time and space regimes. This methodology also accounts for spatial correlations arising in networks with CSMA/CA scheduling and it delivers rigorous closed-form capacity results in terms of probability distributions. Unlike numerous existing asymptotic results, subject to anecdotal practical concerns, our transient results can be used in practical settings, e.g., to compute the time scales at which multi-hop routing is more advantageous than single-hop routing.", "venue": "IEEE INFOCOM 2014 - IEEE Conference on Computer Communications", "authors": ["Florin  Ciucu", "Ramin  Khalili", "Yuming  Jiang", "Liu  Yang", "Yong  Cui"], "year": 2014, "n_citations": 13}
{"id": 872590, "s2_id": "7ab50900483bbe8c5a28eb0a7edaf50b13cefb96", "title": "Performance improvement of the software development project using the Value Management approach", "abstract": "Improving performance and delivering value for customers have become a central theme in business. The software industry has become an increasingly important sector for the economy growth in Tunisia. This study aims to show how using Value Management in the Tunisian software industry for project analysis gives new insight about true project value and performance. This new approach is considered as an appropriate tool for guiding the process of making decisions. It offers tools in order to analyze the service value from the customer and organization perspectives. The results showed that the VM allows to have better performance in the software development project by linking customer satisfaction and cost analysis. The present case shows to service managers how they can benchmark project function to reduce their costs and improve resource allocation taking into consideration what customers consider important during their overall service experience. It can identify best professional practices, orient decisions to improve service value", "venue": "ArXiv", "authors": ["Amel Ben Hadj Salem-Mhamdia", "Bahia Bejar Ghadhab"], "year": 2011, "n_citations": 2}
{"id": 874832, "s2_id": "1b63ebf5883363268b6b3aee3b88c03ecf542d7e", "title": "Queueing Analysis of GPU-Based Inference Servers with Dynamic Batching: A Closed-Form Characterization", "abstract": "GPU-accelerated computing is a key technology to realize high-speed inference servers using deep neural networks (DNNs). An important characteristic of GPU-based inference is that the computational efficiency, in terms of the processing speed and energy consumption, drastically increases by processing multiple jobs together in a batch. In this paper, we formulate GPU-based inference servers as a batch service queueing model with batch-size dependent processing times. We first show that the energy efficiency of the server monotonically increases with the arrival rate of inference jobs, which suggests that it is energy-efficient to operate the inference server under a utilization level as high as possible within a latency requirement of inference jobs. We then derive a closed-form upper bound for the mean latency, which provides a simple characterization of the latency performance. Through simulation and numerical experiments, we show that the exact value of the mean latency is well approximated by this upper bound.", "venue": "Perform. Evaluation", "authors": ["Yoshiaki  Inoue"], "year": 2021, "n_citations": 1}
{"id": 874879, "s2_id": "bd0e8e7b4372c197f62e39815633955f47daa271", "title": "Learning Queuing Networks by Recurrent Neural Networks", "abstract": "It is well known that building analytical performance models in practice is difficult because it requires a considerable degree of proficiency in the underlying mathematics. In this paper, we pro- pose a machine-learning approach to derive performance models from data. We focus on queuing networks, and crucially exploit a deterministic approximation of their average dynamics in terms of a compact system of ordinary differential equations. We encode these equations into a recurrent neural network whose weights can be directly related to model parameters. This allows for an inter- pretable structure of the neural network, which can be trained from system measurements to yield a white-box parameterized model that can be used for prediction purposes such as what-if analyses and capacity planning. Using synthetic models as well as a real case study of a load-balancing system, we show the effectiveness of our technique in yielding models with high predictive power.", "venue": "ICPE", "authors": ["Giulio  Garbi", "Emilio  Incerto", "Mirco  Tribastone"], "year": 2020, "n_citations": 3}
{"id": 875675, "s2_id": "804393840572e3336aa23643cad027d6c5f15b01", "title": "A Framework for QoS-aware Execution of Workflows over the Cloud", "abstract": "The Cloud Computing paradigm is providing system architects with a new powerful tool for building scalable applications. Clouds allow allocation of resources on a \"pay-as-you-go\" model, so that additional resources can be requested during peak loads and released after that. However, this flexibility asks for appropriate dynamic reconfiguration strategies. In this paper we describe SAVER (qoS-Aware workflows oVER the Cloud), a QoS-aware algorithm for executing workflows involving Web Services hosted in a Cloud environment. SAVER allows execution of arbitrary workflows subject to response time constraints. SAVER uses a passive monitor to identify workload fluctuations based on the observed system response time. The information collected by the monitor is used by a planner component to identify the minimum number of instances of each Web Service which should be allocated in order to satisfy the response time constraint. SAVER uses a simple Queueing Network (QN) model to identify the optimal resource allocation. Specifically, the QN model is used to identify bottlenecks, and predict the system performance as Cloud resources are allocated or released. The parameters used to evaluate the model are those collected by the monitor, which means that SAVER does not require any particular knowledge of the Web Services and workflows being executed. Our approach has been validated through numerical simulations, whose results are reported in this paper.", "venue": "CLOSER", "authors": ["Moreno  Marzolla", "Raffaela  Mirandola"], "year": 2012, "n_citations": 3}
{"id": 877959, "s2_id": "85f6fbb74339be3b2d87ff5198ac440035ed63bb", "title": "Deep Reinforcement Learning for Multi-Resource Multi-Machine Job Scheduling", "abstract": "Minimizing job scheduling time is a fundamental issue in data center networks that has been extensively studied in recent years. The incoming jobs require different CPU and memory units, and span different number of time slots. The traditional solution is to design efficient heuristic algorithms with performance guarantee under certain assumptions. In this paper, we improve a recently proposed job scheduling algorithm using deep reinforcement learning and extend it to multiple server clusters. Our study reveals that deep reinforcement learning method has the potential to outperform traditional resource allocation algorithms in a variety of complicated environments.", "venue": "ArXiv", "authors": ["Weijia  Chen", "Yuedong  Xu", "Xiaofeng  Wu"], "year": 2017, "n_citations": 26}
{"id": 879922, "s2_id": "c994037e056d9ef64a789e4b1b1ae9ebbcaf2d15", "title": "Communication over a time correlated channel with an energy harvesting transmitter", "abstract": "In this work, communication over a time-correlated point-to-point wireless channel is studied for an energy harvesting (EH) transmitter. In this model, we take into account the time and energy cost of acquiring channel state information. At the beginning of the time slot, the EH transmitter, has to choose among three possible actions: i) deferring the transmission to save its energy for future use, ii) transmitting without sensing, and iii) sensing the channel before transmission. At each time slot, the transmitter chooses one of the three possible actions to maximize the total expected discounted number of bits transmitted over an infinite time horizon. This problem can be formulated as a partially observable Markov decision process (POMDP) which is then converted to an ordinary MDP by introducing a belief on the channel state, and the optimal policy is shown to exhibit a threshold behavior on the belief state, with battery-dependent threshold values. Optimal threshold values and corresponding optimal performance are characterized through numerical simulations, and it is shown that having the sensing action and intelligently using it to track the channel state improves the achievable long-term throughput significantly.", "venue": "2017 International Symposium on Wireless Communication Systems (ISWCS)", "authors": ["Mehdi Salehi Heydar Abad", "Deniz  G\u00fcnd\u00fcz", "\u00d6zg\u00fcr  Er\u00e7etin"], "year": 2017, "n_citations": 2}
{"id": 880731, "s2_id": "910478a1c1a54f10ec65e397519273ede76a5a80", "title": "SECS: Efficient Deep Stream Processing via Class Skew Dichotomy", "abstract": "Despite that accelerating convolutional neural network (CNN) receives an increasing research focus, the save on resource consumption always comes with a decrease in accuracy. To both increase accuracy and decrease resource consumption, we explore an environment information, called class skew, which is easily available and exists widely in daily life. Since the class skew may switch as time goes, we bring up probability layer to utilize class skew without any overhead during the runtime. Further, we observe class skew dichotomy that some class skew may appear frequently in the future, called hot class skew, and others will never appear again or appear seldom, called cold class skew. Inspired by techniques from source code optimization, two modes, i.e., interpretation and compilation, are proposed. The interpretation mode pursues efficient adaption during runtime for cold class skew and the compilation mode aggressively optimize on hot ones for more efficient deployment in the future. Aggressive optimization is processed by class-specific pruning and provides extra benefit. Finally, we design a systematic framework, SECS, to dynamically detect class skew, processing interpretation and compilation, as well as select the most accurate architectures under the runtime resource budget. Extensive evaluations show that SECS can realize end-to-end classification speedups by a factor of 3x to 11x relative to state-of-the-art convolutional neural networks, at a higher accuracy.", "venue": "ArXiv", "authors": ["Boyuan  Feng", "Kun  Wan", "Shu  Yang", "Yufei  Ding"], "year": 2018, "n_citations": 3}
{"id": 882623, "s2_id": "dae455a93cd98ee814c0c4f0d3338ec7a94d7781", "title": "Accelerating Sparse Matrix-Matrix Multiplication with GPU Tensor Cores", "abstract": "Abstract Sparse general matrix\u2013matrix multiplication (spGEMM) is an essential component in many scientific and data analytics applications. However, the sparsity pattern of the input matrices and the interaction of their patterns make spGEMM challenging. Modern GPUs include Tensor Core Units (TCUs), which specialize in dense matrix multiplication. Our aim is to re-purpose TCUs for sparse matrices. The key idea of our spGEMM algorithm, tSparse, is to multiply sparse rectangular blocks using the mixed precision mode of TCUs. tSparse partitions the input matrices into tiles and operates only on tiles which contain one or more elements. It creates a task list of the tiles, and performs matrix multiplication of these tiles using TCUs. To the best of our knowledge, this is the first time that TCUs are used in the context of spGEMM. We show that spGEMM, with our tiling approach, benefits from TCUs. Our approach significantly improves the performance of spGEMM in comparison to cuSPARSE, CUSP, RMerge2, Nsparse, AC-SpGEMM and spECK.", "venue": "Comput. Electr. Eng.", "authors": ["Orestis  Zachariadis", "Nitin  Satpute", "Juan  G'omez-Luna", "Joaqu'in  Olivares"], "year": 2020, "n_citations": 13}
{"id": 883500, "s2_id": "8322b8a765f5e2714198e62d553886ed5e8497fe", "title": "Efficient Method for Parallel Computation of Geodesic Transformation on CPU", "abstract": "This article introduces a fast Central Processing Unit (CPU) implementation of geodesic morphological operations using stream processing. In contrast to the current state-of-the-art, that focuses on achieving insensitivity to the filter sizes with efficient data structures, the proposed approach achieves efficient computation of long chains of elementary <inline-formula><tex-math notation=\"LaTeX\">$3 \\times 3$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>\u00d7</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"zlaus-ieq1-2953057.gif\"/></alternatives></inline-formula> filters using multicore and Single Instruction Multiple Data (SIMD) processing. In comparison to the related methods, up to 100 times faster computation of common geodesic operators is achieved in this way, allowing for real-time processing (with over 30 FPS) of up to 1500 filters long chains, applied on <inline-formula><tex-math notation=\"LaTeX\">$1024\\times 1024$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1024</mml:mn><mml:mo>\u00d7</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"zlaus-ieq2-2953057.gif\"/></alternatives></inline-formula> images. In addition, the proposed approach outperformed GPGPU, and proved to be more efficient than the comparable streaming method for the computation of morphological erosions and dilations with window sizes up to <inline-formula><tex-math notation=\"LaTeX\">$183\\times 183$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>183</mml:mn><mml:mo>\u00d7</mml:mo><mml:mn>183</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"zlaus-ieq3-2953057.gif\"/></alternatives></inline-formula> in the case of using <monospace>char</monospace> and <inline-formula><tex-math notation=\"LaTeX\">$27\\times 27$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>27</mml:mn><mml:mo>\u00d7</mml:mo><mml:mn>27</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"zlaus-ieq4-2953057.gif\"/></alternatives></inline-formula> when using <monospace>double</monospace> data types.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Danijel  \u017dlaus", "Domen  Mongus"], "year": 2020, "n_citations": 1}
{"id": 883800, "s2_id": "066799b6b45868d0e679e4882e3efdee3e6edefd", "title": "Consensus in Blockchain Systems with Low Network Throughput: A Systematic Mapping Study", "abstract": "Blockchain technologies originate from cryptocurrencies. Thus, most blockchain technologies assume an environment with a fast and stable network. However, in some blockchain-based systems, e.g., supply chain management (SCM) systems, some Internet of Things (IoT) nodes can only rely on the low-quality network sometimes to achieve consensus. Thus, it is critical to understand the applicability of existing consensus algorithms in such environments. We performed a systematic mapping study to evaluate and compare existing consensus mechanisms\u2019 capability to provide integrity and security with varying network properties. Our study identified 25 state-of-the-art consensus algorithms from published and preprint literature. We categorized and compared the consensus algorithms qualitatively based on established performance and integrity metrics and well-known blockchain security issues. Results show that consensus algorithms that rely on synchronous network for correctness cannot provide the expected integrity. Such consensus algorithms may also be vulnerable to distributed-denial-of-service (DDOS) and routing attacks, given limited network throughput. Conversely, asynchronous consensus algorithms, e.g., Honey-BadgerBFT, are deemed more robust against many of these attacks and may provide high integrity in asynchronous events.", "venue": "2021 3rd Blockchain and Internet of Things Conference", "authors": ["Henrik  Knudsen", "Jakob Svennevik Notland", "Peter Halland Haro", "Truls Bakkejord R\u00e6der", "Jingyue  Li"], "year": 2021, "n_citations": 1}
{"id": 889298, "s2_id": "b023e6c346b59c9137d2b10bd0cbdce70db2f1cc", "title": "A Joint Precoding Framework for Wideband Reconfigurable Intelligent Surface-Aided Cell-Free Network", "abstract": "Thanks to the strong ability against the inter-cell interference, cell-free network is considered as a promising technique to improve network capacity. However, further capacity improvement requires to deploy more base stations (BSs) with high cost and power consumption. To address this issue, inspired by the recently developed reconfigurable intelligent surface (RIS) technique, we propose the concept of RIS-aided cell-free network to improve the capacity with low cost and power consumption. The key idea is to replace some of the required BSs by low-cost and energy-efficient RISs. Then, in a wideband RIS-aided cell-free network, we formulate the problem of joint precoding design at BSs and RISs to maximize the network capacity. Due to the non-convexity and high complexity of the formulated problem, we develop an alternating optimization framework to solve this challenging problem. In particular, we decouple this problem via fractional programming, and solve the subproblems alternatively. Note that most of the scenarios considered in existing works are special cases of the general scenario studied in this paper, and the proposed joint precoding framework can serve as a general solution to maximize the capacity in most existing RIS-aided scenarios. Finally, simulation results demonstrate that, compared with the conventional cell-free network, the network capacity under the proposed scheme can be improved significantly.", "venue": "IEEE Transactions on Signal Processing", "authors": ["Zijian  Zhang", "Linglong  Dai"], "year": 2021, "n_citations": 33}
{"id": 891719, "s2_id": "f6b75f5d169c6ae43b4061c4e53ac77b99dc8111", "title": "CloudCoaster: Transient-aware Bursty DatacenterWorkload Scheduling", "abstract": "Today's clusters often have to divide resources among a diverse set of jobs. These jobs are heterogeneous both in execution time and in their rate of arrival. Execution time heterogeneity has lead to the development of hybrid schedulers that can schedule both short and long jobs to ensure good task placement. However, arrival rate heterogeneity, or burstiness, remains a problem in existing schedulers. These hybrid schedulers manage resources on statically provisioned cluster, which can quickly be overwhelmed by bursts in the number of arriving jobs. \nIn this paper we propose CloudCoaster, a hybrid scheduler that dynamically resizes the cluster by leveraging cheap transient servers. CloudCoaster schedules jobs in an intelligent way that increases job performance while reducing overall resource cost. We evaluate the effectiveness of CloudCoaster through simulations on real-world traces and compare it against a state-of-art hybrid scheduler. CloudCoaster improves the average queueing delay time of short jobs by 4.8X while maintaining long job performance. In addition, CloudCoaster reduces the short partition budget by over 29.5%.", "venue": "ArXiv", "authors": ["Samuel S. Ogden", "Tian  Guo"], "year": 2019, "n_citations": 0}
{"id": 892446, "s2_id": "d414f69fc25cb051ec61e53974ed27097f588789", "title": "Performance Evaluation of Sparse Matrix Multiplication Kernels on Intel Xeon Phi", "abstract": "Intel Xeon Phi is a recently released high-performance coprocessor which features 61 cores each supporting 4 hardware threads with 512-bit wide SIMD registers achieving a peak theoretical performance of 1Tflop/s in double precision. Its design differs from classical modern processors; it comes with a large number of cores, the 4-way hyperthreading capability allows many applications to saturate the massive memory bandwidth, and its large SIMD capabilities allow to reach high computation throughput. The core of many scientific applications involves the multiplication of a large, sparse matrix with a single or multiple dense vectors which are not compute-bound but memory-bound. In this paper, we investigate the performance of the Xeon Phi coprocessor for these sparse linear algebra kernels. We highlight the important hardware details and show that Xeon Phi\u2019s sparse kernel performance is very promising and even better than that of cutting-edge CPUs and GPUs.", "venue": "PPAM", "authors": ["Erik  Saule", "Kamer  Kaya", "\u00dcmit V. \u00c7ataly\u00fcrek"], "year": 2013, "n_citations": 157}
{"id": 893422, "s2_id": "257259e9a55c20adf6b7291d9f336db6b1f42283", "title": "Efficient LBM on GPUs for dense moving objects using immersed boundary condition", "abstract": "There exists an increasing interest for using immersed boundary methods (IBMs) (Peskin 2000) to model moving objects in computational fluid dynamics. Indeed, this approach is particularly efficient, because the fluid mesh does not require to be body-fitted or to adjust dynamically to the motion of the body. Frequently, IBMs are implemented in combination with the lattice Boltzmann methods (LBM) (Kruger 2016). They fit elegantly into the framework of this method, and yield impressive parallel performances. It has also become quite common to accelerate LBM simulations with the use of Graphics Processing Units (GPUs) (Tolke 2010), as the underlying algorithm adjusts naturally to the architecture of such platforms. It is not uncommon that speedups of an order of magnitude, or more, at equal financial cost or energy consumption are observed, as compared to classical CPUs. IBM algorithms are however more difficult to adapt to GPUs, because their complex memory access pattern conflicts with a GPU's strategy of broadcasting data to a large number of GPU cores in single memory accesses. In the existing literature, GPU implementations of LBM-IBM codes are therefore restricted to situations in which the immersed surfaces are very small compared to the total number of fluid cells (Valero-Lara 2014), as is often the case in exterior flow simulations around an obstacle. This assumption is however not valid in many other cases of interest. \nWe propose a new method for the implementation of a LBM-IBM on GPUs in the CUDA language, which allows to handle a substantially larger immersed surfaces with acceptable performance than previous implementations.", "venue": "ArXiv", "authors": ["Jo\u00ebl  B\u00e9ny", "Jonas  Latt"], "year": 2019, "n_citations": 4}
{"id": 895073, "s2_id": "a44a7b381a8fc366fcda9294f2318f9815fb80a5", "title": "Delay Asymptotics and Bounds for Multi-Task Parallel Jobs", "abstract": "We study delay of jobs that consist of multiple parallel tasks, which is a critical performance metric in a wide range of applications such as data file retrieval in coded storage systems and parallel computing. In this problem, each job is completed only when all of its tasks are completed, so the delay of a job is the maximum of the delays of its tasks. Despite the wide attention this problem has received, tight analysis is still largely unknown since analyzing job delay requires characterizing the complicated correlation among task delays, which is hard to do.\n We first consider an asymptotic regime where the number of servers, n, goes to infinity, and the number of tasks in a job, k(n), is allowed to increase with n. We establish the asymptotic independence of any k(n) queues under the condition k(n) = o(n1/4). This greatly generalizes the asymptotic-independence type of results in the literature where asymptotic independence is shown only for a fixed constant number of queues. As a consequence of our independence result, the job delay converges to the maximum of independent task delays.\n We next consider the non-asymptotic regime. Here we prove that independence yields a stochastic upper bound on job delay for any n and any k(n) with k(n)\u2264n. The key component of our proof is a new technique we develop, called \"Poisson oversampling\". Our approach converts the job delay problem into a corresponding balls-and-bins problem. However, in contrast with typical balls-and-bins problems where there is a negative correlation among bins, we prove that our variant exhibits positive correlation. A full version of this paper will all proofs appears in [28].", "venue": "PERV", "authors": ["Weina  Wang", "Mor  Harchol-Balter", "Haotian  Jiang", "Alan  Scheller-Wolf", "R.  Srikant"], "year": 2019, "n_citations": 12}
{"id": 895801, "s2_id": "3c2ba59bf9bb1058f091806cf44f438e919aa1e7", "title": "Distributed Fair Scheduling for Information Exchange in Multi-Agent Systems", "abstract": "Information exchange is a crucial component of many realworld multi-agent systems. However, the communication between the agents involves two major challenges: the limited bandwidth, and the shared communication medium between the agents, which restricts the number of agents that can simultaneously exchange information. While both of these issues need to be addressed in practice, the impact of the latter problem on the performance of the multi-agent systems has often been neglected. This becomes even more important when the agents\u2019 information or observations have different importance, in which case the agents require different priorities for accessing the medium and sharing their information. Representing the agents\u2019 priorities by fairness weights and normalizing each agent\u2019s share by the assigned fairness weight, the goal can be expressed as equalizing the agents\u2019 normalized shares of the communication medium. To achieve this goal, we adopt a queueing theoretic approach and propose a distributed fair scheduling algorithm for providing weighted fairness in single-hop networks. Our proposed algorithm guarantees an upper-bound on the normalized share disparity among any pair of agents. This can particularly improve the short-term fairness, which is important in real-time applications. Moreover, our scheduling algorithm adjusts itself dynamically to achieve a high throughput at the same time. The simulation results validate our claims and comparisons with the existing methods show our algorithm\u2019s superiority in providing short-term fairness, while achieving a high throughput. Introduction Many real-world multi-agent systems rely on the information exchange between the agents. However, the communication of the agents involves major practical limitations such as the shared communication medium and the limited bandwidth. Whether the information exchange is required for the coordination of agents, or solely the transfer of information from one point to another, the agents cannot transmit their messages simultaneously over the same communication medium. This becomes even more challenging when there is no coordinator to arbitrate access to the medium and therefore, a distributed algorithm is required for scheduling the agents\u2019 transmissions. Fairness is a key Copyright \u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. concern in distributed scheduling algorithms, where some agents might hog the communication medium for a long period of time and therefore, lead to starvation of the other agents. Moreover, the agents\u2019 messages might have different importance or different latency requirements. Information exchange in multi-agent reinforcement learning (MARL) is one such example, where the importance of each agent\u2019s partially observed information can be different from one another (Kim et al. 2018). Weighted fair queueing, which has long been used as an effective basis for resource allocation and scheduling, is a natural candidate for dealing with these cases. The main goal of the weighted fair queueing algorithms is to provide service in proportion to some specified service shares (also known as fairness weights) to the competing users of a shared resource. The Generalized Processor Sharing (GPS) scheme (Parekh and Gallager 1993) serves as the reference for most of the existing fair queueing algorithms in the literature. The GPS algorithm uses a fluid flow model in which all the users can receive service simultaneously. However, because of the fluid flow assumption, the GPS scheme is not applicable in many real-world applications such as information exchange. Consequently, alternative fair queueing algorithms such as (Golestani 1994; Goyal, Vin, and Cheng 1997) have been proposed to mimic the behaviour of the GPS algorithm in real-world packetbased applications. However, the majority of these algorithms are centralized in implementation and cannot be used directly to provide fair queueing in a distributed manner. In the information exchange context, the communication medium is the shared resource that needs to be scheduled among the agents. In wireless networks, the medium access control (MAC) protocol is responsible for scheduling the users. IEEE 802.11 DCF1 (Wi-Fi) is the dominant distributed MAC protocol in wireless networks. Although there has been some effort in designing distributed fair scheduling algorithms based on 802.11 DCF, almost all these proposed methods take heuristic approaches to emulate a particular centralized fair queueing algorithm without providing any guarantees or theoretical analysis. Moreover, they suffer from short-term unfairness, which is particularly imWe use the terms 802.11 DCF and Wi-Fi interchangeably for referring to the Distributed Coordination Function (DCF) protocol in IEEE 802.11 ar X iv :2 10 2. 08 81 4v 1 [ cs .M A ] 1 7 Fe b 20 21 portant in real-time applications. In contrast, we propose a distributed fair scheduling algorithm, which provides deterministic guarantees on the normalized share disparity among any pair of agents. Specifically, we propose a distributed scheduling algorithm for information exchange in singlehop wireless networks, which guarantees a bounded disparity between the normalized services received by any two agents. To the best of our knowledge, this is the first time that a distributed scheduling algorithm is capable of providing a bounded service disparity among the users. This particularly improves the short-term fairness of our proposed algorithm compared to the existing ones. Furthermore, our algorithm dynamically adjusts itself to balance the trade-off between the fairness and the network throughput. A Brief Background on 802.11 Wi-Fi Wi-Fi is the dominant protocol for wireless communication scheduling, which is based on CSMA (Carrier-Sense Multiple Access). The main idea of CSMA is that each agent is required to listen to the medium before its transmission. The agent is allowed to transmit its message only if the medium is idle. However, to avoid collisions after busy periods, it uses the Binary Exponential Backoff (BEB) mechanism, which requires the agents to choose random backoffs in the interval [0, CW ]. Whenever the medium becomes idle, each agent discretizes the time into time slots of predefined length (denoted by \u03c3) and decrements its backoff counter by one after each idle time slot. The backoff counter will be frozen during the busy periods. When the counter reaches zero, the agent is allowed to attempt a transmission. The parameter CW is called the contention window, which is doubled by the agent if its transmission is unsuccessful. CW is reset to its default value once the agent transmits its message successfully. Moreover, 802.11 uses interframe spaces (IFS), which are specified waiting periods between transmission of frames, to prioritize different traffic types (such as control and data). Related Work Efficient and fair communication between agents of a system is a challenging task. Particularly, it is a key component in applications that require agents\u2019 coordination, such as cooperative multi-agent reinforcement learning. Most of the existing studies on multi-agent communication only focus on the bandwidth limitation of the medium and ignore the shared medium contention between the agents (Goldman and Zilberstein 2003; Mao et al. 2020; Foerster et al. 2016; Zhang and Lesser 2013). For instance, a message pruning mechanism is proposed by (Mao et al. 2020) to improve the bandwidth efficiency of the recent deep RL-based communication methods for multi-agent systems (Foerster et al. 2016; Jiang and Lu 2018; Peng, Zhang, and Luo 2018; Singh, Jain, and Sukhbaatar 2019). In another work, (Zhang, Zhang, and Lin 2019) introduce Variance Based Control (VBC) technique for efficient communication in MARL, which limits the variance of the exchanged messages between the agents in the training phase. (Goldman and Zilberstein 2003) have developed a theoretical model for decentralized control of a cooperative multi-agent system with communication. This Agent 1 Agent 2 Agent 3 Agent $ Agent % ... ...", "venue": "ICAPS", "authors": ["Majid  Raeis", "S. Jamaloddin Golestani"], "year": 2021, "n_citations": 1}
{"id": 896290, "s2_id": "655946c9468944524a7ee19a51c4d62225420f52", "title": "Efficient HTTP Based I/O on Very Large Datasets for High Performance Computing with the Libdavix Library", "abstract": "Remote data access for data analysis in high performance computing is commonly done with specialized data access protocols and storage systems. These protocols are highly optimized for high throughput on very large datasets, multi-streams, high availability, low latency and efficient parallel I/O. The purpose of this paper is to describe how we have adapted a generic protocol, the Hyper Text Transport Protocol (HTTP) to make it a competitive alternative for high performance I/O and data analysis applications in a global computing grid: the Worldwide LHC Computing Grid. In this work, we first analyze the design differences between the HTTP protocol and the most common high performance I/O protocols, pointing out the main performance weaknesses of HTTP. Then, we describe in detail how we solved these issues. Our solutions have been implemented in a toolkit called davix, available through several recent Linux distributions.", "venue": "BPOE@ASPLOS/VLDB", "authors": ["Adrien  Devresse", "Fabrizio  Furano"], "year": 2014, "n_citations": 6}
{"id": 907452, "s2_id": "50581dbca10c9b18f42d65e74d763038bbf054a0", "title": "Enabling microbiome research on personal devices", "abstract": "Microbiome studies have recently transitioned from experimental designs with a few hundred samples to designs spanning tens of thousands of samples. Modern studies such as the Earth Microbiome Project (EMP) afford the statistics crucial for untangling the many factors that influence microbial community composition. Analyzing those data used to require access to a compute cluster, making it both expensive and inconvenient. We show that recent improvements in both hardware and software now allow to compute key bioinformatics tasks on EMP-sized data in minutes using a gaming-class laptop, enabling much faster and broader microbiome science insights.", "venue": "2021 IEEE 17th International Conference on eScience (eScience)", "authors": ["Igor  Sfiligoi", "Daniel  McDonald", "Rob  Knight"], "year": 2021, "n_citations": 0}
{"id": 908711, "s2_id": "fd15384646a386521ee4824c35011bc17e6fa987", "title": "Queue-Learning: A Reinforcement Learning Approach for Providing Quality of Service", "abstract": "End-to-end delay is a critical attribute of quality of service (QoS) in application domains such as cloud computing and computer networks. This metric is particularly important in tandem service systems, where the end-to-end service is provided through a chain of services. Service-rate control is a common mechanism for providing QoS guarantees in service systems. In this paper, we introduce a reinforcement learningbased (RL-based) service-rate controller that provides probabilistic upper-bounds on the end-to-end delay of the system, while preventing the overuse of service resources. In order to have a general framework, we use queueing theory to model the service systems. However, we adopt an RL-based approach to avoid the limitations of queueing-theoretic methods. In particular, we use Deep Deterministic Policy Gradient (DDPG) to learn the service rates (action) as a function of the queue lengths (state) in tandem service systems. In contrast to existing RL-based methods that quantify their performance by the achieved overall reward, which could be hard to interpret or even misleading, our proposed controller provides explicit probabilistic guarantees on the end-to-end delay of the system. The evaluations are presented for a tandem queueing system with non-exponential inter-arrival and service times, the results of which validate our controller\u2019s capability in meeting QoS constraints.", "venue": "AAAI", "authors": ["Majid  Raeis", "Ali  Tizghadam", "Alberto  Leon-Garcia"], "year": 2021, "n_citations": 2}
{"id": 915058, "s2_id": "c478f7ad50c7e42528de4d0eb58e13c4f5c85d20", "title": "Report from GI-Dagstuhl Seminar 16394: Software Performance Engineering in the DevOps World", "abstract": "This report documents the program and the outcomes of GI-Dagstuhl Seminar 16394 \u201cSoftware Performance Engineering in the DevOps World\u201d.", "venue": "ArXiv", "authors": ["Andr\u00e9 van Hoorn", "Pooyan  Jamshidi", "Philipp  Leitner", "Ingo  Weber"], "year": 2017, "n_citations": 7}
{"id": 919318, "s2_id": "1c419e6a481bd02c094ddb995e620711b64773b8", "title": "Priority-Aware Near-Optimal Scheduling for Heterogeneous Multi-Core Systems with Specialized Accelerators", "abstract": "To deliver high performance in power limited systems, architects have turned to using heterogeneous systems, either CPU+GPU or mixed CPU-hardware systems. However, in systems with different processor types and task affinities, scheduling tasks becomes more challenging than in homogeneous multi-core systems or systems without task affinities. The problem is even more complex when specialized accelerators and task priorities are included. In this paper, we provide a formal proof for the optimal scheduling policy for heterogeneous systems with arbitrary number of resource types, including specialized accelerators, independent of the task arrival rate, task size distribution, and resource processing order. We transform the optimal scheduling policy to a nonlinear integer optimization problem and propose a fast, near-optimal algorithm. An additional heuristic is proposed for the case of priority-aware scheduling. Our experimental results demonstrate that the proposed algorithm is only 0.3% from the optimal and superior to conventional scheduling policies.", "venue": "ArXiv", "authors": ["Zhuo  Chen", "Diana  Marculescu"], "year": 2017, "n_citations": 1}
{"id": 920099, "s2_id": "a5e4e6b18ddaeacc75e006342328b7d6f7455c96", "title": "Competitive Online Optimization under Inventory Constraints", "abstract": "This paper studies online optimization under inventory (budget) constraints. While online optimization is a well-studied topic, versions with inventory constraints have proven difficult. We consider a formulation of inventory-constrained optimization that is a generalization of the classic one-way trading problem and has a wide range of applications. We present a new algorithmic framework, \\textsfCR-Pursuit, and prove that it achieves the minimal competitive ratio among all deterministic algorithms (up to a problem-dependent constant factor) for inventory-constrained online optimization. Our algorithm and its analysis not only simplify and unify the state-of-the-art results for the standard one-way trading problem, but they also establish novel bounds for generalizations including concave revenue functions. For example, for one-way trading with price elasticity, the \\textsfCR-Pursuit algorithm achieves a competitive ratio that is within a small additive constant (i.e., 1/3) to the lower bound of ln 0+1, where 0 is the ratio between the maximum and minimum base prices.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Qiulin  Lin", "Hanling  Yi", "John Z. F. Pang", "Minghua  Chen", "Adam  Wierman", "Michael  Honig", "Yuanzhang  Xiao"], "year": 2019, "n_citations": 13}
{"id": 920261, "s2_id": "72f6740aecbb07a8f5c835a02664cee33776f349", "title": "Performance bounds in wormhole routing, a network calculus approach", "abstract": "We present a model of performance bound calculus on feedforward networks where data packets are routed under wormhole routing discipline. We are interested in determining maximum end-to-end delays and backlogs of messages or packets going from a source node to a destination node, through a given virtual path in the network. Our objective here is to give a network calculus approach for calculating the performance bounds. First we propose a new concept of curves that we call packet curves. The curves permit to model constraints on packet lengths of a given data flow, when the lengths are allowed to be different. Second, we use this new concept to propose an approach for calculating residual services for data flows served under non preemptive service disciplines. Third, we model a binary switch (with two input ports and two output ports), where data is served under wormhole discipline. We present our approach for computing the residual services and deduce the worst case bounds for flows passing through a wormhole binary switch. Finally, we illustrate this approach in numerical examples, and show how to extend it to feedforward networks.", "venue": "ArXiv", "authors": ["Nadir  Farhi", "Bruno  Gaujal"], "year": 2010, "n_citations": 1}
{"id": 921429, "s2_id": "726a5023a514fd5fa3db8cc87e28f78ad1812881", "title": "Energy Drain of the Object Detection Processing Pipeline for Mobile Devices: Analysis and Implications", "abstract": "Applying deep learning to object detection provides the capability to accurately detect and classify complex objects in the real world. However, currently, few mobile applications use deep learning because such technology is computation-intensive and energy-consuming. This article, to the best of our knowledge, presents the first detailed experimental study of a mobile augmented reality (AR) client\u2019s energy consumption and the detection latency of executing Convolutional Neural Networks (CNN) based object detection, either locally on the smartphone or remotely on an edge server. In order to accurately measure the energy consumption on the smartphone and obtain the breakdown of energy consumed by each phase of the object detection processing pipeline, we propose a new measurement strategy. Our detailed measurements refine the energy analysis of mobile AR clients and reveal several interesting perspectives regarding the energy consumption of executing CNN-based object detection. Furthermore, several insights and research opportunities are proposed based on our experimental results. These findings from our experimental study will guide the design of energy-efficient processing pipeline of CNN-based object detection.", "venue": "IEEE Transactions on Green Communications and Networking", "authors": ["Haoxin  Wang", "BaekGyu  Kim", "Jiang  Xie", "Zhu  Han"], "year": 2021, "n_citations": 0}
{"id": 924707, "s2_id": "432bd28c0dc03277fdf9cfb544babf4699176d1e", "title": "DV-DVFS: Merging Data Variety and DVFS Technique to Manage the Energy Consumption of Big Data Processing", "abstract": "Data variety is one of the most important features of Big Data. Data variety is the result of aggregating data from multiple sources and uneven distribution of data. This feature of Big Data causes high variation in the consumption of processing resources such as CPU consumption. This issue has been overlooked in previous works. To overcome the mentioned problem, in the present work, we used Dynamic Voltage and Frequency Scaling (DVFS) to reduce the energy consumption of computation. To this goal, we consider two types of deadlines as our constraint. Before applying the DVFS technique to computer nodes, we estimate the processing time and the frequency needed to meet the deadline. In the evaluation phase, we have used a set of data sets and applications. The experimental results show that our proposed approach surpasses the other scenarios in processing real datasets. Based on the experimental results in this paper, DV-DVFS can achieve up to 15% improvement in energy consumption.", "venue": "ArXiv", "authors": ["Hossein  Ahmadvand", "Fouzhan  Foroutan", "Mahmood  Fathy"], "year": 2021, "n_citations": 1}
{"id": 924940, "s2_id": "190912fe7c115a4aa18296940f9cff0913a97810", "title": "Towards scalable pattern\u2010based optimization for dense linear algebra", "abstract": "Linear algebraic expressions are the essence of many computationally intensive problems, including scientific simulations and machine learning applications. However, translating high\u2010level formulations of these expressions to efficient machine\u2010level representations is far from trivial: developers should be assisted by automatic optimization tools so that they can focus their attention on high\u2010level problems, rather than low\u2010level details. The tractability of these optimizations is highly dependent on the choice of the primitive constructs in terms of which the computations are to be expressed. In this work, we propose to describe operations on multi\u2010dimensional arrays using a selection of higher\u2010order functions, inspired by functional programming, and we present rewrite rules for these such that they can be automatically optimized for modern hierarchical and heterogeneous architectures. Using this formalism, we systematically construct and analyze different subdivisions and permutations of the dense matrix multiplication problem.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["D\u00e1niel  Ber\u00e9nyi", "Andr\u00e1s  Leitereg", "G\u00e1bor  Lehel"], "year": 2018, "n_citations": 3}
{"id": 930663, "s2_id": "f5354947a2e1946017585ccbcf97c08ee22e046c", "title": "ZipLine: in-network compression at line speed", "abstract": "Network appliances continue to offer novel opportunities to offload processing from computing nodes directly into the data plane. One popular concern of network operators and their customers is to move data increasingly faster. A common technique to increase data throughput is to compress it before its transmission. However, this requires compression of the data---a time and energy demanding preprocessing phase---and decompression upon reception---a similarly resource consuming operation. Moreover, if multiple nodes transfer similar data chunks across the network hop (e.g., a given pair of switches), each node effectively wastes resources by executing similar steps. This paper proposes ZipLine, an approach to design and implement (de)compression at line speed leveraging the Tofino hardware platform which is programmable using the P416 language. We report on lessons learned while building the system and show throughput, latency and compression measurements on synthetic and real-world traces, showcasing the benefits and trade-offs of our design.", "venue": "CoNEXT", "authors": ["S\u00e9bastien  Vaucher", "Niloofar  Yazdani", "Pascal  Felber", "Daniel Enrique Lucani", "Valerio  Schiavoni"], "year": 2020, "n_citations": 3}
{"id": 931437, "s2_id": "e85b6a7c26522c5447a25040249d5cfcfdf42fea", "title": "Decoding Superposed LoRa Signals", "abstract": "Long-range low-power wireless communications, such as LoRa, are used in many IoT and environmental monitoring applications. They typically increase the communication range to several kilometers, at the cost of reducing the bitrate to a few bits per seconds. Collisions further reduce the performance of these communications. In this paper, we propose two algorithms to decode colliding signals: one algorithm requires the transmitters to be slightly desynchronized, and the other requires the transmitters to be synchronized. To do so, we use the timing information to match the correct symbols to the correct transmitters. We show that our algorithms are able to significantly improve the overall throughput of LoRa.", "venue": "2018 IEEE 43rd Conference on Local Computer Networks (LCN)", "authors": ["Nancy El Rachkidy", "Alexandre  Guitton", "Megumi  Kaneko"], "year": 2018, "n_citations": 21}
{"id": 933316, "s2_id": "84ae7b4ef14fffe3bf7b52266767cb374645794f", "title": "The ARM Scalable Vector Extension", "abstract": "This article describes the ARM Scalable Vector Extension (SVE). Several goals guided the design of the architecture. First was the need to extend the vector processing capability associated with the ARM AArch64 execution state to better address the computational requirements in domains such as high-performance computing, data analytics, computer vision, and machine learning. Second was the desire to introduce an extension that can scale across multiple implementations, both now and into the future, allowing CPU designers to choose the vector length most suitable for their power, performance, and area targets. Finally, the architecture should avoid imposing a software development cost as the vector length changes and where possible reduce it by improving the reach of compiler auto-vectorization technologies. SVE achieves these goals. It allows implementations to choose a vector register length between 128 and 2,048 bits. It supports a vector-length agnostic programming model that lets code run and scale automatically across all vector lengths without recompilation. Finally, it introduces several innovative features that begin to overcome some of the traditional barriers to autovectorization.", "venue": "IEEE Micro", "authors": ["Nigel  Stephens", "Stuart  Biles", "Matthias  Boettcher", "Jacob  Eapen", "Mbou  Eyole", "Giacomo  Gabrielli", "Matt  Horsnell", "Grigorios  Magklis", "Alejandro  Martinez", "Nathana\u00ebl  Pr\u00e9millieu", "Alastair David Reid", "Alejandro  Rico", "Paul  Walker"], "year": 2017, "n_citations": 117}
{"id": 935422, "s2_id": "58741463299ec42ed0859db8f0b98fad8c20e72b", "title": "WCET analysis of multi-level set-associative instruction caches", "abstract": "With the advent of increasingly complex hardware in real-time embedded systems (processors with performance enhancing features such as pipelines, cache hierarchy, multiple cores), many processors now have a set-associative L2 cache. Thus, there is a need for considering cache hierarchies when validating the temporal behavior of real-time systems, in particular when estimating tasks' worst-case execution times (WCETs). To the best of our knowledge, there is only one approach for WCET estimation for systems with cache hierarchies [Mueller, 1997], which turns out to be unsafe for set-associative caches. In this paper, we highlight the conditions under which the approach described in [Mueller, 1997] is unsafe. A safe static instruction cache analysis method is then presented. Contrary to [Mueller, 1997] our method supports set-associative and fully associative caches. The proposed method is experimented on medium-size and large programs. We show that the method is most of the time tight. We further show that in all cases WCET estimations are much tighter when considering the cache hierarchy than when considering only the L1 cache. An evaluation of the analysis time is conducted, demonstrating that analysing the cache hierarchy has a reasonable computation time.", "venue": "ArXiv", "authors": ["Damien  Hardy", "Isabelle  Puaut"], "year": 2008, "n_citations": 22}
{"id": 937866, "s2_id": "be0be6ba985f2e4579002f23fc1aef304c005e70", "title": "On the Fly Orchestration of Unikernels: Tuning and Performance Evaluation of Virtual Infrastructure Managers", "abstract": "Network operators are facing significant challenges meeting the demand for more bandwidth, agile infrastructures, innovative services, while keeping costs low. Network Functions Virtualization (NFV) and Cloud Computing are emerging as key trends of 5G network architectures, providing flexibility, fast instantiation times, support of Commercial Off The Shelf hardware and significant cost savings. NFV leverages Cloud Computing principles to move the data-plane network functions from expensive, closed and proprietary hardware to the so-called Virtual Network Functions (VNFs). In this paper we deal with the management of virtual computing resources (Unikernels) for the execution of VNFs. This functionality is performed by the Virtual Infrastructure Manager (VIM) in the NFV MANagement and Orchestration (MANO) reference architecture. We discuss the instantiation process of virtual resources and propose a generic reference model, starting from the analysis of three open source VIMs, namely OpenStack, Nomad and OpenVIM. We improve the aforementioned VIMs introducing the support for special-purpose Unikernels and aiming at reducing the duration of the instantiation process. We evaluate some performance aspects of the VIMs, considering both stock and tuned versions. The VIM extensions and performance evaluation tools are available under a liberal open source licence.", "venue": "IEEE Transactions on Cloud Computing", "authors": ["Pier Luigi Ventre", "Paolo  Lungaroni", "Giuseppe  Siracusano", "Claudio  Pisa", "Florian  Schmidt", "Francesco  Lombardo", "Stefano  Salsano"], "year": 2021, "n_citations": 8}
{"id": 940942, "s2_id": "9bd81cb02bed5d950e5749042aa89b8f057da2e9", "title": "Introducing SLAMBench, a performance and accuracy benchmarking methodology for SLAM", "abstract": "Real-time dense computer vision and SLAM offer great potential for a new level of scene modelling, tracking and real environmental interaction for many types of robot, but their high computational requirements mean that use on mass market embedded platforms is challenging. Meanwhile, trends in low-cost, low-power processing are towards massive parallelism and heterogeneity, making it difficult for robotics and vision researchers to implement their algorithms in a performance-portable way. In this paper we introduce SLAMBench, a publicly-available software framework which represents a starting point for quantitative, comparable and validatable experimental research to investigate trade-offs in performance, accuracy and energy consumption of a dense RGB-D SLAM system. SLAMBench provides a KinectFusion implementation in C++, OpenMP, OpenCL and CUDA, and harnesses the ICL-NUIM dataset of synthetic RGB-D sequences with trajectory and scene ground truth for reliable accuracy comparison of different implementation and algorithms. We present an analysis and breakdown of the constituent algorithmic elements of KinectFusion, and experimentally investigate their execution time on a variety of multicore and GPU-accelerated platforms. For a popular embedded platform, we also present an analysis of energy efficiency for different configuration alternatives.", "venue": "2015 IEEE International Conference on Robotics and Automation (ICRA)", "authors": ["Luigi  Nardi", "Bruno  Bodin", "M. Zeeshan Zia", "John  Mawer", "Andy  Nisbet", "Paul H. J. Kelly", "Andrew J. Davison", "Mikel  Luj\u00e1n", "Michael F. P. O'Boyle", "Graham D. Riley", "Nigel P. Topham", "Stephen B. Furber"], "year": 2015, "n_citations": 119}
{"id": 942370, "s2_id": "144db1b067b0cbdf3786506150b8e2c717469b48", "title": "Simple Near-Optimal Scheduling for the M/G/1", "abstract": "We consider the problem of preemptively scheduling jobs to minimize mean response time of an M/G/1 queue. When we know each job's size, the shortest remaining processing time (SRPT) policy is optimal. Unfortunately, in many settings we do not have access to each job's size. Instead, we know only the job size distribution. In this setting the Gittins policy is known to minimize mean response time, but its complex priority structure can be computationally intractable. A much simpler alternative to Gittins is the shortest expected remaining processing time (SERPT) policy. While SERPT is a natural extension of SRPT to unknown job sizes, it is unknown whether or not SERPT is close to optimal for mean response time.", "venue": "SIGMETRICS", "authors": ["Ziv  Scully", "Mor  Harchol-Balter", "Alan  Scheller-Wolf"], "year": 2020, "n_citations": 0}
{"id": 945282, "s2_id": "110ecf702330f088704919175a2e274cbaa1b22c", "title": "MBWU: Benefit Quantification for Data Access Function Offloading", "abstract": "The storage industry is considering new kinds of storage devices that support data access function offloading, i.e. the ability to perform data access functions on the storage device itself as opposed to performing it on a separate compute system to which the storage device is connected. But what is the benefit of offloading to a storage device that is controlled by an embedded platform, very different from a host platform? To quantify the benefit, we need a measurement methodology that enables apple-to-apple comparisons between different platforms. We propose a Media-based Work Unit (MBWU, pronounced \"MibeeWu\"), and an MBWU-based measurement methodology to standardize the platform efficiency evaluation so as to quantify the benefit of offloading. To demonstrate the merit of this methodology, we implemented a prototype to automate quantifying the benefit of offloading the key-value data access function.", "venue": "ISC Workshops", "authors": ["Jianshen  Liu", "Philip  Kufeldt", "Carlos  Maltzahn"], "year": 2019, "n_citations": 1}
{"id": 946395, "s2_id": "fb6acbf2239bedd83ea312c328be022e0d4d6da0", "title": "On High Spatial Reuse Link Scheduling in STDMA Wireless Ad Hoc Networks", "abstract": "We consider the point-to-point link scheduling problem in Spatial Time Division Multiple Access (STDMA) wireless ad hoc networks, motivate the use of spatial reuse as performance metric and provide an explicit characterization of spatial reuse. We assume uniform transmission power at all nodes and propose an algorithm based on a graph model of the network as well as Signal to Interference and Noise Ratio (SINR) computations. Our algorithm achieves higher spatial reuse than existing algorithms, without compromising on computational complexity.", "venue": "IEEE GLOBECOM 2007 - IEEE Global Telecommunications Conference", "authors": ["Ashutosh Deepak Gore", "Abhay  Karandikar", "Srikanth  Jagabathula"], "year": 2007, "n_citations": 30}
{"id": 946846, "s2_id": "9a1385650565e536a74bc7c40dcf7a627029fc6b", "title": "A class of equivalent idle-time-order-based routing policies for heterogeneous multi-server systems", "abstract": "We consider an M/M/N/K/FCFS system (N>0, K>=N), where the servers operate at (possibly) heterogeneous service rates. In this situation, the steady state behavior depends on the routing policy that is used to select which idle server serves the next job in queue. We define a class of idle-time-order-based policies (including, for example, Longest Idle Server First (LISF)) and show that all policies in this class result in the same steady state behavior. In particular, they are all equivalent to the naive Random routing policy.", "venue": "ArXiv", "authors": ["Sherwin  Doroudi", "Ragavendran  Gopalakrishnan"], "year": 2013, "n_citations": 1}
{"id": 949280, "s2_id": "4024bb1e2d5daaf8bec407744a6de53a85638adf", "title": "Throughput and Collision Analysis of Multichannel Multistage Spectrum Sensing Algorithms", "abstract": "Multistage sensing is a novel concept that refers to a general class of spectrum sensing algorithms that divide the sensing process into a number of sequential stages. The number of sensing stages and the sensing technique per stage can be used to optimize the performance with respect to secondary user (SU) throughput and the collision probability between primary user (PU) and SU. So far, the impact of multistage sensing on network throughput and collision probability for a realistic network model has been relatively unexplored. Therefore, we present the first analytical framework that enables the performance evaluation of different multichannel multistage spectrum sensing algorithms for opportunistic spectrum access (OSA) networks. The contribution of this paper lies in studying the effect of the following parameters on performance: number of sensing stages, physical layer sensing techniques and durations per each stage, single and parallel channel sensing and access, number of available channels, PU and SU traffic, buffering of incoming SU traffic, and medium access control layer sensing algorithms. The analyzed performance metrics include the average SU throughput and the average collision probability between PU and SU. Our results show that when the probability of PU misdetection is constrained, the performance of multistage sensing is, in most cases, superior to the single-stage sensing counterpart. In addition, prolonged channel observation at the first stage of sensing considerably decreases the collision probability while keeping the throughput at an acceptable level. Finally, in realistic PU traffic scenarios, using two stages of sensing provides a good balance between SU throughput and collision probability while meeting successful detection constraints subjected by OSA communication.", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Wesam  Gabran", "Przemyslaw  Pawelczak", "Danijela  Cabric"], "year": 2011, "n_citations": 15}
{"id": 949690, "s2_id": "35246ad5a915ec62879d16877967a3899843718b", "title": "Heterogeneous MacroTasking (HeMT) for Parallel Processing in the Public Cloud", "abstract": "Using tiny, equal-sized tasks (Homogeneous microTasking, HomT) has long been regarded an effective way of load balancing in parallel computing systems. When combined with nodes pulling in work upon becoming idle, HomT has the desirable property of automatically adapting its load distribution to the processing capacities of participating nodes - more powerful nodes finish their work sooner and, therefore, pull in additional work faster. As a result, HomT is deemed especially desirable in settings with heterogeneous (and possibly possessing dynamically changing) processing capacities. However, HomT does have additional scheduling and I/O overheads that might make this load balancing scheme costly in some scenarios. In this paper, we first analyze these advantages and disadvantages of HomT. We then propose an alternative load balancing scheme - Heterogeneous MacroTasking (HeMT) - wherein workload is intentionally partitioned according to nodes' processing capacity. Our goal is to study when HeMT is able to overcome the performance disadvantages of HomT. We implement a prototype of HeMT within the Apache Spark application framework with complementary enhancements to the Apache Mesos cluster manager. Spark's built-in scheduler, when parameterized appropriately, implements HomT. Our experimental results show that HeMT out-performs HomT when accurate workload-specific estimates of nodes' processing capacities are learned. As representative results, Spark with HeMT offers about 10% better average completion times for realistic data processing workloads over the default system.", "venue": "ArXiv", "authors": ["Yuquan  Shan", "George  Kesidis", "Bhuvan  Urgaonkar", "J\u00f6rg  Schad", "Jalal  Khamse-Ashari", "Ioannis  Lambadaris"], "year": 2018, "n_citations": 1}
{"id": 952629, "s2_id": "6720f61ff7f062be064cb2f7e8262f47843fbd5b", "title": "Guiding Optimizations with Meliora: A Deep Walk down Memory Lane", "abstract": "Performance models can be very useful for understanding the behavior of applications and hence can help guide design and optimization decisions. Unfortunately, performance modeling of nontrivial computations typically requires significant expertise and human effort. Moreover, even when performed by experts, it is necessarily limited in scope, accuracy, or both. However, since models are not typically available, programmers, compilers or autotuners cannot use them easily to guide optimizations and are limited to heuristic-based methods that potentially take a lot of time to perform unnecessary transformations. We believe that streamlining model generation and making it scalable (both in terms of human effort and code size) would enable dramatic improvements in compilation techniques, as well as manual optimization and autotuning. To that end, we are building the Meliora code analysis infrastructure for machine learning-based performance model generation of arbitrary codes based on static analysis of intermediate language representations. We demonstrate good accuracy in matching known codes and show how Meliora can be used to optimize new codes though reusing optimization knowledge, either manually or in conjunction with an autotuner. When autotuning, Meliora eliminates or dramatically reduces the empirical search space, while generally achieving competitive performance.", "venue": "ArXiv", "authors": ["Kewen  Meng", "Boyana  Norris"], "year": 2020, "n_citations": 0}
{"id": 953226, "s2_id": "18f97e0f25ff60651992c30eed70d3b0b6e24e68", "title": "GRETA: Graph-based Real-time Event Trend Aggregation", "abstract": "Streaming applications from algorithmic trading to traffic management deploy Kleene patterns to detect and aggregate arbitrarily-long event sequences, called event trends. State-of-the-art systems process such queries in two steps. Namely, they first construct all trends and then aggregate them. Due to the exponential costs of trend construction, this two-step approach suffers from both a long delays and high memory costs. To overcome these limitations, we propose the Graph-based Real-time Event Trend Aggregation (Greta) approach that dynamically computes event trend aggregation without first constructing these trends. We define the Greta graph to compactly encode all trends. Our Greta runtime incrementally maintains the graph, while dynamically propagating aggregates along its edges. Based on the graph, the final aggregate is incrementally updated and instantaneously returned at the end of each query window. Our Greta runtime represents a win-win solution, reducing both the time complexity from exponential to quadratic and the space complexity from exponential to linear in the number of events. Our experiments demonstrate that Greta achieves up to four orders of magnitude speed-up and up to 50--fold memory reduction compared to the state-of-the-art two-step approaches.", "venue": "Proc. VLDB Endow.", "authors": ["Olga  Poppe", "Chuan  Lei", "Elke A. Rundensteiner", "David  Maier"], "year": 2017, "n_citations": 9}
{"id": 957143, "s2_id": "519be7fda06fdfa7f67a741ebd0d56980110569d", "title": "Analysis of the Leakage Queue: A Queueing Model for Energy Storage Systems with Self-discharge", "abstract": "Energy storage is a crucial component of the smart grid, since it provides the ability to buffer transient fluctuations of the energy supply from renewable sources. Even without a load, energy storage systems experience a reduction of the stored energy through self-discharge. In some storage technologies, the rate of self-discharge can exceed 50% of the stored energy per day. In this paper, we investigate the self-discharge phenomenon in energy storage using a queueing system model, which we refer to as leakage queue. When the average net charge is positive, we discover that the leakage queue operates in one of two regimes: a leakage-dominated regime and a capacity-dominated regime. We find that in the leakage-dominated regime, the stored energy stabilizes at a point that is below the storage capacity. Under suitable independence assumptions for energy supply and demand, the stored energy in this regime closely follows a normal distribution. We present two methods for computing probabilities of underflow and overflow at a leakage queue. The methods are validated in a numerical example where the energy supply resembles a wind energy source.", "venue": "ArXiv", "authors": ["Majid  Raeis", "Almut  Burchard", "J\u00f6rg  Liebeherr"], "year": 2017, "n_citations": 4}
{"id": 961069, "s2_id": "8efb522688fdbae46797519d24c9820254d18ced", "title": "Unix Memory Allocations are Not Poisson", "abstract": "In multitasking operating systems, requests for free memory are traditionally modeled as a stochastic counting process with independent, exponentially-distributed interarrival times because of the analytic simplicity such Poisson models afford. We analyze the distribution of several million unix page commits to show that although this approach could be valid over relatively long timespans, the behavior of the arrival process over shorter periods is decidedly not Poisson. We find that this result holds regardless of the originator of the request: unlike network packets, there is little difference between system- and user-level page-request distributions. We believe this to be due to the bursty nature of page allocations, which tend to occur in either small or extremely large increments. Burstiness and persistent variance have recently been found in self-similar processes in computer networks, but we show that although page commits are both bursty and possess high variance over long timescales, they are probably not self-similar. These results suggest that altogether different models are needed for fine-grained analysis of memory systems, an important consideration not only for understanding behavior but also for the design of online control systems.", "venue": "ArXiv", "authors": ["James  Garnett", "Elizabeth  Bradley"], "year": 2018, "n_citations": 0}
{"id": 961807, "s2_id": "17058b39ff6998b91f9771f354372a6154ee8fd7", "title": "Information Model for Model Driven Safety Requirements Management of Complex Systems", "abstract": "The aim of this paper is to propose a rigorous and complete design framework for complex system based on system engineering (SE) principles. The SE standard EIA-632 is used to guide the approach. Within this framework, two aspects are presented. The first one concerns the integration of safety requirements and management in system engineering process. The objective is to help designers and engineers in managing safety of complex systems. The second aspect concerns model driven design through the definition of an information model. This model is based on SysML (System Modeling Language) to address requirements definition and their traceability towards the solution and the Verification and Validation (V&V) elements.", "venue": "CSDM", "authors": ["Romaric  Guillerm", "Hamid  Demmou", "Nabil  Sadou"], "year": 2010, "n_citations": 10}
{"id": 966196, "s2_id": "9050aebe476414fb4412cbf0d4d78ece01e6ad78", "title": "High performance on-demand de-identification of a petabyte-scale medical imaging data lake", "abstract": "With the increase in Artificial Intelligence driven approaches, researchers are requesting unprecedented volumes of medical imaging data which far exceed the capacity of traditional on-premise client-server approaches for making the data research analysis-ready. We are making available a flexible solution for on-demand de-identification that combines the use of mature software technologies with modern cloud-based distributed computing techniques to enable faster turnaround in medical imaging research. The solution is part of a broader platform that supports a secure high performance clinical data science platform.", "venue": "ArXiv", "authors": ["Joseph  Mesterhazy", "Garrick  Olson", "Somalee  Datta"], "year": 2020, "n_citations": 1}
{"id": 967402, "s2_id": "bbd35050acec8f40c709de0c68e93eabe68bad45", "title": "Semi-Analytical Model for Design and Analysis of On-Orbit Servicing Architecture", "abstract": "Robotic on-orbit servicing (OOS) is expected to be a key technology and concept for future sustainable space exploration. This paper develops a semi-analytical model for OOS systems analysis, responding to the growing needs and ongoing trend of robotic OOS. An OOS infrastructure system is considered whose goal is to provide responsive services to the random failures of a set of customer modular satellites distributed in space (e.g., at the geosynchronous equatorial orbit). The considered OOS architecture is comprised of a servicer that travels and provides module-replacement services to the customer satellites, an on-orbit depot to store the spares, and a series of launch vehicles to replenish the depot. The OOS system performance is analyzed by evaluating the mean waiting time before service completion for a given failure and its relationship with the depot capacity. Leveraging the queueing theory and inventory management methods, the developed semi-analytical model is capable of analyzing the OOS system performance without relying on computationally costly simulations. The effectiveness of the proposed model is demonstrated using a case study compared with simulation results. This paper is expected to provide a critical step to push the research frontier of analytical/semi-analytical models development for complex space systems design.", "venue": "ArXiv", "authors": ["Koki  Ho", "Hai  Wang", "Paul A. DeTrempe", "Tristan Sarton du Jonchay", "Kento  Tomita"], "year": 2019, "n_citations": 2}
{"id": 969966, "s2_id": "0e8a6df259c20a9d8c329a8de5c9794f4c394834", "title": "Enhanced Algorithm for Link to System level Interface Mapping", "abstract": "The current SINR mechanism does not provide the base station (BS) with any knowledge on the frequency selectivity of channel from mobile service station(MSS). This knowledge is important since, contrary to the AWGN channel, in a frequency selective channel there is no longer a 1 to 1 relation between amount of increase in power and amount of improvement in effective SINR 1. Furthermore, the relation is dependent on MCS level. This lack of knowledge in the BS side results in larger fade margins, which translates directly to reduction in capacity. In this paper we propose a enhanced algorithm on the EESM model with weighted beta (\\beta) that provides the BS with sufficient knowledge on the channel-dependent relationship between power increase, MCS change and improvement in effective SINR.", "venue": "ArXiv", "authors": ["Shahid  Mumtaz", "At\u00edlio  Gameiro", "Rasool  Sadeghi"], "year": 2009, "n_citations": 0}
{"id": 973290, "s2_id": "c77da20533553c907a2a42a71d8f082de9491c03", "title": "OR-Benchmark: An Open and Reconfigurable Digital Watermarking Benchmarking Framework", "abstract": "Benchmarking digital watermarking algorithms is not an easy task because different applications of digital watermarking often have very different sets of requirements and trade-offs between conflicting requirements. While there have been some general-purpose digital watermarking benchmarking systems available, they normally do not support complicated benchmarking tasks and cannot be easily reconfigured to work with different watermarking algorithms and testing conditions. In this paper, we propose OR-Benchmark, an open and highly reconfigurable general-purpose digital watermarking benchmarking framework, which has the following two key features: 1) all the interfaces are public and general enough to support all watermarking applications and benchmarking tasks we can think of; 2) end users can easily extend the functionalities and freely configure what watermarking algorithms are tested, what system components are used, how the benchmarking process runs, and what results should be produced. We implemented a prototype of this framework as a MATLAB software package and used it to benchmark a number of digital watermarking algorithms involving two types of watermarks for content authentication and self-restoration purposes. The benchmarking results demonstrated the advantages of the proposed benchmarking framework, and also gave us some useful insights about existing image authentication and self-restoration watermarking algorithms which are an important but less studied topic in digital watermarking.", "venue": "DETIPS/DeSECSys/MPS/SPOSE@ESORICS", "authors": ["Hui  Wang", "Anthony Tung Shuen Ho", "Shujun  Li"], "year": 2020, "n_citations": 2}
{"id": 975411, "s2_id": "7bfb171d830f926c0b8e3c672e57dfff3eea04ee", "title": "The Implementation of a Real-Time Polyphase Filter", "abstract": "In this article we study the suitability of dierent computational accelerators for the task of real-time data processing. The algorithm used for comparison is the polyphase filter, a standard tool in signal processing and a well established algorithm. We measure performance in FLOPs and execution time, which is a critical factor for real-time systems. For our real-time studies we have chosen a data rate of 6.5GB/s, which is the estimated data rate for a single channel on the SKAs Low Frequency Aperture Array. Our findings how that GPUs are the most likely candidate for real-time data processing. GPUs are better in both performance and power consumption.", "venue": "ArXiv", "authors": ["Karel  Ad\u00e1mek", "Jan  Novotn\u00fd", "Wes  Armour"], "year": 2014, "n_citations": 1}
{"id": 976777, "s2_id": "492db3590270946ddd5c8a300f12c8443dcae4b4", "title": "Data-Driven Model-Based Analysis of the Ethereum Verifier's Dilemma", "abstract": "In proof-of-work based blockchains such as Ethereum, verification of blocks is an integral part of establishing consensus across nodes. However, in Ethereum, miners do not receive a reward for verifying. This implies that miners face the Verifier's Dilemma: use resources for verification, or use them for the more lucrative mining of new blocks? We provide an extensive analysis of the Verifier's Dilemma, using a data-driven model-based approach that combines closed-form expressions, machine learning techniques and discrete-event simulation. We collect data from over 300,000 smart contracts and experimentally obtain their CPU execution times. Gaussian Mixture Models and Random Forest Regression transform the data into distributions and inputs suitable for the simulator. We show that, indeed, it is often economically rational not to verify, in particular for miners with less hashing power. We consider two approaches to mitigate the implications of the Verifier's Dilemma, namely parallelization and active insertion of invalid blocks, both will be shown to be effective.", "venue": "2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)", "authors": ["Maher  Alharby", "Roben Castagna Lunardi", "Amjad  Aldweesh", "Aad van Moorsel"], "year": 2020, "n_citations": 5}
{"id": 977510, "s2_id": "a2f949f0e6db37aa5e6c18425241b4ac9589d397", "title": "Zero Queueing for Multi-Server Jobs", "abstract": "Cloud computing today is dominated by multi-server jobs. These are jobs that request multiple servers simultaneously and hold onto all of these servers for the duration of the job. Multi-server jobs add a lot of complexity to the traditional one-server-per-job model: an arrival might not \"fit'' into the available servers and might have to queue, blocking later arrivals and leaving servers idle. From a queueing perspective, almost nothing is understood about multi-server job queueing systems; even understanding the exact stability region is a very hard problem. In this paper, we investigate a multi-server job queueing model under scaling regimes where the number of servers in the system grows. Specifically, we consider a system with multiple classes of jobs, where jobs from different classes can request different numbers of servers and have different service time distributions, and jobs are served in first-come-first-served order. The multi-server job model opens up new scaling regimes where both the number of servers that a job needs and the system load scale with the total number of servers. Within these scaling regimes, we derive the first results on stability, queueing probability, and the transient analysis of the number of jobs in the system for each class. In particular we derive sufficient conditions for zero queueing. Our analysis introduces a novel way of extracting information from the Lyapunov drift, which can be applicable to a broader scope of problems in queueing systems.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Weina  Wang", "Qiaomin  Xie", "Mor  Harchol-Balter"], "year": 2021, "n_citations": 3}
{"id": 978574, "s2_id": "657ec0aac3af5c05fcc69a55f97805955fcfa8f3", "title": "Pinpointing performance inefficiencies in Java", "abstract": "Many performance inefficiencies such as inappropriate choice of algorithms or data structures, developers' inattention to performance, and missed compiler optimizations show up as wasteful memory operations. Wasteful memory operations are those that produce/consume data to/from memory that may have been avoided. We present, JXPerf, a lightweight performance analysis tool for pinpointing wasteful memory operations in Java programs. Traditional byte code instrumentation for such analysis (1) introduces prohibitive overheads and (2) misses inefficiencies in machine code generation. JXPerf overcomes both of these problems. JXPerf uses hardware performance monitoring units to sample memory locations accessed by a program and uses hardware debug registers to monitor subsequent accesses to the same memory. The result is a lightweight measurement at the machine code level with attribution of inefficiencies to their provenance --- machine and source code within full calling contexts. JXPerf introduces only 7% runtime overhead and 7% memory overhead making it useful in production. Guided by JXPerf, we optimize several Java applications by improving code generation and choosing superior data structures and algorithms, which yield significant speedups.", "venue": "ESEC/SIGSOFT FSE", "authors": ["Pengfei  Su", "Qingsen  Wang", "Milind  Chabbi", "Xu  Liu"], "year": 2019, "n_citations": 3}
{"id": 980049, "s2_id": "bc996c534ed0ea7a06a5917b2f160269424f0c7c", "title": "Correlated dynamics in human printing behavior", "abstract": "Arrival times of requests to print in a student laboratory were analyzed. Inter-arrival times between subsequent requests follow a universal scaling law relating time intervals and the size of the request, indicating a scale invariant dynamics with respect to the size. The cumulative distribution of file sizes is well-described by a modified power-law often seen in non-equilibrium critical systems. For each user, waiting times between their individual requests show long range dependence and are broadly distributed from seconds to weeks. All results are incompatible with Poisson models, and may provide evidence of critical dynamics associated with voluntary thought processes in the brain.", "venue": "ArXiv", "authors": ["Uli  Harder", "Maya  Paczuski"], "year": 2004, "n_citations": 65}
{"id": 980227, "s2_id": "93962407a50091bebcfca995e166e34829a20113", "title": "Thread Evolution Kit for Optimizing Thread Operations on CE/IoT Devices", "abstract": "Most modern operating systems have adopted the one-to-one thread model to support fast execution of threads in both multi-core and single-core systems. This thread model, which maps the kernel-space and user-space threads in a one-to-one manner, supports quick thread creation and termination in high-performance server environments. However, the performance of time-critical threads is degraded when multiple threads are being run in low-end CE devices with limited system resources. When a CE device runs many threads to support diverse application functionalities, low-level hardware specifications often lead to significant resource contention among the threads trying to obtain system resources. As a result, the operating system encounters challenges, such as excessive thread context switching overhead, execution delay of time-critical threads, and a lack of virtual memory for thread stacks. This article proposes a state-of-the-art Thread Evolution Kit (TEK) that consists of three primary components: a CPU Mediator, Stack Tuner, and Enhanced Thread Identifier. From the experiment, we can see that the proposed scheme significantly improves user responsiveness ( $7\\times$ faster) under high CPU contention compared to the traditional thread model. Also, TEK solves the segmentation fault problem that frequently occurs when a CE application increases the number of threads during its execution.", "venue": "IEEE Transactions on Consumer Electronics", "authors": ["Geunsik  Lim", "Donghyun  Kang", "Young Ik Eom"], "year": 2020, "n_citations": 0}
{"id": 980844, "s2_id": "2efa8f54399cc2fc545d22264e32e8ff34ad70ab", "title": "Scalable Benchmarks for Gate-Based Quantum Computers", "abstract": "In the near-term \u201cNISQ\u201d-era of noisy, intermediate-scale, quantum hardware and beyond, reliably determining the quality of quantum devices becomes increasingly important: users need to be able to compare them with one another, and make an estimate whether they are capable of performing a given task ahead of time. In this work, we develop and release an advanced quantum benchmarking framework in order to help assess the state of the art of current quantum devices. Our testing framework measures the performance of universal quantum devices in a hardware-agnostic way, with metrics that are aimed to facilitate an intuitive understanding of which device is likely to outperform others on a given task. This is achieved through six structured tests that allow for an immediate, visual assessment of how devices compare. Each test is designed with scalability in mind, making this framework not only suitable for testing the performance of present-day quantum devices, but also of those released in the foreseeable future. The series of tests are motivated by real-life scenarios, and therefore emphasise the interplay between various relevant characteristics of quantum devices, such as qubit count, connectivity, and gate and measurement fidelity. We present the benchmark results of twenty-one different quantum devices from IBM, Rigetti and IonQ.", "venue": "ArXiv", "authors": ["Arjan  Cornelissen", "Johannes  Bausch", "Andr'as  Gily'en"], "year": 2021, "n_citations": 4}
{"id": 981627, "s2_id": "cf009e46410c2bd18309f2e7b15f3b549c142e4e", "title": "Sparse Matrix-vector Multiplication on GPGPU Clusters: A New Storage Format and a Scalable Implementation", "abstract": "Sparse matrix-vector multiplication (spMVM) is the dominant operation in many sparse solvers. We investigate performance properties of spMVM with matrices of various sparsity patterns on the nVidia \"Fermi\" class of GPGPUs. A new \"padded jagged diagonals storage\" (pJDS) format is proposed which may substantially reduce the memory overhead intrinsic to the widespread ELLPACK-R scheme while making no assumptions about the matrix structure. In our test scenarios the pJDS format cuts the overall spMVM memory footprint on the GPGPU by up to 70%, and achieves 91% to 130% of the ELLPACK-R performance. Using a suitable performance model we identify performance bottlenecks on the node level that invalidate some types of matrix structures for efficient multi-GPGPU parallelization. For appropriate sparsity patterns we extend previous work on distributed-memory parallel spMVM to demonstrate a scalable hybrid MPI-GPGPU code, achieving efficient overlap of communication and computation.", "venue": "2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum", "authors": ["Moritz  Kreutzer", "Georg  Hager", "Gerhard  Wellein", "Holger  Fehske", "Achim  Basermann", "Alan R. Bishop"], "year": 2012, "n_citations": 56}
{"id": 985779, "s2_id": "c42e72f1623eb10415ae7ed20a7470995d104a10", "title": "Autonomous Task Dropping Mechanism to Achieve Robustness in Heterogeneous Computing Systems", "abstract": "Robustness of a distributed computing system is defined as the ability to maintain its performance in the presence of uncertain parameters. Uncertainty is a key problem in heterogeneous (and even homogeneous) distributed computing systems that perturbs system robustness. Notably, the performance of these systems is perturbed by uncertainty in both task execution time and arrival. Accordingly, our goal is to make the system robust against these uncertainties. Considering task execution time as a random variable, we use probabilistic analysis to develop an autonomous proactive task dropping mechanism to attain our robustness goal. Specifically, we provide a mathematical model that identifies the optimality of a task dropping decision, so that the system robustness is maximized. Then, we leverage the mathematical model to develop a task dropping heuristic that achieves the system robustness within a feasible time complexity. Although the proposed model is generic and can be applied to any distributed system, we concentrate on heterogeneous computing (HC) systems that have a higher degree of exposure to uncertainty than homogeneous systems. Experimental results demonstrate that the autonomous proactive dropping mechanism can improve the system robustness by up to 20%.", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Ali  Mokhtari", "Chavit  Denninnart", "Mohsen Amini Salehi"], "year": 2020, "n_citations": 4}
{"id": 988478, "s2_id": "a41de567180a0cb0527a06bacab27f61e8c9eb1f", "title": "Availability assessment of SunOS/Solaris Unix systems based on syslogd and wtmpx log files: A case study", "abstract": "This paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 SunOS/Solaris Unix workstations and servers interconnected through a local area network. We focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. Data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. It is widely recognized that the information contained in such event logs might be incomplete or imperfect. The solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the SunOS/Solaris Unix operating system. The results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", "venue": "11th Pacific Rim International Symposium on Dependable Computing (PRDC'05)", "authors": ["Cristina  Simache", "Mohamed  Ka\u00e2niche"], "year": 2005, "n_citations": 33}
{"id": 988853, "s2_id": "dbfda66d4b7e98c09c7df8e42195e19d061c7b91", "title": "On the Power-of-d-choices with Least Loaded Server Selection", "abstract": "Motivated by distributed schedulers that combine the power-of-d-choices with late binding and systems that use replication with cancellation-on-start, we study the performance of the LL(d) policy which assigns a job to a server that currently has the least workload among d randomly selected servers in large-scale homogeneous clusters. We consider general job size distributions and propose a partial integro-differential equation to describe the evolution of the system. This equation relies on the earlier proven ansatz for LL(d) which asserts that the workload distribution of any finite set of queues becomes independent of one another as the number of servers tends to infinity. Based on this equation we propose a fixed point iteration for the limiting workload distribution and study its convergence.", "venue": "Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems", "authors": ["Tim  Hellemans", "Benny  Van Houdt"], "year": 2018, "n_citations": 11}
{"id": 993958, "s2_id": "f40f4de1f04a9f5661704ca147b37ac1ce5c4c7b", "title": "Edge AIBench: Towards Comprehensive End-to-end Edge Computing Benchmarking", "abstract": "In edge computing scenarios, the distribution of data and collaboration of workloads on different layers are serious concerns for performance, privacy, and security issues. So for edge computing benchmarking, we must take an end-to-end view, considering all three layers: client-side devices, edge computing layer, and cloud servers. Unfortunately, the previous work ignores this most important point. This paper presents the BenchCouncil's coordinated e ort on edge AI benchmarks, named Edge AIBench. In total, Edge AIBench models four typical application scenarios: ICU Patient Monitor, Surveillance Camera, Smart Home, and Autonomous Vehicle with the focus on data distribution and workload collaboration on three layers. Edge AIBench is a part of the open-source AIBench project, publicly available from this http URL. We also build an edge computing testbed with a federated learning framework to resolve performance, privacy, and security issues.", "venue": "Bench", "authors": ["Tianshu  Hao", "Yunyou  Huang", "Xu  Wen", "Wanling  Gao", "Fan  Zhang", "Chen  Zheng", "Lei  Wang", "Hainan  Ye", "Kai  Hwang", "Zujie  Ren", "Jianfeng  Zhan"], "year": 2018, "n_citations": 37}
{"id": 994334, "s2_id": "574cf124f055afb61d0a59c7717d0f1f833a4fc0", "title": "Evaluation of Intel Memory Drive Technology Performance for Scientific Applications", "abstract": "In this paper, we present benchmark data for Intel Memory Drive Technology (IMDT), which is a new generation of Software-defined Memory (SDM) based on Intel ScaleMP collaboration and using 3D XPointTM based Intel Solid-State Drives (SSDs) called Optane. We studied IMDT performance for synthetic benchmarks, scientific kernels, and applications. We chose these benchmarks to represent different patterns for computation and accessing data on disks and memory. To put performance of IMDT in comparison, we used two memory configurations: hybrid IMDT DDR4/Optane and DDR4 only systems. The performance was measured as a percentage of used memory and analyzed in detail. We found that for some applications DDR4/Optane hybrid configuration outperforms DDR4 setup by up to 20%.", "venue": "MCHPC@SC", "authors": ["Vladimir  Mironov", "Andrey  Kudryavtsev", "Yuri  Alexeev", "Alexander A. Moskovsky", "Igor M. Kulikov", "Igor G. Chernykh"], "year": 2018, "n_citations": 8}
{"id": 994620, "s2_id": "e3c34e55d1c2f30ad73da032a820074573ba262e", "title": "On Symmetric Sandpiles", "abstract": "A symmetric version of the well-known SPM model for sandpiles is introduced We prove that the new model has fixed point dynamics Although there might be several fixed points, a precise description of the fixed points is given Moreover, we provide a simple closed formula for counting the number of fixed points originated by initial conditions made of a single column of grains.", "venue": "ACRI", "authors": ["Enrico  Formenti", "Beno\u00eet  Masson", "Theophilos  Pisokas"], "year": 2006, "n_citations": 6}
{"id": 995661, "s2_id": "74776f86d7513b4eaf8466f341e3f998b0704e3c", "title": "Hybrid performance modelling of opportunistic networks", "abstract": "We demonstrate the modelling of opportunistic networks using the process algebra stochastic HYPE. Network traffic is modelled as continuous flows, contact between nodes in the network is modelled stochastically, and instantaneous decisions are modelled as discrete events. Our model describes a network of stationary video sensors with a mobile ferry which collects data from the sensors and delivers it to the base station. We consider different mobility models and different buffer sizes for the ferries. This case study illustrates the flexibility and expressive power of stochastic HYPE. We also discuss the software that enables us to describe stochastic HYPE models and simulate them.", "venue": "QAPL", "authors": ["Luca  Bortolussi", "Vashti  Galpin", "Jane  Hillston"], "year": 2012, "n_citations": 19}
{"id": 996067, "s2_id": "a226d38ff45894bb67cbaa5431338b2b6f15a724", "title": "Memory Hierarchy Sensitive Graph Layout", "abstract": "Mining large graphs for information is becoming an increasingly important workload due to the plethora of graph structured data becoming available. An aspect of graph algorithms that has hitherto not received much interest is the effect of memory hierarchy on accesses. A typical system today has multiple levels in the memory hierarchy with differing units of locality; ranging across cache lines, TLB entries and DRAM pages. We postulate that it is possible to allocate graph structured data in main memory in a way as to improve the spatial locality of the data. Previous approaches to improving cache locality have focused only on a single unit of locality, either the cache line or virtual memory page. On the other hand cache oblivious algorithms can optimise layout for all levels of the memory hierarchy but unfortunately need to be specially designed for individual data structures. In this paper we explore hierarchical blocking as a technique for closing this gap. We require as input a specification of the units of locality in the memory hierarchy and lay out the input graph accordingly by copying its nodes using a hierarchy of breadth first searches. We start with a basic algorithm that is limited to trees and then extend it to arbitrary graphs. Our most efficient version requires only a constant amount of additional space. We have implemented versions of the algorithm in various environments: for C programs interfaced with macros, as an extension to the Boost object oriented graph library and finally as a modification to the traversal phase of the semispace garbage collector in the Jikes Java virtual machine. Our results show significant improvements in the access time to graphs of various structure.", "venue": "ArXiv", "authors": ["Amitabha  Roy"], "year": 2012, "n_citations": 1}
{"id": 996890, "s2_id": "2fbcc6ee96a5dd6df4d9f96c0d385a73fcd0214c", "title": "EmBench: Quantifying Performance Variations of Deep Neural Networks across Modern Commodity Devices", "abstract": "In recent years, advances in deep learning have resulted in unprecedented leaps in diverse tasks spanning from speech and object recognition to context awareness and health monitoring. As a result, an increasing number of AI-enabled applications are being developed targeting ubiquitous and mobile devices. While deep neural networks (DNNs) are getting bigger and more complex, they also impose a heavy computational and energy burden on the host devices, which has led to the integration of various specialized processors in commodity devices. Given the broad range of competing DNN architectures and the heterogeneity of the target hardware, there is an emerging need to understand the compatibility between DNN-platform pairs and the expected performance benefits on each platform. This work attempts to demystify this landscape by systematically evaluating a collection of state-of-the-art DNNs on a wide variety of commodity devices. In this respect, we identify potential bottlenecks in each architecture and provide important guidelines that can assist the community in the co-design of more efficient DNNs and accelerators.", "venue": "EMDL '19", "authors": ["Mario  Almeida", "Stefanos  Laskaridis", "Ilias  Leontiadis", "Stylianos I. Venieris", "Nicholas D. Lane"], "year": 2019, "n_citations": 38}
{"id": 997646, "s2_id": "819a1ea79192e47df43942875a780eff5de72839", "title": "Maximizing Service Reward for Queues With Deadlines", "abstract": "In this paper, we consider a real-time queuing system with rewards and deadlines. We assume that the packet processing time is known upon arrival, as is the case in communication networks. This assumption allows us to demonstrate that the well-known earliest-deadline-first policy performance can be improved. We then propose a scheduling policy that provides excellent results for packets with rewards and deadlines. We prove that the policy is optimal under deterministic service time and binomial reward distribution. In the more general case, we prove that the policy processes the maximal number of packets while collecting rewards higher than the expected reward. We present simulation results that show its high performance in more generic cases compared to the most commonly used scheduling policies.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Li-On  Raviv", "Amir  Leshem"], "year": 2018, "n_citations": 5}
{"id": 997952, "s2_id": "26df595c8381838b1a8ea6a7622473ec3dd9a5a6", "title": "GPU Fast Convolution via the Overlap-and-Save Method in Shared Memory", "abstract": "We present an implementation of the overlap-and-save method, a method for the convolution of very long signals with short response functions, which is tailored to GPUs. We have implemented several FFT algorithms (using the CUDA programming language), which exploit GPU shared memory, allowing for GPU accelerated convolution. We compare our implementation with an implementation of the overlap-and-save algorithm utilizing the NVIDIA FFT library (cuFFT). We demonstrate that by using a shared-memory-based FFT, we can achieved significant speed-ups for certain problem sizes and lower the memory requirements of the overlap-and-save method on GPUs.", "venue": "ACM Trans. Archit. Code Optim.", "authors": ["Karel  Ad\u00e1mek", "Sofia  Dimoudi", "Mike  Giles", "Wesley  Armour"], "year": 2020, "n_citations": 4}
{"id": 1004622, "s2_id": "fd41e687feae2f4cc2e95fb7bc142402350f7e23", "title": "Analysis of Interference between RDMA and Local Access on Hybrid Memory System", "abstract": "We can use a hybrid memory system consisting of DRAM and Intel Optane DC Persistent Memory (We call it DCPM in this paper) as DCPM is now commercially available since April 2019. Even if the latency for DCPM is several times higher than that for DRAM, the capacity for DCPM is several times higher than that for DRAM and the cost of DCPM is also several times lower than that for DRAM. In addition, DCPM is non-volatile. A Server with this hybrid memory system could improve the performance for in-memory database systems and virtual machine (VM) systems because these systems often consume a large amount of memory. Moreover, a high-speed shared storage system can be implemented by accessing DCPM via remote direct memory access (RDMA). I assume that some of the DCPM is often assigned as a shared area among other remote servers because applications executed on a server with a hybrid memory system often cannot use the entire capacity of DCPM. This paper evaluates the interference between local memory access and RDMA from a remote server. As a result, I indicate that the interference on this hybrid memory system is significantly different from that on a conventional DRAM-only memory system. I also believe that some kind of throttling implementation is needed when this interference occures.", "venue": "ArXiv", "authors": ["Kazuichi  Oe"], "year": 2020, "n_citations": 0}
{"id": 1004968, "s2_id": "47736e067ce725493a51b3662b52ab93e1145610", "title": "Optimizing real-time RDF data streams", "abstract": "The Resource Description Framework (RDF) provides a common data model for the integration of \"real-time\" social and sensor data streams with the Web and with each other. While there exist numerous protocols and data formats for exchanging dynamic RDF data, or RDF updates, these options should be examined carefully in order to enable a Semantic Web equivalent of the high-throughput, low-latency streams of typical Web 2.0, multimedia, and gaming applications. This paper contains a brief survey of RDF update formats and a high-level discussion of both TCP and UDP-based transport protocols for updates. Its main contribution is the experimental evaluation of a UDP-based architecture which serves as a real-world example of a high-performance RDF streaming application in an Internet-scale distributed environment.", "venue": "ArXiv", "authors": ["Joshua  Shinavier"], "year": 2010, "n_citations": 6}
{"id": 1005513, "s2_id": "09aaa85c6fe92ec2e894700dc2bab4d481c5b27d", "title": "FIRM: An Intelligent Fine-Grained Resource Management Framework for SLO-Oriented Microservices", "abstract": "Modern user-facing latency-sensitive web services include numerous distributed, intercommunicating microservices that promise to simplify software development and operation. However, multiplexing of compute resources across microservices is still challenging in production because contention for shared resources can cause latency spikes that violate the service-level objectives (SLOs) of user requests. This paper presents FIRM, an intelligent fine-grained resource management framework for predictable sharing of resources across microservices to drive up overall utilization. FIRM leverages online telemetry data and machine-learning methods to adaptively (a) detect/localize microservices that cause SLO violations, (b) identify low-level resources in contention, and (c) take actions to mitigate SLO violations via dynamic reprovisioning. Experiments across four microservice benchmarks demonstrate that FIRM reduces SLO violations by up to 16x while reducing the overall requested CPU limit by up to 62%. Moreover, FIRM improves performance predictability by reducing tail latencies by up to 11x.", "venue": "OSDI", "authors": ["Haoran  Qiu", "Subho S. Banerjee", "Saurabh  Jha", "Zbigniew T. Kalbarczyk", "Ravishankar K. Iyer"], "year": 2020, "n_citations": 21}
{"id": 1014000, "s2_id": "05649de33f553329df5732a4b0a6be5464b22056", "title": "Instability of sharing systems in the presence of retransmissions", "abstract": "Retransmissions represent a primary failure recovery mechanism on all layers of communication network architecture. Similarly, fair sharing, for example, processor sharing (PS), is a widely accepted approach to resource allocation among multiple users. Recent work has shown that retransmissions in failure-prone, for example, wireless ad hoc, networks can cause heavy tails and long delays. In this paper, we discover a new phenomenon showing that PS-based scheduling induces complete instability with zero throughput in the presence of retransmissions, regardless of how low the traffic load may be. This phenomenon occurs even when the job sizes are bounded/fragmented, for example, deterministic. Our analytical results are further validated via simulation experiments. Moreover, our work demonstrates that scheduling one job at a time, such as first-come-first-serve, achieves a larger stability region and should be preferred in these systems.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Predrag R. Jelenkovic", "Evangelia D. Skiani"], "year": 2015, "n_citations": 1}
{"id": 1015654, "s2_id": "7ebe18d2a45f709b4d7c285dba98d438eed1f31e", "title": "Removing Dynamic Type Tests with Context-Driven Basic Block Versioning", "abstract": "Dynamic typing is an important feature of dynamic programming languages. Primitive operators such as those for performing arithmetic and comparisons typically operate on a wide variety of in put value types, and as such, must internally implement some form of dynamic type dispatch and type checking. Removing such type tests is important for an efficient implementation. \nIn this paper, we examine the effectiveness of a novel approach to reducing the number of dynamically executed type tests called context-driven basic block versioning. This simple technique clones and specializes basic blocks in such a way as to allow the compiler to accumulate type information while machine code is generated, without a separate type analysis pass. The accumulated information allows the removal of some redundant type tests, particularly in performance-critical paths. \nWe have implemented intraprocedural context-driven basic block versioning in a JavaScript JIT compiler. For comparison, we have also implemented a classical flow-based type analysis operating on the same concrete types. Our results show that basic block versioning performs better on most benchmarks and removes a large fraction of type tests at the expense of a moderate code size increase. We believe that this technique offers a good tradeoff between implementation complexity and performance, and is suitable for integration in production JIT compilers.", "venue": "ArXiv", "authors": ["Maxime  Chevalier-Boisvert", "Marc  Feeley"], "year": 2014, "n_citations": 0}
{"id": 1017818, "s2_id": "153af84d534cdbd613f63e53613c5e7adfc868b2", "title": "Computational performance of a parallelized high-order spectral and mortar element toolbox", "abstract": "In this paper, a comprehensive performance review of a MPI-based high-order spec- tral and mortar element method C++ toolbox is presented. The focus is put on the performance evaluation of several aspects at the hardware and software levels with a particular emphasis on the parallel efficiency, theC++ implementation weak- nesses and the influence of the concurrent and subsequent implementation layers in a multi-programming environment. The performance evaluation is analyzed and compared to predictions given by a heuristic model, the so-called model. Three tailor-made CFD computation benchmark cases are introduced and used to carry out this review on different serial and parallel architectures, stressing the particular interest for commodity clusters. Conclusions are drawn from this extensive series of analyses and modeling leading to specific recommendations concerning such toolbox development and implementation.", "venue": "ArXiv", "authors": ["Roland  Bouffanais", "Vincent  Keller", "Ralf  Gruber", "Michel O. Deville"], "year": 2007, "n_citations": 0}
{"id": 1020313, "s2_id": "21d737942a0687dd694c779657a9c7a66d783702", "title": "An Axiomatic Theory of Fairness in Network Resource Allocation", "abstract": "We present five axioms for fairness measures in resource allocation. A family of fairness measures satisfying the axioms is constructed. Special cases of this family include &#945;-fairness, Jain's index, and entropy. Properties of fairness measures satisfying the axioms are proven, including Schur-concavity. Among the engineering implications is a generalized Jain's index that tunes the resolution of fairness measure, a new understanding of &#945;-fair utility functions, and an interpretation of ``larger &#945; is more fair''. We also construct an alternative set of axioms to capture system efficiency and feasibility constraints.", "venue": "2010 Proceedings IEEE INFOCOM", "authors": ["Tian  Lan", "David T. H. Kao", "Mung  Chiang", "Ashutosh  Sabharwal"], "year": 2010, "n_citations": 310}
{"id": 1021314, "s2_id": "f016d87779e5008f3dbd3a79233bed6739d2d67a", "title": "Revocation Statuses on the Internet", "abstract": "The modern Internet is highly dependent on the trust communicated via X.509 certificates. However, in some cases certificates become untrusted and it is necessary to revoke them. In practice, the problem of secure certificate revocation has not yet been solved, and today no revocation procedure (similar to Certificate Transparency w.r.t. certificate issuance) has been adopted to provide transparent and immutable history of all revocations. Instead, the status of most certificates can only be checked with Online Certificate Status Protocol (OCSP) and/or Certificate Revocation Lists (CRLs). In this paper, we present the first longitudinal characterization of the revocation statuses delivered by CRLs and OCSP servers from the time of certificate expiration to status disappearance. The analysis captures the status history of over 1 million revoked certificates, including 773K certificates mass-revoked by Let\u2019s Encrypt. Our characterization provides a new perspective on the Internet\u2019s revocation rates, quantifies how short-lived the revocation statuses are, highlights differences in revocation practices within and between different CAs, and captures biases and oddities in the handling of revoked certificates. Combined, the findings motivate the development and adoption of a revocation transparency standard.", "venue": "PAM", "authors": ["Nikita  Korzhitskii", "Niklas  Carlsson"], "year": 2021, "n_citations": 0}
{"id": 1022406, "s2_id": "9bbe32368ecb0982664ad048d07956009f284824", "title": "Quality of Service (QoS): Measurements of Video Streaming", "abstract": "Nowadays video streaming is growing over the social clouds, where end-users always want to share High Definition (HD) videos among friends. Mostly videos were recorded via smartphones and other HD devices and short time videos have a big file size. The big file size of videos required high bandwidth to upload and download on the Internet and also required more time to load in a web page for play. So avoiding this problem social cloud compress videos during the upload for smooth play and fast loading in a web page. Compression decreases the video quality which also decreases the quality of experience of end users. In this paper we measure the QoS of different standard video file formats on social clouds; they varied from each other in resolution, audio/video bitrate, and storage size.", "venue": "ArXiv", "authors": ["Sajida  Karim", "Hui  He", "Asif Ali Laghari", "Hina  Madiha"], "year": 2020, "n_citations": 1}
{"id": 1022486, "s2_id": "423e43f48ecb446132645c05edb5c19886799cc3", "title": "Operational Semantics for Product-Form Solution", "abstract": "In this paper we present product-form solutions from the point of view of stochastic process algebra. In previous work [16] we have shown how to derive product-form solutions for a formalism called Labelled Markov Automata (LMA). LMA are very useful as their relation with the Continuous Time Markov Chains is very direct. The disadvantage of using LMA is that the proofs of properties are cumbersome. In fact, in LMA it is not possible to use the inductive structure of the language in a proof. In this paper we consider a simple stochastic process algebra that has the great advantage of simplifying the proofs. This simple language has been inspired by PEPA [10], however, detailed analysis of the semantics of cooperation will show the differences between the two formalisms. It will also be shown that the semantics of the cooperation in process algebra influences the correctness of the derivation of the product-form solutions.", "venue": "EPEW/UKPEW", "authors": ["Maria Grazia Vigliotti"], "year": 2012, "n_citations": 0}
{"id": 1023571, "s2_id": "82d9513c212cd1f37dbc7c0a2e624bdfd1110e7c", "title": "Fluid Model Checking", "abstract": "In this paper we investigate a potential use of fluid approximation techniques in the context of stochastic model checking of CSL formulae. We focus on properties describing the behaviour of a single agent in a (large) population of agents, exploiting a limit result known also as fast simulation. In particular, we will approximate the behaviour of a single agent with a time-inhomogeneous CTMC which depends on the environment and on the other agents only through the solution of the fluid differential equation. We will prove the asymptotic correctness of our approach in terms of satisfiability of CSL formulae and of reachability probabilities. We will also present a procedure to model check time-inhomogeneous CTMC against CSL formulae.", "venue": "CONCUR", "authors": ["Luca  Bortolussi", "Jane  Hillston"], "year": 2012, "n_citations": 114}
{"id": 1027555, "s2_id": "055359bf3807f067db7b3518540b719759df2388", "title": "An Analysis of Core- and Chip-Level Architectural Features in Four Generations of Intel Server Processors", "abstract": "This paper presents a survey of architectural features among four generations of Intel server processors (Sandy Bridge, Ivy Bridge, Haswell, and Broadwell) with a focus on performance with floating point workloads. Starting at the core level and going down the memory hierarchy we cover instruction throughput for floating-point instructions, L1 cache, address generation capabilities, core clock speed and its limitations, L2 and L3 cache bandwidth and latency, the impact of Cluster on Die (CoD) and cache snoop modes, and the Uncore clock speed. Using microbenchmarks we study the influence of these factors on code performance. We show that the energy efficiency of the LINPACK and HPCG benchmarks can be improved significantly by tuning the Uncore clock speed without sacrificing performance, and that the Graph500 benchmark performance may benefit from a suitable choice of cache snoop mode settings.", "venue": "ISC", "authors": ["Johannes  Hofmann", "Georg  Hager", "Gerhard  Wellein", "Dietmar  Fey"], "year": 2017, "n_citations": 24}
{"id": 1030846, "s2_id": "1d61c07aefc2f85b49a881421fdacd1b80b82d99", "title": "Multicast-based Architecture for IP Mobility: Simulation Analysis and Comparison with Basic Mobile IP", "abstract": "With the introduction of a newer generation of wireless devices and technologies, the need for an efficient architecture for IP mobility is becoming more apparent. Several architectures have been proposed to support IP mobility. Most studies, however, show that current architectures, in general, fall short from satisfying the performance requirements for wireless applications, mainly audio. Other studies have shown performance improvement by using multicast to reduce latency and packet loss during handoff. In this study, we propose a multicast-based architecture to support IP mobility. We evaluate our approach through simulation, and we compare it to mainstream approaches for IP mobility, mainly, the Mobile IP protocol. Comparison is performed according to the required performance criteria, such as smooth handoff and efficient routing. \nOur simulation results show significant improvement for the proposed architecture. On average, basic Mobile IP consumes almost twice as much network bandwidth, and experiences more than twice as much end-to-end and handoff delays, as does our proposed architecture. Furthermore, we propose an extension to Mobile IP to support our architecture with minimal modification.", "venue": "ArXiv", "authors": ["Ahmed  Helmy"], "year": 2000, "n_citations": 11}
{"id": 1033982, "s2_id": "2aaa4a88938b3f2df6baa78c2b323e3a1bb3328c", "title": "NTP : A Neural Network Topology Profiler", "abstract": "Performance of end-to-end neural networks on a given hardware platform is a function of its compute and memory signature, which in-turn, is governed by a wide range of parameters such as topology size, primitives used, framework used, batching strategy, latency requirements, precision etc. Current benchmarking tools suffer from limitations such as a) being either too granular like DeepBench [1] (or) b) mandate a working implementation that is either framework specific or hardware-architecture specific or both (or) c) provide only high level benchmark metrics. In this paper, we present NTP (Neural Net Topology Profiler), a sophisticated benchmarking framework, to effectively identify memory and compute signature of an end-to-end topology on multiple hardware architectures, without the need for an actual implementation. NTP is tightly integrated with hardware specific benchmarking tools to enable exhaustive data collection and analysis. Using NTP, a deep learning researcher can quickly establish baselines needed to understand performance of an end-to-end neural network topology and make high level architectural decisions. Further, integration of NTP with frameworks like Tensorflow, Pytorch, Intel OpenVINO etc. allows for performance comparison along several vectors like a) Comparison of different frameworks on a given hardware b) Comparison of different hardware using a given framework c) Comparison across different heterogeneous hardware configurations for given framework etc. These capabilities empower a researcher to effortlessly make architectural decisions needed for achieving optimized performance on any hardware platform. The paper documents the architectural approach of NTP and demonstrates the capabilities of the tool by benchmarking Mozilla DeepSpeech, a popular Speech Recognition topology.", "venue": "ArXiv", "authors": ["Raghavendra  Bhat", "Pravin  Chandran", "Juby  Jose", "Viswanath  Dibbur", "Prakash Sirra Ajith"], "year": 2019, "n_citations": 1}
{"id": 1038594, "s2_id": "9596bd9486a93dc24a682543354e0d27b9fbd6ba", "title": "On the Effectiveness of Polynomial Realization of Reed-Solomon Codes for Storage Systems", "abstract": "There are different ways to realize Reed Solomon (RS) codes. While in the storage community, using the generator matrices to implement RS codes is more popular, in the coding theory community the generator polynomials are typically used to realize RS codes. Prominent exceptions include HDFS-RAID, which uses generator polynomial based erasure codes, and extends the Apache Hadoop's file system. \nIn this paper we evaluate the performance of an implementation of polynomial realization of Reed-Solomon codes, along with our optimized version of it, against that of a widely-used library (Jerasure) that implements the main matrix realization alternatives. Our experimental study shows that despite significant performance gains yielded by our optimizations, the polynomial implementations' performance is constantly inferior to those of matrix realization alternatives in general, and that of Cauchy bit matrices in particular.", "venue": "ArXiv", "authors": ["Kyumars Sheykh Esmaili", "Anwitaman  Datta"], "year": 2013, "n_citations": 2}
{"id": 1038675, "s2_id": "fc2246157fffbcd02310307d875717b0e63717aa", "title": "System Performance with varying L1 Instruction and Data Cache Sizes: An Empirical Analysis", "abstract": "In this project, we investigate the fluctuations in performance caused by changing the Instruction (I-cache) size and the Data (D-cache) size in the L1 cache. We employ the Gem5 framework to simulate a system with varying specifications on a single host machine. We utilize the FreqMine benchmark available under the PARSEC suite as the workload program to benchmark our simulated system. The Out-order CPU (O3) with Ruby memory model was simulated in a Full-System X86 environment with Linux OS. The chosen metrics deal with Hit Rate, Misses, Memory Latency, Instruction Rate, and Bus Traffic within the system. Performance observed by varying L1 size within a certain range of values was used to compute Confidence Interval based statistics for relevant metrics. Our expectations, corresponding experimental observations, and discrepancies are also discussed in this report.", "venue": "ArXiv", "authors": ["Ramya  Akula", "Kartik  Jain", "Deep Jigar Kotecha"], "year": 2019, "n_citations": 2}
{"id": 1039757, "s2_id": "443701309d27b41bc23e28c950a052e1199fc0df", "title": "Fast and Generalized Polynomial Time Memory Consistency Verification", "abstract": "The problem of verifying multi-threaded execution against the memory consistency model of a processor is known to be an NP hard problem. However polynomial time algorithms exist that detect almost all failures in such execution. These are often used in practice for microprocessor verification. We present a low complexity and fully parallelized algorithm to check program execution against the processor consistency model. In addition our algorithm is general enough to support a number of consistency models without any degradation in performance. An implementation of this algorithm is currently used in practice to verify processors in the post silicon stage for multiple architectures.", "venue": "CAV", "authors": ["Amitabha  Roy", "Stephan  Zeisset", "Charles J. Fleckenstein", "John C. Huang"], "year": 2006, "n_citations": 47}
{"id": 1041917, "s2_id": "49ecb49f3541db01869eac1e6a95fbea13ef278b", "title": "GraphZero: Breaking Symmetry for Efficient Graph Mining", "abstract": "Graph mining for structural patterns is a fundamental task in many applications. Compilation-based graph mining systems, represented by AutoMine, generate specialized algorithms for the provided patterns and substantially outperform other systems. However, the generated code causes substantial computation redundancy and the compilation process incurs too much overhead to be used online, both due to the inherent symmetry in the structural patterns. \nIn this paper, we propose an optimizing compiler, GraphZero, to completely address these limitations through symmetry breaking based on group theory. GraphZero implements three novel techniques. First, its schedule explorer efficiently prunes the schedule space without missing any high-performance schedule. Second, it automatically generates and enforces a set of restrictions to eliminate computation redundancy. Third, it generalizes orientation, a surprisingly effective optimization that was mainly used for clique patterns, to apply to arbitrary patterns. Evaluated on multiple graph mining applications and complex patterns with 7 real-world graph datasets, GraphZero demonstrates up to 40X performance improvement and up to 197X reduction on schedule generation overhead over AutoMine.", "venue": "ArXiv", "authors": ["Daniel  Mawhirter", "Sam  Reinehr", "Connor  Holmes", "Tongping  Liu", "Bo  Wu"], "year": 2019, "n_citations": 10}
{"id": 1044168, "s2_id": "659f1a3b5edd3ee6151db4f25f54846780715244", "title": "On the Behavior of the Distributed Coordination Function of IEEE 802.11 with Multirate Capability under General Transmission Conditions", "abstract": "The aim of this paper is threefold. First, it presents a multi-dimensional Markovian state transition model characterizing the behavior of the IEEE 802.11 protocol at the Medium Access Control layer which accounts for packet transmission failures due to channel errors modeling both saturated and non-saturated traffic conditions. Second, it provides a throughput analysis of the IEEE 802.11 protocol at the data link layer in both saturated and non-saturated traffic conditions taking into account the impact of both the physical propagation channel and multirate transmission in Rayleigh fading environment. The general traffic model assumed is M/M/1/K. Finally, it shows that the behavior of the throughput in non-saturated traffic conditions is a linear combination of two system parameters; the payload size and the packet rates, $\\lambda^{(s)}$, of each contending station. The validity interval of the proposed model is also derived. \nSimulation results closely match the theoretical derivations, confirming the effectiveness of the proposed models.", "venue": "ArXiv", "authors": ["Fred  Daneshgaran", "Massimiliano  Laddomada", "Fabio  Mesiti", "Marina  Mondin"], "year": 2007, "n_citations": 3}
{"id": 1048067, "s2_id": "3516073ffb7bf38ca5837a4f48c843089f72225c", "title": "Sequential Algorithms and Independent Sets Discovering on Large Sparse Random Graphs", "abstract": "Computing the size of maximum independent sets is a NP-hard problem for fixed graphs. Characterizing and designing efficient algorithms to estimate this independence number for random graphs are notoriously difficult and still largely open issues. In a companion paper, we showed that a low complexity degree-greedy exploration is actually asymptotically optimal on a large class of sparse random graphs. Encouraged by this result, we present and study two variants of sequential exploration algorithms: static and dynamic degree-aware explorations. We derive hydrodynamic limits for both of them, which in turn allow us to compute the size of the resulting independent set. Whereas the former is simpler to compute, the latter may be used to arbitrarily approximate the degree-greedy algorithm. Both can be implemented in a distributed manner. The corresponding hydrodynamic limits constitute an efficient method to compute or bound the independence number for a large class of sparse random graphs. As an application, we then show how our method may be used to estimate the capacity of a large 802.11-based wireless network. We finally consider further indicators such as the fairness of the resulting configuration, and show how an unexpected trade-off between fairness and capacity can be achieved.", "venue": "ArXiv", "authors": ["Paola  Bermolen", "Matthieu  Jonckheere", "Federico  Larroca", "Manuel  Saenz"], "year": 2020, "n_citations": 1}
{"id": 1049207, "s2_id": "8d64f808c933bdd3ada626e49f83bb238198af66", "title": "A domain-specific language and matrix-free stencil code for investigating electronic properties of Dirac and topological materials", "abstract": "We introduce PVSC-DTM (Parallel Vectorized Stencil Code for Dirac and Topological Materials), a library and code generator based on a domain-specific language tailored to implement the specific stencil-like algorithms that can describe Dirac and topological materials such as graphene and topological insulators in a matrix-free way. The generated hybrid-parallel (MPI+OpenMP) code is fully vectorized using Single Instruction Multiple Data (SIMD) extensions. It is significantly faster than matrix-based approaches on the node level and performs in accordance with the roofline model. We demonstrate the chip-level performance and distributed-memory scalability of basic building blocks such as sparse matrix-(multiple-) vector multiplication on modern multicore CPUs. As an application example, we use the PVSC-DTM scheme to (i) explore the scattering of a Dirac wave on an array of gate-defined quantum dots, to (ii) calculate a bunch of interior eigenvalues for strong topological insulators, and to (iii) discuss the photoemission spectra of a disordered Weyl semimetal.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Andreas  Pieper", "Georg  Hager", "Holger  Fehske"], "year": 2021, "n_citations": 0}
{"id": 1053643, "s2_id": "158b774a1deeb95fa7fcd315c19b1d99ab5d20b9", "title": "Higher aggregation of gNodeBs in Cloud-RAN architectures via parallel computing", "abstract": "In this paper, we address the virtualization and the centralization of real-time network functions, notably in the framework of Cloud RAN (C-RAN). We thoroughly analyze the required fronthaul capacity for the deployment of the proposed C-RAN architecture. We are specifically interested in the performance of the software based channel coding function. We develop a dynamic multi-threading approach to achieve parallel computing on a multi-core platform. Measurements from an OAI-based testbed show important gains in terms of latency; this enables the increase of the distance between the radio elements and the virtualized RAN functions and thus a higher aggregation of gNodeBs in edge data centers, referred to as Central Offices (COs).", "venue": "2019 22nd Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)", "authors": ["Veronica Quintuna Rodriguez", "Fabrice  Guillemin"], "year": 2019, "n_citations": 4}
{"id": 1054315, "s2_id": "de7bf50c1e1953e727d3f2d46a596643b723ded5", "title": "Faster-than-light effects and negative group delays in optics and electronics, and their applications", "abstract": "Recent manifestations of apparently faster-than-light effects confirmed our predictions that the group velocity in transparent optical media can exceed c. Special relativity is not violated by these phenomena. Moreover, in the electronic domain, the causality principle does not forbid negative group delays of analytic signals in electronic circuits, in which the peak of an output pulse leaves the exit port of a circuit before the peak of the input pulse enters the input port. Furthermore, pulse distortion for these 'superluminal' analytic signal scan be negligible in both the optical and electronic domains. Here we suggest an extension of these ideas to the microelectronic domain. The underlying principle is that negative feedback can be used to produce negative group delays. Such negative group delay scan be used to cancel out the positive group delays due to 'transistor latency' as well as the 'propagation delays' due to the interconnects between transistors. Using this principle, it is possible to speed up computer systems.", "venue": "SPIE OPTO", "authors": ["Raymond Y. Chiao", "Jandir M. Hickmann", "Daniel  Solli"], "year": 2001, "n_citations": 6}
{"id": 1055440, "s2_id": "a127cfd083d94b39ca3d3059525541bc80d1ecae", "title": "Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization", "abstract": "A common pain point in differentially private machine learning is the significant runtime overhead incurred when executing Differentially Private Stochastic Gradient Descent (DPSGD), which may be as large as two orders of magnitude. We thoroughly demonstrate that by exploiting powerful language primitives, including vectorization, just-in-time compilation, and static graph optimization, one can dramatically reduce these overheads, in many cases nearly matching the best non-private running times. These gains are realized in two frameworks: JAX and TensorFlow. JAX provides rich support for these primitives as core features of the language through the XLA compiler. We also rebuild core parts of TensorFlow Privacy, integrating features from TensorFlow 2 as well as XLA compilation, granting significant memory and runtime improvements over the current release version. These approaches allow us to achieve up to 50x speedups in comparison to the best alternatives. Our code is available at this https URL.", "venue": "ArXiv", "authors": ["Pranav  Subramani", "Nicholas  Vadivelu", "Gautam  Kamath"], "year": 2020, "n_citations": 16}
{"id": 1062335, "s2_id": "ef8da20db0fa64cb783aa5b6f1ac5fe4c8053cdd", "title": "On throughput capacity for a class of buffer-limited MANETs", "abstract": "Available throughput performance studies for mobile ad hoc networks (MANETs) suffer from two major limitations: they mainly focus on the scaling law study of throughput, while the exact throughput of such networks remains largely unknown; they usually consider the infinite buffer scenarios, which are not applicable to the practical networks with limited buffer. As a step to address these limitations, this paper develops a general framework for the exact throughput capacity study of a class of buffer-limited MANETs with the two-hop relay. We first provide analysis to reveal how the throughput capacity of such a MANET is determined by its relay-buffer blocking probability (RBP). Based on the Embedded Markov Chain Theory and Queuing Theory, a novel theoretical framework is then developed to enable the RBP and closed-form expression for exact throughput capacity to be derived. We further conduct case studies under two typical transmission scheduling schemes to illustrate the applicability of our framework and to explore the corresponding capacity optimization as well as capacity scaling law. Finally, extensive simulation and numerical results are provided to validate the efficiency of our framework and to show the impacts brought by the buffer constraint.", "venue": "Ad Hoc Networks", "authors": ["Jia  Liu", "Min  Sheng", "Yang  Xu", "Jiandong  Li", "Xiaohong  Jiang"], "year": 2016, "n_citations": 11}
{"id": 1068742, "s2_id": "6eb7ab884b3f4eef4272fcb5a40b7ef7b349baef", "title": "Towards Performance Clarity of Edge Video Analytics", "abstract": "Edge video analytics is becoming the solution to many safety and management tasks. Its wide deployment, however, must first address the tension between inference accuracy and resource (compute/network) cost. This has led to the development of video analytics pipelines (VAPs), which reduce resource cost by combining DNN compression/speedup techniques with video processing heuristics. Our measurement study, however, shows that today\u2019s methods for evaluating VAPs are incomplete, often producing premature conclusions or ambiguous results. This is because each VAP\u2019s performance varies substantially across videos and time, and is sensitive to different subsets of video content characteristics. We argue that accurate VAP evaluation must first characterize the complex interaction between VAPs and video characteristics, which we refer to as VAP performance clarity. We design and implement Yoda, the first VAP benchmark to achieve performance clarity. Using primitive-based profiling and a carefully curated benchmark video set, Yoda builds a performance clarity profile for each VAP to precisely define its accuracy/cost tradeoff and its relationship with video characteristics. We show that Yoda substantially improves VAP evaluations by (1) providing a comprehensive, transparent assessment of VAP performance and its dependencies on video characteristics; (2) explicitly identifying fine-grained VAP behaviors that were previously hidden by large performance Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SEC \u201921, December 14\u201317, 2021, San Jose, CA, USA \u00a9 2021 Association for Computing Machinery. ACM ISBN XXX-X-XXXX-XXXX-X/XX/XX. . . $15.00 https://doi.org/10.1145/3341302.XXXXXXX variance; and (3) revealing strengths/weaknesses among different VAPs and new design opportunities. CCS CONCEPTS \u2022 Networks \u2192 Mobile networks; Network performance analysis; \u2022 Information systems \u2192 Multimedia information systems.", "venue": "ArXiv", "authors": ["Zhujun  Xiao", "Zhengxu  Xia", "Haitao  Zheng", "Ben Y. Zhao", "Junchen  Jiang"], "year": 2021, "n_citations": 1}
{"id": 1071838, "s2_id": "2b71e4053be465dcd68064a3a45dedc9fa368726", "title": "AIBench: An Industry Standard Internet Service AI Benchmark Suite", "abstract": "Today's Internet Services are undergoing fundamental changes and shifting to an intelligent computing era where AI is widely employed to augment services. In this context, many innovative AI algorithms, systems, and architectures are proposed, and thus the importance of benchmarking and evaluating them rises. However, modern Internet services adopt a microservice-based architecture and consist of various modules. The diversity of these modules and complexity of execution paths, the massive scale and complex hierarchy of datacenter infrastructure, the confidential issues of data sets and workloads pose great challenges to benchmarking. In this paper, we present the first industry-standard Internet service AI benchmark suite---AIBench with seventeen industry partners, including several top Internet service providers. AIBench provides a highly extensible, configurable, and flexible benchmark framework that contains loosely coupled modules. We identify sixteen prominent AI problem domains like learning to rank, each of which forms an AI component benchmark, from three most important Internet service domains: search engine, social network, and e-commerce, which is by far the most comprehensive AI benchmarking effort. On the basis of the AIBench framework, abstracting the real-world data sets and workloads from one of the top e-commerce providers, we design and implement the first end-to-end Internet service AI benchmark, which contains the primary modules in the critical paths of an industry scale application and is scalable to deploy on different cluster scales. The specifications, source code, and performance numbers are publicly available from the benchmark council web site this http URL.", "venue": "ArXiv", "authors": ["Wanling  Gao", "Fei  Tang", "Lei  Wang", "Jianfeng  Zhan", "Chunxin  Lan", "Chunjie  Luo", "Yunyou  Huang", "Chen  Zheng", "Jiahui  Dai", "Zheng  Cao", "Daoyi  Zheng", "Haoning  Tang", "Kunlin  Zhan", "Biao  Wang", "Defei  Kong", "Tong  Wu", "Minghe  Yu", "Chongkang  Tan", "Huan  Li", "Xinhui  Tian", "Yatao  Li", "Junchao  Shao", "Zhenyu  Wang", "Xiaoyu  Wang", "Hainan  Ye"], "year": 2019, "n_citations": 37}
{"id": 1077410, "s2_id": "e3b1f12a240ff79e7e6e53aadb36fc683b157e9b", "title": "How Data Volume Affects Spark Based Data Analytics on a Scale-up Server", "abstract": "Sheer increase in volume of data over the last decade has triggered research in cluster computing frameworks that enable web enterprises to extract big insights from big data. While Apache Spark is gaining popularity for exhibiting superior scale-out performance on the commodity machines, the impact of data volume on the performance of Spark based data analytics in scale-up configuration is not well understood. We present a deep-dive analysis of Spark based applications on a large scale-up server machine. Our analysis reveals that Spark based data analytics are DRAM bound and do not benefit by using more than 12 cores for an executor. By enlarging input data size, application performance degrades significantly due to substantial increase in wait time during I/O operations and garbage collection, despite 10\\% better instruction retirement rate (due to lower L1 cache misses and higher core utilization). We match memory behaviour with the garbage collector to improve performance of applications between 1.6x to 3x.", "venue": "BPOE", "authors": ["Ahsan Javed Awan", "Mats  Brorsson", "Vladimir  Vlassov", "Eduard  Ayguad\u00e9"], "year": 2015, "n_citations": 18}
{"id": 1079077, "s2_id": "cdc602ed161a6b36b03be1346637eb2be0b8ecd6", "title": "Optimizing Performance of Continuous-Time Stochastic Systems using Timeout Synthesis", "abstract": "We consider parametric version of fixed-delay continuous-time Markov chains or equivalently deterministic and stochastic Petri nets, DSPN where fixed-delay transitions are specified by parameters, rather than concrete values. Our goal is to synthesize values of these parameters that, for a given cost function, minimise expected total cost incurred before reaching a given set of target states. We show that under mild assumptions, optimal values of parameters can be effectively approximated using translation to a Markov decision process MDP whose actions correspond to discretized values of these parameters. To this end we identify and overcome several interesting phenomena arising in systems with fixed delays.", "venue": "QEST", "authors": ["Tom\u00e1s  Br\u00e1zdil", "Lubos  Korenciak", "Jan  Krc\u00e1l", "Petr  Novotn\u00fd", "Vojtech  Reh\u00e1k"], "year": 2015, "n_citations": 11}
{"id": 1080368, "s2_id": "4e6fed8dff7e47cbaa07ffaa01976f56e4d4b960", "title": "Performance analysis of Markov modulated 1-persistent CSMA/CA protocols with exponential backoff scheduling", "abstract": "This paper proposes a Markovian model of 1-persistent CSMA/CA protocols with K-Exponential backoff scheduling algorithm. The input buffer of each access node is modeled as a Geo/G/1 queue, and the service time distribution of each individual head-of-line packet is derived from the Markov chain of the underlying scheduling algorithm. From the queueing model, we derive the characteristic equation of network throughput and obtain the stable throughput and bounded delay regions with respect to the retransmission factor. Our results show that the stable throughput region of the exponential backoff scheme exists even for an infinite population. Moreover, we find that the bounded delay region of exponential backoff is only a sub-set of its stable throughput region due to the large variance of the service time of input packets caused by the capture effect. All analytical results presented in this paper are verified by simulations.", "venue": "Wirel. Networks", "authors": ["Pui King Wong", "Dongjie  Yin", "Tony T. Lee"], "year": 2011, "n_citations": 8}
{"id": 1082305, "s2_id": "d18affb41ce868503efd90ca390befd9f59bc0b5", "title": "Performance Optimization and Modeling of Fine-Grained Irregular Communication in UPC", "abstract": "The Unified Parallel C (UPC) programming language offers parallelism via logically partitioned shared memory, which typically spans physically disjoint memory subsystems. One convenient feature of UPC is its ability to automatically execute between-thread data movement, such that the entire content of a shared data array appears to be freely accessible by all the threads. The programmer friendliness, however, can come at the cost of substantial performance penalties. This is especially true when indirectly indexing the elements of a shared array, for which the induced between-thread data communication can be irregular and have a fine-grained pattern. In this paper, we study performance enhancement strategies specifically targeting such fine-grained irregular communication in UPC. Starting from explicit thread privatization, continuing with block-wise communication, and arriving at message condensing and consolidation, we obtained considerable performance improvement of UPC programs that originally require fine-grained irregular communication. Besides the performance enhancement strategies, the main contribution of the present paper is to propose performance models for the different scenarios, in the form of quantifiable formulas that hinge on the actual volumes of various data movements plus a small number of easily obtainable hardware characteristic parameters. These performance models help to verify the enhancements obtained, while also providing insightful predictions of similar parallel implementations, not limited to UPC, that also involve between-thread or between-process irregular communication. As a further validation, we also apply our performance modeling methodology and hardware characteristic parameters to an existing UPC code for solving a 2D heat equation on a uniform mesh.", "venue": "Sci. Program.", "authors": ["J\u00e9r\u00e9mie  Lagravi\u00e8re", "Johannes  Langguth", "Martina  Prugger", "Lukas  Einkemmer", "Phuong Hoai Ha", "Xing  Cai"], "year": 2019, "n_citations": 1}
{"id": 1083022, "s2_id": "44355907c841d2dbc81fb17d8d2d69d8a3154951", "title": "The Feasibility of Using OpenCL Instead of OpenMP for Parallel CPU Programming", "abstract": "OpenCL, along with CUDA, is one of the main tools used to program GPGPUs. However, it allows running the same code on multi-core CPUs too, making it a rival for the long-established OpenMP. In this paper we compare OpenCL and OpenMP when developing and running compute-heavy code on a CPU. Both ease of programming and performance aspects are considered. Since, unlike a GPU, no memory copy operation is involved, our comparisons measure the code generation quality, as well as thread management efficiency of OpenCL and OpenMP. We evaluate the performance of these development tools under two conditions: a large number of short-running compute-heavy parallel code executions, when more thread management is performed, and a small number of long-running parallel code executions, when less thread management is required. The results show that OpenCL and OpenMP each win in one of the two conditions. We argue that while using OpenMP requires less setup, OpenCL can be a viable substitute for OpenMP from a performance point of view, especially when a high number of thread invocations is required. We also provide a number of potential pitfalls to watch for when moving from OpenMP to OpenCL.", "venue": "ArXiv", "authors": ["Kamran  Karimi"], "year": 2015, "n_citations": 2}
{"id": 1083968, "s2_id": "2ed09ce69ec5e46e55c44e894aed20022bc97772", "title": "Reliable Generation of High-Performance Matrix Algebra", "abstract": "Scientific programmers often turn to vendor-tuned Basic Linear Algebra Subprograms (BLAS) to obtain portable high performance. However, many numerical algorithms require several BLAS calls in sequence, and those successive calls do not achieve optimal performance. The entire sequence needs to be optimized in concert. Instead of vendor-tuned BLAS, a programmer could start with source code in Fortran or C (e.g., based on the Netlib BLAS) and use a state-of-the-art optimizing compiler. However, our experiments show that optimizing compilers often attain only one-quarter of the performance of hand-optimized code. In this article, we present a domain-specific compiler for matrix kernels, the Build to Order BLAS (BTO), that reliably achieves high performance using a scalable search algorithm for choosing the best combination of loop fusion, array contraction, and multithreading for data parallelism. The BTO compiler generates code that is between 16% slower and 39% faster than hand-optimized code.", "venue": "ACM Trans. Math. Softw.", "authors": ["Thomas  Nelson", "Geoffrey  Belter", "Jeremy G. Siek", "Elizabeth R. Jessup", "Boyana  Norris"], "year": 2015, "n_citations": 12}
{"id": 1085253, "s2_id": "6c06ee9811fc2af7877477a4e164cda574cd2b3b", "title": "Slow and Stale Gradients Can Win the Race", "abstract": "Distributed Stochastic Gradient Descent (SGD) when run in a synchronous manner, suffers from delays in runtime as it waits for the slowest workers (stragglers). Asynchronous methods can alleviate stragglers, but cause gradient staleness that can adversely affect the convergence error. In this work, we present a novel theoretical characterization of the speedup offered by asynchronous methods by analyzing the trade-off between the error in the trained model and the actual training runtime (wallclock time). The main novelty in our work is that our runtime analysis considers random straggling delays, which helps us design and compare distributed SGD algorithms that strike a balance between straggling and staleness. We also provide a new error convergence analysis of asynchronous SGD variants without bounded or exponential delay assumptions. Finally, based on our theoretical characterization of the error-runtime trade-off, we propose a method of gradually varying synchronicity in distributed SGD and demonstrate its performance on the CIFAR10 dataset.", "venue": "IEEE Journal on Selected Areas in Information Theory", "authors": ["Sanghamitra  Dutta", "Gauri  Joshi", "Soumyadip  Ghosh", "Parijat  Dube", "Priya  Nagpurkar"], "year": 2021, "n_citations": 96}
{"id": 1086273, "s2_id": "cfd096ea0419079b7122d3e00af28d390ba3e2db", "title": "Synthesizing Credit Card Transactions", "abstract": "Two elements have been essential to AI's recent boom: (1) deep neural nets and the theory and practice behind them; and (2) cloud computing with its abundant labeled data and large computing resources. \nAbundant labeled data is available for key domains such as images, speech, natural language processing, and recommendation engines. However, there are many other domains where such data is not available, or access to it is highly restricted for privacy reasons, as with health and financial data. Even when abundant data is available, it is often not labeled. Doing such labeling is labor-intensive and non-scalable. \nAs a result, to the best of our knowledge, key domains still lack labeled data or have at most toy data; or the synthetic data must have access to real data from which it can mimic new data. This paper outlines work to generate realistic synthetic data for an important domain: credit card transactions. \nSome challenges: there are many patterns and correlations in real purchases. There are millions of merchants and innumerable locations. Those merchants offer a wide variety of goods. Who shops where and when? How much do people pay? What is a realistic fraudulent transaction? \nWe use a mixture of technical approaches and domain knowledge including mechanics of credit card processing, a broad set of consumer domains: electronics, clothing, hair styling, etc. Connecting everything is a virtual world. This paper outlines some of our key techniques and provides evidence that the data generated is indeed realistic. \nBeyond the scope of this paper: (1) use of our data to develop and train models to predict fraud; (2) coupling models and the synthetic dataset to assess performance in designing accelerators such as GPUs and TPUs.", "venue": "ArXiv", "authors": ["Erik R. Altman"], "year": 2019, "n_citations": 3}
{"id": 1086449, "s2_id": "95735893a2e4acda9aec1805522d35344867ad34", "title": "AITuning: Machine Learning-based Tuning Tool for Run-Time Communication Libraries", "abstract": "In this work, we address the problem of tuning communication libraries by using a deep reinforcement learning approach. Reinforcement learning is a machine learning technique incredibly effective in solving game-like situations. In fact, tuning a set of parameters in a communication library in order to get better performance in a parallel application can be expressed as a game: Find the right combination/path that provides the best reward. Even though AITuning has been designed to be utilized with different run-time libraries, we focused this work on applying it to the OpenCoarrays run-time communication library, built on top of MPI-3. This work not only shows the potential of using a reinforcement learning algorithm for tuning communication libraries, but also demonstrates how the MPI Tool Information Interface, introduced by the MPI-3 standard, can be used effectively by run-time libraries to improve the performance without human intervention.", "venue": "PARCO", "authors": ["Alessandro  Fanfarillo", "Davide Del Vento"], "year": 2019, "n_citations": 1}
{"id": 1086705, "s2_id": "4b9ae83763cf3e135b3e9664f7883f69b9afec9f", "title": "Asymptotics of Insensitive Load Balancing and Blocking Phases", "abstract": "Load balancing with various types of load information has become a key component of modern communication and information systems. In many systems, characterizing precisely the blocking probability allows to establish a performance trade-off between delay and losses. We address here the problem of giving robust performance bounds based on the study of the asymptotic behavior of the insensitive load balancing schemes when the number of servers and the load scales jointly. These schemes have the desirable property that the stationary distribution of the resulting stochastic network depends on the distribution of job sizes only through its mean. It was shown that they give good estimates of performance indicators for systems with finite buffers, generalizing henceforth Erlang's formula whereas optimal policies are already theoretically and computationally out of reach for networks of moderate size. We study a single class of traffic acting on a symmetric set of processor sharing queues with finite buffers and we consider the case where the load scales with the number of servers.We characterize the response of symmetric systems under those schemes at different scales and show that three amplitudes of deviations can be identified according to whether \u03c1 < 1, \u03c1 = 1, and \u03c1 > 1. A central limit scaling takes place for a sub-critical load;for \u03c1=1, the number of free servers scales like n{\u03b8/\u03b8+1} (\u03b8 being the buffer depth and n being the number of servers) and is of order 1 for super-critical loads. This further implies the existence of different phases for the blocking probability. Before a (refined) critical load \u03c1c(n)=1-a n- {\u03b8/\u03b8+1}, the blocking is exponentially small and becomes of order n- {\u03b8/\u03b8+1} at \u03c1c(n). This generalizes the well-known Quality and Efficiency Driven (QED) regime or Halfin-Whitt regime for a one-dimensional queue, and leads to a generalized staffing rule for a given target blocking probability.", "venue": "SIGMETRICS 2016", "authors": ["Matthieu  Jonckheere", "Balakrishna J. Prabhu"], "year": 2016, "n_citations": 11}
{"id": 1090097, "s2_id": "64d639ba7d27381104b4d57e4aca904caeb30597", "title": "Reconfigurable Intelligent Surface for MISO Systems with Proportional Rate Constraints", "abstract": "This paper investigates the spectral efficiency (SE) in reconfigurable intelligent surface (RIS)-aided multiuser multiple-input single-output (MISO) systems, where RIS can reconFigure the propagation environment via a large number of controllable and intelligent phase shifters. In order to explore the SE performance with user proportional fairness for such a system, an optimization problem is formulated to maximize the SE by jointly considering the power allocation at the base station (BS) and phase shift at the RIS, under nonlinear proportional rate fairness constraints. To solve the non-convex optimization problem, an effective solution is developed, which capitalizes on an iterative algorithm with closed-form expressions, i.e., alternatively optimizing the transmit power at the BS and the reflecting phase shift at the RIS. Numerical simulations are provided to validate the theoretical analysis and assess the performance of the proposed alternative algorithm.", "venue": "ICC 2020 - 2020 IEEE International Conference on Communications (ICC)", "authors": ["Yulan  Gao", "Chao  Yong", "Zehui  Xiong", "Dusit  Niyato", "Yue  Xiao", "Jun  Zhao"], "year": 2020, "n_citations": 13}
{"id": 1090337, "s2_id": "c14da1ee52fbc8d2a2cb5b72fc1b98e333654854", "title": "Parallel Sparse Matrix-Vector Multiplication as a Test Case for Hybrid MPI+OpenMP Programming", "abstract": "We evaluate optimized parallel sparse matrix-vector operations for two representative application areas on widespread multicore-based cluster configurations. First the single-socket baseline performance is analyzed and modeled with respect to basic architectural properties of standard multicore chips. Going beyond the single node, parallel sparse matrix-vector operations often suffer from an unfavorable communication to computation ratio. Starting from the observation that nonblocking MPI is not able to hide communication cost using standard MPI implementations, we demonstrate that explicit overlap of communication and computation can be achieved by using a dedicated communication thread, which may run on a virtual core. We compare our approach to pure MPI and the widely used \"vector-like'' hybrid programming strategy.", "venue": "2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum", "authors": ["Gerald  Schubert", "Georg  Hager", "Holger  Fehske", "Gerhard  Wellein"], "year": 2011, "n_citations": 19}
{"id": 1093106, "s2_id": "2229ac756f89c3db017293918548555734d2f891", "title": "TicTac: Accelerating Distributed Deep Learning with Communication Scheduling", "abstract": "State-of-the-art deep learning systems rely on iterative distributed training to tackle the increasing complexity of models and input data. The iteration time in these communication-heavy systems depends on the computation time, communication time and the extent of overlap of computation and communication. \nIn this work, we identify a shortcoming in systems with graph representation for computation, such as TensorFlow and PyTorch, that result in high variance in iteration time --- random order of received parameters across workers. We develop a system, TicTac, to improve the iteration time by fixing this issue in distributed deep learning with Parameter Servers while guaranteeing near-optimal overlap of communication and computation. TicTac identifies and enforces an order of network transfers which improves the iteration time using prioritization. Our system is implemented over TensorFlow and requires no changes to the model or developer inputs. TicTac improves the throughput by up to $37.7\\%$ in inference and $19.2\\%$ in training, while also reducing straggler effect by up to $2.3\\times$. Our code is publicly available.", "venue": "MLSys", "authors": ["Sayed Hadi Hashemi", "Sangeetha Abdu Jyothi", "Roy H. Campbell"], "year": 2019, "n_citations": 75}
{"id": 1093429, "s2_id": "805b8a967bc5a3ae6c3f811de3d65253bba82bd8", "title": "Optimized Implementation of Elliptic Curve Based Additive Homomorphic Encryption for Wireless Sensor Networks", "abstract": "When deploying wireless sensor networks (WSNs) in public environments it may become necessary to secure their data storage and transmission against possible attacks such as node-compromise and eavesdropping. The nodes feature only small computational and energy resources, thus requiring efficient algorithms. As a solution for this problem the TinyPEDS approach was proposed in [7], which utilizes the Elliptic Curve ElGamal (EC-ElGamal) cryptosystem for additive homomorphic encryption allowing concealed data aggregation. This work presents an optimized implementation of EC-ElGamal on a MicaZ mote, which is a typical sensor node platform with 8-bit processor for WSNs. Compared to the best previous result, our implementation is at least 44% faster for fixed-point multiplication. Because most parts of the algorithm are similar to standard Elliptic Curve algorithms, the results may be reused in other realizations on constrained devices as well.", "venue": "ArXiv", "authors": ["Osman  Ugus", "Dirk  Westhoff", "Ralf  Laue", "Abdulhadi  Shoufan", "Sorin A. Huss"], "year": 2009, "n_citations": 59}
{"id": 1107633, "s2_id": "1ed6e5c88c2cfbb34cf1b58bd82269f884bf6785", "title": "Integrating Post-Newtonian Equations on Graphics Processing Units", "abstract": "We report on early results of a numerical and statistical study of binary black hole inspirals. The two black holes are evolved using post-Newtonian approximations starting with initially randomly distributed spin vectors. We characterize certain aspects of the distribution shortly before merger. In particular we note the uniform distribution of black hole spin vector dot products shortly before merger and a high correlation between the initial and final black hole spin vector dot products in the equal-mass, maximally spinning case. These simulations were performed on Graphics Processing Units, and we demonstrate a speed-up of a factor 50 over a more conventional CPU implementation.", "venue": "ArXiv", "authors": ["Frank  Herrmann", "John  Silberholz", "Matias  Bellone", "Gustavo  Guerberoff", "Manuel  Tiglio"], "year": 2009, "n_citations": 13}
{"id": 1108614, "s2_id": "19a3610c030d9ab8009bb12ceafaa3ad31ffc51d", "title": "Brittle System Analysis", "abstract": "The goal of this paper is to define and analyze systems which exhibit brittle behavior. This behavior is characterized by a sudden and steep decline in performance as the system approaches the limits of tolerance. This can be due to input parameters which exceed a specified input, or environmental conditions which exceed specified operating boundaries. An analogy is made between brittle commmunication systems in particular and materials science.", "venue": "ArXiv", "authors": ["Stephen F. Bush", "John E. Hershey", "Kirby  Vosburgh"], "year": 1999, "n_citations": 7}
{"id": 1108900, "s2_id": "bd2b6bedc7bf7a56480fab1086b01656838fede7", "title": "A Virtual Queue Approach for Online Estimation of Loss Probability Based on MVA Theory", "abstract": "In network quality of service provisioning, premium services generally require to keep a very small loss probability, which is infeasible to measure directly. The proposed virtual queue scheme estimates the small packet loss probability of a real queueing system by measuring queue statistics in a set of separate virtual queues. A novel scaling property between the real queue and the virtual queues is deduced on the basis of the maximum variance asymptotic (MVA) theory. The new scheme retains the high accuracy and wide applicability of the MVA method for aggregated traffic while avoiding the high computational complexity in a direct application of the original MVA analysis in real time. This makes it suitable for online measurement applications such as network performance monitoring and measurement-based admission control.", "venue": "ArXiv", "authors": ["Guoqiang  Hu", "Yuming  Jiang", "Anne  Nevin"], "year": 2010, "n_citations": 0}
{"id": 1109116, "s2_id": "5e211677567d2713741c67d8d31a6395b28749c1", "title": "A PHY Layer Security Analysis of Uplink Cooperative Jamming-Based Underlay CRNs With Multi-Eavesdroppers", "abstract": "In this paper, the physical layer security of a dual-hop underlay uplink cognitive radio network is investigated over Nakagami-<inline-formula> <tex-math notation=\"LaTeX\">${m}$ </tex-math></inline-formula> fading channels. Specifically, multiple secondary sources <inline-formula> <tex-math notation=\"LaTeX\">$(S_{i})_{1\\leq i \\leq N}$ </tex-math></inline-formula> are taking turns in accessing the licensed spectrum of the primary users and communicating with a multi-antenna secondary base station (<inline-formula> <tex-math notation=\"LaTeX\">${D}$ </tex-math></inline-formula>) through the aid of a multi-antenna relay <inline-formula> <tex-math notation=\"LaTeX\">${R}$ </tex-math></inline-formula> in the presence of <inline-formula> <tex-math notation=\"LaTeX\">${M}$ </tex-math></inline-formula> eavesdroppers <inline-formula> <tex-math notation=\"LaTeX\">$(E_{k})_{1\\leq k \\leq M}$ </tex-math></inline-formula> that are also equipped with multiple antennas. Among the remaining nodes, one jammer is randomly selected to transmit an artificial noise to disrupt all the eavesdroppers that are attempting to intercept the communication of the legitimate links, i.e., <inline-formula> <tex-math notation=\"LaTeX\">$S_{i}$ </tex-math></inline-formula>-<inline-formula> <tex-math notation=\"LaTeX\">${R}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">${R}$ </tex-math></inline-formula>-<inline-formula> <tex-math notation=\"LaTeX\">${D}$ </tex-math></inline-formula>. The received signals at each node are combined using maximum-ratio combining. Secrecy analysis is provided by deriving closed-form and asymptotic expressions for the secrecy outage probability. The impact of several key parameters on the system\u2019s secrecy, e.g., transmit power of the sources, number of eavesdroppers, maximum tolerated interference power, and the number of diversity branches is investigated. Importantly, by considering two scenarios, namely (i) absence and (ii) presence of a friendly jammer, new insights are obtained for the considered communication system. Especially, we tend to answer to the following question: Can better secrecy be achieved without jamming by considering a single antenna at eavesdroppers and multiple-ones at the legitimate users (i.e., relay and end-user) rather than sending permanently an artificial noise and considering that both the relay and the destination are equipped with a single antenna, while multiple antennas are used by the eavesdroppers? The obtained results are corroborated through Monte Carlo simulation and show that the system\u2019s security can be enhanced by adjusting the aforementioned parameters.", "venue": "IEEE Transactions on Cognitive Communications and Networking", "authors": ["Mounia  Bouabdellah", "Faissal El Bouanani", "Mohamed-Slim  Alouini"], "year": 2020, "n_citations": 7}
{"id": 1115203, "s2_id": "e5adfeeb911746b09dbfd6a4723e42e56b9aeb4a", "title": "Performance Impact of Memory Channels on Sparse and Irregular Algorithms", "abstract": "Graph processing is typically considered to be a memory-bound rather than compute-bound problem. One common line of thought is that more available memory bandwidth corresponds to better graph processing performance. However, in this work we show that this is not necessarily the case. We demonstrate that the key factor in the utilization of the memory system for graph algorithms is not the raw bandwidth, or even latency of memory requests, but instead is the number of memory channels available to handle small data transfers with low locality. Using several widely used graph frameworks, including Gunrock (on the GPU) and GAPBS & Ligra (for CPUs), we characterize two very distinct memory hierarchies with respect to key graph analytics kernels. Our results show that the differences in peak bandwidths of several of the latest Pascal-generation GPU memory subsystems aren't reflected in the performance of various analytics. Furthermore, our experiments on CPU and Xeon Phi systems show that the number of memory channels utilized can be a decisive factor in performance across several different applications. For CPU systems with smaller thread counts, the memory channels can be underutilized while systems with high thread counts can oversaturate the memory subsystem, which leads to limited performance. Lastly, we model the performance of including more channels with narrower access widths than those found in existing memory subsystems, and we analyze the trade-offs in terms of the two most prominent types of memory accesses found in graph algorithms, streaming and random accesses.", "venue": "2019 IEEE/ACM 9th Workshop on Irregular Applications: Architectures and Algorithms (IA3)", "authors": ["Oded  Green", "James  Fox", "Jeffrey  Young", "Jun  Shirako", "David  Bader"], "year": 2019, "n_citations": 1}
{"id": 1115424, "s2_id": "0a1ce4855a8631b04765c86e1c4fc37750365415", "title": "ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data", "abstract": "Causal inference from observational data is a subject of active research and development in statistics and computer science. Many toolkits have been developed for this purpose that depends on statistical software. However, these toolkits do not scale to large datasets. In this paper we describe a suite of techniques for expressing causal inference tasks from observational data in SQL. This suite supports the state-of-the-art methods for causal inference and run at scale within a database engine. In addition, we introduce several optimization techniques that significantly speedup causal inference, both in the online and offline setting. We evaluate the quality and performance of our techniques by experiments of real datasets.", "venue": "ArXiv", "authors": ["Babak  Salimi", "Dan  Suciu"], "year": 2016, "n_citations": 0}
{"id": 1118418, "s2_id": "3bde8fea845dc34e6b2aeb433f7e0dc993c67916", "title": "Performance portability study of linear algebra kernels in OpenCL", "abstract": "The performance portability of OpenCL kernel implementations for common memory bandwidth limited linear algebra operations across different hardware generations of the same vendor as well as across vendors is studied. Certain combinations of kernel implementations and work sizes are found to exhibit good performance across compute kernels, hardware generations, and, to a lesser degree, vendors. As a consequence, it is demonstrated that the optimization of a single kernel is often sufficient to obtain good performance for a large class of more complicated operations.", "venue": "IWOCL '14", "authors": ["Karl  Rupp", "Philippe  Tillet", "Florian  Rudolf", "Josef  Weinbub", "Tibor  Grasser", "Ansgar  J\u00fcngel"], "year": 2014, "n_citations": 8}
{"id": 1123663, "s2_id": "9850aaef8b81e3bf0799ce033ccc3ecc31b9cae1", "title": "Towards a Geometry Automated Provers Competition", "abstract": "The geometry automated theorem proving area distinguishes itself by a large number of specific methods and implementations, different approaches (synthetic, algebraic, semi-synthetic) and different goals and applications (from research in the area of artificial intelligence to applications in education). \nApart from the usual measures of efficiency (e.g. CPU time), the possibility of visual and/or readable proofs is also an expected output against which the geometry automated theorem provers (GATP) should be measured. \nThe implementation of a competition between GATP would allow to create a test bench for GATP developers to improve the existing ones and to propose new ones. It would also allow to establish a ranking for GATP that could be used by \"clients\" (e.g. developers of educational e-learning systems) to choose the best implementation for a given intended use.", "venue": "ThEdu@CADE", "authors": ["Nuno  Baeta", "Pedro  Quaresma", "Zolt'an  Kov'acs"], "year": 2019, "n_citations": 2}
{"id": 1123976, "s2_id": "810b5113fbce4912ce69d77c907719e0f4ff9ee6", "title": "A Newcomer In The PGAS World - UPC++ vs UPC: A Comparative Study", "abstract": "A newcomer in the Partitioned Global Address Space (PGAS) \u2019world\u2019 has arrived in its version 1.0: Unified Parallel C++ (UPC++). UPC++ targets distributed data structures where communication is irregular or fine-grained. The key abstractions are global pointers, asynchronous programming via RPC, futures and promises. UPC++ API for moving non-contiguous data and handling memories with different optimal access methods resemble those used in modern C++. In this study we provide two kernels implemented in UPC++: a sparse-matrix vector multiplication (SpMV) as part of a PartialDifferential Equation solver, and an implementation of the Heat Equation on a 2D-domain. Code listings of these two kernels are available in the article in order to show the differences in programming style between UPC and UPC++. We provide a performance comparison between UPC and UPC++ using single-node, multi-node hardware and many-core hardware (Intel Xeon Phi Knight\u2019s Landing).", "venue": "ArXiv", "authors": ["J'er'emie  Lagraviere", "Johannes  Langguth", "Martina  Prugger", "Phuong H. Ha", "Xing  Cai"], "year": 2021, "n_citations": 0}
{"id": 1125143, "s2_id": "2e54d6b402311e4a96e1ba58cb3983839893ad4e", "title": "Rethinking the Intercept Probability of Random Linear Network Coding", "abstract": "This letter considers a network comprising a transmitter, which employs random linear network coding to encode a message, a legitimate receiver, which can recover the message if it gathers a sufficient number of linearly independent coded packets, and an eavesdropper. Closed-form expressions for the probability of the eavesdropper intercepting enough coded packets to recover the message are derived. Transmission with and without feedback is studied. Furthermore, an optimization model that minimizes the intercept probability under delay and reliability constraints is presented. Results validate the proposed analysis and quantify the secrecy gain offered by a feedback link from the legitimate receiver.", "venue": "IEEE Communications Letters", "authors": ["Amjad Saeed Khan", "Andrea  Tassi", "Ioannis  Chatzigeorgiou"], "year": 2015, "n_citations": 17}
{"id": 1126828, "s2_id": "1601193fae5d6b30601992096482515ce1c90cab", "title": "SoK: Diving into DAG-based Blockchain Systems", "abstract": "Blockchain plays an important role in cryptocurrency markets and technology services. However, limitations on high latency and low scalability retard their adoptions and applications in classic designs. Reconstructed blockchain systems have been proposed to avoid the consumption of competitive transactions caused by linear sequenced blocks. These systems, instead, structure transactions/blocks in the form of Directed Acyclic Graph (DAG) and consequently re-build upper layer components including consensus, incentives, \\textit{etc.} The promise of DAG-based blockchain systems is to enable fast confirmation (complete transactions within million seconds) and high scalability (attach transactions in parallel) without significantly compromising security. However, this field still lacks systematic work that summarises the DAG technique. To bridge the gap, this Systematization of Knowledge (SoK) provides a comprehensive analysis of DAG-based blockchain systems. Through deconstructing open-sourced systems and reviewing academic researches, we conclude the main components and featured properties of systems, and provide the approach to establish a DAG. With this in hand, we analyze the security and performance of several leading systems, followed by discussions and comparisons with concurrent (scaling blockchain) techniques. We further identify open challenges to highlight the potentiality of DAG-based solutions and indicate their promising directions for future research.", "venue": "ArXiv", "authors": ["Qin  Wang", "Jiangshan  Yu", "Shiping  Chen", "Yang  Xiang"], "year": 2020, "n_citations": 16}
{"id": 1127623, "s2_id": "60ddba70eb343da6eff0c7c3fc91812357c06587", "title": "Performance Impact of Lock-Free Algorithms on Multicore Communication APIs", "abstract": "Data race conditions in multi-tasking software applications are prevented by serializing access to shared memory resources, ensuring data consistency and deterministic behavior. Traditionally tasks acquire and release locks to synchronize operations on shared memory. Unfortunately, lock management can add significant processing overhead especially for multicore deployments where tasks on different cores convoy in queues waiting to acquire a lock. Implementing more than one lock introduces the risk of deadlock and using spinlocks constrains which cores a task can run on. The better alternative is to eliminate locks and validate that real-time properties are met, which is not directly considered in many embedded applications. Removing the locks is non-trivial and packaging lock-free algorithms for developers reduces the possibility of concurrency defects. This paper details how a multicore communication API implementation is enhanced to support lock-free messaging and the impact this has on data exchange latency between tasks. Throughput and latency are compared on Windows and Linux between lock-based and lock-free implementations for data exchange of messages, packets, and scalars. A model of the lock-free exchange predicts performance at the system architecture level and provides a stop criterion for the refactoring. The results show that migration from single to multicore hardware architectures degrades lock-based performance, and increases lock-free performance.", "venue": "ArXiv", "authors": ["K. Eric Harper", "Thijmen de Gooijer"], "year": 2014, "n_citations": 2}
{"id": 1129279, "s2_id": "cfd798b3048603cd1afca26bd88db6b997d46b6d", "title": "Deadline-Aware Scheduling for Maximizing Information Freshness in Industrial Cyber-Physical System", "abstract": "\u201cAge of Information\u201d is an interesting metric that captures the freshness of information in the underlying applications. It is a combination of both packet inter-arrival time and packet transmission delay. In recent times, advanced real-time systems rely on this metric for delivering status updates as timely as possible. This article aims to accomplish optimal transmission scheduling policy to maintain the information freshness of real-time updates in the industrial cyber-physical systems. Here, the coexistence of both cyber and physical units and their individual requirements to provide the quality of service is one of the most critical challenges to handle. A greedy scheduling policy called Deadline-aware Highest Latency First has been proposed for this purpose. This article also gives the analytical proof of its optimality, and finally, the claim is validated by comparing the performance of our algorithm with other scheduling policies by extensive simulations.", "venue": "IEEE Sensors Journal", "authors": ["Devarpita  Sinha", "Rajarshi  Roy"], "year": 2021, "n_citations": 3}
{"id": 1129546, "s2_id": "dc08171d150ea7c99af80d310adda54adf3974f2", "title": "Age of Information for Small Buffer Systems", "abstract": "Consider a message processing system whose objective is to produce the most current information as measured by the quantity known as \u201cage of information\u201d. We have argued in previous papers that if we are allowed to design the message processing policy ad libitum, we should keep a small buffer and operate according to a LIFO policy. In this small note we provide an analysis for the AoI of the Pm system which uses a buffer of size m, a single server, operating without service preemption and in a LIFO manner for stored messages. Analytical expressions for the mean (or even distribution) of the AoI in steady-state are possible but with the aid computer algebra. We explain the the analysis for m = 3. 1 The background Here is the definition of the Age-of-Information (AoI) stochastic process \u03b1(t), t \u2208 R, for a fairly general message processing system. Messages arrive at the times of a point process. A message may be rejected upon arrival or at any point during its sojourn in the system. If the message eventually departs it is called succesful. Fix time t and consider the time D(t) of the last successful departure before t. Tag this message and let A(t) be the time it arrived in the system. Hence A(t) \u2264 D(t) \u2264 t, and D(t) \u2212 A(t) is its total sojourn time in the system. The quantity \u03b1(t) = t\u2212A(t) is called AoI at time t. As mentioned, the definition is quite general and doesn\u2019t care about the particular system. For further information on the topic see [2, 6, 3, 4, 5] and references therein. One would like to have certain deterministic functionals of the process \u03b1 as performance measures of the system. Typical examples are the expectation of the (possibly unique) stationary version of \u03b1 at a fixed (and hence any) time, or the tail of its distribution. In [3, 4, 5] we analyzed a variety of systems always aiming at considering ones that have \u201cleast\u201d AoI. We complement these studies, in particular that of [4], by examining the system of the following paragraph. Consider a buffer of size m where m is a positive integer. Position (cell) 1 of the buffer is occupied by the message currently in service, if any. Messages arrive at the times of a Poisson process with rate \u03bb. The service times of the messages are i.i.d. positive random variables and independent of the arrival process. A message receiving service is never interrupted by newly arrived ones. Messages stored in cells 2, 3, . . . ,m are in decreasing order of their arrival times. We use the letter Pm for this system. For example, supposing that m = 3, and that messages labeled, say, 1, 2, 3, . . ., arrive at times T1 < T2 < T3 < \u00b7 \u00b7 \u00b7 , and if message 1 has a very long service time then here is what the contents of the buffer look like, while message 1 is in the system: 1\u2205\u2205, 12\u2205, 132, 143, 154, . . ., at times T1 + \u03b5, T2 + \u03b5, T3 + \u03b5, T4 + \u03b5, T5 + \u03b5, . . ., respectively, where 0 < \u03b5 1. When message 1 departs all messages move one cell forward. *gik2@psu.edu; Pennsylvania State Univ., USA \u2020takiskonst@gmail.com; Univ. of Liverpool, UK; research supported by Cast. Co. IIS-75 \u2021zazanis@aueb.gr; Athens Univ. of Economics and Business, Greece 1 ar X iv :2 10 6. 08 47 3v 1 [ cs .P F] 1 5 Ju n 20 21", "venue": "ArXiv", "authors": ["George  Kesidis", "Takis  Konstantopoulos", "Michael  Zazanis"], "year": 2021, "n_citations": 1}
{"id": 1142580, "s2_id": "112db80c4921cccbefa6e319bb92e471f83519c6", "title": "Using Genetic Algorithms to Benchmark the Cloud", "abstract": "This paper presents a novel application of Genetic Algorithms(GAs) to quantify the performance of Platform as a Service (PaaS), a cloud service model that plays a critical role in both industry and academia. While Cloud benchmarks are not new, in this novel concept, the authors use a GA to take advantage of the elasticity in Cloud services in a graceful manner that was not previously possible. Using Google App Engine, Heroku, and Python Anywhere with three distinct classes of client computers running our GA codebase, we quantified the completion time for application of the GA to search for the parameters of controllers for dynamical systems. Our results show statistically significant differences in PaaS performance by vendor, and also that the performance of the PaaS performance is dependent upon the client that uses it. Results also show the effectiveness of our GA in determining the level of service of PaaS providers, and for determining if the level of service of one PaaS vendor is repeatable with another. Such a concept could then increase the appeal of PaaS Cloud services by making them more financially appealing.", "venue": "ArXiv", "authors": ["Jeff  Kinnison", "Sekou  Remy"], "year": 2015, "n_citations": 0}
{"id": 1144242, "s2_id": "e46e560adb4d190f84b493d33f5a7fd6e52e7850", "title": "Free-Space Optical Communication With Reconfigurable Intelligent Surfaces", "abstract": "Despite the promising gains, free-space optical (FSO) communication is severely influenced by atmospheric turbulence and pointing error issues, which make its practical design a bit challenging. In this paper, with the aim to increase the communication coverage and improve the system performance, reconfigurable intelligent surfaces (RISs) are considered in an FSO communication setup, in which both atmospheric turbulence and pointing errors are considered. Closed-form expressions for the outage probability, average bit error rate, and channel capacity are derived assuming large number of reflecting elements at the RIS. Specifically, according to central limit theorem (CLT), while assuming multiple reflecting elements approximate expressions are proposed. It is shown that the respective accuracies increase as the number of elements at the RIS increases. Illustrative numerical examples are shown along with insightful discussions. Finally, Monte Carlo simulations are presented to verify the correctness of the analytical results.", "venue": "ArXiv", "authors": ["Liang  Yang", "Wang  Guo", "Daniel Benevides da Costa", "Mohamed-Slim  Alouini"], "year": 2020, "n_citations": 7}
{"id": 1144733, "s2_id": "0aff3fa4236cde9bfe5cad72c39e874aa3368c0e", "title": "Performance / Price Sort", "abstract": "NTsort is an external sort on WindowsNT 5.0. It has minimal functionality but excellent price performance. In particular, running on mail-order hardware it can sort 1.5 GB for a penny. For commercially available sorts, Postman Sort from Robert Ramey Software Development has elapsed time performance comparable to NTsort, while using less processor time. It can sort 1.27 GB for a penny (12.7 million records.) These sorts set new price-performance records. This paper documents this and proposes that the PennySort benchmark be revised to Performance/Price sort: a simple GB/$ sort metric based on a two-pass external sort.", "venue": "ArXiv", "authors": ["Jim  Gray", "Joshua  Coates", "Chris  Nyberg"], "year": 1998, "n_citations": 3}
{"id": 1146557, "s2_id": "b4befeee33704e44c535721f471b4f0f32552aa0", "title": "Latency Analysis of ROS2 Multi-Node Systems", "abstract": "The Robot Operating System 2 (ROS2) targets distributed real-time systems and is widely used in the robotics community. Especially in these systems, latency in data processing and communication can lead to instabilities. Though being highly configurable with respect to latency, ROS2 is often used with its default settings. In this paper, we investigate the end-to-end latency of ROS2 for distributed systems with default settings and different Data Distribution Service (DDS) middlewares. In addition, we profile the ROS2 stack and point out latency bottlenecks. Our findings indicate that end-to-end latency strongly depends on the used DDS middleware. Moreover, we show that ROS2 can lead to 50 % latency overhead compared to using low-level DDS communications. Our results imply guidelines for designing distributed ROS2 architectures and indicate possibilities for reducing the ROS2 overhead.", "venue": "2021 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)", "authors": ["Tobias  Kronauer", "Joshwa  Pohlmann", "Maximilian  Matthe", "Till  Smejkal", "Gerhard  Fettweis"], "year": 2021, "n_citations": 1}
{"id": 1148193, "s2_id": "3829fe34d03cd290526f516834028edb230991ab", "title": "Parallel Discrete Convolutions on Adaptive Particle Representations of Images", "abstract": "We present data structures and algorithms for native implementations of discrete convolution operators over Adaptive Particle Representations (APR) of images on parallel computer architectures. The APR is a contentadaptive image representation that locally adapts the sampling resolution to the image signal. It has been developed as an alternative to pixel representations for large, sparse images as they typically occur in fluorescence microscopy. It has been shown to reduce the memory and runtime costs of storing, visualizing, and processing such images. This, however, requires that image processing natively operates on APRs, without intermediately reverting to pixels. Designing efficient and scalable APR-native image processing primitives, however, is complicated by the APR\u2019s irregular memory structure. Here, we provide the algorithmic building blocks required to efficiently and natively process APR images using a wide range of algorithms that can be formulated in terms of discrete convolutions. We show that APR convolution naturally leads to scale-adaptive algorithms that efficiently parallelize on multi-core CPU and GPU architectures. We quantify the speedups in comparison to pixel-based algorithms and convolutions on evenly sampled data. We achieve pixel-equivalent throughputs of up to 1 TB/s on a single Nvidia GeForce RTX 2080 gaming GPU, requiring up to two orders of magnitude less memory than a pixel-based implementation.", "venue": "ArXiv", "authors": ["Joel  Jonsson", "Bevan L. Cheeseman", "Suryanarayana  Maddu", "Krzysztof  Gonciarz", "Ivo F. Sbalzarini"], "year": 2021, "n_citations": 0}
{"id": 1150858, "s2_id": "2f2ce11dc70fd4fc0bb394edbcf71a8dc057f47e", "title": "DuctTeip: An efficient programming model for distributed task based parallel computing", "abstract": "Current high-performance computer systems used for scientific computing typically combine shared memory computational nodes in a distributed memory environment. Extracting high performance from these complex systems requires tailored approaches. Task based parallel programming has been successful both in simplifying the programming and in exploiting the available hardware parallelism for shared memory systems. In this paper we focus on how to extend task parallel programming to distributed memory systems. We use a hierarchical decomposition of tasks and data in order to accommodate the different levels of hardware. We test the proposed programming model on two different applications, a Cholesky factorization, and a solver for the Shallow Water Equations. We also compare the performance of our implementation with that of other frameworks for distributed task parallel programming, and show that it is competitive.", "venue": "Parallel Comput.", "authors": ["Afshin  Zafari", "Elisabeth  Larsson", "Martin  Tillenius"], "year": 2019, "n_citations": 10}
{"id": 1160937, "s2_id": "ee82245af5a9b242daa8b03642ff5175b4a1c683", "title": "GraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra", "abstract": "We propose GraphMineSuite (GMS): the rst benchmarking suite for graph mining that facilitates evaluating and constructing highperformance graph mining algorithms. First, GMS comes with a benchmark specication based on extensive literature review, prescribing representative problems, algorithms, and datasets. Second, GMS oers a carefully designed software platform for seamless testing of dierent ne-grained elements of graph mining algorithms, such as graph representations or algorithm subroutines. The platform includes parallel implementations of more than 40 considered baselines, and it facilitates developing complex and fast mining algorithms. High modularity is possible by harnessing set algebra operations such as set intersection and dierence, which enables breaking complex graph mining algorithms into simple building blocks that can be separately experimented with. GMS is supported with a broad concurrency analysis for portability in performance insights, and a novel performance metric to assess the throughput of graph mining algorithms, enabling more insightful evaluation. As use cases, we harness GMS to rapidly redesign and accelerate state-of-the-art baselines of core graph mining problems: degeneracy reordering (by >2\u21e5), maximal clique listing (by >9\u21e5), k-clique listing (by up to 1.1\u21e5), and subgraph isomorphism (by 2.5\u21e5), also obtaining better theoretical performance bounds. PVLDB Reference Format: M. Besta et al.. GraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra. PVLDB, 14(11): 1922 1936, 2021. doi:10.14778/3476249.3476252 PVLDB Artifact Availability: The source code, data, and/or other artifacts have been made available at https://graphminesuite.spcl.inf.ethz.ch/. This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 14, No. 11 ISSN 2150-8097. doi:10.14778/3476249.3476252", "venue": "Proc. VLDB Endow.", "authors": ["Maciej  Besta", "Zur  Vonarburg-Shmaria", "Yannick  Schaffner", "Leonardo  Schwarz", "Grzegorz  Kwasniewski", "Lukas  Gianinazzi", "Jakub  Ber\u00e1nek", "Kacper  Janda", "Tobias  Holenstein", "Sebastian  Leisinger", "Peter  Tatkowski", "Esref  \u00d6zdemir", "Adrian  Balla", "Marcin  Copik", "Philipp  Lindenberger", "Pavel  Kalvoda", "Marek  Konieczny", "Onur  Mutlu", "Torsten  Hoefler"], "year": 2021, "n_citations": 6}
{"id": 1161569, "s2_id": "b0b6a5df5e70d09e9c20e5a1df600520c306ab7c", "title": "Enabling and optimizing pilot jobs using xen based virtual machines for the HPC grid applications", "abstract": "The primary motivation for uptake of virtualization have been resource isolation, capacity management and resource customization: isolation and capacity management allow providers to isolate users from the site and control their resources usage while customization allows end-users to easily project the required environment onto a variety of sites. Various approaches have been taken to integrate virtualization with Grid technologies. In this paper, we propose an approach that combines virtualization on the existing software infrastructure such as Pilot Jobs with minimum change on the part of resource providers. We also present a standard API to enable a wider set of applications including Batch systems to deploy virtual machines on-demand as isolated job sandboxes. To illustrate the usefulness of this approach, we also evaluate the impact of Xen virtualization on memory and compute intensive tasks, and present our results that how memory and scheduling parameters could be tweaked to optimize job performance.", "venue": "VTDC '09", "authors": ["Omer  Khalid", "Richard J. Anthony", "Paul  Nilsson", "Katarzyna  Keahey", "Markus  Schulz", "Miltos  Petridis", "Kevin  Parrott"], "year": 2009, "n_citations": 11}
{"id": 1163954, "s2_id": "a475dfd2eea1dca60347fc70b04522355a764a7a", "title": "On the flow-level stability of data networks without congestion control: the case of linear networks and upstream trees", "abstract": "In this paper, flow models of networks without congestion control are considered. Users generate data transfers according to some Poisson processes and transmit corresponding packet at a fixed rate equal to their access rate until the entire document is received at the destination; some erasure codes are used to make the transmission robust to packet losses. We study the stability of the stochastic process representing the number of active flows in two particular cases: linear networks and upstream trees. For the case of linear networks, we notably use fluid limits and an interesting phenomenon of \u201ctime scale separation\u201d occurs. Bounds on the stability region of linear networks are given. For the case of upstream trees, underlying monotonic properties are used. Finally, the asymptotic stability of those processes is analyzed when the access rate of the users decreases to\u00a00. An appropriate scaling is introduced and used to prove that the stability region of those networks is asymptotically maximized.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Mathieu  Feuillet"], "year": 2012, "n_citations": 6}
{"id": 1164673, "s2_id": "cb2906571ee2aeb659acf93003568eabeb6af149", "title": "Online Scheduling of Spark Workloads with Mesos using Different Fair Allocation Algorithms", "abstract": "In the following, we present example illustrative and experimental results comparing fair schedulers allocating resources from multiple servers to distributed application frameworks. Resources are allocated so that at least one resource is exhausted in every server. Schedulers considered include DRF (DRFH) and Best-Fit DRF (BF-DRF), TSF, and PS-DSF. We also consider server selection under Randomized Round Robin (RRR) and based on their residual (unreserved) resources. In the following, we consider cases with frameworks of equal priority and without server-preference constraints. We first give typical results of a illustrative numerical study and then give typical results of a study involving Spark workloads on Mesos which we have modified and open-sourced to prototype different schedulers.", "venue": "ArXiv", "authors": ["Yuquan  Shan", "Aman  Jain", "George  Kesidis", "Bhuvan  Urgaonkar", "Jalal  Khamse-Ashari", "Ioannis  Lambadaris"], "year": 2018, "n_citations": 1}
{"id": 1165935, "s2_id": "bafef31d0cfc079712f201928fed16155e2b1e4a", "title": "Analysis of Network Traffic in Switched Ethernet Systems", "abstract": "A 100 Mbps Ethernet link between a college campus and the outside world was monitored with a dedicated PC and the measured data analysed for its statistical properties. Similar measurements were taken at an internal node of the network. The networks in both cases are a full-duplex switched Ethernet. Inter-event interval histograms and power spectra of the throughput aggregated for 10ms bins were used to analyse the measured traffic. For most investigated cases both methods reveal that the traffic behaves according to a power law. The results will be used in later studies to parameterise models for network traffic.", "venue": "ArXiv", "authors": ["Tony  Field", "Uli  Harder", "Peter G. Harrison"], "year": 2001, "n_citations": 1}
{"id": 1168420, "s2_id": "b8cca4db8421f05beafbdc35f38a197174923ca6", "title": "Distributed Link Scheduling With Constant Overhead", "abstract": "This paper proposes a new class of simple, distributed algorithms for scheduling in multihop wireless networks under the primary interference model. The class is parameterized by integers k ges 1. We show that algorithm k of our class achieves k/(k + 2) of the capacity region, for every k ges 1. The algorithms have small and constant worst-case overheads. In particular, algorithm k generates a new schedule using a) time less than 4k + 2 round-trip times between neighboring nodes in the network and b) at most three control transmissions by any given node for any k. The control signals are explicitly specified and face the same interference effects as normal data transmissions. Our class of distributed wireless scheduling algorithms are the first ones guaranteed to achieve any fixed fraction of the capacity region while using small and constant overheads that do not scale with network size. The parameter k explicitly captures the tradeoff between control overhead and throughput performance and provides a tuning-knob protocol designers can use to harness this tradeoff in practice.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Sujay  Sanghavi", "Loc  Bui", "R.  Srikant"], "year": 2009, "n_citations": 229}
{"id": 1169253, "s2_id": "0ea34c5c0f2fdb2dedadd07c349b98c4cfecac0a", "title": "Cost-Performance Tradeoffs in Fusing Unreliable Computational Units", "abstract": "We investigate fusing several unreliable computational units that perform the same task. We model an unreliable computational outcome as an additive perturbation to its error-free result in terms of its fidelity and cost. We analyze performance of repetition-based strategies that distribute cost across several unreliable units and fuse their outcomes. When the cost is a convex function of fidelity, the optimal repetition-based strategy in terms of incurred cost while achieving a target mean-square error (MSE) performance may fuse several computational units. For concave and linear costs, a single more reliable unit incurs lower cost compared to fusion of several lower cost and less reliable units while achieving the same MSE performance. We show how our results give insight into problems from theoretical neuroscience, circuits, and crowdsourcing.", "venue": "ArXiv", "authors": ["Mehmet A. Donmez", "Maxim  Raginsky", "Andrew C. Singer", "Lav R. Varshney"], "year": 2017, "n_citations": 0}
{"id": 1174100, "s2_id": "59cb2048e92a01d7650615cadefbc16eea1ac9dc", "title": "Big enterprise registration data imputation: Supporting spatiotemporal analysis of industries in China", "abstract": "Big, fine-grained enterprise registration data that includes time and location information enables us to quantitatively analyze, visualize, and understand the patterns of industries at multiple scales across time and space. However, data quality issues like incompleteness and ambiguity, hinder such analysis and application. These issues become more challenging when the volume of data is immense and constantly growing. High Performance Computing (HPC) frameworks can tackle big data computational issues, but few studies have systematically investigated imputation methods for enterprise registration data in this type of computing environment. In this paper, we propose a big data imputation workflow based on Apache Spark as well as a bare-metal computing cluster, to impute enterprise registration data. We integrated external data sources, employed Natural Language Processing (NLP), and compared several machine-learning methods to address incompleteness and ambiguity problems found in enterprise registration data. Experimental results illustrate the feasibility, efficiency, and scalability of the proposed HPC-based imputation framework, which also provides a reference for other big georeferenced text data processing. Using these imputation results, we visualize and briefly discuss the spatiotemporal distribution of industries in China, demonstrating the potential applications of such data when quality issues are resolved.", "venue": "Comput. Environ. Urban Syst.", "authors": ["Fa  Li", "Zhipeng  Gui", "Huayi  Wu", "Jianya  Gong", "Yuan  Wang", "Siyu  Tian", "Jiawen  Zhang"], "year": 2018, "n_citations": 13}
{"id": 1177974, "s2_id": "1f1ad870c9c80578d7d48e83959e4c74bc2d5bf3", "title": "On Resource Pooling and Separation for LRU Caching", "abstract": "Caching systems using the Least Recently Used (LRU) principle have now become ubiquitous. A fundamental question for these systems is whether the cache space should be pooled together or divided to serve multiple flows of data item requests in order to minimize the miss probabilities. In this paper, we show that there is no straight yes or no answer to this question, and depends on complex combinations of critical factors, including, e.g., request rates, overlapped data items across different request flows, data item popularities and their sizes. To this end, we characterize the performance of multiple flows of data item requests under resource pooling and separation when the cache size is large. Analytically we show that it is asymptotically optimal to jointly serve multiple flows if their data item sizes and popularity distributions are similar, and their arrival rates do not differ significantly; the self-organizing property of LRU caching automatically optimizes the resource allocation among them asymptotically. Otherwise, separating these flows could be better, e.g., when data sizes vary significantly. We also quantify critical points beyond which resource pooling is better than separation for each of the flows when the overlapped data items exceed certain levels. These results provide new insights on the performance of caching systems.", "venue": "SIGMETRICS", "authors": ["Jian  Tan", "Guocong  Quan", "Kaiyi  Ji", "Ness B. Shroff"], "year": 2018, "n_citations": 0}
{"id": 1180559, "s2_id": "1f89227160b8c905d89a0f894d14fe80493185ae", "title": "LASSI: Metric Based I/O Analytics for HPC", "abstract": "LASSi is a tool aimed at analyzing application usage and contention caused by use of shared resources (filesystem or network) in a HPC system. LASSi was initially developed to support the ARCHER system where there are large variations in application requirements and occasional user complaints regarding filesystem performance manifested by variation in job runtimes or poor interactive response. LASSi takes an approach of defining derivativerisk andops metrics that relate to unusually high application I/O behaviour. The metrics are shown to correlate to applications that can experience variable performance or that may impact the performance of other applications. LASSi uses I/O statistics over time to provide application I/O profiles and has been automated to generate daily reports for ARCHER. We demonstrate how LASSi provides holistic I/O analysis by monitoring filesystem I/O, generating coarse profiles of filesystems and application runs and automating analysis of application slowdown using metrics.", "venue": "2019 Spring Simulation Conference (SpringSim)", "authors": ["Karthee  Sivalingam", "Harvey  Richardson", "Adrian  Tate", "Martin  Lafferty"], "year": 2019, "n_citations": 8}
{"id": 1181250, "s2_id": "d818433e18c420d622b6031c09b077ea9bdc29e3", "title": "Computing System Congestion Management Using Exponential Smoothing Forecasting", "abstract": "An overloaded computer must finish what it starts and not start what will fail or hang. A congestion management algorithm, the author developed, effectively manages traffic overload with its unique formulation of Exponential Smoothing forecasting. This set of equations resolve forecasting startup issues that have limited the model's adoption as a discrete time series predictor. These expressions also satisfy implementation requirements to perform calculations using integer math and be able to reset the forecast seamlessly. A computer program, written in C language, which exercises the methodology, is downloadable from GitHub.", "venue": "ICCDA", "authors": ["James F Brady"], "year": 2020, "n_citations": 0}
{"id": 1181521, "s2_id": "49b9ff5fa15523e275a4cce6c5e5d543aa9e95f7", "title": "Sharing pandemic vaccination certificates through blockchain: Case study and performance evaluation", "abstract": "During 2021, different worldwide initiatives have been established for the development of digital vaccination certificates to alleviate the restrictions associated with the COVID-19 pandemic to vaccinated individuals. Although diverse technologies can be considered for the deployment of such certificates, the use of blockchain has been suggested as a promising approach due to its decentralization and transparency features. However, the proposed solutions often lack realistic experimental evaluation that could help to determine possible practical challenges for the deployment of a blockchain platform for this purpose. To fill this gap, this work introduces a scalable, blockchain-based platform for the secure sharing of COVID-19 or other disease vaccination certificates. As an indicative use case, we emulate a large-scale deployment by considering the countries of the European Union. The platform is evaluated through extensive experiments measuring computing resource usage, network response time, and bandwidth. Based on the results, the proposed scheme shows satisfactory performance across all major evaluation criteria, suggesting that it can set the pace for real implementations. Vis-\u00e0-vis the related work, the proposed platform is novel, especially through the prism of a large-scale, full-fledged implementation and its assessment.", "venue": "Wirel. Commun. Mob. Comput.", "authors": ["Jos\u00e9 Luis Hern\u00e1ndez Ramos", "Georgios  Karopoulos", "Dimitris  Geneiatakis", "Tania  Martin", "Georgios  Kambourakis", "Igor Nai Fovino"], "year": 2021, "n_citations": 6}
{"id": 1183072, "s2_id": "db26c34f9ea9177862d792f3975ff5c963e1284c", "title": "kNet: A Deep kNN Network To Handle Label Noise", "abstract": "Deep Neural Networks require large amounts of labeled data for their training. Collecting this data at scale inevitably causes label noise. Hence, the need to develop learning algorithms that are robust to label noise. In recent years, k Nearest Neighbors (kNN) emerged as a viable solution to this problem. Despite its success, kNN is not without its problems. Mainly, it requires a huge memory footprint to store all the training samples and it needs an advanced data structure to allow for fast retrieval of the relevant examples, given a query sample. We propose a neural network, termed kNet, that learns to perform kNN. Once trained, we no longer need to store the training data, and processing a query sample is a simple matter of inference. To use kNet, we first train a preliminary network on the data set, and then train kNet on the penultimate layer of the preliminary network. We find that kNet gives a smooth approximation of kNN, and cannot handle the sharp label changes between samples that kNN can exhibit. This indicates that currently kNet is best suited to approximate kNN with a fairly large k. Experiments on two data sets show that this is the regime in which kNN works best, and can therefore be replaced by kNet. In practice, kNet consistently improve the results of all preliminary networks, in all label noise regimes, by up to 3%.", "venue": "ArXiv", "authors": ["Itzik  Mizrahi", "Shai  Avidan"], "year": 2021, "n_citations": 0}
{"id": 1184147, "s2_id": "dd1fe201dbab2b99f1ce299d0bf5a1c3ea617ed8", "title": "Efficient Pattern Matching in Python", "abstract": "Pattern matching is a powerful tool for symbolic computations. Applications include term rewriting systems, as well as the manipulation of symbolic expressions, abstract syntax trees, and XML and JSON data. It also allows for an intuitive description of algorithms in the form of rewrite rules. We present the open source Python module MatchPy, which offers functionality and expressiveness similar to the pattern matching in Mathematica. In particular, it includes syntactic pattern matching, as well as matching for commutative and/or associative functions, sequence variables, and matching with constraints. MatchPy uses new and improved algorithms to efficiently find matches for large pattern sets by exploiting similarities between patterns. The performance of MatchPy is investigated on several real-world problems.", "venue": "PyHPC@SC", "authors": ["Manuel  Krebber", "Henrik  Barthels", "Paolo  Bientinesi"], "year": 2017, "n_citations": 7}
{"id": 1187351, "s2_id": "d475d316a874f1959b289a51cf902865e40ae133", "title": "GridMonitor: Integration of Large Scale Facility Fabric Monitoring with Meta Data Service in Grid Environment", "abstract": "Grid computing consists of the coordinated use of large sets of diverse, geographically distributed resources for high performance computation. Effective monitoring of these computing resources is extremely important to allow efficient use on the Grid. The large number of heterogeneous computing entities available in Grids makes the task challenging. In this work, we describe a Grid monitoring system, called GridMonitor, that captures and makes available the most important information from a large computing facility. The Grid monitoring system consists of four tiers: local monitoring, archiving, publishing and harnessing. This architecture was applied on a large scale linux farm and network infrastructure. It can be used by many higher-level Grid services including scheduling services and resource brokering.", "venue": "ArXiv", "authors": ["Richard  Baker", "Dantong  Yu", "Jason  Smith", "Anthony  Chan", "Kaushik  De", "Patrick  McGuigan"], "year": 2003, "n_citations": 4}
{"id": 1194856, "s2_id": "c286b83bd1d793390db0cc1b1f0a86458d4e4f99", "title": "Hierarchical Roofline Performance Analysis for Deep Learning Applications", "abstract": "This paper presents a practical methodology for collecting performance data necessary to conduct hierarchical Roofline analysis on NVIDIA GPUs. It discusses the extension of the Empirical Roofline Toolkit for broader support of a range of data precisions and Tensor Core support and introduces a Nsight Compute based method to accurately collect application performance information. This methodology allows for automated machine characterization and application characterization for Roofline analysis across the entire memory hierarchy on NVIDIA GPUs, and it is validated by a complex deep learning application used for climate image segmentation. We use two versions of the code, in TensorFlow and PyTorch respectively, to demonstrate the use and effectiveness of this methodology. We highlight how the application utilizes the compute and memory capabilities on the GPU and how the implementation and performance differ in two deep learning frameworks.", "venue": "ArXiv", "authors": ["Yunsong  Wang", "Charlene  Yang", "Steven  Farrell", "Thorsten  Kurth", "Samuel  Williams"], "year": 2020, "n_citations": 5}
{"id": 1195772, "s2_id": "9bd18fd39c48cd21781ba38690f25effceef46a3", "title": "Model Slicing for Supporting Complex Analytics with Elastic Inference Cost and Resource Constraints", "abstract": "\n Deep learning models have been used to support analytics beyond simple aggregation, where deeper and wider models have been shown to yield great results. These models consume a huge amount of memory and computational operations. However, most of the large-scale industrial applications are often computational budget constrained. In practice, the peak workload of inference service could be 10x higher than the average cases, with the presence of unpredictable extreme cases. Lots of computational resources could be wasted during off-peak hours and the system may crash when the workload exceeds system capacity. How to support deep learning services with dynamic workload cost-efficiently remains a challenging problem. In this paper, we address the challenge with a general and novel training scheme called\n model slicing\n , which enables deep learning models to provide predictions within the prescribed computational resource budget dynamically.\n Model slicing\n could be viewed as an elastic computation solution without requiring more computational resources. Succinctly, each layer in the model is divided into\n groups\n of contiguous block of basic components (i.e. neurons in dense layers and channels in convolutional layers), and then partially ordered relation is introduced to these groups by enforcing that groups participated in each forward pass always starts from the\n first\n group to the\n dynamically-determined rightmost\n group. Trained by dynamically indexing the rightmost group with a single parameter\n slice rate\n , the network is engendered to build up group-wise and residual representation. Then during inference, a sub-model with fewer groups can be readily deployed for efficiency whose computation is roughly quadratic to the width controlled by the\n slice rate.\n Extensive experiments show that models trained with\n model slicing\n can effectively support on-demand workload with elastic inference cost.\n", "venue": "Proc. VLDB Endow.", "authors": ["Shaofeng  Cai", "Gang  Chen", "Beng Chin Ooi", "Jinyang  Gao"], "year": 2019, "n_citations": 10}
{"id": 1196152, "s2_id": "5bff34afa7fac2263633fb4c5b8afd1a492120e4", "title": "Smart-PGSim: Using Neural Network to Accelerate AC-OPF Power Grid Simulation", "abstract": "The optimal power flow (OPF) problem is one of the most important optimization problems for the operation of the power grid. It calculates the optimum scheduling of the committed generation units. In this paper, we develop a neural network approach to the problem of accelerating the current optimal power flow (AC-OPF) by generating an intelligent initial solution. The high quality of the initial solution and guidance of other outputs generated by the neural network enables faster convergence to the solution without losing optimality of final solution as computed by traditional methods. Smart-PGSim generates a novel multitask-learning neural network model to accelerate the AC-OPF simulation. Smart-PGSim also imposes the physical constraints of the simulation on the neural network automatically. Smart-PGSim brings an average of 49.2% performance improvement (up to 91%), computed over 10,000 problem simulations, with respect to the original AC-OPF implementation, without losing the optimality of the final solution.", "venue": "SC", "authors": ["Wenqian  Dong", "Zhen  Xie", "Gokcen  Kestor", "Dong  Li"], "year": 2020, "n_citations": 11}
{"id": 1197966, "s2_id": "970b3ec9458609c13bf8f1ddfef1ab2fdcc2a2b9", "title": "Optimising finite-difference methods for PDEs through parameterised time-tiling in Devito", "abstract": "Finite-difference methods are widely used in solving partial differential equations. In a large problem set, approximations can take days or weeks to evaluate, yet the bulk of computation may occur within a single loop nest. The modelling process for researchers is not straightforward either, requiring models with differential equations to be translated into stencil kernels, then optimised separately. One tool that seeks to speed up and eliminate mistakes from this tedious procedure is Devito, used to efficiently employ finite-difference methods. \nIn this work, we implement time-tiling, a loop nest optimisation, in Devito yielding a decrease in runtime of up to 45%, and at least 20% across stencils from the acoustic wave equation family, widely used in Devito's target domain of seismic imaging. We present an estimator for arithmetic intensity under time-tiling and a model to predict runtime improvements in stencil computations. We also consider generalisation of time-tiling to imperfect loop nests, a less widely studied problem.", "venue": "ArXiv", "authors": ["Nicholas  Sim"], "year": 2018, "n_citations": 0}
{"id": 1201072, "s2_id": "52c8b52092c8e2c1321a8052c008189af01f418e", "title": "A Modeling Framework for Gossip-based Information Spread", "abstract": "We present an analytical framework for gossip protocols based on the pair wise information exchange between interacting nodes. This framework allows for studying the impact of protocol parameters on the performance of the protocol. Previously, gossip-based information dissemination protocols have been analyzed under the assumption of perfect, lossless communication channels. We extend our framework for the analysis of networks with lossy channels. We show how the presence of message loss, coupled with specific topology configurations, impacts the expected behavior of the protocol. We validate the obtained models against simulations for two protocols.", "venue": "2011 Eighth International Conference on Quantitative Evaluation of SysTems", "authors": ["Rena  Bakhshi", "Daniela  Gavidia", "Wan  Fokkink", "Maarten van Steen"], "year": 2011, "n_citations": 10}
{"id": 1202657, "s2_id": "145587b0fbd9d063f77a0e15aa9370729084dd01", "title": "A Case for Malleable Thread-Level Linear Algebra Libraries: The LU Factorization With Partial Pivoting", "abstract": "We propose two novel techniques for overcoming load-imbalance encountered when implementing so-called look-ahead mechanisms in relevant dense matrix factorizations for the solution of linear systems. Both techniques target the scenario where two thread teams are created/activated during the factorization, with each team in charge of performing an independent task/branch of execution. The first technique promotes worker sharing (WS) between the two tasks, allowing the threads of the task that completes first to be reallocated for use by the costlier task. The second technique allows a fast task to alert the slower task of completion, enforcing the early termination (ET) of the second task, and a smooth transition of the factorization procedure into the next iteration. The two mechanisms are instantiated via a new malleable thread-level implementation of the basic linear algebra subprograms, and their benefits are illustrated via an implementation of the LU factorization with partial pivoting enhanced with look-ahead. Concretely, our experimental results on an Intel-Xeon system with 12 cores show the benefits of combining WS+ET, reporting competitive performance in comparison with a task-parallel runtime-based solution.", "venue": "IEEE Access", "authors": ["Sandra  Catal\u00e1n", "Jos\u00e9 R. Herrero", "Enrique S. Quintana-Ort\u00ed", "Rafael  Rodr\u00edguez-S\u00e1nchez", "Robert  Van De Geijn"], "year": 2019, "n_citations": 14}
{"id": 1202749, "s2_id": "b555873cfc3d581d3b235a5beabe2fa62cbef2d8", "title": "Extending Memory Capacity in Consumer Devices with Emerging Non-Volatile Memory: An Experimental Study", "abstract": "The number and diversity of consumer devices are growing rapidly, alongside their target applications' memory consumption. Unfortunately, DRAM scalability is becoming a limiting factor to the available memory capacity in consumer devices. As a potential solution, manufacturers have introduced emerging non-volatile memories (NVMs) into the market, which can be used to increase the memory capacity of consumer devices by augmenting or replacing DRAM. Since entirely replacing DRAM with NVM in consumer devices imposes large system integration and design challenges, recent works propose extending the total main memory space available to applications by using NVM as swap space for DRAM. However, no prior work analyzes the implications of enabling a real NVM-based swap space in real consumer devices. In this work, we provide the first analysis of the impact of extending the main memory space of consumer devices using off-the-shelf NVMs. We extensively examine system performance and energy consumption when the NVM device is used as swap space for DRAM main memory to effectively extend the main memory capacity. For our analyses, we equip real web-based Chromebook computers with the Intel Optane SSD, which is a state-of-the-art low-latency NVM-based SSD device. We compare the performance and energy consumption of interactive workloads running on our Chromebook with NVM-based swap space, where the Intel Optane SSD capacity is used as swap space to extend main memory capacity, against two state-of-the-art systems: (i) a baseline system with double the amount of DRAM than the system with the NVM-based swap space; and (ii) a system where the Intel Optane SSD is naively replaced with a state-of-the-art (yet slower) off-the-shelf NAND-flash-based SSD, which we use as a swap space of equivalent size as the NVM-based swap space.", "venue": "ArXiv", "authors": ["Geraldo F. Oliveira", "Saugata  Ghose", "Juan  G\u00f3mez-Luna", "Amirali  Boroumand", "Alexis  Savery", "Sonny  Rao", "Salman  Qazi", "Gwendal  Grignou", "Rahul  Thakur", "Eric  Shiu", "Onur  Mutlu"], "year": 2021, "n_citations": 1}
{"id": 1208809, "s2_id": "e7983a5e072d51b4b219f15c84948eccb1a86b42", "title": "Sensing matrix setting schemes for cognitive networks and their performance analysis", "abstract": "Powerful spectrum decision schemes enable cognitive radios (CRs) to find transmission opportunities in spectral resources allocated exclusively to the primary users. One of the key effecting factors on the CR network throughput is the spectrum sensing sequence used by each secondary user (SU). In this study, SUs' throughput maximisation through finding an appropriate sensing matrix (SM) is investigated. First, the average throughput of the CR network is evaluated for a given SM. Then, an optimisation problem based on the maximisation of the network throughput is formulated in order to find the optimal SM. As the optimum solution is very complicated, three novel suboptimal solutions are proposed for various cases including perfect and non-perfect sensing. Despite having less computational complexities as well as lower consumed energies, the proposed solutions perform quite well compared to the optimum solution. The structure and performance of the proposed SM setting schemes are discussed in detail and a set of illustrative numerical results is presented to validate their efficiencies.", "venue": "IET Commun.", "authors": ["Hossein Shokri Ghadikolaei", "Masoumeh  Nasiri-Kenari"], "year": 2012, "n_citations": 18}
{"id": 1210976, "s2_id": "c85190ca45fe4702708740b243c5d6cb1cc6034d", "title": "Distribution of the Number of Retransmissions of Bounded Documents", "abstract": "Retransmission-based failure recovery represents a primary approach in existing communication networks that guarantees data delivery in the presence of channel failures. Recent work has shown that, when data sizes have infinite support, retransmissions can cause long (-tailed) delays even if all traffic and network characteristics are light-tailed. In this paper we investigate the practically important case of bounded data units 0 \u2264 L b \u2264 b under the condition that the hazard functions of the distributions of data sizes and channel statistics are proportional. To this end, we provide an explicit and uniform characterization of the entire body of the retransmission distribution \u2119[N b > n] in both n and b. Our main discovery is that this distribution can be represented as the product of a power law and gamma distribution. This rigorous approximation clearly demonstrates the coupling of a power law distribution, dominating the main body, and the gamma distribution, determining the exponential tail. Our results are validated via simulation experiments and can be useful for designing retransmission-based systems with the required performance characteristics. From a broader perspective, this study applies to any other system, e.g. computing, where restart mechanisms are employed after a job processing failure.", "venue": "Advances in Applied Probability", "authors": ["Predrag R. Jelenkovic", "Evangelia D. Skiani"], "year": 2015, "n_citations": 9}
{"id": 1213211, "s2_id": "72b6c4a6d593f15df5406e5ee9389197e558c4f6", "title": "Unbounded Product-Form Petri Nets", "abstract": "Computing steady-state distributions in infinite-state stochastic systems is in general a very dificult task. Product-form Petri nets are those Petri nets for which the steady-state distribution can be described as a natural product corresponding, up to a normalising constant, to an exponentiation of the markings. However, even though some classes of nets are known to have a product-form distribution, computing the normalising constant can be hard. The class of (closed) {\\Pi}3-nets has been proposed in an earlier work, for which it is shown that one can compute the steady-state distribution efficiently. However these nets are bounded. In this paper, we generalise queuing Markovian networks and closed {\\Pi}3-nets to obtain the class of open {\\Pi}3-nets, that generate infinite-state systems. We show interesting properties of these nets: (1) we prove that liveness can be decided in polynomial time, and that reachability in live {\\Pi}3-nets can be decided in polynomial time; (2) we show that we can decide ergodicity of such nets in polynomial time as well; (3) we provide a pseudo-polynomial time algorithm to compute the normalising constant.", "venue": "CONCUR", "authors": ["Patricia  Bouyer", "Serge  Haddad", "Vincent  Jug\u00e9"], "year": 2017, "n_citations": 0}
{"id": 1214793, "s2_id": "fd3c541bd348c9577c6f37effe975ead7bdacb1e", "title": "Data access optimizations for highly threaded multi-core CPUs with multiple memory controllers", "abstract": "Processor and system architectures that feature multiple memory controllers are prone to show bottlenecks and erratic performance numbers on codes with regular access patterns. Although such effects are well known in the form of cache thrashing and aliasing conflicts, they become more severe when memory access is involved. Using the new Sun UltraSPARC T2 processor as a prototypical multi-core design, we analyze performance patterns in low-level and application benchmarks and show ways to circumvent bottlenecks by careful data layout and padding.", "venue": "2008 IEEE International Symposium on Parallel and Distributed Processing", "authors": ["Georg  Hager", "Thomas  Zeiser", "Gerhard  Wellein"], "year": 2008, "n_citations": 29}
{"id": 1214994, "s2_id": "21360a0082a02e55685f9f5c56290171eaf113cc", "title": "Energy Predictive Models for Convolutional Neural Networks on Mobile Platforms", "abstract": "Energy use is a key concern when deploying deep learning models on mobile and embedded platforms. Current studies develop energy predictive models based on application-level features to provide researchers a way to estimate the energy consumption of their deep learning models. This information is useful for building resource-aware models that can make efficient use of the hard-ware resources. However, previous works on predictive modelling provide little insight into the trade-offs involved in the choice of features on the final predictive model accuracy and model complexity. To address this issue, we provide a comprehensive analysis of building regression-based predictive models for deep learning on mobile devices, based on empirical measurements gathered from the SyNERGY framework.Our predictive modelling strategy is based on two types of predictive models used in the literature:individual layers and layer-type. Our analysis of predictive models show that simple layer-type features achieve a model complexity of 4 to 32 times less for convolutional layer predictions for a similar accuracy compared to predictive models using more complex features adopted by previous approaches. To obtain an overall energy estimate of the inference phase, we build layer-type predictive models for the fully-connected and pooling layers using 12 representative Convolutional NeuralNetworks (ConvNets) on the Jetson TX1 and the Snapdragon 820using software backends such as OpenBLAS, Eigen and CuDNN. We obtain an accuracy between 76% to 85% and a model complexity of 1 for the overall energy prediction of the test ConvNets across different hardware-software combinations.", "venue": "ArXiv", "authors": ["Crefeda Faviola Rodrigues", "Graham  Riley", "Mikel  Lujan"], "year": 2020, "n_citations": 0}
{"id": 1216271, "s2_id": "85ce98e3b8abe49afc3823ad69121d54b8997f12", "title": "Accelerator-level parallelism", "abstract": "Charging computer scientists to develop the science needed to best achieve the performance and cost goals of accelerator-level parallelism hardware and software.", "venue": "Communications of the ACM", "authors": ["Mark D. Hill", "Vijay Janapa Reddi"], "year": 2021, "n_citations": 11}
{"id": 1218592, "s2_id": "5a2c040b8c912b860598c422082a7b79eeaac57f", "title": "Scouting the Path to a Million-Client Server", "abstract": "To keep up with demand, servers will scale up to handle hundreds of thousands of clients simultaneously. Much of the focus of the community has been on scaling servers in terms of aggregate traffic intensity (packets transmitted per second). However, bottlenecks caused by the increasing number of concurrent clients, resulting in a large number of concurrent flows, have received little attention. In this work, we focus on identifying such bottlenecks. In particular, we define two broad categories of problems; namely, admitting more packets into the network stack than can be handled efficiently, and increasing per-packet overhead within the stack. We show that these problems contribute to high CPU usage and network performance degradation in terms of aggregate throughput and RTT. Our measurement and analysis are performed in the context of the Linux networking stack, the the most widely used publicly available networking stack. Further, we discuss the relevance of our findings to other network stacks. The goal of our work is to highlight considerations required in the design of future networking stacks to enable efficient handling of large numbers of clients and flows.", "venue": "PAM", "authors": ["Yimeng  Zhao", "Ahmed  Saeed", "Mostafa H. Ammar", "Ellen W. Zegura"], "year": 2021, "n_citations": 0}
{"id": 1218909, "s2_id": "e65a0544311686c66d10565a5edaa409487e7c9e", "title": "Flexible Queueing Architectures", "abstract": "We study a multiserver model with n flexible servers and n queues, connected through a bipartite graph, where the level of flexibility is captured by an upper bound on the graph\u2019s average degree, dn. Applications in content replication in data centers, skill-based routing in call centers, and flexible supply chains are among our main motivations. We focus on the scaling regime where the system size n tends to infinity, while the overall traffic intensity stays fixed. We show that a large capacity region and an asymptotically vanishing queueing delay are simultaneously achievable even under limited flexibility (dn \u226a n). Our main results demonstrate that, when dn \u226b ln n, a family of expander-graph-based flexibility architectures has a capacity region that is within a constant factor of the maximum possible, while simultaneously ensuring a diminishing queueing delay for all arrival rate vectors in the capacity region. Our analysis is centered around a new class of virtual-queue-based scheduling policies that...", "venue": "Oper. Res.", "authors": ["John N. Tsitsiklis", "Kuang  Xu"], "year": 2017, "n_citations": 33}
{"id": 1218918, "s2_id": "c32399b3390edd972096515ceb0c6ea55ca827aa", "title": "A Computational Framework for the Mixing Times in the QBD Processes with Infinitely-Many Levels", "abstract": "In this paper, we develop some matrix Poisson\u2019s equations satisfied by the mean and variance of the mixing time in an irreducible positive-recurrent discrete-time Markov chain with infinitely-many levels, and provide a computational framework for the solution to the matrix Poisson\u2019s equations by means of the UL-type of RGfactorization as well as the generalized inverses. In an important special case: the level-dependent QBD processes, we provide a detailed computation for the mean and variance of the mixing time. Based on this, we give new highlight on computation of the mixing time in the block-structured Markov chains with infinitely-many levels through the matrix-analytic method.", "venue": "ArXiv", "authors": ["Quan-Lin  Li", "Jing  Cao"], "year": 2013, "n_citations": 1}
{"id": 1219125, "s2_id": "7e337f09baad62b42eaaef26c0512b251e24a518", "title": "Video Tester \u2014 A multiple-metric framework for video quality assessment over IP networks", "abstract": "This paper presents an extensible and reusable framework which addresses the problem of video quality assessment over IP networks. The proposed tool (referred to as Video-Tester) supports raw uncompressed video encoding and decoding. It also includes different video over IP transmission methods (i.e.: RTP over UDP unicast and multicast, as well as RTP over TCP). In addition, it is furnished with a rich set of offline analysis capabilities. Video-Tester analysis includes QoS and bitstream parameters estimation (i.e.: bandwidth, packet inter-arrival time, jitter and loss rate, as well as GOP size and I-frame loss rate). Our design facilitates the integration of virtually any existing video quality metric thanks to the adopted Python-based modular approach. Video-Tester currently provides PSNR, SSIM, ITU-T G.1070 video quality metric, DIV and PSNR-based MOS estimations. In order to promote its use and extension, Video-Tester is open and publicly available.", "venue": "IEEE international Symposium on Broadband Multimedia Systems and Broadcasting", "authors": ["I\u00f1aki  Ucar", "Jorge  Navarro-Ortiz", "Pablo  Ameigeiras", "Juan M. L\u00f3pez-Soler"], "year": 2012, "n_citations": 17}
{"id": 1224799, "s2_id": "ec112fe0ffc0a77163a74fc44ed2f1ef34bd02ac", "title": "Self-Learning Threshold-Based Load Balancing", "abstract": "We consider a large-scale service system where incoming tasks have to be instantaneously dispatched to one out of many parallel server pools. The user-perceived performance degrades with the number of concurrent tasks and the dispatcher aims at maximizing the overall quality of service by balancing the load through a simple threshold policy. We demonstrate that such a policy is optimal on the fluid and diffusion scales, while only involving a small communication overhead, which is crucial for large-scale deployments. In order to set the threshold optimally, it is important, however, to learn the load of the system, which may be unknown. For that purpose, we design a control rule for tuning the threshold in an online manner. We derive conditions that guarantee that this adaptive threshold settles at the optimal value, along with estimates for the time until this happens. In addition, we provide numerical experiments that support the theoretical results and further indicate that our policy copes effectively with time-varying demand patterns. Summary of Contribution: Data centers and cloud computing platforms are the digital factories of the world, and managing resources and workloads in these systems involves operations research challenges of an unprecedented scale. Due to the massive size, complex dynamics, and wide range of time scales, the design and implementation of optimal resource-allocation strategies is prohibitively demanding from a computation and communication perspective. These resource-allocation strategies are essential for certain interactive applications, for which the available computing resources need to be distributed optimally among users in order to provide the best overall experienced performance. This is the subject of the present article, which considers the problem of distributing tasks among the various server pools of a large-scale service system, with the objective of optimizing the overall quality of service provided to users. A solution to this load-balancing problem cannot rely on maintaining complete state information at the gateway of the system, since this is computationally unfeasible, due to the magnitude and complexity of modern data centers and cloud computing platforms. Therefore, we examine a computationally light load-balancing algorithm that is yet asymptotically optimal in a regime where the size of the system approaches infinity. The analysis is based on a Markovian stochastic model, which is studied through fluid and diffusion limits in the aforementioned large-scale regime. The article analyzes the load-balancing algorithm theoretically and provides numerical experiments that support and extend the theoretical results.", "venue": "INFORMS Journal on Computing", "authors": ["Diego  Goldsztajn", "Sem C. Borst", "Johan S. H. van Leeuwaarden", "Debankur  Mukherjee", "Philip A. Whiting"], "year": 2021, "n_citations": 4}
{"id": 1225283, "s2_id": "bd980a77c95d5a10b3880605a0908a26da5b4e91", "title": "Network Coding as a Service", "abstract": "Network Coding (NC) shows great potential in various communication scenarios through changing the packet forwarding principles of current networks. It can improve not only throughput, latency, reliability and security but also alleviates the need of coordination in many cases. However, it is still controversial due to widespread misunderstandings on how to exploit the advantages of it. The aim of the paper is to facilitate the usage of NC by $(i)$ explaining how it can improve the performance of the network (regardless the existence of any butterfly in the network), $(ii)$ showing how Software Defined Networking (SDN) can resolve the crucial problems of deployment and orchestration of NC elements, and $(iii)$ providing a prototype architecture with measurement results on the performance of our network coding capable software router implementation compared by fountain codes.", "venue": "ArXiv", "authors": ["D\u00e1vid  Szab\u00f3", "Attila  Csoma", "P\u00e9ter  Megyesi", "Andr\u00e1s  Guly\u00e1s", "Frank H. P. Fitzek"], "year": 2016, "n_citations": 7}
{"id": 1226479, "s2_id": "9809e4cc81ced2f58055adcc14a303f9b608f539", "title": "Enhanced Performance and Privacy for TLS over TCP Fast Open", "abstract": "Abstract Small TCP flows make up the majority of web flows. For them, the TCP three-way handshake induces significant delay overhead. The TCP Fast Open (TFO) protocol can significantly decrease this delay via zero round-trip time (0-RTT) handshakes for all TCP handshakes that follow a full initial handshake to the same host. However, this comes at the cost of privacy limitations and also has some performance limitations. In this paper, we investigate the TFP deployment on popular websites and browsers. We found that a client revisiting a web site for the first time fails to use an abbreviated TFO handshake in 40% of all cases due to web server load-balancing using multiple IP addresses. Our analysis further reveals significant privacy problems of the protocol design and implementation. Network-based attackers and online trackers can exploit TFO to track the online activities of users. As a countermeasure, we introduce a novel protocol called TCP Fast Open Privacy (FOP). TCP FOP prevents tracking by network attackers and impedes third-party tracking, while still allowing 0-RTT handshakes as in TFO. As a proof-of-concept, we have implemented the proposed protocol for the Linux kernel and a TLS library. Our measurements indicate that TCP FOP outperforms TLS over TFO when websites are served from multiple IP addresses.", "venue": "Proc. Priv. Enhancing Technol.", "authors": ["Erik  Sy", "Tobias  Mueller", "Christian  Burkert", "Hannes  Federrath", "Mathias  Fischer"], "year": 2020, "n_citations": 4}
{"id": 1229208, "s2_id": "0a2c50119679491c02e9f58fb3cc98a3035df253", "title": "Some Properties of Length Rate Quotient Shapers", "abstract": "Length Rate Quotient (LRQ) is the first algorithm of interleaved shaping \u2013 a novel concept proposed to provide per-flow shaping for a flow aggregate without per-flow queuing. This concept has been adopted by Time-Sensitive Networking (TSN) and Deterministic Networking (DetNet). An appealing property of interleaved shaping is that, when an interleaved shaper is appended to a FIFO system, it does not increase the worst-case delay of the system. Based on this \u201cshaping-for-free\u201d property, an approach has been introduced to deliver bounded end-to-end latency. Specifically, at each output link of a node, class-based aggregate scheduling is used together with one interleaved shaper per-input link and per-class, and the interleaved shaper re-shapes every flow to its initial traffic constraint. In this paper, we investigate other properties of interleaved LRQ shapers, particularly as stand-alone elements. In addition, under per-flow setting, we also investigate per-flow LRQ based flow aggregation and derive its properties. The analysis focuses directly on the timing of operations, such as shaping and scheduling, in the network. This timing based method can be found in the Guaranteed Rate (GR) server model and more generally the max-plus branch of network calculus. With the derived properties, we not only show that an improved end-to-end latency bound can be obtained for the current approach, but also demonstrate with two examples that new approaches may be devised. End-to-end delay bounds for the three approaches are derived and compared. As a highlight, the two new approaches do not require different node architectures in allocating (shaping / scheduling) queues, which implies that they can be readily adapted for use in TSN and DetNet. This together with the derived properties of LRQ shed new insights on providing the TSN / DetNet qualities of service.", "venue": "ArXiv", "authors": ["Yuming  Jiang"], "year": 2021, "n_citations": 0}
{"id": 1230046, "s2_id": "770d5dd8b59d12f90c7c0c69acccd1ae25d2e4fe", "title": "A new tool for the performance analysis of massively parallel computer systems", "abstract": "We present a new tool, GPA, that can generate key performance measures for very large systems. Based on solving systems of ordinary differential equations (ODEs), this method of performance analysis is far more scalable than stochastic simulation. The GPA tool is the first to produce higher moment analysis from differential equation approximation, which is essential, in many cases, to obtain an accurate performance prediction. We identify so-called switch points as the source of error in the ODE approximation. We investigate the switch point behaviour in several large models and observe that as the scale of the model is increased, in general the ODE performance prediction improves in accuracy. In the case of the variance measure, we are able to justify theoretically that in the limit of model scale, the ODE approximation can be expected to tend to the actual variance of the model.", "venue": "QAPL", "authors": ["Anton  Stefanek", "Richard A. Hayden", "Jeremy T. Bradley"], "year": 2010, "n_citations": 35}
{"id": 1230334, "s2_id": "c81b875b55520f7840dd208546f51d06f6d5f699", "title": "Fundamentals of the Backoff Process in 802.11: Dichotomy of the Aggregation", "abstract": "This paper discovers fundamental principles of the backoff process that governs the performance of IEEE 802.11. A simplistic principle founded upon regular variation theory is that the backoff time has a truncated Pareto-type tail distribution with an exponent of (log y)/ log m (m is the multiplicative factor and \u03b3 is the collision probability). This reveals that the per-node backoff process is heavy-tailed in the strict sense for \u03b3 > 1/m2, and paves the way for the following unifying result. The state-of-the-art theory on the superposition of the heavy-tailed processes is applied to establish a dichotomy exhibited by the aggregate backoff process, putting emphasis on the importance of time-scales on which we view the backoff processes. While the aggregation on normal time-scales leads to a Poisson process, it is approximated by a new limiting process possessing long-range dependence (LRD) on coarse time-scales. This dichotomy turns out to be instrumental in formulating short-term fairness, extending existing formulas to arbitrary population, and to elucidate the absence of LRD in practical situations. A refined wavelet analysis is conducted to strengthen this argument.", "venue": "IEEE Transactions on Information Theory", "authors": ["Jeong-woo  Cho", "Yuming  Jiang"], "year": 2015, "n_citations": 11}
{"id": 1233446, "s2_id": "75d70b320fc06720f49be82d8f62c4be74d14b91", "title": "Significance relations for the benchmarking of meta-heuristic algorithms", "abstract": "The experimental analysis of meta-heuristic algorithm performance is usually based on comparing average performance metric values over a set of algorithm instances. When algorithms getting tight in performance gains, the additional consideration of significance of a metric improvement comes into play. However, from this moment the comparison changes from an absolute to a relative mode. Here the implications of this paradigm shift are investigated. Significance relations are formally established. Based on this, a trade-off between increasing cycle-freeness of the relation and small maximum sets can be identified, allowing for the selection of a proper significance level and resulting ranking of a set of algorithms. The procedure is exemplified on the CEC'05 benchmark of real parameter single objective optimization problems. The significance relation here is based on awarding ranking points for relative performance gains, similar to the Borda count voting method or the Wilcoxon signed rank test. In the particular CEC'05 case, five ranks for algorithm performance can be clearly identified.", "venue": "2013 13th International Conference on Intellient Systems Design and Applications", "authors": ["Mario  K\u00f6ppen", "Kei  Ohnishi"], "year": 2013, "n_citations": 0}
{"id": 1235675, "s2_id": "be24c4b9e474737a7660569ee1e9adad483eaba0", "title": "Towards automated kernel selection in machine learning systems: A SYCL case study", "abstract": "Automated tuning of compute kernels is a popular area of research, mainly focused on finding optimal kernel parameters for a problem with fixed input sizes. This approach is good for deploying machine learning models, where the network topology is constant, but machine learning research often involves changing network topologies and hyperparameters. Traditional kernel auto-tuning has limited impact in this case; a more general selection of kernels is required for libraries to accelerate machine learning research. In this paper we present initial results using machine learning to select kernels in a case study deploying high performance SYCL kernels in libraries that target a range of heterogeneous devices from desktop GPUs to embedded accelerators. The techniques investigated apply more generally and could similarly be integrated with other heterogeneous programming systems. By combining auto-tuning and machine learning these kernel selection processes can be deployed with little developer effort to achieve high performance on new hardware.", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["John  Lawson"], "year": 2020, "n_citations": 3}
{"id": 1236325, "s2_id": "8a105cc3adb9ff5573d4f48812dec0869ef390ce", "title": "Modelling load balancing and Carrier Aggregation in mobile networks", "abstract": "In this paper, we propose analytical models to derive the performance of dual carrier mobile HSDPA mobile networks. Specifically, we analyze the flow-level performance of two inter-carrier load balancing schemes and the gain engendered by Carrier Aggregation (CA). CA is one of the most important features of HSPA+ networks; it allows devices to be served simultaneously by several carriers. We propose Volume Balancing (VB) and Join the Fastest Queue (JFQ), a load-balancing mechanism that allows the traffic of non-CA users to be distributed over the aggregated carriers. We then evaluate the performance of both CA and non-CA users by means of analytical modeling. We show that the proposed schemes achieve efficient load balancing. We also investigate the impact of mixing traffic of CA and non-CA users in the same cell and show that performance is practically insensitive to the traffic mix.", "venue": "2014 12th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)", "authors": ["Florence  B\u00e9n\u00e9zit", "Salah-Eddine  Elayoubi", "Raluca-Maria  Indre", "Alain  Simonian"], "year": 2014, "n_citations": 3}
{"id": 1239920, "s2_id": "6b343dd1814738da2d01edf5d81332701f005bcb", "title": "Scaling Datalog for Machine Learning on Big Data", "abstract": "In this paper, we present the case for a declarative foundation for data-intensive machine learning systems. Instead of creating a new system for each specific flavor of machine learning task, or hardcoding new optimizations, we argue for the use of recursive queries to program a variety of machine learning systems. By taking this approach, database query optimization techniques can be utilized to identify effective execution plans, and the resulting runtime plans can be executed on a single unified data-parallel query processing engine. As a proof of concept, we consider two programming models--Pregel and Iterative Map-Reduce-Update---from the machine learning domain, and show how they can be captured in Datalog, tuned for a specific task, and then compiled into an optimized physical plan. Experiments performed on a large computing cluster with real data demonstrate that this declarative approach can provide very good performance while offering both increased generality and programming ease.", "venue": "ArXiv", "authors": ["Yingyi  Bu", "Vinayak R. Borkar", "Michael J. Carey", "Joshua  Rosen", "Neoklis  Polyzotis", "Tyson  Condie", "Markus  Weimer", "Raghu  Ramakrishnan"], "year": 2012, "n_citations": 51}
{"id": 1240048, "s2_id": "21809bad9cbacfef2f30822753d483a829f541ba", "title": "Using Silent Writes in Low-Power Traffic-Aware ECC", "abstract": "Using Error Detection Code (EDC) and Error Correction Code (ECC) is a noteworthy way to increase cache memories robustness against soft errors. EDC enables detecting errors in cache memory while ECC is used to correct erroneous cache blocks. \n \nECCs are often costly as they impose considerable area and energy overhead on cache memory. Reducing this overhead has been the subject of many studies. In particular, a previous study has suggested mapping ECC to the main memory at the expense of high cache traffic and energy. A major source of this excessive traffic and energy is the high frequency of cache writes. In this work, we show that a significant portion of cache writes are silent, i.e., they write the same data already existing. We build on this observation and introduce Trafficaware ECC (or simply TCC). TCC detects silent writes by an efficient mechanism. Once such writes are detected updating their ECC is avoided effectively reducing L2 cache traffic and access frequency. \n \nUsing our solution, we reduce L2 cache access frequency by 8% while maintaining performance. We reduce L2 cache dynamic and overall cache energy by up to 32% and 8%, respectively. Furthermore, TCC reduces L2 cache miss rate by 3%.", "venue": "PATMOS", "authors": ["Mostafa  Kishani", "Amirali  Baniasadi", "Hossein  Pedram"], "year": 2011, "n_citations": 1}
{"id": 1247915, "s2_id": "b477e32c29c5c65ebfa369c5ab820e7199cead61", "title": "Implementation of PFC and RCM for RoCEv2 Simulation in OMNeT++", "abstract": "As traffic patterns and network topologies become more and more complicated in current enterprise data centers and TOP500 supercomputers, the probability of network congestion increases. If no countermeasures are taken, network congestion causes long communication delays and degrades network performance. A congestion control mechanism is often provided to reduce the consequences of congestion. However, it is usually difficult to configure and activate a congestion control mechanism in production clusters and supercomputers due to concerns that it may negatively impact jobs if the mechanism is not appropriately configured. Therefore, simulations for these situations are necessary to identify congestion points and sources, and more importantly, to determine optimal settings that can be utilized to reduce congestion in those complicated networks. In this paper, we use OMNeT++ to implement the IEEE 802.1Qbb Priority-based Flow Control (PFC) and RoCEv2 Congestion Management (RCM) in order to simulate clusters with RoCEv2 interconnects.", "venue": "ArXiv", "authors": ["Qian  Liu", "Robert D. Russell", "Fabrice  Mizero", "Malathi  Veeraraghavan", "John M. Dennis", "Benjamin F. Jamroz"], "year": 2015, "n_citations": 1}
{"id": 1251828, "s2_id": "fba9599612871544918dc4271cdb126736593a43", "title": "Analyzing distributed Join-Idle-Queue: A fluid limit approach", "abstract": "In the context of load balancing, Lu et al. [2] introduced the distributed Join-Idle-Queue algorithm, where a group of dispatchers distribute jobs to a cluster of parallel servers. Each dispatcher maintains a queue of recently idle servers; when a job arrives to a dispatcher, it sends it to a server on its queue, or to a random server if the queue is empty. In turn, when a server becomes idle, it requests to be placed on the queue of a randomly chosen dispatcher.", "venue": "2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "authors": ["Michael  Mitzenmacher"], "year": 2016, "n_citations": 26}
{"id": 1256076, "s2_id": "ff36fb850779c4faaaa29b2c09143ccedb056697", "title": "Amdahl's and Gustafson-Barsis laws revisited", "abstract": "The paper presents a simple derivation of the Gustafson-Barsis law from the Amdahl's law. In the computer literature these two laws describing the speedup limits of parallel applications are derived separately. It is shown, that treating the time of the execution of the sequential part of the application as a constant, in few lines the Gustafson-Barsis law can be obtained from the Amdahl's law and that the popular claim, that Gustafson-Barsis law overthrows Amdahl's law is a mistake.", "venue": "ArXiv", "authors": ["Andrzej  Karbowski"], "year": 2008, "n_citations": 5}
{"id": 1260231, "s2_id": "d23581e021205927ea937f753222870a74dd7138", "title": "On the Reliability of RAID Systems: An Argument for More Check Drives", "abstract": "In this paper we address issues of reliability of RAID systems. We focus on \"big data\" systems with a large number of drives and advanced error correction schemes beyond \\RAID{6}. Our RAID paradigm is based on Reed-Solomon codes, and thus we assume that the RAID consists of $N$ data drives and $M$ check drives. The RAID fails only if the combined number of failed drives and sector errors exceeds $M$, a property of Reed-Solomon codes. \nWe review a number of models considered in the literature and build upon them to construct models usable for a large number of data and check drives. We attempt to account for a significant number of factors that affect RAID reliability, such as drive replacement or lack thereof, mistakes during service such as replacing the wrong drive, delayed repair, and the finite duration of RAID reconstruction. We evaluate the impact of sector failures that do not result in drive replacement. \nThe reader who needs to consider large $M$ and $N$ will find applicable mathematical techniques concisely summarized here, and should be able to apply them to similar problems. Most methods are based on the theory of continuous time Markov chains, but we move beyond this framework when we consider the fixed time to rebuild broken hard drives, which we model using systems of delay and partial differential equations. \nOne universal statement is applicable across various models: increasing the number of check drives in all cases increases the reliability of the system, and is vastly superior to other approaches of ensuring reliability such as mirroring.", "venue": "ArXiv", "authors": ["Sarah Edge Mann", "Michael  Anderson", "Marek  Rychlik"], "year": 2012, "n_citations": 2}
{"id": 1265397, "s2_id": "35d0ded6376d95fdc1bab4d66f9bb838e2ec84b4", "title": "Load-aware channel selection for 802.11 WLANs with limited measurement", "abstract": "It has been known that load unaware channel selection in 802.11 networks results in high level interference, and can significantly reduce the network throughput. In current implementation, the only way to determine the traffic load on a channel is to measure that channel for a certain duration of time. Therefore, in order to find the best channel with the minimum load all channels have to be measured, which is costly and can cause unacceptable communication interruptions between the AP and the stations. In this paper, we propose a learning based approach which aims to find the channel with the minimum load by measuring only limited number of channels. Our method uses Gaussian Process Regressing to accurately track the traffic load on each channel based on the previous measured load. We confirm the performance of our algorithm by using experimental data, and show that the time consumed for the load measurement can be reduced up to 46% compared to the case where all channels are monitored.", "venue": "2016 23rd International Conference on Telecommunications (ICT)", "authors": ["Mehmet  Karaca", "Bj\u00f6rn  Landfeldt"], "year": 2016, "n_citations": 2}
{"id": 1265788, "s2_id": "a17d06a58c1825ceaf61668a729e91f2923c0657", "title": "Global attraction of ODE-based mean field models with hyperexponential job sizes", "abstract": "Mean field modeling is a popular approach to assess the performance of large scale computer systems. The evolution of many mean field models is characterized by a set of ordinary differential equations that have a unique fixed point. In order to prove that this unique fixed point corresponds to the limit of the stationary measures of the finite systems, the unique fixed point must be a global attractor. While global attraction was established for various systems in case of exponential job sizes, it is often unclear whether these proof techniques can be generalized to non-exponential job sizes. In this paper we show how simple monotonicity arguments can be used to prove global attraction for a broad class of ordinary differential equations that capture the evolution of mean field models with hyperexponential job sizes. This class includes both existing as well as previously unstudied load balancing schemes and can be used for systems with either finite or infinite buffers. The main novelty of the approach exists in using a Coxian representation for the hyperexponential job sizes and a partial order that is stronger than the componentwise partial order used in the exponential case.", "venue": "Abstracts of the 2019 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems", "authors": ["Benny Van Houdt"], "year": 2019, "n_citations": 12}
{"id": 1267688, "s2_id": "3f8c11bc6452a8750bb3b943d6aaf8a41eb31d29", "title": "Get More With Less: Near Real-Time Image Clustering on Mobile Phones", "abstract": "Machine learning algorithms, in conjunction with user data, hold the promise of revolutionizing the way we interact with our phones, and indeed their widespread adoption in the design of apps bear testimony to this promise. However, currently, the computationally expensive segments of the learning pipeline, such as feature extraction and model training, are offloaded to the cloud, resulting in an over-reliance on the network and under-utilization of computing resources available on mobile platforms. In this paper, we show that by combining the computing power distributed over a number of phones, judicious optimization choices, and contextual information it is possible to execute the end-to-end pipeline entirely on the phones at the edge of the network, efficiently. We also show that by harnessing the power of this combination, it is possible to execute a computationally expensive pipeline at near real-time. \nTo demonstrate our approach, we implement an end-to-end image-processing pipeline -- that includes feature extraction, vocabulary learning, vectorization, and image clustering -- on a set of mobile phones. Our results show a 75% improvement over the standard, full pipeline implementation running on the phones without modification -- reducing the time to one minute under certain conditions. We believe that this result is a promising indication that fully distributed, infrastructure-less computing is possible on networks of mobile phones; enabling a new class of mobile applications that are less reliant on the cloud.", "venue": "ArXiv", "authors": ["Jorge  Ortiz", "Chien-Chin  Huang", "Supriyo  Chakraborty"], "year": 2015, "n_citations": 1}
{"id": 1271404, "s2_id": "bdc2b4c9eadd7872c9b7e4c7a79f84f1ff55a227", "title": "UNIX Resource Managers: Capacity Planning and Resource Issues", "abstract": "The latest implementations of commercial UNIX to offer mainframe style capacity management on enterprise servers include: AIX Workload Manager (WLM), HP-UX Process Resource Manager (PRM), Solaris Resource Manager (SRM), as well as SGI and Compaq. The ability to manage server capacity is achieved by making significant modifications to the standard UNIX operating system so that processes are inherently tied to specific users. Those users, in turn, are granted only a certain fraction of system resources. Resource usage is monitored and compared with each users grant to ensure that the assigned entitlement constraints are met. In this paper, we begin by clearing up some of the confusion that has surrounded the motivation and the terminology behind the new technology. The common theme across each of the commercial implementations is the introduction of the fair-share scheduler. After reviewing some potential performance pitfalls, we present capacity planning guidelines for migrating to automated UNIX resource management.", "venue": "ArXiv", "authors": ["Neil J. Gunther"], "year": 2000, "n_citations": 0}
{"id": 1272613, "s2_id": "031c37b102f7dd75e14d4156e2ecbdf385401592", "title": "Transmission Performance Analysis of Digital Wire and Wireless Optical Links in Local and Wide Areas Optical Networks", "abstract": "In the present paper, the transmission performance analysis of digital wire and wireless optical links in local and wide areas optical networks have been modeled and parametrically investigated over wide range of the affecting parameters. Moreover, we have analyzed the basic equations of the comparative study of the performance of digital fiber optic links with wire and wireless optical links. The development of optical wireless communication systems is accelerating as a high cost effective to wire fiber optic links. The optical wireless technology is used mostly in wide bandwidth data transmission applications. Finally, we have investigated the maximum transmission distance and data transmission bit rates that can be achieved within digital wire and wireless optical links for local and wide areas optical network applications.", "venue": "ArXiv", "authors": ["Abd El Naser A. Mohammed", "Mohamed M. E. El-Halawany", "Ahmed Nabih Zaki Rashed", "Amina E. M. El-Nabawy"], "year": 2009, "n_citations": 15}
{"id": 1272763, "s2_id": "1326d4eaf89ac3e3980a49b4d77078141c68d496", "title": "Analytics of Longitudinal System Monitoring Data for Performance Prediction", "abstract": "In recent years, several HPC facilities have started continuous monitoring of their systems and jobs to collect performance-related data for understanding performance and operational efficiency. Such data can be used to optimize the performance of individual jobs and the overall system by creating data-driven models that can predict the performance of pending jobs. In this paper, we model the performance of representative control jobs using longitudinal system-wide monitoring data to explore the causes of performance variability. Using machine learning, we are able to predict the performance of unseen jobs before they are executed based on the current system state. We analyze these prediction models in great detail to identify the features that are dominant predictors of performance. We demonstrate that such models can be application-agnostic and can be used for predicting performance of applications that are not included in training.", "venue": "ArXiv", "authors": ["Ian J. Costello", "Abhinav  Bhatele"], "year": 2020, "n_citations": 0}
{"id": 1274031, "s2_id": "bd9eef3507119ee1c16fcb9990ef210f0ed30061", "title": "CoCoPIE: Making Mobile AI Sweet As PIE -Compression-Compilation Co-Design Goes a Long Way", "abstract": "Assuming hardware is the major constraint for enabling real-time mobile intelligence, the industry has mainly dedicated their efforts to developing specialized hardware accelerators for machine learning and inference. This article challenges the assumption. By drawing on a recent real-time AI optimization framework CoCoPIE, it maintains that with effective compression-compiler co-design, it is possible to enable real-time artificial intelligence on mainstream end devices without special hardware. CoCoPIE is a software framework that holds numerous records on mobile AI: the first framework that supports all main kinds of DNNs, from CNNs to RNNs, transformer, language models, and so on; the fastest DNN pruning and acceleration framework, up to 180X faster compared with current DNN pruning on other frameworks such as TensorFlow-Lite; making many representative AI applications able to run in real-time on off-the-shelf mobile devices that have been previously regarded possible only with special hardware support; making off-the-shelf mobile devices outperform a number of representative ASIC and FPGA solutions in terms of energy efficiency and/or performance.", "venue": "ArXiv", "authors": ["Shaoshan  Liu", "Bin  Ren", "Xipeng  Shen", "Yanzhi  Wang"], "year": 2020, "n_citations": 4}
{"id": 1274352, "s2_id": "a6c215717c60036eccdac6f0a38bda12bbca3024", "title": "Towards a Performance Model for Byzantine Fault Tolerant (Storage) Services", "abstract": "Byzantine fault-tolerant systems have been researched for more than four decades, and although shown possible early, the solutions were impractical for a long time. With PBFT the first practical solution was proposed in 1999 and spawned new research which culminated in novel applications using it today. Although the safety and liveness properties of PBFT-type protocols have been rigorously analyzed, when it comes to practical performance only empirical results often in artificial settings are known and imperfections on the communication channels are not specifically considered. In this work we present the first performance model for PBFT specifically considering the impact of unreliable channels and the use of different transport protocols over them. We also did extensive simulations to verify the model and to gain more insight on the impact of deployment parameters on the overall transaction time. We show that the usage of UDP can lead to significant speedup for PBFT protocols compared to TCP when tuned accordingly even over lossy channels. Finally, we compared the simulation to a real implementation and measure the benefits of a developed improvement directly. We found that the impact on the design of the network layer has been overlooked in the past but offers some additional room for improvement when it comes to practical performance. In this work we are focusing on the optimistic case with no node failures, as this is hopefully the most relevant situation.", "venue": "ArXiv", "authors": ["Thomas  Loruenser", "Benjamin  Rainer", "Florian  Wohner"], "year": 2021, "n_citations": 0}
{"id": 1274432, "s2_id": "2c5de78bf586ed6dbf20a104a82207c9ff8f126c", "title": "Algorithmic Performance-Accuracy Trade-off in 3D Vision Applications Using HyperMapper", "abstract": "In this paper we investigate an emerging application, 3D scene understanding, likely to be significant in the mobile space in the near future. The goal of this exploration is to reduce execution time while meeting our quality of result objectives. In previous work, we showed for the first time that it is possible to map this application to power constrained embedded systems, highlighting that decision choices made at the algorithmic design-level have the most significant impact. As the algorithmic design space is too large to be exhaustively evaluated, we use a previously introduced multi-objective random forest active learning prediction framework dubbed HyperMapper, to find good algorithmic designs. We show that HyperMapper generalizes on a recent cutting edge 3D scene understanding algorithm and on a modern GPU-based computer architecture. HyperMapper is able to beat an expert human hand-tuning the algorithmic parameters of the class of computer vision applications taken under consideration in this paper automatically. In addition, we use crowd-sourcing using a 3D scene understanding Android app to show that the Pareto front obtained on an embedded system can be used to accelerate the same application on all the 83 smart-phones and tablets with speedups ranging from 2x to over 12x.", "venue": "2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Luigi  Nardi", "Bruno  Bodin", "Sajad  Saeedi", "Emanuele  Vespa", "Andrew J. Davison", "Paul H. J. Kelly"], "year": 2017, "n_citations": 19}
{"id": 1274829, "s2_id": "26e121822fd4d91f2be28a3912dedac431d25986", "title": "Breakdown of a Benchmark Score Without Internal Analysis of Benchmarking Program", "abstract": "A breakdown of a benchmark score is how much each aspect of the system performance affects the score. Existing methods require internal analysis on the benchmarking program and then involve the following problems: (1) require a certain amount of labor for code analysis, profiling, simulation, and so on and (2) require the benchmarking program itself. In this paper, we present a method for breaking down a benchmark score without internal analysis of the benchmarking program. The method utilizes regression analysis of benchmark scores on a number of systems. Experimental results with 3 benchmarks on 15 Android smartphones showed that our method could break down those benchmark scores even though there is room for improvement in accuracy.", "venue": "ArXiv", "authors": ["Naoki  Matagawa", "Kazuyuki  Shudo"], "year": 2016, "n_citations": 1}
{"id": 1278971, "s2_id": "85b3dfcf986ec3c0c98ea16bfaeb13726cb11233", "title": "On the optimal tradeoff of average service cost rate, average utility rate, and average delay for the state dependent M/M/1 queue", "abstract": "The optimal tradeoff between average service cost rate, average utility rate, and average delay is addressed for a state dependent M/M/1 queueing model, with controllable queue length dependent service rates and arrival rates. For a model with a constant arrival rate $\\lambda$ for all queue lengths, we obtain an asymptotic characterization of the minimum average delay, when the average service cost rate is a small positive quantity, $V$, more than the minimum average service cost rate required for queue stability. We show that depending on the value of the arrival rate $\\lambda$, the assumed service cost rate function, and the possible values of the service rates, the minimum average delay either: a) increases only to a finite value, b) increases without bound as $\\log\\frac{1}{V}$, c) increases without bound as $\\frac{1}{V}$, or d) increases without bound as $\\frac{1}{\\sqrt{V}}$, when $V \\downarrow 0$. We then extend our analysis to (i) a complementary problem, where the tradeoff of average utility rate and average delay is analysed for a M/M/1 queueing model, with controllable queue length dependent arrival rates, but a constant service rate $\\mu$ for all queue lengths, and (ii) a M/M/1 queueing model, with controllable queue length dependent service rates and arrival rates, for which we obtain an asymptotic characterization of the minimum average delay under constraints on both the average service cost rate as well as the average utility rate. The results that we obtain are useful in obtaining intuition as well guidance for the derivation of similar asymptotic lower bounds, such as the Berry-Gallager asymptotic lower bound, for discrete time queueing models.", "venue": "ArXiv", "authors": ["Vineeth Bala Sukumaran", "Utpal  Mukherji"], "year": 2013, "n_citations": 0}
{"id": 1280547, "s2_id": "2077c644ab0c9cfae3b76454b1e4d3211e20f486", "title": "On Deep Neural Networks for Detecting Heart Disease", "abstract": "Heart disease is the leading cause of death, and experts estimate that approximately half of all heart attacks and strokes occur in people who have not been flagged as \"at risk.\" Thus, there is an urgent need to improve the accuracy of heart disease diagnosis. To this end, we investigate the potential of using data analysis, and in particular the design and use of deep neural networks (DNNs) for detecting heart disease based on routine clinical data. Our main contribution is the design, evaluation, and optimization of DNN architectures of increasing depth for heart disease diagnosis. This work led to the discovery of a novel five layer DNN architecture - named Heart Evaluation for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields best prediction accuracy. HEARO-5's design employs regularization optimization and automatically deals with missing data and/or data outliers. To evaluate and tune the architectures we use k-way cross-validation as well as Matthews correlation coefficient (MCC) to measure the quality of our classifications. The study is performed on the publicly available Cleveland dataset of medical information, and we are making our developments open source, to further facilitate openness and research on the use of DNNs in medicine. The HEARO-5 architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms currently published research in the area.", "venue": "ArXiv", "authors": ["Nathalie-Sofia  Tomov", "Stanimire  Tomov"], "year": 2018, "n_citations": 17}
{"id": 1282308, "s2_id": "d631613b0fe46c78b73f2c7389a357239a3351dc", "title": "Towards an Objective Metric for the Performance of Exact Triangle Count", "abstract": "The performance of graph algorithms is often measured in terms of the number of traversed edges per second (TEPS). However, this performance metric is inadequate for a graph operation such as exact triangle counting. In triangle counting, execution times on graphs with a similar number of edges can be distinctly different as demonstrated by results from the past Graph Challenge entries. We discuss the need for an objective performance metric for graph operations and the desired characteristics of such a metric such that it more accurately captures the interactions between the amount of work performed and the capabilities of the hardware on which the code is executed. Using exact triangle counting as an example, we derive a metric that captures how certain techniques employed in many implementations improve performance. We demonstrate that our proposed metric can be used to evaluate and compare multiple approaches for triangle counting, using a SIMD approach as a case study against a scalar baseline.", "venue": "2020 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Mark P. Blanco", "Scott  McMillan", "Tze Meng Low"], "year": 2020, "n_citations": 0}
{"id": 1282826, "s2_id": "de8f510ed5eda4924faa71e91aeca46520ff656b", "title": "Patch-based Hybrid Modelling of Spatially Distributed Systems by Using Stochastic HYPE - ZebraNet as an Example", "abstract": "Individual-based hybrid modelling of spatially distributed systems is usually expensive. Here, we consider a hybrid system in which mobile agents spread over the space and interact with each other when in close proximity. An individual-based model for this system needs to capture the spatial attributes of every agent and monitor the interaction between each pair of them. As a result, the cost of simulating this model grows exponentially as the number of agents increases. For this reason, a patch-based model with more abstraction but better scalability is advantageous. In a patch-based model, instead of representing each agent separately, we model the agents in a patch as an aggregation. This property significantly enhances the scalability of the model. In this paper, we convert an individual-based model for a spatially distributed network system for wild-life monitoring, ZebraNet, to a patch-based stochastic HYPE model with accurate performance evaluation. We show the ease and expressiveness of stochastic HYPE for patch-based modelling of hybrid systems. Moreover, a mean-field analytical model is proposed as the fluid flow approximation of the stochastic HYPE model, which can be used to investigate the average behaviour of the modelled system over an infinite number of simulation runs of the stochastic HYPE model.", "venue": "QAPL", "authors": ["Cheng  Feng"], "year": 2014, "n_citations": 11}
{"id": 1286074, "s2_id": "e62f23a6474a2dea29d4497767de0fc38c26ab37", "title": "A Modeling Framework for Reliability of Erasure Codes in SSD Arrays", "abstract": "Emergence of Solid-State Drives (SSDs) have evolved the data storage industry where they are rapidly replacing Hard Disk Drives (HDDs) due to their superiority in performance and power. Meanwhile, SSDs have reliability issues due to bit errors, bad blocks, and bad chips. To help reliability, Redundant Array of Independent Disks (RAID) configurations, originally proposed to increase both performance and reliability of HDDs, are also applied to SSD arrays. However, the conventional reliability models of HDD RAID cannot be intactly applied to SSD arrays, as the nature of failures in SSDs are totally different from HDDs. Previous studies on the reliability of SSD arrays are based on the deprecated SSD failure data, and only focus on limited failure types, device failures, and page failures caused by the bit errors, while recent field studies have reported other failure types including bad blocks and bad chips, and a high correlation between failures. In this paper, we investigate the reliability of SSD arrays using field storage traces and real-system implementation of conventional and emerging erasure codes. The reliability is evaluated by statistical fault injection experiments that post-process the usage logs obtained from the real-system implementation, while the fault/failure attributes are obtained from the state-of-the-art field data by previous works. As a case study, we examine conventional RAID5 and RAID6 and emerging Partial-MDS (PMDS) codes, Sector-Disk (SD) codes, and STAIR codes in terms of both reliability and performance using an open-source software RAID controller, MD (in Linux kernel version 3.10.0-327), and arrays of Samsung 850 Pro SSDs. Our detailed analysis on the data loss breakdown shows that a) emerging erasure codes fail to replace RAID6 in terms of reliability, b) row-wise erasure codes are the most efficient choices for contemporary SSD devices, and c) previous models overestimate the SSD array reliability by up to six orders of magnitude, as they just focus on the coincidence of bad pages (bit errors) and bad chips within a data stripe that holds the minority of root cause of data loss in SSD arrays. Our experiments show that the combination of bad chips with bad blocks is recognized as the major source of data loss in RAID5 and emerging codes (contributing more than 54 and 90 percent of data loss in RAID5 and emerging codes, respectively), while RAID6 remains robust under these failure combinations. Finally, the fault injection results reveal that SSD array reliability, as well as the failure breakdown is significantly correlated with SSD type.", "venue": "IEEE Transactions on Computers", "authors": ["Mostafa  Kishani", "Saba  Ahmadian", "Hossein  Asadi"], "year": 2020, "n_citations": 2}
{"id": 1291342, "s2_id": "194f82f9163dfb842fab9ec62a947fcd6d66d998", "title": "Advanced Antenna Techniques and High Order Sectorization with Novel Network Tessellation for Enhancing Macro Cell Capacity in DC-HSDPA Network", "abstract": "Mobile operators commonly use macro cells with traditional wide beam antennas for wider coverage in the cell, but future capacity demands cannot be achieved by using them only. It is required to achieve maximum practical capacity from macro cells by employing higher order sectorization and by utilizing all possible antenna solutions including smart antennas. This paper presents enhanced tessellation for 6-sector sites and proposes novel layout for 12-sector sites. The main target of this paper is to compare the performance of conventional wide beam antenna, switched beam smart antenna, adaptive beam antenna and different network layouts in terms of offering better received signal quality and user throughput. Splitting macro cell into smaller micro or pico cells can improve the capacity of network, but this paper highlights the importance of higher order sectorization and advance antenna techniques to attain high Signal to Interference plus Noise Ratio (SINR), along with improved network capacity. Monte Carlo simulations at system level were done for Dual Cell High Speed Downlink Packet Access (DC-HSDPA) technology with multiple (five) users per Transmission Time Interval (TTI) at different Intersite Distance (ISD). The obtained results validate and estimate the gain of using smart antennas and higher order sectorization with proposed network layout.", "venue": "ArXiv", "authors": ["Muhammad Usman Sheikh", "Jukka  Lempi\u00e4inen"], "year": 2013, "n_citations": 23}
{"id": 1292616, "s2_id": "945ea3d9b094381ff0c06894d538fc461427e15a", "title": "Mithril: mining sporadic associations for cache prefetching", "abstract": "The growing pressure on cloud application scalability has accentuated storage performance as a critical bottleneck. Although cache replacement algorithms have been extensively studied, cache prefetching - reducing latency by retrieving items before they are actually requested - remains an underexplored area. Existing approaches to history-based prefetching, in particular, provide too few benefits for real systems for the resources they cost. We propose Mithril, a prefetching layer that efficiently exploits historical patterns in cache request associations. Mithril is inspired by sporadic association rule mining and only relies on the timestamps of requests. Through evaluation of 135 block-storage traces, we show that Mithril is effective, giving an average of a 55% hit ratio increase over LRU and Probability Graph, and a 36% hit ratio gain over Amp at reasonable cost. Finally, we demonstrate the improvement comes from Mithril being able to capture mid-frequency blocks.", "venue": "SoCC", "authors": ["Juncheng  Yang", "Reza  Karimi", "Trausti  S\u00e6mundsson", "Avani  Wildani", "Ymir  Vigfusson"], "year": 2017, "n_citations": 10}
{"id": 1295046, "s2_id": "daa701c4d6aee1a817fa8e04ca5e8b439feda8ec", "title": "OMP2MPI: Automatic MPI code generation from OpenMP programs", "abstract": "In this paper, we present OMP2MPI a tool that generates automatically MPI source code from OpenMP. With this transformation the original program can be adapted to be able to exploit a larger number of processors by surpassing the limits of the node level on large HPC clusters. The transformation can also be useful to adapt the source code to execute in distributed memory many-cores with message passing support. In addition, the resulting MPI code can be used as an starting point that still can be further optimized by software engineers. The transformation process is focused on detecting OpenMP parallel loops and distributing them in a master/worker pattern. A set of micro-benchmarks have been used to verify the correctness of the the transformation and to measure the resulting performance. Surprisingly not only the automatically generated code is correct by construction, but also it often performs faster even when executed with MPI.", "venue": "ArXiv", "authors": ["Albert  Sa\u00e0-Garriga", "David  Castells-Rufas", "Jordi  Carrabina"], "year": 2015, "n_citations": 6}
{"id": 1297186, "s2_id": "d7330aca256929fc8dad593c1f2561a89f10184d", "title": "Join-Idle-Queue with Service Elasticity: Large-Scale Asymptotics of a Non-monotone System", "abstract": "We consider the model of a token-based joint auto-scaling and load balancing strategy, proposed in a recent paper by Mukherjee, Dhara, Borst, and van Leeuwaarden (SIGMETRICS '17, arXiv:1703.08373), which offers an efficient scalable implementation and yet achieves asymptotically optimal steady-state delay performance and energy consumption as the number of servers $N\\to\\infty$. In the above work, the asymptotic results are obtained under the assumption that the queues have fixed-size finite buffers, and therefore the fundamental question of stability of the proposed scheme with infinite buffers was left open. In this paper, we address this fundamental stability question. The system stability under the usual subcritical load assumption is not automatic. Moreover, the stability may not even hold for all $N$. The key challenge stems from the fact that the process lacks monotonicity, which has been the powerful primary tool for establishing stability in load balancing models. We develop a novel method to prove that the subcritically loaded system is stable for large enough $N$, and establish convergence of steady-state distributions to the optimal one, as $N \\to \\infty$. The method goes beyond the state of the art techniques -- it uses an induction-based idea and a \"weak monotonicity\" property of the model; this technique is of independent interest and may have broader applicability.", "venue": "ArXiv", "authors": ["Debankur  Mukherjee", "Alexander L. Stolyar"], "year": 2018, "n_citations": 8}
{"id": 1301000, "s2_id": "6ec8e9fe1b6150aaa0995134c001234a71304730", "title": "A Performance Comparison of CUDA and OpenCL", "abstract": "CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is an open standard that can be used to program CPUs, GPUs, and other devices from different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL promises a portable language for GPU programming, its generality may entail a performance penalty. In this paper, we use complex, near-identical kernels from a Quantum Monte Carlo application to compare the performance of CUDA and OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel to an OpenCL kernel involves minimal modifications. Making such a kernel compile with ATI's build tools involves more modifications. Our performance tests measure and compare data transfer times to and from the GPU, kernel execution times, and end-to-end application execution times for both CUDA and OpenCL.", "venue": "ArXiv", "authors": ["Kamran  Karimi", "Neil G. Dickson", "Firas  Hamze"], "year": 2010, "n_citations": 235}
{"id": 1301745, "s2_id": "12be666c8d25dc29497ffc96152755bf0d8ea3c9", "title": "Optimal routing in two-queue polling systems", "abstract": "\nWe consider a polling system with two queues, exhaustive service, no switchover times, and exponential service times with rate \u00b5 in each queue. The waiting cost depends on the position of the queue relative to the server: it costs a customer c per time unit to wait in the busy queue (where the server is) and d per time unit in the idle queue (where there is no server). Customers arrive according to a Poisson process with rate \u03bb. We study the control problem of how arrivals should be routed to the two queues in order to minimize the expected waiting costs and characterize individually and socially optimal routeing policies under three scenarios of available information at decision epochs: no, partial, and complete information. In the complete information case, we develop a new iterative algorithm to determine individually optimal policies (which are symmetric Nash equilibria), and show that such policies can be described by a switching curve. We use Markov decision processes to compute the socially optimal policies. We observe numerically that the socially optimal policy is well approximated by a linear switching curve. We prove that the control policy described by this linear switching curve is indeed optimal for the fluid version of the two-queue polling system.\n", "venue": "J. Appl. Probab.", "authors": ["Ivo J. B. F. Adan", "Vidyadhar G. Kulkarni", "Namyoon  Lee", "A. A. J. Lefeber"], "year": 2018, "n_citations": 7}
{"id": 1302256, "s2_id": "9dfbe27d7927ee3c3c07c5fd2416459220800c60", "title": "Efficient Saliency Maps for Explainable AI", "abstract": "We describe an explainable AI saliency map method for use with deep convolutional neural networks (CNN) that is much more efficient than popular fine-resolution gradient methods. It is also quantitatively similar or better in accuracy. Our technique works by measuring information at the end of each network scale which is then combined into a single saliency map. We describe how saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence. We visualize individual scale/layer contributions by using a Layer Ordered Visualization of Information. This provides an interesting comparison of scale information contributions within the network not provided by other saliency map methods. Using our method instead of Guided Backprop, coarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem to yield demonstrably superior results without sacrificing speed. This will make fine-resolution saliency methods feasible on resource limited platforms such as robots, cell phones, low-cost industrial devices, astronomy and satellite imagery.", "venue": "ArXiv", "authors": ["T. Nathan Mundhenk", "Barry Y. Chen", "Gerald  Friedland"], "year": 2019, "n_citations": 15}
{"id": 1302837, "s2_id": "d0cf9e9726d2ae2df801771136f006e6b58770eb", "title": "Product Forms for FCFS Queueing Models with Arbitrary Server-Job Compatibilities: An Overview", "abstract": "In recent years, a number of models involving different compatibilities between jobs and servers in queueing systems, or between agents and resources in matching systems, have been studied, and, under Markov assumptions and appropriate stability conditions, the stationary distributions have been shown to have product forms. We survey these results and show how, under an appropriate detailed description of the state, many existing product-form results are corollaries of similar results for the Order Independent queue. We also discuss how to use the product-form results to determine distributions for steady-state response times.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Kristen  Gardner", "Rhonda  Righter"], "year": 2020, "n_citations": 16}
{"id": 1302984, "s2_id": "65f138ead83d5dd3530bf617b08804216708563c", "title": "Debian Package usage profiler for Debian based Systems", "abstract": "The embedded devices of today due to their CPU, RAM capabilities can run various Linux distributions but in most cases they are different from general purpose distributions as they are usually lighter and specific to the needs of that particular system. In this project, we share the problems associated in adopting a fully heavy-weight Debian based system like Ubuntu in embedded/automotive platforms and provide solutions to optimize them to identify unused/redundant content in the system. This helps developer to reduce the hefty general purpose distribution to an application specific distribution. The solution involves collecting usage data in the system in a non-invasive manner (to avoid any drop in performance) to suggest users the redundant, unused parts of the system that can be safely removed without impacting the system functionality.", "venue": "ArXiv", "authors": ["Bharath Honnesara Sreenivasa", "Ajay  Rajan"], "year": 2019, "n_citations": 0}
{"id": 1308773, "s2_id": "48763418bf66c623aa5f1bfda68467f9c60a4806", "title": "Performance landscape of resource-constrained platforms targeting DNNs", "abstract": "Over the recent years, a significant number of complex, deep neural networks have been developed for a variety of applications including speech and face recognition, computer vision in the areas of health-care, automatic translation, image classification, etc. Moreover, there is an increasing demand in deploying these networks in resource-constrained edge devices. As the computational demands of these models keep increasing, pushing to their limits the targeted devices, the constant development of new hardware systems tailored to those workloads has been observed. Since programmability of these diverse and complex platforms -compounded by the rapid development of new DNN modelsis a major challenge, platform vendors have developed Machine Learning tailored SDKs to maximize the platform\u2019s performance. This work investigates the performance achieved on a number of modern commodity embedded platforms coupled with the vendors\u2019 provided software support when state-of-the-art DNN models from image classification, object detection and image segmentation are targeted. The work quantifies the relative latency gains of the particular embedded platforms and provides insights on the relationship between the required minimum batch size for achieving maximum throughput, concluding that modern embedded systems reach their maximum performance even for modest batch sizes when a modern state of the art DNN model is targeted. Overall, the presented results provide a guide for the expected performance for a number of state-of-the-art DNNs on popular embedded platforms across the image classification, detection and segmentation domains.", "venue": "ArXiv", "authors": ["Panagiotis  Miliadis", "Christos-Savvas  Bouganis", "Dionisios  Pnevmatikatos"], "year": 2021, "n_citations": 0}
{"id": 1308927, "s2_id": "bcb8496e88c8572449ea83be4994a29dbac94ca7", "title": "Analysis and Optimization of Sparse Random Linear Network Coding for Reliable Multicast Services", "abstract": "Point-to-multipoint communications are expected to play a pivotal role in next-generation networks. This paper refers to a cellular system transmitting layered multicast services to a multicast group of users. Reliability of communications is ensured via different random linear network coding (RLNC) techniques. We deal with a fundamental problem: the computational complexity of the RLNC decoder. The higher the number of decoding operations is, the more the user's computational overhead grows and, consequently, the faster the battery of mobile devices drains. By referring to several sparse RLNC techniques, and without any assumption on the implementation of the RLNC decoder in use, we provide an efficient way to characterize the performance of users targeted by ultra-reliable layered multicast services. The proposed modeling allows to efficiently derive the average number of coded packet transmissions needed to recover one or more service layers. We design a convex resource allocation framework that allows to minimize the complexity of the RLNC decoder by jointly optimizing the transmission parameters and the sparsity of the code. The designed optimization framework also ensures service guarantees to predetermined fractions of users. The performance of the proposed optimization framework is then investigated in a LTE-A eMBMS network multicasting H.264/SVC video services.", "venue": "IEEE Transactions on Communications", "authors": ["Andrea  Tassi", "Ioannis  Chatzigeorgiou", "Daniel E. Lucani"], "year": 2016, "n_citations": 40}
{"id": 1310818, "s2_id": "4996247026abc9145d01b45e4c17035079688369", "title": "On the Importance of Nonlinear Modeling in Computer Performance Prediction", "abstract": "Computers are nonlinear dynamical systems that exhibit complex and sometimes even chaotic behavior. The low-level performance models used in the computer systems community, however, are linear. This paper is an exploration of that disconnect: when linear models are adequate for predicting computer performance and when they are not. Specifically, we build linear and nonlinear models of the processor load of an Intel i7-based computer as it executes a range of different programs. We then use those models to predict the processor loads forward in time and compare those forecasts to the true continuations of the time series.", "venue": "IDA", "authors": ["Joshua  Garland", "Elizabeth  Bradley"], "year": 2013, "n_citations": 8}
{"id": 1311979, "s2_id": "10e0141d6ac25307389ae535a6c947dd96d46db9", "title": "GPU-based Commonsense Paradigms Reasoning for Real-Time Query Answering and Multimodal Analysis", "abstract": "We utilize commonsense knowledge bases to address the problem of real- time multimodal analysis. In particular, we focus on the problem of multimodal sentiment analysis, which consists in the simultaneous analysis of different modali- ties, e.g., speech and video, for emotion and polarity detection. Our approach takes advantages of the massively parallel processing power of modern GPUs to enhance the performance of feature extraction from different modalities. In addition, in order to ex- tract important textual features from multimodal sources we generate domain-specific graphs based on commonsense knowledge and apply GPU-based graph traversal for fast feature detection. Then, powerful ELM classifiers are applied to build the senti- ment analysis model based on the extracted features. We conduct our experiments on the YouTube dataset and achieve an accuracy of 78% which outperforms all previous systems. In term of processing speed, our method shows improvements of several orders of magnitude for feature extraction compared to CPU-based counterparts.", "venue": "ArXiv", "authors": ["Ha Nguyen Tran", "Erik  Cambria"], "year": 2018, "n_citations": 2}
{"id": 1319453, "s2_id": "d00a671a81c17b512627a4f7c8cfc0be110df6d7", "title": "Applying the Roofline model for Deep Learning performance optimizations", "abstract": "In this paper We present a methodology for creating Roofline models automatically for Non-Unified Memory Access (NUMA) using Intel Xeon as an example. Finally, we present an evaluation of highly efficient deep learning primitives as implemented in the Intel oneDNN Library.", "venue": "ArXiv", "authors": ["Jacek  Czaja", "Michal  Gallus", "Joanna  Wozna", "Adam  Grygielski", "Luo  Tao"], "year": 2020, "n_citations": 1}
{"id": 1320882, "s2_id": "2310fd01b720ca411bba5f76c494ad5c342a9751", "title": "Bounding Mean First Passage Times in Population Continuous-Time Markov Chains", "abstract": "We consider the problem of bounding mean first passage times for a class of continuous-time Markov chains that captures stochastic interactions between groups of identical agents. The quantitative analysis of such probabilistic population models is notoriously difficult since typically neither state-based numerical approaches nor methods based on stochastic sampling give efficient and accurate results. Here, we propose a technique that extends recently developed methods using semi-definite programming to determine bounds on mean first passage times. We further apply the technique to hybrid models and demonstrate its accuracy and efficiency for some examples from biology.", "venue": "QEST", "authors": ["Michael  Backenk\u00f6hler", "Luca  Bortolussi", "Verena  Wolf"], "year": 2020, "n_citations": 2}
{"id": 1321030, "s2_id": "f8d4ebee17b086fc2fadd17044c74fdc8a039abf", "title": "Efficient replication of queued tasks for latency reduction in cloud systems", "abstract": "In cloud computing systems, assigning a job to multiple servers and waiting for the earliest copy to finish is an effective method to combat the variability in response time of individual servers. Although adding redundant replicas always reduces service time, the total computing time spent per job may be higher, thus increasing waiting time in queue. The total time spent per job is also proportional to the cost of computing resources. We analyze how different redundancy strategies, for eg. number of replicas, and the time when they are issued and canceled, affect the latency and computing cost. We get the insight that the log-concavity of the service time distribution is a key factor in determining whether adding redundancy reduces latency and cost. If the service distribution is log-convex, then adding maximum redundancy reduces both latency and cost. And if it is log-concave, then having fewer replicas and canceling the redundant requests early is more effective.", "venue": "2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "authors": ["Gauri  Joshi", "Emina  Soljanin", "Gregory W. Wornell"], "year": 2015, "n_citations": 44}
{"id": 1324428, "s2_id": "0075afed51343aca4361f526d4319244fe260847", "title": "Smart Streaming for Online Video Services", "abstract": "Bandwidth cost is a significant concern for online video service providers. Today's video streaming systems mostly use HTTP streaming, with users accessing video segments as HTTP requests. A frequently used strategy is to serve all user requests as fast as possible, as if the user is downloading a file. The downloading rate can often far exceed the playback rate, when the system is below the peak load. This is known as progressive downloading. Since users may quit before viewing the complete video, however, much of the downloaded video can be \u201cwasted.\u201d By studying and exploiting the predictability of users' departure behavior , the authors developed a smart streaming strategy that can significantly improve overall streaming service quality under given server bandwidth. The improvement is achieved by avoiding the waste based on predicted user departure behavior. The proposed smart streaming technique is evaluated by modeling, analysis, and simulation, as well as experimentation using a prototype implementation.", "venue": "IEEE Transactions on Multimedia", "authors": ["Liang  Chen", "Yipeng  Zhou", "Dah-Ming  Chiu"], "year": 2015, "n_citations": 30}
{"id": 1325453, "s2_id": "133a518fda996d3a4e07d8b2edd547e949a672da", "title": "FFT Convolutions are Faster than Winograd on Modern CPUs, Here is Why", "abstract": "Winograd-based convolution has quickly gained traction as a preferred approach to implement convolutional neural networks (ConvNet) on various hardware platforms because it requires fewer floating point operations than FFT-based or direct convolutions. \nThis paper compares three highly optimized implementations (regular FFT--, Gauss--FFT--, and Winograd--based convolutions) on modern multi-- and many--core CPUs. Although all three implementations employed the same optimizations for modern CPUs, our experimental results with two popular ConvNets (VGG and AlexNet) show that the FFT--based implementations generally outperform the Winograd--based approach, contrary to the popular belief. \nTo understand the results, we use a Roofline performance model to analyze the three implementations in detail, by looking at each of their computation phases and by considering not only the number of floating point operations, but also the memory bandwidth and the cache sizes. The performance analysis explains why, and under what conditions, the FFT--based implementations outperform the Winograd--based one, on modern CPUs.", "venue": "ArXiv", "authors": ["Aleksandar  Zlateski", "Zhen  Jia", "Kai  Li", "Fr\u00e9do  Durand"], "year": 2018, "n_citations": 10}
{"id": 1331659, "s2_id": "0ab017f6f5f4453bac1a9c4b463871a7d80a2646", "title": "Towards High Performance, Portability, and Productivity: Lightweight Augmented Neural Networks for Performance Prediction", "abstract": "Writing high-performance code requires significant expertise in the programming language, compiler optimizations, and hardware knowledge. This often leads to poor productivity and portability and is inconvenient for a non-programmer domain-specialist such as a Physicist. More desirable is a high-level language where the domain-specialist simply specifies the workload in terms of high-level operations (e.g., matrix-multiply(A, B)), and the compiler identifies the best implementation fully utilizing the heterogeneous platform. For creating a compiler that supports productivity, portability, and performance simultaneously, it is crucial to predict the performance of various available implementations (variants) of the dominant operations (kernels) contained in the workload on various hardware to decide (a) which variant should be chosen for each kernel in the workload, and (b) on which hardware resource the variant should run. To enable the performance prediction, we propose lightweight augmented neural networks for arbitrary combinations of kernel-variant-hardware. A key innovation is utilizing the mathematical complexity of the kernels as a feature to achieve higher accuracy. These models are compact to reduce training time and allow fast inference during compile-time and run-time. Using models with less than 75 parameters, and only 250 training data instances, we are able to obtain accurate performance predictions, significantly outperforming traditional feed-forward neural networks on 48 kernel-variant-hardware combinations. We further demonstrate that our variant-selection approach can be used in Halide implementations to obtain up to 1.7x speedup over Halide auto-scheduler.", "venue": "2020 IEEE 27th International Conference on High Performance Computing, Data, and Analytics (HiPC)", "authors": ["Ajitesh  Srivastava", "Naifeng  Zhang", "Rajgopal  Kannan", "Viktor K. Prasanna"], "year": 2020, "n_citations": 2}
{"id": 1332199, "s2_id": "48fb99ede828bee5b55d447a0332a3a07b3dc0d0", "title": "Inhomogeneous CTMC Model of a Call Center with Balking and Abandonment", "abstract": "This paper considers a nonstationary multiserver queuing model with abandonment and balking for inbound call centers. We present a continuous time Markov chain (CTMC) model which captures the important characteristics of an inbound call center and obtain a numerical solution for its transient state probabilities using uniformization method with steady-state detection.", "venue": "ArXiv", "authors": ["Maciej Rafal Burak"], "year": 2015, "n_citations": 5}
{"id": 1332596, "s2_id": "e526e6b11995e8b873151e7f4274654dbb1dfdd7", "title": "Exploiting the Space Filling Curve Ordering of Particles in the Neighbour Search of Gadget3", "abstract": "Gadget3 is nowadays one of the most frequently used high performing parallel codes for cosmological hydrodynamical simulations. Recent analyses have shown t\\ hat the Neighbour Search process of Gadget3 is one of the most time-consuming parts. Thus, a considerable speedup can be expected from improvements of the u\\ nderlying algorithms. In this work we propose a novel approach for speeding up the Neighbour Search which takes advantage of the space-filling-curve particle ordering. Instead of performing Neighbour Search for all particles individually, nearby active particles can be grouped and one single Neighbour Search can be performed to obta\\ in a common superset of neighbours. Thus, with this approach we reduce the number of searches. On the other hand, tree walks are performed within a larger searching radius. There is an optimal size of grouping that maximize the speedup, which we found by numerical experiments. We tested the algorithm within the boxes of the Magneticum project. As a result we obtained a speedup of $1.65$ in the Density and of $1.30$ in the Hydrodynamics computation, respectively, and a total speedup of $1.34.$", "venue": "PARCO", "authors": ["Antonio  Ragagnin", "Nikola  Tchipev", "Michael  Bader", "Klaus  Dolag", "Nicolay J. Hammer"], "year": 2015, "n_citations": 5}
{"id": 1332665, "s2_id": "2b4c4a9bba64a2437a52e74bc398f1395d2665a2", "title": "Significant Diagnostic Counterexamples in Probabilistic Model Checking", "abstract": "This paper presents a novel technique for counterexample generation in probabilistic model checking of Markov chains and Markov Decision Processes. (Finite) paths in counterexamples are grouped together in witnesses that are likely to provide similar debugging information to the user. We list five properties that witnesses should satisfy in order to be useful as debugging aid: similarity, accuracy, originality, significance, and finiteness. Our witnesses contain paths that behave similarly outside strongly connected components. \n \nThen, we show how to compute these witnesses by reducing the problem of generating counterexamples for general properties over Markov Decision Processes, in several steps, to the easy problem of generating counterexamples for reachability properties over acyclic Markov chains.", "venue": "Haifa Verification Conference", "authors": ["Miguel E. Andr\u00e9s", "Pedro R. D'Argenio", "Peter van Rossum"], "year": 2008, "n_citations": 61}
{"id": 1336946, "s2_id": "523f2452cf4180ec7adca4cbefad05101d221799", "title": "On Mean Field Convergence and Stationary Regime", "abstract": "Assume that a family of stochastic processes on some Polish space $E$ converges to a deterministic process; the convergence is in distribution (hence in probability) at every fixed point in time. This assumption holds for a large family of processes, among which many mean field interaction models and is weaker than previously assumed. We show that any limit point of an invariant probability of the stochastic process is an invariant probability of the deterministic process. The results are valid in discrete and in continuous time.", "venue": "ArXiv", "authors": ["Michel  Bena\u00efm", "Jean-Yves Le Boudec"], "year": 2011, "n_citations": 32}
{"id": 1337236, "s2_id": "8be3b581e6a4ead4c7d4e97176c06f2c3337ad5d", "title": "An In-Depth Investigation of Performance Characteristics of Hyperledger Fabric", "abstract": "Private permissioned blockchains, such as Hyperledger Fabric, are widely deployed across the industry to facilitate cross-organizational processes and promise improved performance compared to their public counterparts. However, the lack of empirical and theoretical results prevent precise prediction of the real-world performance. We address this gap by conducting an in-depth performance analysis of Hyperledger Fabric. The paper presents a detailed compilation of various performance characteristics using an enhanced version of the Distributed Ledger Performance Scan. Researchers and practitioners alike can use the results as guidelines to better configure and implement their blockchains and utilize the DLPS framework to conduct their measurements.", "venue": "ArXiv", "authors": ["Tobias  Guggenberger", "Johannes  Sedlmeir", "Gilbert  Fridgen", "Andr'e  Luckow"], "year": 2021, "n_citations": 1}
{"id": 1337348, "s2_id": "e0e9415dd76cb3981454f40805e9acb94c80495e", "title": "Durable Queues: The Second Amendment", "abstract": "We consider durable data structures for non-volatile main memory, such as the new Intel Optane memory architecture. Substantial recent work has concentrated on making concurrent data structures durable with low overhead, by adding a minimal number of blocking persist operations (i.e., flushes and fences). In this work we show that focusing on minimizing the number of persist instructions is important, but not enough. We show that access to flushed content is of high cost due to cache invalidation in current architectures. Given this finding, we present a design of the queue data structure that properly takes care of minimizing blocking persist operations as well as minimizing access to flushed content. The proposed design outperforms state-of-the-art durable queues. We start by providing a durable version of the Michael Scott queue (MSQ ). We amend MSQ by adding a minimal number of persist instructions, fewer than in available durable queues, and meeting the theoretical lower bound on the number of blocking persist operations. We then proceed with a second amendment to this design, that eliminates accesses to flushed data. Evaluation shows that the second amendment yields substantial performance improvement, outperforming the state of the art and demonstrating the importance of reduced accesses to flushed content. The presented queues are durably linearizable and lock-free. Finally, we discuss the theoretical optimal number of accesses to flushed content.", "venue": "SPAA", "authors": ["Gal  Sela", "Erez  Petrank"], "year": 2021, "n_citations": 0}
{"id": 1338230, "s2_id": "5a6bbe1f1bac479a66af053b17c5710539e130cc", "title": "uFLIP: Understanding Flash IO Patterns", "abstract": "Does the advent of flash devices constitute a radical change for secondary storage? How should database systems adapt to this new form of secondary storage? Before we can answer these questions, we need to fully understand the performance characteristics of flash devices. More specifically, we want to establish what kind of IOs should be favored (or avoided) when designing algorithms and architectures for flash-based systems. In this paper, we focus on flash IO patterns, that capture relevant distribution of IOs in time and space, and our goal is to quantify their performance. We define uFLIP, a benchmark for measuring the response time of flash IO patterns. We also present a benchmarking methodology which takes into account the particular characteristics of flash devices. Finally, we present the results obtained by measuring eleven flash devices, and derive a set of design hints that should drive the development of flash-based systems on current devices.", "venue": "CIDR", "authors": ["Luc  Bouganim", "Bj\u00f6rn \u00de\u00f3r J\u00f3nsson", "Philippe  Bonnet"], "year": 2009, "n_citations": 206}
{"id": 1344060, "s2_id": "f6168120c4a541a7a57fd95b79847786637780c4", "title": "Power Consumption of Virtualization Technologies: An Empirical Investigation", "abstract": "Virtualization is growing rapidly as a result of the increasing number of alternative solutions in this area, and of the wide range of application field. Until now, hypervisor-based virtualization has been the de facto solution to perform server virtualization. Recently, container-based virtualization -- an alternative to hypervisors -- has gained more attention because of lightweight characteristics, attracting cloud providers that have already made use of it to deliver their services. However, a gap in the existing research on containers exists in the area of power consumption. This paper presents the results of a performance comparison in terms of power consumption of four different virtualization technologies: KVM and Xen, which are based on hypervisor virtualization, Docker and LXC which are based on container virtualization. The aim of this empirical investigation, carried out by means of a testbed, is to understand how these technologies react to particular workloads. Our initial results show how, despite of the number of virtual entities running, both kinds of virtualization alternatives behave similarly in idle state and in CPU/Memory stress test. Contrarily, the results on network performance show differences between the two technologies.", "venue": "2015 IEEE/ACM 8th International Conference on Utility and Cloud Computing (UCC)", "authors": ["Roberto  Morabito"], "year": 2015, "n_citations": 70}
{"id": 1350266, "s2_id": "83ebbe77f66084334cb6bde58aba392a23e4327c", "title": "FIRESTARTER 2: Dynamic Code Generation for Processor Stress Tests", "abstract": "Processor stress tests target to maximize processor power consumption by executing highly demanding workloads. They are typically used to test the cooling and electrical infrastructure of compute nodes or larger systems in labs or data centers. While multiple of these tools already exists, they have to be re-evaluated and updated regularly to match the developments in computer architecture. This paper presents the first major update of FIRESTARTER, an Open Source tool specifically designed to create near-peak power consumption. The main new features concern the online generation of workloads and automatic self-tuning for specific hardware configurations. We further apply these new features on an AMD Rome system and demonstrate the optimization process. Our analysis shows how accesses to the different levels of the memory hierarchy contribute to the overall power consumption. Finally, we demonstrate how the auto-tuning algorithm can cope with different processor configurations and how these influence the effectiveness of the created workload.", "venue": "2021 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Robert  Sch\u00f6ne", "Markus  Schmidl", "Mario  Bielert", "Daniel  Hackenberg"], "year": 2021, "n_citations": 1}
{"id": 1350739, "s2_id": "39a8a6ff8bae414476090dcaa7787afb50136143", "title": "Incentivizing sharing in realtime D2D streaming networks: A mean field game perspective", "abstract": "We consider the problem of streaming live content to a cluster of co-located wireless devices that have both an expensive unicast base-station-to-device (B2D) interface, as well as an inexpensive broadcast device-to-device (D2D) interface, which can be used simultaneously. Our setting is a streaming system that uses a block-by-block random linear coding approach to achieve a target percentage of on-time deliveries with minimal B2D usage. Our goal is to design an incentive framework that would promote such cooperation across devices, while ensuring good quality of service. Based on ideas drawn from truth-telling auctions, we design a mechanism that achieves this goal via appropriate transfers (monetary payments or rebates) in a setting with a large number of devices, and with peer arrivals and departures. Here, we show that a Mean Field Game can be used to accurately approximate our system. Furthermore, the complexity of calculating the best responses under this regime is low. We implement the proposed system on an Android testbed, and illustrate its efficient performance using real world experiments.", "venue": "2015 IEEE Conference on Computer Communications (INFOCOM)", "authors": ["Jian  Li", "Rajarshi  Bhattacharyya", "Suman  Paul", "Srinivas  Shakkottai", "Vijay  Subramanian"], "year": 2015, "n_citations": 57}
{"id": 1353759, "s2_id": "239c369b091083b2580f36ecd96f287d80de0a0f", "title": "Performance Evaluation of Mixed-Precision Runge-Kutta Methods", "abstract": "Additive Runge-Kutta methods designed for preserving highly accurate solutions in mixed-precision computation were proposed and analyzed in [8]. These specially designed methods use reduced precision for the implicit computations and full precision for the explicit computations. We develop a FORTRAN code to solve a nonlinear system of ordinary differential equations using the mixed precision additive Runge-Kutta (MP-ARK) methods on IBM POWER9 and Intel x86_64 chips. The convergence, accuracy, runtime, and energy consumption of these methods is explored. We show that these MP-ARK methods efficiently produce accurate solutions with significant reductions in runtime (and by extension energy consumption).", "venue": "2021 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Ben  Burnett", "Sigal  Gottlieb", "Zachary J. Grant", "Alfa R. H. Heryudono"], "year": 2021, "n_citations": 1}
{"id": 1355766, "s2_id": "85f7efff823e8868c72fe8e818490ef0986c7cdc", "title": "Accelerating Lossless Data Compression with GPUs", "abstract": "Huffman compression is a statistical, lossless, data compression algorithm that compresses data by assigning variable length codes to symbols, with the more frequently appearing symbols given shorter codes than the less. This work is a modification of the Huffman algorithm which permits uncompressed data to be decomposed into indepen- dently compressible and decompressible blocks, allowing for concurrent compression and decompression on multiple processors. We create implementations of this modified algorithm on a current NVIDIA GPU using the CUDA API as well as on a current Intel chip and the performance results are compared, showing favorable GPU performance for nearly all tests. Lastly, we discuss the necessity for high performance data compression in today's supercomputing ecosystem.", "venue": "ArXiv", "authors": ["Robert Louis Cloud", "Matthew L. Curry", "Lee  Ward", "Anthony  Skjellum", "Purushotham  Bangalore"], "year": 2011, "n_citations": 20}
{"id": 1358735, "s2_id": "4540ad64da31531cbb00ad26ae050bcbf856238a", "title": "Comparative study of finite element methods using the Time-Accuracy-Size (TAS) spectrum analysis", "abstract": "We present a performance analysis appropriate for comparing algorithms using different numerical discretizations. By taking into account the total time-to-solution, numerical accuracy with respect to an error norm, and the computation rate, a cost-benefit analysis can be performed to determine which algorithm and discretization are particularly suited for an application. This work extends the performance spectrum model in Chang et. al. 2017 for interpretation of hardware and algorithmic tradeoffs in numerical PDE simulation. As a proof-of-concept, popular finite element software packages are used to illustrate this analysis for Poisson's equation.", "venue": "SIAM J. Sci. Comput.", "authors": ["Justin  Chang", "Maurice S. Fabien", "Matthew G. Knepley", "Richard T. Mills"], "year": 2018, "n_citations": 7}
{"id": 1359002, "s2_id": "2e674b27897f174c718ea60000627c0bd0e499bb", "title": "Enhancing Scalability of a Matrix-Free Eigensolver for Studying Many-Body Localization", "abstract": "In [Van Beeumen, et. al, HPC Asia 2020, https://www.doi.org/10.1145/3368474.3368497] a scalable and matrix-free eigensolver was proposed for studying the many-body localization (MBL) transition of two-level quantum spin chain models with nearest-neighbor $XX+YY$ interactions plus $Z$ terms. This type of problem is computationally challenging because the vector space dimension grows exponentially with the physical system size, and averaging over different configurations of the random disorder is needed to obtain relevant statistical behavior. For each eigenvalue problem, eigenvalues from different regions of the spectrum and their corresponding eigenvectors need to be computed. Traditionally, the interior eigenstates for a single eigenvalue problem are computed via the shift-and-invert Lanczos algorithm. Due to the extremely high memory footprint of the LU factorizations, this technique is not well suited for large number of spins $L$, e.g., one needs thousands of compute nodes on modern high performance computing infrastructures to go beyond $L = 24$. The matrix-free approach does not suffer from this memory bottleneck, however, its scalability is limited by a computation and communication imbalance. We present a few strategies to reduce this imbalance and to significantly enhance the scalability of the matrix-free eigensolver. To optimize the communication performance, we leverage the consistent space runtime, CSPACER, and show its efficiency in accelerating the MBL irregular communication patterns at scale compared to optimized MPI non-blocking two-sided and one-sided RMA implementation variants. The efficiency and effectiveness of the proposed algorithm is demonstrated by computing eigenstates on a massively parallel many-core high performance computer.", "venue": "ArXiv", "authors": ["Roel Van Beeumen", "Khaled Z. Ibrahim", "Gregory D. Kahanamoku-Meyer", "Norman Y. Yao", "Chao  Yang"], "year": 2020, "n_citations": 0}
{"id": 1360950, "s2_id": "57938e2023946288f5060d4330d87ff2f4f59076", "title": "CoShare: An Efficient Approach for Redundancy Allocation in NFV", "abstract": "An appealing feature of Network Function Virtualization (NFV) is that in an NFV-based network, a network function (NF) instance may be placed at any node. This, on the one hand, offers great flexibility in redundancy allocation to meet the availability requirements of flows; on the other hand, it makes the challenge unique and difficult. One particular highlight is that there is inherent correlation among nodes due to the structure of the network, implying that special care is needed for redundancy allocation in NFV-based networks. To this aim, a novel approach, called CoShare, is proposed. Originally, its design takes into consideration the effect of network structural dependency. In addition, to efficiently make use of resources, CoShare proposes the idea of shared reservation, where multiple flows may be allowed to share the same reserved backup capacity at an NF instance. Furthermore, CoShare factors in the heterogeneity in nodes, NF instances and availability requirements of flows in the design. The results from a number of experiments conducted using realistic network topologies show that CoShare is able to meet diverse availability requirements in a resource-efficient manner, requiring less resource overbuild than using the idea of dedicated reservation commonly adopted for redundancy allocation in NFV.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Yordanos Tibebu Woldeyohannes", "Besmir  Tola", "Yuming  Jiang", "K. K. Ramakrishnan"], "year": 2021, "n_citations": 0}
{"id": 1364425, "s2_id": "6690109fd149cd5e6770ac8697af04feda6e5869", "title": "Hybrid CPU-GPU Generation of the Hamiltonian and Overlap Matrices in FLAPW Methods", "abstract": "In this paper we focus on the integration of high-performance numerical libraries in ab initio codes and the portability of performance and scalability. The target of our work is FLEUR, a software for electronic structure calculations developed in the Forschungszentrum Julich over the course of two decades. The presented work follows up on a previous effort to modernize legacy code by re-engineering and rewriting it in terms of highly optimized libraries. We illustrate how this initial effort to get efficient and portable shared-memory code enables fast porting of the code to emerging heterogeneous architectures. More specifically, we port the code to nodes equipped with multiple GPUs. We divide our study in two parts. First, we show considerable speedups attained by minor and relatively straightforward code changes to off-load parts of the computation to the GPUs. Then, we identify further possible improvements to achieve even higher performance and scalability. On a system consisting of 16-cores and 2 GPUs, we observe speedups of up to 5\\(\\times \\) with respect to our optimized shared-memory code, which in turn means between 7.5\\(\\times \\) and 12.5\\(\\times \\) speedup with respect to the original FLEUR code.", "venue": "JHPCS", "authors": ["Diego  Fabregat-Traver", "Davor  Davidovic", "Markus  H\u00f6hnerbach", "Edoardo Di Napoli"], "year": 2016, "n_citations": 1}
{"id": 1364998, "s2_id": "e3855ef20b300611647cb755fc2f68977420d63a", "title": "Performance Evaluation of an OMPR Algorithm for Route Discovery in Noisy MANETs", "abstract": "It has been revealed in the literature that pure multipoint relaying (MPR) algorithms demonstrate both simplicity and outstanding performance, as compared to other flooding algorithms in wireless networks. One drawback of pure MPR algorithms is that the selected forwarding set may not represent the optimum selection. In addition, little efforts have been carried-out to investigate the performance of such algorithms in noisy mobile ad hoc networks (MANETs) suffering from high packet-loss and node mobility. In this paper, we develop and evaluate the performance of an optimal MPR (OMPR) algorithm for route discovery in noisy MANETs. The main feature of this new algorithm is that it calculates all possible sets of multipoint relays (MPRs) and then selects the set with minimum number of nodes. The algorithm demonstrates an excellent performance when it is compared with other route discovery algorithms as it achieves the highest cost-effective reachability.", "venue": "ArXiv", "authors": ["Hussein  Al-Bahadili", "Rami  Jaradat"], "year": 2010, "n_citations": 17}
{"id": 1367130, "s2_id": "4c77fb4565922930fe97d699250949e693c409f6", "title": "Where to Encode: A Performance Analysis of x86 and Arm-based Amazon EC2 Instances", "abstract": "Video streaming became an undivided part of the Internet. To efficiently utilise the limited network bandwidth it is essential to encode the video content. However, encoding is a computationally intensive task, involving high-performance resources provided by private infrastructures or public clouds. Public clouds, such as Amazon EC2, provide a large portfolio of services and instances optimized for specific purposes and budgets. The majority of Amazon\u2019s instances use x86 processors, such as Intel Xeon or AMD EPYC. However, following the recent trends in computer architecture, Amazon introduced Arm-based instances that promise up to 40% better cost performance ratio than comparable x86 instances for specific workloads. We evaluate in this paper the video encoding performance of x86 and Arm instances of four instance families using the latest FFmpeg version and two video codecs. We examine the impact of the encoding parameters, such as different presets and bitrates, on the time and cost for encoding. Our experiments reveal that Arm instances show high time and cost saving potential of up to 33.63% for specific bitrates and presets, especially for the x264 codec. However, the x86 instances are more general and achieve low encoding times, regardless of the codec.", "venue": "2021 IEEE 17th International Conference on eScience (eScience)", "authors": ["Roland  Math'a", "Dragi  Kimovski", "Anatoliy  Zabrovskiy", "Christian  Timmerer", "Radu  Prodan"], "year": 2021, "n_citations": 0}
{"id": 1369507, "s2_id": "899fc7a94a138eb82e0a988fc01e0c8aafd5481a", "title": "An Analytical Model for Evaluating Outage and Handover Probability of Cellular Wireless Networks", "abstract": "We consider stochastic cellular networks where base stations locations form a homogeneous Poisson point process and each mobile is attached to the base station that provides the best mean signal power. The mobile is in outage if the signal-to-noise-plus-interference ratio falls below some threshold. The handover decision has to be made if the mobile is in outage during several time slots. The outage probability and the handover probabilities are evaluated taking into account the effect of path loss, shadowing, Rayleigh fast fading, frequency factor reuse and conventional beamforming. The main assumption is that the Rayleigh fast fading changes at each time slot while other network components remain static during the period of study.", "venue": "The 15th International Symposium on Wireless Personal Multimedia Communications", "authors": ["T. T. Vu", "Laurent  Decreusefond", "Philippe  Martins"], "year": 2012, "n_citations": 43}
{"id": 1370596, "s2_id": "7f71394dcfcff3df6265c6080e20ac02c794b199", "title": "Criticisms of Modelling Packet Traffic Using Long-Range Dependence", "abstract": "This paper criticises the notion that long-range dependence is an important contributor to the queuing behaviour of real Internet traffic. The idea is questioned in two different ways. Firstly, a class of models used to simulate Internet traffic is shown to have important theoretical flaws. It is shown that this behaviour is inconsistent with the behaviour of real traffic traces. Secondly, the notion that long-range correlations significantly affects the queuing performance of traffic is investigated by destroying those correlations in real traffic traces (by reordering). It is shown that the longer ranges of correlations are not important to mean queue length except in one case with an extremely high load.", "venue": "2009 Proceedings of 18th International Conference on Computer Communications and Networks", "authors": ["Richard G. Clegg", "Raul  Landa", "Miguel  Rio"], "year": 2009, "n_citations": 2}
{"id": 1374389, "s2_id": "3fa5ba7ab2c2fe5f6851383d2656efd7059e1f84", "title": "Session Initiation Protocol (SIP) Server Overload Control: Design and Evaluation", "abstract": "A Session Initiation Protocol (SIP) server may be overloaded by emergency-induced call volume, \"American Idol\" style flash crowd effects or denial of service attacks. The SIP server overload problem is interesting especially because the costs of serving or rejecting a SIP session can be similar. For this reason, the built-in SIP overload control mechanism based on generating rejection messages cannot prevent the server from entering congestion collapse under heavy load. The SIP overload problem calls for a pushback control solution in which the potentially overloaded receiving server may notify its upstream sending servers to have them send only the amount of load within the receiving server's processing capacity. The pushback framework can be achieved by either a rate-based feedback or a window-based feedback. The centerpiece of the feedback mechanism is the algorithm used to generate load regulation information. We propose three new window-based feedback algorithms and evaluate them together with two existing rate-based feedback algorithms. We compare the different algorithms in terms of the number of tuning parameters and performance under both steady and variable load. Furthermore, we identify two categories of fairness requirements for SIP overload control, namely, user-centric and provider-centric fairness. With the introduction of a new double-feed SIP overload control architecture, we show how the algorithms can meet those fairness criteria.", "venue": "IPTComm", "authors": ["Charles  Shen", "Henning  Schulzrinne", "Erich M. Nahum"], "year": 2008, "n_citations": 84}
{"id": 1374910, "s2_id": "9a3e443822f2891f7726cd6aaff2d76c0a6c7337", "title": "On the catalyzing effect of randomness on the per-flow throughput in wireless networks", "abstract": "This paper investigates the throughput capacity of a flow crossing a multi-hop wireless network, whose geometry is characterized by general randomness laws including Uniform, Poisson, Heavy-Tailed distributions for both the nodes' densities and the number of hops. The key contribution is to demonstrate how the per-flow throughput depends on the distribution of 1) the number of nodes Nj inside hops' interference sets, 2) the number of hops K, and 3) the degree of spatial correlations. The randomness in both Nj's and K is advantageous, i.e., it can yield larger scalings (as large as \u0398(n)) than in non-random settings. An interesting consequence is that the per-flow capacity can exhibit the opposite behavior to the network capacity, which was shown to suffer from a logarithmic decrease in the presence of randomness. In turn, spatial correlations along the end-to-end path are detrimental by a logarithmic term.", "venue": "IEEE INFOCOM 2014 - IEEE Conference on Computer Communications", "authors": ["Florin  Ciucu", "Jens B. Schmitt"], "year": 2014, "n_citations": 6}
{"id": 1377961, "s2_id": "91a74b9ff7425c427c17f0f670dd7fc0f1bef069", "title": "Data Motif-based Proxy Benchmarks for Big Data and AI Workloads", "abstract": "For the architecture community, reasonable simulation time is a strong requirement in addition to performance data accuracy. However, emerging big data and AI workloads are too huge at binary size level and prohibitively expensive to run on cycle-accurate simulators. The concept of data motif, which is identified as a class of units of computation performed on initial or intermediate data, is the first step towards building proxy benchmark to mimic the real-world big data and AI workloads. However, there is no practical way to construct a proxy benchmark based on the data motifs to help simulation based research.In this paper, we embark on a study to bridge the gap between data motif and a practical proxy benchmark. We propose a data motif-based proxy benchmark generating methodology by means of machine learning method, which combine data motifs with different weights to mimic the big data and AI workloads. Furthermore, we implement various data motifs using light-weight stacks and apply the methodology to five real-world workloads to construct a suite of proxy benchmarks, considering the data types, patterns, and distributions. The evaluation results show that our proxy benchmarks shorten the execution time by 100s times on real systems while maintaining the average system and micro-architecture performance data accuracy above 90%, even changing the input data sets or cluster configurations. Moreover, the generated proxy benchmarks reflect consistent performance trends across different architectures. To facilitate the community, we will release the proxy benchmarks on the project homepage http://prof.ict.ac.cn/BigDataBench.", "venue": "2018 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Wanling  Gao", "Jianfeng  Zhan", "Lei  Wang", "Chunjie  Luo", "Zhen  Jia", "Daoyi  Zheng", "Chen  Zheng", "Xiwen  He", "Hainan  Ye", "Haibin  Wang", "Rui  Ren"], "year": 2018, "n_citations": 7}
{"id": 1378035, "s2_id": "131fb7d5c178e255e4ce0b5afa5e858693839898", "title": "Strong, Weak and Branching Bisimulation for Transition Systems and Markov Reward Chains: A Unifying Matrix Approach", "abstract": "We first study labeled transition systems with explicit successful termination. We establish the notions of strong, weak, and branching bisimulation in terms of boolean matrix theory, introducing thus a novel and powerful algebraic apparatus. Next we consider Markov reward chains which are standardly presented in real matrix theory. By interpreting the obtained matrix conditions for bisimulations in this setting, we automatically obtain the definitions of strong, weak, and branching bisimulation for Markov reward chains. The obtained strong and weak bisimulations are shown to coincide with some existing notions, while the obtained branching bisimulation is new, but its usefulness is questionable.", "venue": "QFM", "authors": ["Nikola  Trcka"], "year": 2009, "n_citations": 5}
{"id": 1380031, "s2_id": "330d51b4706d9f4f9886b6370d1407ac86b08a11", "title": "An online delay efficient packet scheduler for M2M traffic in industrial automation", "abstract": "Some Machine-to-Machine (M2M) communication links particularly those in a industrial automation plant have stringent latency requirements. In this paper, we study the delay-performance for the M2M uplink from the sensors to a Programmable Logic Controller (PLC) in a industrial automation scenario. The uplink traffic can be broadly classified as either Periodic Update (PU) and Event Driven (ED). The PU arrivals from different sensors are periodic, synchronized by the PLC and need to be processed by a prespecified firm latency deadline. On the other hand, the ED arrivals are random, have low-arrival rate, but may need to be processed quickly depending upon the criticality of the application. To accommodate these contrasting Quality-of-Service (QoS) requirements, we model the utility of PU and ED packets using step function and sigmoidal functions of latency respectively. Our goal is to maximize the overall system utility while being proportionally fair to both PU and ED data. To this end, we propose a novel online QoS-aware packet scheduler that gives priority to ED data as long as that results the latency deadline is met for PU data. However as the size of networks increases, we drop the PU packets that fail to meet latency deadline which reduces congestion and improves overall system utility. Using extensive simulations, we compare the performance of our scheme with various scheduling policies such as First-Come-First-Serve (FCFS), Earliest-Due-Date (EDD) and (preemptive) priority. We show that our scheme outperforms the existing schemes for various simulation scenarios.", "venue": "2016 Annual IEEE Systems Conference (SysCon)", "authors": ["Akshay  Kumar", "Ahmed  Abdel-Hadi", "T. Charles Clancy"], "year": 2016, "n_citations": 26}
{"id": 1383354, "s2_id": "722d16deba0bb17da6e63a4c05829aee1c7f65ee", "title": "Quantitative Impact Evaluation of an Abstraction Layer for Data Stream Processing Systems", "abstract": "With the demand to process ever-growing data volumes, a variety of new data stream processing frameworks have been developed. Moving an implementation from one such system to another, e.g., for performance reasons, requires adapting existing applications to new interfaces. Apache Beam addresses these high substitution costs by providing an abstraction layer that enables executing programs on any of the supported streaming frameworks. In this paper, we present a novel benchmark architecture for comparing the performance impact of using Apache Beam on three streaming frameworks: Apache Spark Streaming, Apache Flink, and Apache Apex. We find significant performance penalties when using Apache Beam for application development in the surveyed systems. Overall, usage of Apache Beam for the examined streaming applications caused a high variance of query execution times with a slowdown of up to a factor of 58 compared to queries developed without the abstraction layer. All developed benchmark artifacts are publicly available to ensure reproducible results.", "venue": "2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)", "authors": ["Guenter  Hesse", "Christoph  Matthies", "Kelvin  Glass", "Johannes  Huegle", "Matthias  Uflacker"], "year": 2019, "n_citations": 0}
{"id": 1387359, "s2_id": "2dd9be6ebeb921592c4e9e44d585ba62fcb49a7e", "title": "What slows you down? Your network or your device?", "abstract": "This study takes a close look at mobile web performance. The two main parameters determining web page load time are the network speed and the computing power of the end-user device. Based on data from real users, this paper quantifies the relative importance of network and device. The findings suggest that increased processing power of latest generation smart phones and optimized browsers have a significant impact on web performance; up to 56% reduction in median page load time from one generation to the following. The cellular networks, on the other hand, have become so mature that the median page load time on one fiber-to-the-home network (using wifi for the last meter) is only 18-28% faster than cellular and the median page load time on one DSL network is 19% slower compared to a well-deployed cellular network.", "venue": "ArXiv", "authors": ["Moritz  Steiner", "Ruomei  Gao"], "year": 2016, "n_citations": 5}
{"id": 1388866, "s2_id": "e97ab7b6b4e68d3ad794595a88bc8761fb28d694", "title": "Event Trend Aggregation Under Rich Event Matching Semantics", "abstract": "Streaming applications from cluster monitoring to algorithmic trading deploy Kleene queries to detect and aggregate event trends. Rich event matching semantics determine how to compose events into trends. The expressive power of state-of-the-art streaming systems remains limited since they do not support many of these semantics. Worse yet, they suffer from long delays and high memory costs because they maintain aggregates at a fine granularity. To overcome these limitations, our Coarse-Grained Event Trend Aggregation (Cogra) approach supports a rich variety of event matching semantics within one system. Better yet, Cogra incrementally maintains aggregates at the coarsest granularity possible for each of these semantics. In this way, Cogra minimizes the number of aggregates -- reducing both time and space complexity. Our experiments demonstrate that Cogra achieves up to six orders of magnitude speed-up and up to seven orders of magnitude memory reduction compared to state-of-the-art approaches.", "venue": "SIGMOD Conference", "authors": ["Olga  Poppe", "Chuan  Lei", "Elke A. Rundensteiner", "David  Maier"], "year": 2019, "n_citations": 6}
{"id": 1389416, "s2_id": "fe8e30310a6ed07cfd0cd85551c35a246f95aa8e", "title": "ExPAN(N)D: Exploring Posits for Efficient Artificial Neural Network Design in FPGA-Based Systems", "abstract": "The high computational complexity, memory footprints, and energy requirements of machine learning models, such as Artificial Neural Networks (ANNs), hinder their deployment on resource-constrained embedded systems. Most state-of-the-art works have considered this problem by proposing various low bit-width data representation schemes and optimized arithmetic operators\u2019 implementations. To further elevate the implementation gains offered by these individual techniques, there is a need to cross-examine and combine these techniques\u2019 unique features. This paper presents ExPAN(N)D, a framework to analyze and ingather the efficacy of the Posit number representation scheme and the efficiency of fixed-point arithmetic implementations for ANNs. The Posit scheme offers a better dynamic range and higher precision for various applications than IEEE 754 single-precision floating-point format. However, due to the dynamic nature of the various fields of the Posit scheme, the corresponding arithmetic circuits have higher critical path delay and resource requirements than the single-precision-based arithmetic units. Towards this end, we propose a novel Posit to fixed-point converter for enabling high-performance and energy-efficient hardware implementations for ANNs with minimal drop in the output accuracy. We also propose a modified Posit-based representation to store the trained parameters of a network. With the proposed Posit to fixed-point converter-based designs, we provide multiple design points with varying accuracy-performance trade-offs for an ANN. For instance, compared to the lowest power dissipating Posit-only accelerator design, one of our proposed designs results in 80% and 48% reduction in power dissipation and LUT utilization respectively, with marginal increase in classification error for Imagenet dataset classification using VGG-16.", "venue": "IEEE Access", "authors": ["Suresh  Nambi", "Salim  Ullah", "Aditya  Lohana", "Siva Satyendra Sahoo", "Farhad  Merchant", "Akash  Kumar"], "year": 2021, "n_citations": 4}
{"id": 1393819, "s2_id": "f6a91b9c4bb49bbde172636f417fb6640cb95b96", "title": "Achieving Optimal Throughput and Near-Optimal Asymptotic Delay Performance in Multichannel Wireless Networks With Low Complexity: A Practical Greedy Scheduling Policy", "abstract": "In this paper, we focus on the scheduling problem in multichannel wireless networks, e.g., the downlink of a single cell in fourth-generation (4G) OFDM-based cellular networks. Our goal is to design practical scheduling policies that can achieve provably good performance in terms of both throughput and delay, at a low complexity. While a class of O(n2.5 log n)-complexity hybrid scheduling policies is recently developed to guarantee both rate-function delay optimality (in the many-channel many-user asymptotic regime) and throughput optimality (in the general non-asymptotic setting), their practical complexity is typically high. To address this issue, we develop a simple greedy policy called Delay-based Server-Side-Greedy (D-SSG) with a lower complexity 2n2+2n, and rigorously prove that D-SSG not only achieves throughput optimality, but also guarantees near-optimal asymptotic delay performance. Specifically, the rate-function of the delay-violation probability attained by D-SSG for any fixed integer delay threshold b > 0 is no smaller than the maximum achievable rate-function by any scheduling policy for threshold b-1. Thus, we are able to achieve a reduction in complexity (from O(n2.5 logn) of the hybrid policies to 2n2 + 2n) with a minimal drop in the delay performance. More importantly, in practice, D-SSG generally has a substantially lower complexity than the hybrid policies that typically have a large constant factor hidden in the O(\u00b7) notation. Finally, we conduct simulations to validate our theoretical results in various scenarios. The simulation results show that in all scenarios we consider, D-SSG not only guarantees a near-optimal rate-function, but also empirically has a similar delay performance to the rate-function delay-optimal policies.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Bo  Ji", "Gagan Raj Gupta", "Manu  Sharma", "Xiaojun  Lin", "Ness B. Shroff"], "year": 2015, "n_citations": 16}
{"id": 1394039, "s2_id": "cadf2ab42e9e3bb3cf4bd6301e95a9537ad36029", "title": "IFOSMONDI Co-simulation Algorithm with Jacobian-Free Methods in PETSc", "abstract": "IFOSMONDI iterative algorithm for implicit co-simulation of coupled physical systems (introduced by the authors in july 2019 during the Simultech conference, p.176-186) enables us to solve the nonlinear coupling function while keeping the smoothness of interfaces without introducing a delay. Moreover, it automatically adapts the size of the steps between data exchanges among the systems according to the difficulty of the solving of the coupling constraint. The latter was solved by a fixed-point algorithm in the original implementation whereas this paper introduces the JFM version (standing for Jacobian-Free Methods). Most implementations of Newton-like methods require a jacobian matrix which can be difficult to compute in the co-simulation context, except in the case where the interfaces are represented by a Zero-Order-Hold (ZOH). As far as IFOSMONDI coupling algorithm uses Hermite interpolation for smoothness enhancement (up to Third-Order-Hold), we propose hereafter a new formulation of the non-linear coupling function including both the values and the time-derivatives of the coupling variables. This formulation is well designed for solving the coupling through jacobianfree Newton type methods. Consequently, successive function evaluations consist in multiple simulations of the systems on a co-simulation timestep using rollback. The orchestrator-workers structure of the algorithm enables us to combine the PETSc framework on the orchestrator side for the non-linear Newton-type solvers with the parallel integrations of the systems on the workers side thanks to MPI processes. Different nonlinear methods will be compared to one another and to the original fixedpoint implementation on a newly proposed 2-systems academic test-case (mass-spring-damper type) with direct feedthrough on both sides.", "venue": "ArXiv", "authors": ["Yohan  Eguillon", "Bruno  Lacabanne", "Damien  Tromeur-Dervout"], "year": 2021, "n_citations": 0}
{"id": 1394268, "s2_id": "cbababa11981ff9b6a95d30ab1fb4710f1f42b76", "title": "Ergodic Theory for Controlled Markov Chains with Stationary Inputs", "abstract": "Consider a stochastic process $\\{X(t)\\}$ on a finite state space $ {\\sf X}=\\{1,\\dots, d\\}$. It is conditionally Markov, given a real-valued `input process' $\\{\\zeta(t)\\}$. This is assumed to be small, which is modeled through the scaling, \\[ \\zeta_t = \\varepsilon \\zeta^1_t, \\qquad 0\\le \\varepsilon \\le 1\\,, \\] where $\\{\\zeta^1(t)\\}$ is a bounded stationary process. The following conclusions are obtained, subject to smoothness assumptions on the controlled transition matrix and a mixing condition on $\\{\\zeta(t)\\}$: \n(i) A stationary version of the process is constructed, that is coupled with a stationary version of the Markov chain $\\{X^\\bullet$(t)\\}obtained with $\\{\\zeta(t)\\}\\equiv 0$. The triple $(\\{X(t)\\}, \\{X^\\bullet(t)\\},\\{\\zeta(t)\\})$ is a jointly stationary process satisfying \\[ {\\sf P}\\{X(t) \\neq X^\\bullet(t)\\} = O(\\varepsilon) \\] Moreover, a second-order Taylor-series approximation is obtained: \\[ {\\sf P}\\{X(t) =i \\} ={\\sf P}\\{X^\\bullet(t) =i \\} + \\varepsilon^2 \\varrho(i) + o(\\varepsilon^2),\\quad 1\\le i\\le d, \\] with an explicit formula for the vector $\\varrho\\in\\mathbb{R}^d$. \n(ii) For any $m\\ge 1$ and any function $f\\colon \\{1,\\dots,d\\}\\times \\mathbb{R}\\to\\mathbb{R}^m$, the stationary stochastic process $Y(t) = f(X(t),\\zeta(t))$ has a power spectral density $\\text{S}_f$ that admits a second order Taylor series expansion: A function $\\text{S}^{(2)}_f\\colon [-\\pi,\\pi] \\to \\mathbb{C}^{ m\\times m}$ is constructed such that \\[ \\text{S}_f(\\theta) = \\text{S}^\\bullet_f(\\theta) + \\varepsilon^2 \\text{S}_f^{(2)}(\\theta) + o(\\varepsilon^2),\\quad \\theta\\in [-\\pi,\\pi] . \\] An explicit formula for the function $\\text{S}_f^{(2)}$ is obtained, based in part on the bounds in (i). \nThe results are illustrated using a version of the timing channel of Anantharam and Verdu.", "venue": "ArXiv", "authors": ["Yue  Chen", "Ana  Busic", "Sean P. Meyn"], "year": 2016, "n_citations": 3}
{"id": 1394908, "s2_id": "32efd6317f69970ff4a696aca985d3a315873365", "title": "Using HEP experiment workflows for the benchmarking and accounting of WLCG computing resources", "abstract": "Benchmarking of CPU resources in WLCG has been based on the HEP-SPEC06 (HS06) suite for over a decade. It has recently become clear that HS06, which is based on real applications from non-HEP domains, no longer describes typical HEP workloads. The aim of the HEP-Benchmarks project is to develop a new benchmark suite for WLCG compute resources, based on real applications from the LHC experiments. By construction, these new benchmarks are thus guaranteed to have a score highly correlated to the throughputs of HEP applications, and a CPU usage pattern similar to theirs. Linux containers and the CernVM-FS filesystem are the two main technologies enabling this approach, which had been considered impossible in the past. In this paper, we review the motivation, implementation and outlook of the new benchmark suite.", "venue": "EPJ Web of Conferences", "authors": ["Andrea  Valassi", "Manfred  Alef", "Jean-Michel  Barbet", "Olga  Datskova", "Riccardo De Maria", "Miguel Fontes Medeiros", "Domenico  Giordano", "Costin  Grigoras", "Christopher  Hollowell", "Martina  Javurkova", "Viktor  Khristenko", "David  Lange", "Michele  Michelotto", "Lorenzo  Rinaldi", "Andrea  Sciaba", "Cas Van Der Laan"], "year": 2020, "n_citations": 4}
{"id": 1398679, "s2_id": "feb6e9133376bfdbfa1a25504d415af6df8c21d0", "title": "Movers and Shakers: Kinetic Energy Harvesting for the Internet of Things", "abstract": "Numerous energy harvesting wireless devices that will serve as building blocks for the Internet of Things (IoT) are currently under development. However, there is still only limited understanding of the properties of various energy sources and their impact on energy harvesting adaptive algorithms. Hence, we focus on characterizing the kinetic (motion) energy that can be harvested by a wireless node with an IoT form factor and on developing energy allocation algorithms for such nodes. In this paper, we describe methods for estimating harvested energy from acceleration traces. To characterize the energy availability associated with specific human activities (e.g., relaxing, walking, cycling), we analyze a motion dataset with over 40 participants. Based on acceleration measurements that we collected for over 200 hours, we study energy generation processes associated with day-long human routines. We also briefly summarize our experiments with moving objects. We develop energy allocation algorithms that take into account practical IoT node design considerations, and evaluate the algorithms using the collected measurements. Our observations provide insights into the design of motion energy harvesters, IoT nodes, and energy harvesting adaptive algorithms.", "venue": "IEEE J. Sel. Areas Commun.", "authors": ["Maria  Gorlatova", "John  Sarik", "Guy  Grebla", "Mina  Cong", "Ioannis  Kymissis", "Gil  Zussman"], "year": 2015, "n_citations": 187}
{"id": 1403976, "s2_id": "c819d607bc3372815619327d15f9032e2536ac65", "title": "Cuckoo++ hash tables: high-performance hash tables for networking applications", "abstract": "Hash tables are essential data-structures for networking applications (e.g., connection tracking, firewalls, network address translators). Among these, cuckoo hash tables provide excellent performance by processing lookups with very few memory accesses (2 to 3 per lookup). Yet, they remain memory bound and each memory access impacts performance. In this paper, we propose algorithmic improvements to cuckoo hash tables to eliminate unnecessary memory accesses, without altering the properties of the original cuckoo hash table so that all existing theoretical analysis remain applicable. We also present an implementation tailored to run efficiently on Intel Xeon processors, thus supporting NFV and softwarization trends and compare it to the optimized implementation of DPDK. On a single core, our implementation achieves 37M positive lookups per second (i.e., when the key looked up is present in the table), and 60M negative lookups per second, a 45% to 70% improvement over DPDK.", "venue": "ANCS", "authors": ["Nicolas Le Scouarnec"], "year": 2018, "n_citations": 17}
{"id": 1405346, "s2_id": "000daf31674a10a51fe623e6b01a723ac651bafc", "title": "Heavy traffic optimal resource allocation algorithms for cloud computing clusters", "abstract": "Cloud computing is emerging as an important platform for business, personal and mobile computing applications. In this paper, we study a stochastic model of cloud computing, where jobs arrive according to a stochastic process and request resources like CPU, memory and storage space. We consider a model where the resource allocation problem can be separated into a routing or load balancing problem and a scheduling problem. We study the join-the-shortest-queue routing and power-of-two-choices routing algorithms with MaxWeight scheduling algorithm. It was known that these algorithms are throughput optimal. In this paper, we show that these algorithms are queue length optimal in the heavy traffic limit.", "venue": "2012 24th International Teletraffic Congress (ITC 24)", "authors": ["Siva Theja Maguluri", "R.  Srikant", "Lei  Ying"], "year": 2012, "n_citations": 107}
{"id": 1405781, "s2_id": "de8ce9adf533dad3f59b4bd971c6ae1b500adafb", "title": "Effect of payload size on goodput when message segmentations occur for wireless networks: Case of packet corruptions recovered by stop-and-wait protocol", "abstract": "This paper investigates the effect of payload size on goodput for wireless networks where packets created from a message through a segmentation function are lost due to bit errors and they are recovered by a stop-and-wait protocol. To achieve this, we derive the exact analytical form of goodput using the analytical form of a packet-size distribution, given a message-size distribution and a payload size. In previous work, the packet sizes are assumed to be constant, which are payload size plus header size, although actual segmented packets are not constant in size. Hence, this constant packet-size assumption may be not justified for goodput analysis. From numerical results, we show that the constant packet-size assumption is not justified under low bit-error rates. Furthermore, we indicate that the curves of goodput are concave in payload size under high bit-error rates. In addition, we show that the larger mean bit-error burst length yields less concave curves of goodput.", "venue": "ArXiv", "authors": ["Takashi  Ikegawa"], "year": 2019, "n_citations": 0}
{"id": 1407808, "s2_id": "eb1942bbdad390ec75c9f517d720d5539c9fa58b", "title": "A robust queueing network analyzer based on indices of dispersion", "abstract": "We develop a robust queueing network analyzer algorithm to approximate the steady-state performance of a single-class open queueing network of single-server queues with Markovian routing. The algorithm allows non-renewal external arrival processes, general service-time distributions and customer feedback. We focus on the customer flows, defined as the continuous-time processes counting customers flowing into or out of the network, or flowing from one queue to another. Each flow is partially characterized by its rate and a continuous function that measures the stochastic variability over time. This function is a scaled version of the variance-time curve, called the index of dispersion for counts (IDC). The required IDC functions for the flows can be calculated from the model primitives, estimated from data or approximated by solving a set of linear equations. A robust queueing technique is used to generate approximations of the mean steady-state performance at each queue from the IDC of the total arrival flow and the service specification at that queue. The algorithm effectiveness is supported by extensive simulation studies and heavy-traffic limits.", "venue": "Naval Research Logistics (NRL)", "authors": ["Ward  Whitt", "Wei  You"], "year": 2021, "n_citations": 1}
{"id": 1410646, "s2_id": "1b5d768a9be25f406ca97bfa3095b2176a210c73", "title": "The Pitfall of Evaluating Performance on Emerging AI Accelerators", "abstract": "In recent years, domain-specific hardware has brought significant performance improvements in deep learning (DL). Both industry and academia only focus on throughput when evaluating these AI accelerators, which usually are custom ASICs deployed in datacenter to speed up the inference phase of DL workloads. Pursuing higher hardware throughput such as OPS (Operation Per Second) using various optimizations seems to be their main design target. However, they ignore the importance of accuracy in the DL nature. Motivated by this, this paper argue that a single throughput metric can not comprehensively reflect the real-world performance of AI accelerators. To reveal this pitfall, we evaluates several frequently-used optimizations on a typical AI accelerator and quantifies their impact on accuracy and throughout under representative DL inference workloads. Based on our experimental results, we find that some optimizations cause significant loss on accuracy in some workloads, although it can improves the throughout. Furthermore, our results show the importance of end-to-end evaluation in DL.", "venue": "ArXiv", "authors": ["Zihan  Jiang", "Jiansong  Li", "Jiangfeng  Zhan"], "year": 2019, "n_citations": 2}
{"id": 1412720, "s2_id": "1da94cf1d1bd4b31bf3a216479a8f123b7752be7", "title": "The X-Files: Investigating Alien Performance in a Thin-Client World", "abstract": "Many scientific applications use the X11 window environment; an open source windows GUI standard employing a client/server architecture. X11 promotes: distributed computing, thin-client functionality, cheap desktop displays, compatibility with heterogeneous servers, remote services and administration, and greater maturity than newer web technologies. This paper details the author's investigations into close encounters with alien performance in X11-based seismic applications running on a 200-node cluster, backed by 2 TB of mass storage. End-users cited two significant UFOs (Unidentified Faulty Operations) i) long application launch times and ii) poor interactive response times. The paper is divided into three major sections describing Close Encounters of the 1st Kind: citings of UFO experiences, the 2nd Kind: recording evidence of a UFO, and the 3rd Kind: contact and analysis. UFOs do exist and this investigation presents a real case study for evaluating workload analysis and other diagnostic tools.", "venue": "Int. CMG Conference", "authors": ["Neil J. Gunther"], "year": 1999, "n_citations": 0}
{"id": 1413092, "s2_id": "85bb45a623f217e5732c62f555b22736e9e90e12", "title": "A stochastic calculus for network systems with renewable energy sources", "abstract": "We consider the performance modeling and evaluation of network systems powered with renewable energy sources such as solar and wind energy. Such energy sources largely depend on environmental conditions, which are hard to predict accurately. As such, it may only make sense to require the network systems to support a soft quality of service (QoS) guarantee, i.e., to guarantee a service requirement with a certain high probability. In this paper, we build a solid mathematical foundation to help better understand the stochastic energy constraint and the inherent correlation between QoS and the uncertain energy supply. We utilize a calculus approach to model the cumulative amount of charged energy and the cumulative amount of consumed energy. We derive upper and lower bounds on the remaining energy level based on a stochastic energy charging rate and a stochastic energy discharging rate. By building the bridge between energy consumption and task execution (i.e., service), we study the QoS guarantee under the constraint of uncertain energy sources.", "venue": "2012 Proceedings IEEE INFOCOM Workshops", "authors": ["Kui  Wu", "Yuming  Jiang", "Dimitri  Marinakis"], "year": 2012, "n_citations": 32}
{"id": 1415438, "s2_id": "0983b4ce862c6d60bead74dc1daa6e1e586171c6", "title": "Architecture-Specific Performance Optimization of Compute-Intensive FaaS Functions", "abstract": "FaaS allows an application to be decomposed into functions that are executed on a FaaS platform. The FaaS platform is responsible for the resource provisioning of the functions. Recently, there is a growing trend towards the execution of compute-intensive FaaS functions that run for several seconds. However, due to the billing policies followed by commercial FaaS offerings, the execution of these functions can incur significantly higher costs. Moreover, due to the abstraction of underlying processor architectures on which the functions are executed, the performance optimization of these functions is challenging. As a result, most FaaS functions use pre-compiled libraries generic to x86-64 leading to performance degradation. In this paper, we examine the underlying processor architectures for Google Cloud Functions (GCF) and determine their prevalence across the 19 available GCF regions. We modify, adapt, and optimize three compute-intensive FaaS workloads written in Python using Numba, a JIT compiler based on LLVM, and present results wrt performance, memory consumption, and costs on GCF. Results from our experiments show that the optimization of FaaS functions can improve performance by 12.8x (geometric mean) and save costs by 73.4% on average for the three functions. Our results show that optimization of the FaaS functions for the specific architecture is very important. We achieved a maximum speedup of 1.79x by tuning the function especially for the instruction set of the underlying processor architecture.", "venue": "2021 IEEE 14th International Conference on Cloud Computing (CLOUD)", "authors": ["Mohak  Chadha", "Anshul  Jindal", "Michael  Gerndt"], "year": 2021, "n_citations": 3}
{"id": 1420287, "s2_id": "bc7be47f82f09e745e0e8f25f28d55f56999ad7b", "title": "C++ Templates as Partial Evaluation", "abstract": "This paper explores the relationship between C++ templates and partial evaluation. Templates were designed to support generic programming, but unintentionally provided the ability to perform compile-time computations and code generation. These features are completely accidental, and as a result their syntax is awkward. By recasting these features in terms of partial evaluation, a much simpler syntax can be achieved. C++ may be regarded as a two-level language in which types are first-class values. Template instantiation resembles an offline partial evaluator. This paper describes preliminary work toward a single mechanism based on Partial Evaluation which unifies generic programming, compile-time computation and code generation. The language Catat is introduced to illustrate these ideas.", "venue": "PEPM", "authors": ["Todd L. Veldhuizen"], "year": 1999, "n_citations": 105}
{"id": 1421001, "s2_id": "c31a043346c2a9673118b09604c8c5ffe3c5fbce", "title": "Improving the performance of bagging ensembles for data streams through mini-batching", "abstract": "Abstract Often, machine learning applications have to cope with dynamic environments where data are collected in the form of continuous data streams with potentially infinite length and transient behavior . Compared to traditional (batch) data mining , stream processing algorithms have additional requirements regarding computational resources and adaptability to data evolution. They must process instances incrementally because the data\u2019s continuous flow prohibits storing data for multiple passes. Ensemble learning achieved remarkable predictive performance in this scenario. Implemented as a set of (several) individual classifiers , ensembles are naturally amendable for task parallelism . However, the incremental learning and dynamic data structures used to capture the concept drift increase the cache misses and hinder the benefit of parallelism . This paper proposes a mini-batching strategy that can improve memory access locality and performance of several ensemble algorithms for stream mining in multi-core environments. With the aid of a formal framework, we demonstrate that mini-batching can significantly decrease the reuse distance (and the number of cache misses). Experiments on six different state-of-the-art ensemble algorithms applying four benchmark datasets with varied characteristics show speedups of up to 5X on 8-core processors. These benefits come at the expense of a small reduction in predictive performance.", "venue": "Inf. Sci.", "authors": ["Guilherme  Cassales", "Heitor Murilo Gomes", "Albert  Bifet", "Bernhard  Pfahringer", "Hermes  Senger"], "year": 2021, "n_citations": 1}
{"id": 1423361, "s2_id": "5c1fe03707e6f80d8eb0606bd9895d1380335f53", "title": "On the Efficiency of Decentralized File Storage for Personal Information Management Systems", "abstract": "This paper presents an architecture, based on Distributed Ledger Technologies (DLTs) and Decentralized File Storage (DFS) systems, to support the use of Personal Information Management Systems (PIMS). DLT and DFS are used to manage data sensed by mobile users equipped with devices with sensing capability. DLTs guarantee the immutability, traceability and verifiability of references to personal data, that are stored in DFS. In fact, the inclusion of data digests in the DLT makes it possible to obtain an unalterable reference and a tamper-proof log, while remaining compliant with the regulations on personal data, i.e. GDPR. We provide an experimental evaluation on the feasibility of the use of DFS. Three different scenarios have been studied: i) a proprietary IPFS approach with a dedicated node interfacing with the data producers, ii) a public IPFS service and iii) Sia Skynet. Results show that through proper configuration of the system infrastructure, it is viable to build a decentralized Personal Data Storage (PDS).", "venue": "2020 IEEE Symposium on Computers and Communications (ISCC)", "authors": ["Mirko  Zichichi", "Stefano  Ferretti", "Gabriele  D\u2019Angelo"], "year": 2020, "n_citations": 5}
{"id": 1424644, "s2_id": "63fcb530e36c2b973a9f2aa96b884d8822d6e95f", "title": "An optimized Parallel Failure-less Aho-Corasick algorithm for DNA sequence matching", "abstract": "The Aho-Corasick algorithm is a multiple patterns searching algorithm running sequentially in various applications like network intrusion detection and bioinformatics for finding several input strings within a given large input string. The parallel version of the Aho-Corasick algorithm is called as Parallel Failure-less Aho-Corasick algorithm because it doesnt need failure links like in the original Aho-Corasick algorithm. In this research, we implemented an application specific parallel failureless Aho-Corasick algorithm on the general purpose graphic processing unit by applying several cache optimization techniques for matching DNA sequences. Our parallel Aho-Corasick algorithm shows better performance than the available parallel Aho-Corasick algorithm library due to its simplicity and optimized cache memory usage of graphic processing units for matching DNA sequences.", "venue": "2016 IEEE International Conference on Information and Automation for Sustainability (ICIAfS)", "authors": ["Vajira  Thambawita", "Roshan G. Ragel", "Dhammika  Elkaduwe"], "year": 2016, "n_citations": 1}
{"id": 1426047, "s2_id": "49940f9dfa0afbe618c6b9d074c2087e27758737", "title": "Application of the Computer Capacity to the Analysis of Processors Evolution", "abstract": "The concept of the so-called computer capacity was proposed in 2012 and applied for analysis of processors of different kinds. Here, we analyze the evolution of processors using the computer capacity as the main tool of analysis. It is shown that during the transition \u201cfrom old to new\u201d the manufacturers change the parameters that affect the computer capacity. It allows us to predict the values of parameters of following processors. Intel processors are used as the main example due to their high popularity and the accessibility of detailed description of all the technical characteristics.", "venue": "J. Circuits Syst. Comput.", "authors": ["Boris  Ryabko", "Anton  Rakitskiy"], "year": 2020, "n_citations": 2}
{"id": 1426054, "s2_id": "393679ff5a156854c13ad2571ed76b9ffadb3c1a", "title": "Design and optimisation of an efficient HDF5 I/O Kernel for massive parallel fluid flow simulations", "abstract": "More and more massive parallel codes running on several hundreds of thousands of cores are entering the computational science and engineering domain, allowing high\u2010fidelity computations on up to trillions of unknowns for very detailed analyses of the underlying problems. Such runs typically produce gigabytes of data, hindering both efficient storage and (interactive) data exploration. Advanced approaches based on inherently distributed data formats such as hierarchical data format version 5 become necessary here to avoid long latencies when storing the data and to support fast (random) access when retrieving the data for visual processing. This paper shows considerations and implementation aspects of an I/O kernel based on hierarchical data format version 5 that supports fast checkpointing, restarting, and selective visualisation using a single shared output file for an existing computational fluid dynamics framework. This functionality is achieved by including the framework's hierarchical data structure in the file, which also opens the door for additional steering functionality. Finally, the performance of the kernel's write routines are presented. Bandwidths close to the theoretical peak on modern supercomputing clusters were achieved by avoiding file\u2010locking and using collective buffering.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Christoph  Ertl", "J\u00e9r\u00f4me  Frisch", "Ralf-Peter  Mundani"], "year": 2017, "n_citations": 7}
{"id": 1428689, "s2_id": "843a21c47308bd9a87809c940a3c5366b5981080", "title": "Just-In-Time compilation of OCaml byte-code", "abstract": "This paper presents various improvements that were applied to OCamlJIT2, a Just-In-Time compiler for the OCaml byte-code virtual machine. OCamlJIT2 currently runs on various Unix-like systems with x86 or x86-64 processors. The improvements, including the new x86 port, are described in detail, and performance measures are given, including a direct comparison of OCamlJIT2 to OCamlJIT.", "venue": "ArXiv", "authors": ["Benedikt  Meurer"], "year": 2010, "n_citations": 2}
{"id": 1432723, "s2_id": "2ed5909443d6d8df0eabd3f6b0080b9f80c07449", "title": "A model-driven approach to broaden the detection of software performance antipatterns at runtime", "abstract": "Performance antipatterns document bad design patterns that have negative influence on system performance. In our previous work we formalized such antipatterns as logical predicates that build on four different views: (i) the static view that captures the software elements (e.g. classes, components) and the static relationships among them; (ii) the dynamic view that represents the interaction (e.g. messages) that occurs between the software elements to provide system functionalities; (iii) the deployment view that describes the hardware elements (e.g. processing nodes) and the mapping of software resources onto hardware platforms; (iv) the performance view that collects a set of specific performance indices. In this paper we present a lightweight infrastructure that enables the detection of software performance antipatterns at runtime through the monitoring of specific performance indices. The proposed approach precalculates the logical predicates of antipatterns and identifies the ones whose static, dynamic and deployment sub-predicates occur in the current system configuration and brings at runtime the verification of performance sub-predicates. The proposed infrastructure leverages model-driven techniques to generate probes for monitoring the performance sub-predicates thus to support the detection of antipatterns at runtime.", "venue": "FESCA", "authors": ["Antinisca Di Marco", "Catia  Trubiani"], "year": 2014, "n_citations": 5}
{"id": 1432901, "s2_id": "1d4093ce51c477256d19a78e90a4a5b7676bf924", "title": "Scalable Load Balancing Algorithms in Networked Systems", "abstract": "A fundamental challenge in large-scale networked systems viz., data centers and cloud networks is to distribute tasks to a pool of servers, using minimal instantaneous state information, while providing excellent delay performance. In this thesis we design and analyze load balancing algorithms that aim to achieve a highly efficient distribution of tasks, optimize server utilization, and minimize communication overhead.", "venue": "ArXiv", "authors": ["Debankur  Mukherjee"], "year": 2018, "n_citations": 2}
{"id": 1435095, "s2_id": "b76410bb79bd6de5a2508d19cc78923219f672a7", "title": "Beam-searching and transmission scheduling in millimeter wave communications", "abstract": "Millimeter wave (mmWave) wireless networks rely on narrow beams to support multi-gigabit data rates. Nevertheless, the alignment of transmitter and receiver beams is a time-consuming operation, which introduces an alignment-throughput tradeoff. A wider beamwidth reduces the alignment overhead, but leads also to reduced directivity gains. Moreover, existing mmWave standards schedule a single transmission in each time slot, although directional communications facilitate multiple concurrent transmissions. In this paper, a joint consideration of the problems of beamwidth selection and scheduling is proposed to maximize effective network throughput. The resulting optimization problem requires exact knowledge of network topology, which may not be available in practice. Therefore, two standard-compliant approximation algorithms are developed, which rely on underestimation and overestimation of interference. The first one aims to maximize the reuse of available spectrum, whereas the second one is a more conservative approach that schedules together only links that cause no interference. Extensive performance analysis provides useful insights on the directionality level and the number of concurrent transmissions that should be pursued. Interestingly, extremely narrow beams are in general not optimal.", "venue": "2015 IEEE International Conference on Communications (ICC)", "authors": ["Hossein Shokri Ghadikolaei", "Lazaros  Gkatzikis", "Carlo  Fischione"], "year": 2015, "n_citations": 133}
{"id": 1435332, "s2_id": "c0ac4dde9b9c6c8d3a8e44276128d30bda9d4c83", "title": "EPOBF: Energy Efficient Allocation of Virtual Machines in High Performance Computing Cloud", "abstract": "Cloud computing has become more popular in provision of computing resources under virtual machine (VM) abstraction for high performance computing (HPC) users to run their applications. A HPC cloud is such cloud computing environment. One of challenges of energy efficient resource allocation for VMs in HPC cloud is tradeoff between minimizing total energy consumption of physical machines (PMs) and satisfying Quality of Service (e.g. performance). On one hand, cloud providers want to maximize their profit by reducing the power cost (e.g. using the smallest number of running PMs). On the other hand, cloud customers (users) want highest performance for their applications. In this paper, we focus on the scenario that scheduler does not know global information about user jobs and user applications in the future. Users will request shortterm resources at fixed start times and non interrupted durations. We then propose a new allocation heuristic (named Energy-aware and Performance per watt oriented Bestfit (EPOBF)) that uses metric of performance per watt to choose which most energy-efficient PM for mapping each VM (e.g. maximum of MIPS per Watt). Using information from Feitelson's Parallel Workload Archive to model HPC jobs, we compare the proposed EPOBF to state of the art heuristics on heterogeneous PMs (each PM has multicore CPU). Simulations show that the EPOBF can reduce significant total energy consumption in comparison with state of the art allocation heuristics.", "venue": "Trans. Large Scale Data Knowl. Centered Syst.", "authors": ["Nguyen  Quang-Hung", "Nam  Thoai", "Nguyen Thanh Son"], "year": 2014, "n_citations": 46}
{"id": 1437196, "s2_id": "22e3a6a18319a00a178e1416942f875da0e5f5bc", "title": "On the Impact of Device and Behavioral Heterogeneity in Federated Learning", "abstract": "Federated learning (FL) is becoming a popular paradigm for collaborative learning over distributed, private datasets owned by non-trusting entities. FL has seen successful deployment in production environments, and it has been adopted in services such as virtual keyboards, autocompletion, item recommendation, and several IoT applications. However, FL comes with the challenge of performing training over largely heterogeneous datasets, devices, and networks that are out of the control of the centralized FL server. Motivated by this inherent setting, we make a first step towards characterizing the impact of device and behavioral heterogeneity on the trained model. We conduct an extensive empirical study spanning close to 1.5K unique configurations on five popular FL benchmarks. Our analysis shows that these sources of heterogeneity have a major impact on both model performance and fairness, thus sheds light on the importance of considering heterogeneity in FL system design.", "venue": "ArXiv", "authors": ["Ahmed M. Abdelmoniem", "Chen-Yu  Ho", "Pantelis  Papageorgiou", "Muhammad  Bilal", "Marco  Canini"], "year": 2021, "n_citations": 5}
{"id": 1438354, "s2_id": "41fcc5eacaddb0d5c04a91058775373c2a3bcfef", "title": "Implicit Renewal Theory and Power Tails on Trees", "abstract": "We extend Goldie's (1991) implicit renewal theorem to enable the analysis of recursions on weighted branching trees. We illustrate the developed method by deriving the power-tail asymptotics of the distributions of the solutions R to and similar recursions, where (Q, N, C 1, C 2,\u2026) is a nonnegative random vector with N \u2208 {0, 1, 2, 3,\u2026} \u222a {\u221e}, and are independent and identically distributed copies of R, independent of (Q, N, C 1, C 2,\u2026); here \u2018\u2228\u2019 denotes the maximum operator.", "venue": "Advances in Applied Probability", "authors": ["Predrag R. Jelenkovic", "Mariana  Olvera-Cravioto"], "year": 2012, "n_citations": 47}
{"id": 1441743, "s2_id": "14d9a4cdbf6257a00d5c353d77116ea5ad722e48", "title": "Stochastic HYPE: Flow-based modelling of stochastic hybrid systems", "abstract": "Stochastic HYPE is a novel process algebra that models stochastic, instantaneous and continuous behaviour. It develops the flow-based approach of the hybrid process algebra HYPE by replacing non-urgent events with events with exponentially-distributed durations and also introduces random resets. The random resets allow for general stochasticity, and in particular allow for the use of event durations drawn from distributions other than the exponential distribution. To account for stochasticity, the semantics of stochastic HYPE target piecewise deterministic Markov processes (PDMPs), via intermediate transition-driven stochastic hybrid automata (TDSHA) in contrast to the hybrid automata used as semantic target for HYPE. Stochastic HYPE models have a specific structure where the controller of a system is separate from the continuous aspect of this system providing separation of concerns and supporting reasoning. A novel equivalence is defined which captures when two models have the same stochastic behaviour (as in stochastic bisimulation), instantaneous behaviour (as in classical bisimulation) and continuous behaviour. These techniques are illustrated via an assembly line example.", "venue": "ArXiv", "authors": ["Luca  Bortolussi", "Vashti  Galpin", "Jane  Hillston"], "year": 2014, "n_citations": 4}
{"id": 1443209, "s2_id": "9a2668a27303e4cbd921ae53a84d4cc264068728", "title": "COZ: Finding Code that Counts with Causal Profiling", "abstract": "Improving performance is a central concern for software developers. To locate optimization opportunities, developers rely on software profilers. However, these profilers only report where programs spend their time: optimizing that code may have no impact on performance. Past profilers thus both waste developer time and make it difficult for them to uncover significant optimization opportunities.This paper introduces causal profiling. Unlike past profiling approaches, causal profiling indicates exactly where programmers should focus their optimization efforts, and quantifies their potential impact. Causal profiling works by running performance experiments during program execution. Each experiment calculates the impact of any potential optimization by virtually speeding up code: inserting pauses that slow down all other code running concurrently. The key insight is that this slowdown has the same relative effect as running that line faster, thus \"virtually\" speeding it up.We present Coz, a causal profiler, which we evaluate on a range of highly-tuned applications such as Memcached, SQLite, and the PARSEC benchmark suite. Coz identifies previously unknown optimization opportunities that are both significant and targeted. Guided by Coz, we improve the performance of Memcached by 9%, SQLite by 25%, and accelerate six PARSEC applications by as much as 68%; in most cases, these optimizations involve modifying under 10 lines of code.", "venue": "USENIX Annual Technical Conference", "authors": ["Charlie  Curtsinger", "Emery D. Berger"], "year": 2016, "n_citations": 77}
{"id": 1444899, "s2_id": "51d85909e0faa9bff20f0c6a92431f82900cc831", "title": "Scalability in Computing and Robotics", "abstract": "Efficient engineered systems require scalability. A scalable system has increasing performance with increasing system size. In an ideal case, the increase in performance (e.g., speedup) corresponds to the number of units that are added to the system. However, if multiple units work on the same task, then coordination among these units is required. This coordination can introduce overheads with an impact on system performance. The coordination costs can lead to sublinear improvement or even diminishing performance with increasing system size. However, there are also systems that implement efficient coordination and exploit collaboration of units to attain superlinear improvement. Modeling the scalability dynamics is key to understanding efficient systems. Known laws of scalability, such as Amdahl's law, Gustafson's law, and Gunther's Universal Scalability Law, are minimalistic phenomenological models that explain a rich variety of system behaviors through concise equations. While useful to gain general insights, the phenomenological nature of these models may limit the understanding of the underlying dynamics, as they are detached from first principles that could explain coordination overheads among units. Through a decentralized system approach, we propose a general model based on generic interactions between units that is able to describe, as specific cases, any general pattern of scalability included by previously reported laws. The proposed general model of scalability is built on first principles, or at least on a microscopic description of interaction between units, and therefore has the potential to contribute to a better understanding of system behavior and scalability. We show that this model can be applied to a diverse set of systems, such as parallel supercomputers, robot swarms, or wireless sensor networks, creating a unified view on interdisciplinary design for scalability.", "venue": "ArXiv", "authors": ["Heiko  Hamann", "Andreagiovanni  Reina"], "year": 2020, "n_citations": 1}
{"id": 1445896, "s2_id": "0629bcbc4ce2d7265fd6269c9f49bc6e0bea090e", "title": "heSRPT: Optimal Parallel Scheduling of Jobs With Known Sizes", "abstract": "When parallelizing a set of jobs across many servers, one must balance a trade-off between granting priority to short jobs and maintaining the overall efficiency of the system. When the goal is to minimize the mean flow time of a set of jobs, it is usually the case that one wants to complete short jobs before long jobs. However, since jobs usually cannot be parallelized with perfect efficiency, granting strict priority to the short jobs can result in very low system efficiency which in turn hurts the mean flow time across jobs. In this paper, we derive the optimal policy for allocating servers to jobs at every moment in time in order to minimize mean flow time across jobs. We assume that jobs follow a sublinear, concave speedup function, and hence jobs experience diminishing returns from being allocated additional servers. We show that the optimal policy, heSRPT, will complete jobs according to their size order, but maintains overall system efficiency by allocating some servers to each job at every moment in time. We compare heSRPT with state-of-the-art allocation policies from the literature and show that heSRPT outperforms its competitors by at least 30%, and often by much more.", "venue": "ArXiv", "authors": ["Benjamin  Berg", "Rein  Vesilo", "Mor  Harchol-Balter"], "year": 2019, "n_citations": 0}
{"id": 1446560, "s2_id": "0b6c613d118e7093d9a8fab89d4d4760076e7ffd", "title": "Devito: Towards a Generic Finite Difference DSL Using Symbolic Python", "abstract": "Domain specific languages (DSL) have been used in a variety of fields to express complex scientific problems in a concise manner and provide automated performance optimization for a range of computational architectures. As such DSLs provide a powerful mechanism to speed up scientific Python computation that goes beyond traditional vectorization and pre-compilation approaches, while allowing domain scientists to build applications within the comforts of the Python software ecosystem. In this paper we present Devito, a new finite difference DSL that provides optimized stencil computation from high-level problem specifications based on symbolic Python expressions. We demonstrate Devito's symbolic API and performance advantages over traditional Python acceleration methods before highlighting its use in the scientific context of seismic inversion problems.", "venue": "2016 6th Workshop on Python for High-Performance and Scientific Computing (PyHPC)", "authors": ["Michael  Lange", "Navjot  Kukreja", "Mathias  Louboutin", "Fabio  Luporini", "Felippe  Vieira", "Vincenzo  Pandolfo", "Paulius  Velesko", "Paulius  Kazakas", "Gerard  Gorman"], "year": 2016, "n_citations": 23}
{"id": 1447073, "s2_id": "468854bd45d8a28df610d25238f584db6216f56f", "title": "Proximity Based Load Balancing Policies on Graphs: A Simulation Study", "abstract": "Distributed load balancing is the act of allocating jobs among a set of servers as evenly as possible. There are mainly two versions of the load balancing problem that have been studied in the literature: static and dynamic. The static interpretation leads to formulating the load balancing problem as a case with jobs (balls) never leaving the system and accumulating at the servers (bins) whereas the dynamic setting deals with the case when jobs arrive and leave the system after service completion. This paper designs and evaluates server proximity aware job allocation policies for treating load balancing problems with a goal to reduce the communication cost associated with the jobs. We consider a class of proximity aware Power of Two (POT) choice based assignment policies for allocating jobs to servers, where servers are interconnected as an n-vertex graph G(V, E). For the static version, we assume each job arrives at one of the servers, u. For the dynamic setting, we assume G to be a circular graph and job arrival process at each server is described by a Poisson point process with the job service time exponentially distributed. For both settings, we then assign each job to the server with minimum load among servers u and v where v is chosen according to one of the following two policies: (i) Unif-POT(k): Sample a server v uniformly at random from k-hop neighborhood of u (ii) InvSq-POT(k): Sample a server v from k-hop neighborhood of u with probability proportional to the inverse square of the distance between u and v. Our simulation results show that both the policies consistently produce a load distribution which is much similar to that of a classical proximity oblivious POT policy.", "venue": "ArXiv", "authors": ["Nitish K. Panigrahy", "Thirupathaiah  Vasantam", "Prithwish  Basu", "Don  Towsley"], "year": 2020, "n_citations": 0}
{"id": 1447832, "s2_id": "2e0dad8057a54d0b7c76982930dc7040f1fbafdc", "title": "An Empirical Guide to the Behavior and Use of Scalable Persistent Memory", "abstract": "After nearly a decade of anticipation, scalable nonvolatile memory DIMMs are finally commercially available with the release of Intel's 3D XPoint DIMM. This new nonvolatile DIMM supports byte-granularity accesses with access times on the order of DRAM, while also providing data storage that survives power outages. Researchers have not idly waited for real nonvolatile DIMMs (NVDIMMs) to arrive. Over the past decade, they have written a slew of papers proposing new programming models, file systems, libraries, and applications built to exploit the performance and flexibility that NVDIMMs promised to deliver. Those papers drew conclusions and made design decisions without detailed knowledge of how real NVDIMMs would behave or how industry would integrate them into computer architectures. Now that 3D XPoint NVDIMMs are actually here, we can provide detailed performance numbers, concrete guidance for programmers on these systems, reevaluate prior art for performance, and reoptimize persistent memory software for the real 3D XPoint DIMM. In this paper, we explore the performance properties and characteristics of Intel's new 3D XPoint DIMM at the micro and macro level. First, we investigate the basic characteristics of the device, taking special note of the particular ways in which its performance is peculiar relative to traditional DRAM or other past methods used to emulate NVM. From these observations, we recommend a set of best practices to maximize the performance of the device. With our improved understanding, we then explore the performance of prior art in application-level software for persistent memory, taking note of where their performance was influenced by our guidelines.", "venue": "FAST", "authors": ["Jian  Yang", "Juno  Kim", "Morteza  Hoseinzadeh", "Joseph  Izraelevitz", "Steven  Swanson"], "year": 2020, "n_citations": 116}
{"id": 1462700, "s2_id": "b2378f274b49e817df703ff39be8ebd9fe75f087", "title": "Sub-realtime simulation of a neuronal network of natural density", "abstract": "Full scale simulations of neuronal network models of the brain are challenging due to the high density of connections between neurons. This contribution reports run times shorter than the simulated span of biological time for a full scale model of the local cortical microcircuit with explicit representation of synapses on a recent conventional compute node. Realtime performance is relevant for robotics and closed-loop applications while sub-realtime is desirable for the study of learning and development in the brain, processes extending over hours and days of biological time. 1 ar X iv :2 11 1. 04 39 8v 2 [ cs .D C ] 2 4 N ov 2 02 1", "venue": "ArXiv", "authors": ["Anno C. Kurth", "Johanna  Senk", "Dennis  Terhorst", "Justin  Finnerty", "Markus  Diesmann"], "year": 2021, "n_citations": 2}
{"id": 1462961, "s2_id": "bf8946cf3832eef92f7cafdd8968a142d842b829", "title": "Network Load Balancing Methods: Experimental Comparisons and Improvement", "abstract": "Load balancing algorithms play critical roles in systems where the workload has to be distributed across multiple resources, such as cores in multiprocessor system, computers in distributed computing, and network links. In this paper, we study and evaluate four load balancing methods: random, round robin, shortest queue, and shortest queue with stale load information. We build a simulation model and compare mean delay of the systems for the load balancing methods. We also provide a method to improve shortest queue with stale load information load balancing. A performance analysis for the improvement is also presented in this paper.", "venue": "ArXiv", "authors": ["Shafinaz  Islam"], "year": 2017, "n_citations": 3}
{"id": 1463510, "s2_id": "f340d1da09a5c0e60c7bf99d8764c21bd0e8b534", "title": "Chebyshev Filter Diagonalization on Modern Manycore Processors and GPGPUs", "abstract": "Chebyshev filter diagonalization is well established in quantum chemistry and quantum physics to compute bulks of eigenvalues of large sparse matrices. Choosing a block vector implementation, we investigate optimization opportunities on the new class of high-performance compute devices featuring both high-bandwidth and low-bandwidth memory. We focus on the transparent access to the full address space supported by both architectures under consideration: Intel Xeon Phi \"Knights Landing\" and Nvidia \"Pascal.\" \nWe propose two optimizations: (1) Subspace blocking is applied for improved performance and data access efficiency. We also show that it allows transparently handling problems much larger than the high-bandwidth memory without significant performance penalties. (2) Pipelining of communication and computation phases of successive subspaces is implemented to hide communication costs without extra memory traffic. \nAs an application scenario we use filter diagonalization studies on topological insulator materials. Performance numbers on up to 512 nodes of the OakForest-PACS and Piz Daint supercomputers are presented, achieving beyond 100 Tflop/s for computing 100 inner eigenvalues of sparse matrices of dimension one billion.", "venue": "ISC", "authors": ["Moritz  Kreutzer", "Georg  Hager", "Dominik  Ernst", "Holger  Fehske", "Alan R. Bishop", "Gerhard  Wellein"], "year": 2018, "n_citations": 15}
{"id": 1464209, "s2_id": "dbbafc1b3c8b8ba4308c025d7e5ab291b6ef00e0", "title": "Nuzzer: A Large-Scale Device-Free Passive Localization System for Wireless Environments", "abstract": "The widespread usage of WLANs and mobile devices has fostered the interest in localization systems for wireless environments. The majority of research in the context of wireless-based localization systems has focused on device-based active localization, in which devices are attached to tracked entities. Recently, device-free passive localization (DfP) has been proposed where the tracked entity is neither required to carry devices nor to participate actively in the localization process. Previous studies have focused on small areas and/or controlled environments. In this paper, we present the design, implementation, and analysis of Nuzzer, a large-scale DfP localization system, which tracks entities in real environments, rich in multipath. We first present probabilistic techniques for DfP localization of a single entity and evaluate their performance both analytically and in typical office buildings. Our results show that Nuzzer gives location estimates with less than 2-meters median distance error. We then give an algorithm for estimating the number of entities in an area of interest and localizing them into coarse-grained zones to enhance the scalability of the system. This indicates the suitability of Nuzzer to a large number of application domains.", "venue": "IEEE Transactions on Mobile Computing", "authors": ["Moustafa  Seifeldin", "Ahmed  Saeed", "Ahmed E. Kosba", "Amr  El-Keyi", "Moustafa  Youssef"], "year": 2013, "n_citations": 318}
{"id": 1465759, "s2_id": "cd47bb373a65633a12305b704033aabcb12cd8fb", "title": "A Monitoring System for the BaBar INFN Computing Cluster", "abstract": "Monitoring large clusters is a challenging problem. It is necessary to observe a large quantity of devices with a reasonably short delay between consecutive observations. The set of monitored devices may include PCs, network switches, tape libraries and other equipments. The monitoring activity should not impact the performances of the system. In this paper we present PerfMC, a monitoring system for large clusters. PerfMC is driven by an XML configuration file, and uses the Simple Network Management Protocol (SNMP) for data collection. SNMP is a standard protocol implemented by many networked equipments, so the tool can be used to monitor a wide range of devices. System administrators can display informations on the status of each device by connecting to a WEB server embedded in PerfMC. The WEB server can produce graphs showing the value of different monitored quantities as a function of time; it can also produce arbitrary XML pages by applying XSL Transformations to an internal XML representation of the cluster\u2019s status. XSL Transformations may be used to produce HTML pages which can be displayed by ordinary WEB browsers. PerfMC aims at being relatively easy to configure and operate, and highly efficient. It is currently being used t monitor the Italian Reprocessing farm for the BaBar experiment, which is made of about 200 dual-CPU Linux machines.", "venue": "ArXiv", "authors": ["Moreno  Marzolla", "V.  Melloni"], "year": 2003, "n_citations": 3}
{"id": 1466792, "s2_id": "ae7cdd310d5e97f7400e02397a79b9734e62dc93", "title": "A NUMA-Aware Provably-Efficient Task-Parallel Platform Based on the Work-First Principle", "abstract": "Task parallelism is designed to simplify the task of parallel programming. When executing a task parallel program on modern NUMA architectures, it can fail to scale due to the phenomenon called work inflation, where the overall processing time that multiple cores spend on doing useful work is higher compared to the time required to do the same amount of work on one core, due to effects experienced only during parallel executions such as additional cache misses, remote memory accesses, and memory bandwidth issues.One can mitigate work inflation by co-locating the computation with its data, but this is nontrivial to do with task parallel programs. First, by design, the scheduling for task parallel programs is automated, giving the user little control over where the computation is performed. Second, the platforms tend to employ work stealing, which provides strong theoretical guarantees, but its randomized protocol for load balancing does not discern between work items that are far away versus ones that are closer.In this work, we propose NUMA-WS, a NUMA-aware task parallel platform engineered based on the work-first principle. By abiding by the work-first principle, we are able to obtain a platform that is work efficient, provides the same theoretical guarantees as a classic work stealing scheduler, and mitigates work inflation. We have extended Cilk Plus runtime system to implemented NUMA-WS. Empirical results indicate that the NUMA-WS is work efficient and can provide better scalability by mitigating work inflation.", "venue": "2018 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Justin  Deters", "Jiaye  Wu", "Yifan  Xu", "I-Ting Angelina Lee"], "year": 2018, "n_citations": 4}
{"id": 1470172, "s2_id": "41d08b6ec34bae0600c7cc5b913249a077e26d71", "title": "Bayesian Optimization for auto-tuning GPU kernels", "abstract": "Finding optimal parameter configurations for tunable GPU kernels is a non-trivial exercise for large search spaces, even when automated. This poses an optimization task on a nonconvex search space, using an expensive to evaluate function with unknown derivative. These characteristics make a good candidate for Bayesian Optimization, which has not been applied to this problem before. However, the application of Bayesian Optimization to this problem is challenging. We demonstrate how to deal with the rough, discrete, constrained search spaces, containing invalid configurations. We introduce a novel contextual variance exploration factor, as well as new acquisition functions with improved scalability, combined with an informed acquisition function selection mechanism. By comparing the performance of our Bayesian Optimization implementation on various test cases to the existing search strategies in Kernel Tuner, as well as other Bayesian Optimization implementations, we demonstrate that our search strategies generalize well and consistently outperform other search strategies by a wide margin.", "venue": "2021 International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)", "authors": ["Floris-Jan  Willemsen", "Rob  van Nieuwpoort", "Ben  van Werkhoven"], "year": 2021, "n_citations": 0}
{"id": 1473047, "s2_id": "0c4f752d34a4d7c35f99a57f2ce0f3bbb2afef4b", "title": "Desynchronization and Wave Pattern Formation in MPI-Parallel and Hybrid Memory-Bound Programs", "abstract": "Analytic, first-principles performance modeling of distributed-memory parallel codes is notoriously imprecise. Even for applications with extremely regular and homogeneous compute-communicate phases, simply adding communication time to computation time does often not yield a satisfactory prediction of parallel runtime due to deviations from the expected simple lockstep pattern caused by system noise, variations in communication time, and inherent load imbalance. In this paper, we highlight the specific cases of provoked and spontaneous desynchronization of memory-bound, bulk-synchronous pure MPI and hybrid MPI+OpenMP programs. Using simple microbenchmarks we observe that although desynchronization can introduce increased waiting time per process, it does not necessarily cause lower resource utilization but can lead to an increase in available bandwidth per core. In case of significant communication overhead, even natural noise can shove the system into a state of automatic overlap of communication and computation, improving the overall time to solution. The saturation point, i.e., the number of processes per memory domain required to achieve full memory bandwidth, is pivotal in the dynamics of this process and the emerging stable wave pattern. We also demonstrate how hybrid MPI-OpenMP programming can prevent desirable desynchronization by eliminating the bandwidth bottleneck among processes. A Chebyshev filter diagonalization application is used to demonstrate some of the observed effects in a realistic setting.", "venue": "ISC", "authors": ["Ayesha  Afzal", "Georg  Hager", "Gerhard  Wellein"], "year": 2020, "n_citations": 4}
{"id": 1476245, "s2_id": "47ce31a21eadfb5a2ce9ce0605f70ef02177c1f7", "title": "Digital blood in massively parallel CPU/GPU systems for the study of platelet transport", "abstract": "We propose a highly versatile computational framework for the simulation of cellular blood flow focusing on extreme performance without compromising accuracy or complexity. The tool couples the lattice Boltzmann solver Palabos for the simulation of blood plasma, a novel finite-element method (FEM) solver for the resolution of deformable blood cells, and an immersed boundary method for the coupling of the two phases. The design of the tool supports hybrid CPU\u2013GPU executions (fluid, fluid\u2013solid interaction on CPUs, deformable bodies on GPUs), and is non-intrusive, as each of the three components can be replaced in a modular way. The FEM-based kernel for solid dynamics outperforms other FEM solvers and its performance is comparable to state-of-the-art mass\u2013spring systems. We perform an exhaustive performance analysis on Piz Daint at the Swiss National Supercomputing Centre and provide case studies focused on platelet transport, implicitly validating the accuracy of our tool. The tests show that this versatile framework combines unprecedented accuracy with massive performance, rendering it suitable for upcoming exascale architectures.", "venue": "Interface Focus", "authors": ["Christos  Kotsalos", "Jonas  Latt", "Joel  Beny", "Bastien  Chopard"], "year": 2020, "n_citations": 13}
{"id": 1478377, "s2_id": "1438cf1ad9b1467f872b721d27df17cb43e0b528", "title": "Programmable Ethernet Switches and Their Applications", "abstract": "Simplicity, cost effectiveness, scalability, and economies of scale make Ethernet a popular choice for (a) local area networks (LAN), as well as for (b) storage area networks (SAN), and increasingly (c) metropolitan-area networks (MAN). Applications of Ethernet in the SAN and MAN arena elevate it from a LAN technology to a ubiquitous networking technology. With the expanded applicability of Ethernet there are certain adaptability issues which prompt rethinking of some of its architectural features. The Spanning-Tree based switching mechanism, considered to be very efficient at avoiding loops in LAN environments, is a performance bottleneck in the metro network context. Absence of an explicit switching path selection mechanism prohibits traffic engineering in metro networks. Prolonged spanning tree reconstruction periods after failures make Ethernet unsuitable to support critical applications. Lack of usage regulation mechanisms leads to insufficient isolation between different users, resulting in QoS problems. \nModern Ethernet switches support many advanced features which can be controlled through programmable interfaces. These features are VLAN tagging, rate limiting, and status monitoring. Conventionally, these features are mostly used to statically configure an Ethernet switch. This research proposes to use these features as dynamic control mechanisms in the context of metro and cluster networks to: (1) Maximize physical network link resources by enabling MPLS like traffic engineering, (2) Minimize failure recovery time, and (3) Enforce QoS requirements. \nWith these programmatic configurable control mechanisms, standard Ethernet switches can be used as effective building blocks for metropolitan-area Ethernet networks (MEN), storage-area networks (SAN), and computation cluster interconnects. We validated the usefulness of this new level of control over Ethernet switches with a MEN architecture that features multi-fold throughput gains and sub-second failure recovery time. \nWe discuss how a comprehensive resource management system can be devised using these mechanisms that can result in high performance and fault tolerant metro Ethernet, Storage, and Cluster networks. We also discuss how a network topology can be efficiently evolved by correlating the traffic profile characteristics of the end users and the traffic engineering required in the network. We describe a design methodology for Ethernet-based SAN fabrics utilizing this network evolution technique. \nTo this effect, we develop a network topology planning tool to minimize network infrastructure deployment cost. This topology planning work is specifically targeted towards providing automated tools to design Ethernet storage area network and cluster interconnect topologies with redundancy and fault-tolerance support.", "venue": "ArXiv", "authors": ["Srikant  Sharma", "Tzi-cker  Chiueh"], "year": 2004, "n_citations": 1}
{"id": 1483884, "s2_id": "df1ae2331588a6b232a76e5ff3028e696eb4d86c", "title": "Solving large number of non-stiff, low-dimensional ordinary differential equation systems on GPUs and CPUs: performance comparisons of MPGOS, ODEINT and DifferentialEquations.jl", "abstract": "In this paper, the performance characteristics of different solution techniques and program packages to solve a large number of independent ordinary differential equation systems is examined. The employed hardware are an Intel Core i7-4820K CPU with 30.4 GFLOPS peak double-precision performance per cores and an Nvidia GeForce Titan Black GPU that has a total of 1707 GFLOPS peak double-precision performance. The tested systems (Lorenz equation, Keller--Miksis equation and a pressure relief valve model) are non-stiff and have low dimension. Thus, the performance of the codes are not limited by memory bandwidth, and Runge--Kutta type solvers are efficient and suitable choices. The tested program packages are MPGOS written in C++ and specialised only for GPUs; ODEINT implemented in C++, which supports execution on both CPUs and GPUs; finally, DifferentialEquations.jl written in Julia that also supports execution on both CPUs and GPUs. Using GPUs, the program package MPGOS is superior. For CPU computations, the ODEINT program package has the best performance.", "venue": "ArXiv", "authors": ["D\u00e1niel  Nagy", "Lambert  Plavecz", "Ferenc  Heged\u00fcs"], "year": 2020, "n_citations": 1}
{"id": 1486032, "s2_id": "c5608a031040ba2308f8957a14e54e55fcb91726", "title": "SDN helps Big Data to optimize access to data", "abstract": "This chapter introduces the state of the art in the emerging area of combining high performance computing (HPC) with Big Data Analysis. To understand the new area, the chapter first surveys the existing approaches to integrating HPC with Big Data. Next, the chapter introduces several optimization solutions that focus on how to minimize the data transfer time from computation-intensive applications to analysis intensive applications as well as minimizing the end-to-end time-to-solution. The solutions utilize Software Defined Network (SDN) to adaptively use both high speed interconnect network and high performance parallel file systems to optimize the application performance. A computational framework called DataBroker is designed and developed to enable a tight integration of HPC with data analysis. Multiple types of experiments have been conducted to show different performance issues in both message passing and parallel file systems and to verify the effectiveness of the proposed research approaches.", "venue": "ArXiv", "authors": ["Yuankun  Fu", "Fengguang  Song"], "year": 2020, "n_citations": 0}
{"id": 1486809, "s2_id": "7f27e5f6f6ab1a65132a36027c91fe598c9145a4", "title": "Evaluating the Cost of Atomic Operations on Modern Architectures", "abstract": "Atomic operations (atomics) such as Compare-and-Swap (CAS) or Fetch-and-Add (FAA) are ubiquitous in parallel programming. Yet, performance tradeoffs between these operations and various characteristics of such systems, such as the structure of caches, are unclear and have not been thoroughly analyzed. In this paper we establish an evaluation methodology, develop a performance model, and present a set of detailed benchmarks for latency and bandwidth of different atomics. We consider various state-of-the-art x86 architectures: Intel Haswell, Xeon Phi, Ivy Bridge, and AMD Bulldozer. The results unveil surprising performance relationships between the considered atomics and architectural properties such as the coherence state of the accessed cache lines. One key finding is that all the tested atomics have comparable latency and bandwidth even if they are characterized by different consensus numbers. Another insight is that the design of atomics prevents any instruction-level parallelism even if there are no dependencies between the issued operations. Finally, we discuss solutions to the discovered performance issues in the analyzed architectures. Our analysis can be used for making better design and algorithmic decisions in parallel programming on various architectures deployed in both off-the-shelf machines and large compute systems.", "venue": "2015 International Conference on Parallel Architecture and Compilation (PACT)", "authors": ["Hermann  Schweizer", "Maciej  Besta", "Torsten  Hoefler"], "year": 2015, "n_citations": 72}
{"id": 1493775, "s2_id": "2b4fccfc00fad446717315d312f855103e09e2d9", "title": "Dynamic Performance Management: An Approach for Managing the Common Goods", "abstract": "Public organizations need innovative approaches for managing common goods and to explain the dynamics linking the (re)generation of common goods and organizational performance. Although system dynamics is recognised as a useful approach for managing common goods, public organizations rarely adopt the system dynamics for this goal. The paper aims to review the literature on the system dynamics and its recent application, known as dynamic performance management, to highlight the state of the art and future opportunities on the management of common goods. The authors analyzed 144 documents using a systematic literature review. The results obtained outline a fair number of documents, countries and journals involving the study of system dynamics, but do not cover sufficient research on the linking between the (re)generation of common goods and organizational performance. This paper outlines academic and practical contributions. Firstly, it contributes to the theory of common goods. It provides insight for linking the management of common goods and organizational performance through the use of dynamic performance management approach. Furthermore, it shows scholars the main research opportunities. Secondly, it indicates to practitioners the documents providing useful ideas on the adoption of system dynamics for managing common goods.", "venue": "Sustainability", "authors": ["Alberto  Sardi", "Enrico  Sorano"], "year": 2019, "n_citations": 12}
{"id": 1494075, "s2_id": "8e0cc9df3154184ebefee3502e245b7e3f375166", "title": "Understanding HPC Benchmark Performance on Intel Broadwell and Cascade Lake Processors", "abstract": "Hardware platforms in high performance computing are constantly getting more complex to handle even when considering multicore CPUs alone. Numerous features and configuration options in the hardware and the software environment that are relevant for performance are not even known to most application users or developers. Microbenchmarks, i.e., simple codes that fathom a particular aspect of the hardware, can help to shed light on such issues, but only if they are well understood and if the results can be reconciled with known facts or performance models. The insight gained from microbenchmarks may then be applied to real applications for performance analysis or optimization. In this paper we investigate two modern Intel x86 server CPU architectures in depth: Broadwell EP and Cascade Lake SP. We highlight relevant hardware configuration settings that can have a decisive impact on code performance and show how to properly measure on-chip and off-chip data transfer bandwidths. The new victim L3 cache of Cascade Lake and its advanced replacement policy receive due attention. Finally we use DGEMM, sparse matrix-vector multiplication, and the HPCG benchmark to make a connection to relevant application scenarios.", "venue": "ISC", "authors": ["Christie L. Alappat", "Johannes  Hofmann", "Georg  Hager", "Holger  Fehske", "Alan R. Bishop", "Gerhard  Wellein"], "year": 2020, "n_citations": 10}
{"id": 1494708, "s2_id": "fdd0b785b246458bdde668167804ab7fa0f55616", "title": "Enhancing application performance by memory partitioning in Android platforms", "abstract": "This paper suggests a new memory partitioning scheme that can enhance process lifecycle, while avoiding Low Memory Killer and Out-of-Memory Killer operations on mobile devices. Our proposed scheme offers the complete concept of virtual memory nodes in operating systems of Android devices.", "venue": "2013 IEEE International Conference on Consumer Electronics (ICCE)", "authors": ["Geunsik  Lim", "Changwoo  Min", "Young Ik Eom"], "year": 2013, "n_citations": 11}
{"id": 1495106, "s2_id": "c03ea9a685a121b2f9a9124fdf063325475d29d6", "title": "High Frequency Radio Network Simulation Using OMNeT++", "abstract": "Harris Corporation has an interest in making HF radios a suitable medium for wireless information networks using standard Internet protocols. Although HF radio links have many unique characteristics, HF wireless subnets can be subject to many of the same traffic flow characteristics and topologies as existing line-of-sight (LOS) radio networks, giving rise to similar issues (media access, connectivity, routing) which lend themselves to investigation through simulation. Accordingly, we have undertaken to develop efficient, high-fidelity simulations of various aspects of HF radio communications and networking using the OMNeT++ framework. Essential aspects of these simulations include HF channel models simulating relevant channel attributes such as Signal to Noise Ratio, multipath, and Doppler spread; a calibrated physical layer model reproducing the error statistics (including burst error distributions) of the MIL-STD-188-110B/C HF modem waveforms, both narrowband (3 kHz) and wideband (up to 24 kHz) on the simulated HF channels; a model of the NATO STANAG 5066 data link protocol; and integration of these models with the OMNeT++ network simulation framework and its INET library of Internet protocol models. This simulation is used to evaluate the impacts of different STANAG 5066 configuration settings on TCP network performance, and to evaluate strategies for optimizing throughput over HF links using TCP Performance Enhancing Proxy (PEP) techniques.", "venue": "ArXiv", "authors": ["Jeffery  Weston", "Eric N. Koski"], "year": 2015, "n_citations": 0}
{"id": 1500370, "s2_id": "dc0d8e378a1e8d9dbcab66803bb085978bb74a26", "title": "Applications of polling systems", "abstract": "Since the first paper on polling systems, written by Mack in 1957, a huge number of papers on this topic has been written. A typical polling system consists of a number of queues, attended by a single server. In several surveys, the most notable ones written by Takagi, detailed and comprehensive descriptions of the mathematical analysis of polling systems are provided. The goal of the present survey paper is to complement these papers by putting the emphasis on applications of polling models. We discuss not only the capabilities, but also the limitations of polling models in representing various applications. The present survey is directed at both academicians and practitioners.", "venue": "ArXiv", "authors": ["Marko A. A. Boon", "Robert D. van der Mei", "Erik M. M. Winands"], "year": 2014, "n_citations": 117}
{"id": 1501921, "s2_id": "f56ccab2002323c0b6a5fcb1139aa88639799dc7", "title": "A Performance Comparison of Network Simulators for Wireless Networks", "abstract": "Network simulation is the most useful and common methodology used to evaluate different network to-pologies without real world implementation. Network simulators are widely used by the research community to evaluate new theories and hypotheses. There are a number of network simulators, for instance, ns-2, ns-3, OMNET++, SWAN, OPNET, Jist, and GloMoSiM etc. Therefore, the selection of a network simulator for evaluating research work is a crucial task for researchers. The main focus of this paper is to compare the state-of-the-art, open source network simulators based on the following parameters: CPU utilization, memory usage, computational time, and scalability by simulating a MANET routing protocol, to identify an optimal network simulator for the research community.", "venue": "ArXiv", "authors": ["Atta ur Rehman Khan", "Sardar Muhammad Bilal", "Mazliza  Othman"], "year": 2013, "n_citations": 54}
{"id": 1503873, "s2_id": "6d9544c32c4c96dda8bd714ae7e45aceb1698dc6", "title": "MLP Aware Scheduling Techniques in Multithreaded Processors", "abstract": "Major chip manufacturers have all introduced Multithreaded processors. These processors are used for running a variety of workloads. Efficient resource utilization is an important design aspect in such processors. Particularly, it is important to take advantage of available memory-level parallelism(MLP). In this paper I propose a MLP aware operating system (OS) scheduling algorithm for Multithreaded Multi-core processors. By observing the MLP available in each thread and by balancing it with available MLP resources in the system the OS will come up with a new schedule of threads for the next quantum that could potentially improve overall performance. We do a qualitative comparison of our solution with other hardware and software techniques. This work can be extended by doing a quantitative evaluation and by further refining the scheduling optimization.", "venue": "ArXiv", "authors": ["Murthy  Durbhakula"], "year": 2019, "n_citations": 0}
{"id": 1504186, "s2_id": "8076aa210220f370cc69a52869e36609c9260436", "title": "Enabling Homomorphically Encrypted Inference for Large DNN Models", "abstract": "The proliferation of machine learning services in the last few years has raised data privacy concerns. Homomorphic encryption (HE) enables inference using encrypted data but it incurs 100x-10,000x memory and runtime overheads. Secure deep neural network (DNN) inference using HE is currently limited by computing and memory resources, with frameworks requiring hundreds of gigabytes of DRAM to evaluate small models. To overcome these limitations, in this paper we explore the feasibility of leveraging hybrid memory systems comprised of DRAM and persistent memory. In particular, we explore the recently-released Intel Optane PMem technology and the Intel HE-Transformer nGraph to run large neural networks such as MobileNetV2 (in its largest variant) and ResNet-50 for the first time in the literature. We present an in-depth analysis of the efficiency of the executions with different hardware and software configurations. Our results conclude that DNN inference using HE incurs on friendly access patterns for this memory configuration, yielding efficient executions.", "venue": "IEEE Transactions on Computers", "authors": ["Guillermo  Lloret-Talavera", "Marc  Jorda", "Harald  Servat", "Fabian  Boemer", "Chetan  Chauhan", "Shigeki  Tomishima", "Nilesh N. Shah", "Antonio J. Pena"], "year": 2021, "n_citations": 3}
{"id": 1508283, "s2_id": "9465f1712911070ce4c99e56400e815efc876e91", "title": "An analytical model of information dissemination for a gossip-based protocol", "abstract": "We develop an analytical model of information dissemination for a gossiping protocol that combines both pull and push approaches. With this model we analyse how fast an item is replicated through a network, and how fast the item covers the network. We also determine the optimal size of the exchange buffer, to obtain fast replication. Our results are confirmed by large-scale simulation experiments.", "venue": "Comput. Networks", "authors": ["Rena  Bakhshi", "Daniela  Gavidia", "Wan  Fokkink", "Maarten van Steen"], "year": 2009, "n_citations": 23}
{"id": 1509109, "s2_id": "84ed3bf689aa41c4222f06a47e5282dec937d87f", "title": "Restricted Mobility Improves Delay-Throughput Tradeoffs in Mobile Ad Hoc Networks", "abstract": "In this paper, we analyze asymptotic delay-throughput tradeoffs in mobile ad hoc networks comprising heterogeneous nodes with restricted mobility. We show that node spatial heterogeneity has the ability to drastically improve upon existing scaling laws established under the assumption that nodes are identical and uniformly visit the entire network area. In particular, we consider the situation in which each node moves around its own home-point according to a restricted mobility process which results into a spatial stationary distribution that decays as a power law of exponent \u03b4 with the distance from the home-point. For such restricted mobility model, we propose a novel class of scheduling and routing schemes, which significantly outperforms all delay-throughput results previously obtained in the case of identical nodes. In particular, for \u03b4 = 2 it is possible to achieve almost constant delay and almost constant per-node throughput (except for a polylogarithmic factor) as the number of nodes increases, even without resorting to sophisticated coding or signal processing techniques.", "venue": "IEEE Transactions on Information Theory", "authors": ["Michele  Garetto", "Emilio  Leonardi"], "year": 2010, "n_citations": 63}
{"id": 1511246, "s2_id": "6bb846b12c634f4512b676823cf5ed802ec43432", "title": "Analysis of the Energy-Performance Tradeoff for Delayed Mobile Offloading", "abstract": "Mobile cloud offloading that migrates heavy computation from mobile devices to powerful cloud servers through communication networks can alleviate the hardware limitations of mobile devices for higher performance and energy saving. Different applications usually give different relative importance to the factors of response time and energy consumption. If a delay- tolerant job is deferred up to a given deadline, or until a fast and energy-efficient network becomes available, the transmission time will be reduced, which could lead to energy savings. However, if the reduced service time fails to cover the extra waiting time, this policy may not be suitable for the delay- sensitive applications. In this paper, we investigate two types of delayed offloading policies, the partial model where jobs can leave from the slow phase of the offloading process and then executed locally on the mobile device, and the full offloading model, where jobs can abandon the WiFi Queue and then offloaded via the Cellular Queue. In both models we minimise the Energy-Response time Weighted Product (ERWP) metric. We find that jobs abandon the queue very often especially when the availability ratio (AR) of the WiFi network is relatively small. We can optimally choose the reneging deadline to achieve different energy-performance tradeoff by optimizing the ERWP metric. The amount of delay a job can tolerate closely de- pends on the application type and the potential energy saving for the mobile device. In general one can say that for delay- sensitive applications, the partial offloading model is preferred when having a suitable reneging rate, while for delay-tolerant applications, the full offloading model shows very good results and outperforms the other offloading models when setting the deadline a large value.", "venue": "EAI Endorsed Trans. Energy Web", "authors": ["Huaming  Wu", "Katinka  Wolter"], "year": 2016, "n_citations": 9}
{"id": 1512889, "s2_id": "651943e1d09bf3a5e5691ca1ed595c49babd51a7", "title": "GEMMbench: a framework for reproducible and collaborative benchmarking of matrix multiplication", "abstract": "The generic matrix-matrix multiplication (GEMM) is arguably the most popular computational kernel of the 20th century. Yet, surprisingly, no common methodology for evaluating GEMM performance has been established over the many decades of using GEMM for comparing architectures, compilers and ninja-class programmers. \nWe introduce GEMMbench, a framework and methodology for evaluating performance of GEMM implementations. GEMMbench is implemented on top of Collective Knowledge (CK), a lightweight framework for reproducible and collaborative R&D in computer systems. Using CK allows the R&D community to crowdsource hand-written and compiler-generated GEMM implementations and to study their performance across multiple platforms, data sizes and data types. \nOur initial implementation supports hand-written OpenCL kernels operating on matrices consisting of single- and double-precision floating-point values, and producing single or multiple output elements per work-item (via thread coarsening and vectorization).", "venue": "ArXiv", "authors": ["Anton  Lokhmotov"], "year": 2015, "n_citations": 2}
{"id": 1513731, "s2_id": "ddf7190d582ad7a1b1b070c53fc4fa09039a8bdb", "title": "The Effect of Scheduling on Link Capacity in Multi-hopWireless Networks", "abstract": "Existing models of Multi-Hop Wireless Networks (MHWNs) assume that interference estimators of link quality such as observed busy time predict the capacity of the links. We show that these estimators do not capture the intricate interactions that occur at the scheduling level, which have a large impact on effective link capacity under contention based MAC protocols. We observe that scheduling problems arise only among those interfering sources whose concurrent transmissions cannot be prevented by the MAC protocol's collision management mechanisms; other interfering sources can arbitrate the medium and coexist successfully. Based on this observation, we propose a methodology for rating links and show that it achieves high correlation with observed behavior in simulation. We then use this rating as part of a branch-and-bound framework based on a linear programming formulation for traffic engineering in static MHWNs and show that it achieves considerable improvement in performance relative to interference based models.", "venue": "ArXiv", "authors": ["Vinay  Kolar", "Nael B. Abu-Ghazaleh"], "year": 2006, "n_citations": 10}
{"id": 1518487, "s2_id": "39c983891ca99ed79b06a027ba2965f91b1a46fe", "title": "On Metric Skyline Processing by PM-tree", "abstract": "The task of similarity search in multimedia databases is usually accomplished by range or k nearest neighbor queries. However, the expressing power of these \"single-example\" queries fails when the user's delicate query intent is not available as a single example. Recently, the well-known skyline operator was reused in metric similarity search as a \"multi-example\" query type. When applied on a multi-dimensional database (i.e., on a multi-attribute table), the traditional skyline operator selects all database objects that are not dominated by other objects. The metric skyline query adopts the skyline operator such that the multiple attributes are represented by distances (similarities) to multiple query examples. Hence, we can view the metric skyline as a set of representative database objects which are as similar to all the examples as possible and, simultaneously, are semantically distinct. In this paper we propose a technique of processing the metric skyline query by use of PM-tree, while we show that our technique significantly outperforms the original M-tree based implementation in both time and space costs. In experiments we also evaluate the partial metric skyline processing, where only a controlled number of skyline objects is retrieved.", "venue": "ArXiv", "authors": ["Tom\u00e1s  Skopal", "Jakub  Lokoc"], "year": 2009, "n_citations": 0}
{"id": 1519138, "s2_id": "574be88e94dbdcbc1e091c463457fefa4788244f", "title": "Large Fork-Join Queues with Nearly Deterministic Arrival and Service Times", "abstract": "In this paper, we study an N server fork-join queue with nearly deterministic arrival and service times. Specifically, we present a fluid limit for the maximum queue length as [Formula: see text]. This fluid limit depends on the initial number of tasks. In order to prove these results, we develop extreme value theory and diffusion approximations for the queue lengths.", "venue": "Mathematics of Operations Research", "authors": ["Dennis  Schol", "Maria  Vlasiou", "Bert  Zwart"], "year": 2021, "n_citations": 0}
{"id": 1519430, "s2_id": "3c6a19ea6d9600f1b3141f6ece1c1636bbd42b53", "title": "Correlation Coefficient Analysis of the Age of Information in Multi-Source Systems", "abstract": "This paper studies the age of information (AoI) on an information updating system such that multiple sources share one server to process packets of updated information. In such systems, packets from different sources compete for the server, and thus they may suffer from being interrupted, being backlogged, and becoming stale. Therefore, in order to grasp structures of such systems, it is crucially important to study a metric indicating a correlation of different sources. In this paper, we aim to analyze the correlation of AoIs on a single-server queueing system with multiple sources. As our contribution, we provide the closed-form expression of the correlation coefficient of the AoIs. To this end, we first derive the Laplace-Stieltjes transform of the stationary distribution of each AoI for the multiple sources. Some nontrivial properties on the systems are revealed from our analysis results.", "venue": "ArXiv", "authors": ["Yukang  Jiang", "Kiichi  Tokuyama", "Yuichiro  Wada", "Moeko  Yajima"], "year": 2020, "n_citations": 0}
{"id": 1523584, "s2_id": "db15a0b731207a6933468522e6335cbffc3f8e1f", "title": "Benchmarking in Optimization: Best Practice and Open Issues", "abstract": "This survey compiles ideas and recommendations from more than a dozen researchers with different backgrounds and from different institutes around the world. Promoting best practice in benchmarking is its main goal. The article discusses eight essential topics in benchmarking: clearly stated goals, well-specified problems, suitable algorithms, adequate performance measures, thoughtful analysis, effective and efficient designs, comprehensible presentations, and guaranteed reproducibility. The final goal is to provide well-accepted guidelines (rules) that might be useful for authors and reviewers. As benchmarking in optimization is an active and evolving field of research this manuscript is meant to co-evolve over time by means of periodic updates.", "venue": "ArXiv", "authors": ["Thomas  Bartz-Beielstein", "Carola  Doerr", "Jakob  Bossek", "Sowmya  Chandrasekaran", "Tome  Eftimov", "Andreas  Fischbach", "Pascal  Kerschke", "Manuel  L\u00f3pez-Ib\u00e1\u00f1ez", "Katherine M. Malan", "Jason H. Moore", "Boris  Naujoks", "Patryk  Orzechowski", "Vanessa  Volz", "Markus  Wagner", "Thomas  Weise"], "year": 2020, "n_citations": 24}
{"id": 1523664, "s2_id": "2624797db824de23a896f96d27f3542e7c7357af", "title": "Performance Analysis for Multi-Antenna Small Cell Networks with Clustered Dynamic TDD", "abstract": "Small cell networks with dynamic time-division duplex (D-TDD) have emerged as a potential solution to address the asymmetric traffic demands in 5G wireless networks. By allowing the dynamic adjustment of cell-specific UL/DL configuration, D-TDD flexibly allocates percentage of subframes to UL and DL transmissions to accommodate the traffic within each cell. However, the unaligned transmissions bring in extra interference which degrades the potential gain achieved by DTDD. In this work, we propose an analytical framework to study the performance of multi-antenna small cell networks with clustered D-TDD, where cell clustering is employed to mitigate the interference from opposite transmission direction in neighboring cells. With tools from stochastic geometry, we derive explicit expressions and tractable tight upper bounds for success probability and network throughput. The proposed analytical framework allows to quantify the effect of key system parameters, such as UL/DL configuration, cluster size, antenna number, and SINR threshold. Our results show the superiority of the clustered D-TDD over the traditional D-TDD, and reveal the fact that there exists an optimal cluster size for DL performance, while UL performance always benefits from a larger cluster.", "venue": "GLOBECOM 2020 - 2020 IEEE Global Communications Conference", "authors": ["Hongguang  Sun", "Howard H. Yang", "Xijun  Wang", "Chao  Xu", "Tony Q.S. Quek"], "year": 2020, "n_citations": 0}
{"id": 1526271, "s2_id": "e2c717f83b0d44711cb344c6148d80de1b01f3c8", "title": "On the Capacity Region of Bipartite and Tripartite Entanglement Switching", "abstract": "We study a quantum switch serving a set of users in a star topology. The function of the switch is to create bipartite or tripartite entangled state among users at the highest possible rates at a fixed ratio. We model a set of randomized switching policies. Discovering that some are better than others, we present analytical results for the case where the switch stores one qubit per user, and find that the best policies outperform a time division multiplexing (TDM) policy for sharing the switch between bipartite and tripartite state generation. This performance improvement decreases as the number of users grows. The model is easily augmented to study the capacity region in the presence of qubit decoherence, obtaining similar results. Moreover, decoherence appears to have little effect on capacity. We also study a smaller class of policies when the switch stores two qubits per user.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Gayane  Vardoyan", "Saikat  Guha", "Philippe  Nain", "Donald F. Towsley"], "year": 2020, "n_citations": 17}
{"id": 1527125, "s2_id": "1c3f548a39c46528d7cc861608e2632a8c2bff15", "title": "Mean-Field Approximation and Quasi-Equilibrium Reduction of Markov Population Models", "abstract": "Markov Population Model is a commonly used framework to describe stochastic systems. Their exact analysis is unfeasible in most cases because of the state space explosion. Approximations are usually sought, often with the goal of reducing the number of variables. Among them, the mean field limit and the quasi-equilibrium approximations stand out. We view them as techniques that are rooted in independent basic principles. At the basis of the mean field limit is the law of large numbers. The principle of the quasi-equilibrium reduction is the separation of temporal scales. It is common practice to apply both limits to an MPM yielding a fully reduced model. Although the two limits should be viewed as completely independent options, they are applied almost invariably in a fixed sequence: MF limit first, QE-reduction second. We present a framework that makes explicit the distinction of the two reductions, and allows an arbitrary order of their application. By inverting the sequence, we show that the double limit does not commute in general: the mean field limit of a time-scale reduced model is not the same as the time-scale reduced limit of a mean field model. An example is provided to demonstrate this phenomenon. Sufficient conditions for the two operations to be freely exchangeable are also provided.", "venue": "QEST", "authors": ["Luca  Bortolussi", "Rytis  Paskauskas"], "year": 2014, "n_citations": 4}
{"id": 1529022, "s2_id": "a92845a3fee0fda73f4263d7763221e4b2f6d637", "title": "Queue Model of Leaf Degree Keeping Process in Gnutella Network", "abstract": "Leaf degree keeping process of Gnutella is discussed in this paper. Queue system based on rules of Gnutella protocol are introduced to modeling this process. The leaf degree distributions resulted from the queue system and from our real measurement are compared. The well match of those distributions reveal that the leaf degree distribution in Gnutella network should not be power law or power law like as reported before. It is more likely a distribution driven by certain queue process specified by the protocol.", "venue": "ArXiv", "authors": ["Chunxi  Li", "Changjia  Chen"], "year": 2006, "n_citations": 0}
{"id": 1535047, "s2_id": "32a844c5cf7b5b3bc6ad30f1f2719d0e9d4c526a", "title": "Toward Transparent Heterogeneous Systems", "abstract": "Heterogeneous parallel systems are widely spread nowadays. Despite their availability, their usage and adoption are still limited, and even more rarely they are used to full power. Indeed, compelling new technologies are constantly developed and keep changing the technological landscape, but each of them targets a limited sub-set of supported devices, and nearly all of them require new programming paradigms and specific toolsets. Software, however, can hardly keep the pace with the growing number of computational capabilities, and developers are less and less motivated in learning skills that could quickly become obsolete. In this paper we present our effort in the direction of a transparent system optimization based on automatic code profiling and Just-In-Time compilation, that resulted in a fully-working embedded prototype capable of dynamically detect computing-intensive code blocks and automatically dispatch them to different computation units. Experimental results show that our system allows gains up to 32x in performance --- after an initial warm-up phase --- without requiring any human intervention.", "venue": "ArXiv", "authors": ["Baptiste  Delporte", "Roberto  Rigamonti", "Alberto  Dassatti"], "year": 2015, "n_citations": 5}
{"id": 1537140, "s2_id": "3c321ba262ffc816f268c1d52ee7c1097d6d5a06", "title": "Advanced Python Performance Monitoring with Score-P", "abstract": "Within the last years, Python became more prominent in the scientific community and is now used for simulations, machine learning, and data analysis. All these tasks profit from additional compute power offered by parallelism and offloading. In the domain of High Performance Computing (HPC), we can look back to decades of experience exploiting different levels of parallelism on the core, node or inter-node level, as well as utilising accelerators. By using performance analysis tools to investigate all these levels of parallelism, we can tune applications for unprecedented performance. Unfortunately, standard Python performance analysis tools cannot cope with highly parallel programs. Since the development of such software is complex and error-prone, we demonstrate an easy-to-use solution based on an existing tool infrastructure for performance analysis. In this paper, we describe how to apply the established instrumentation framework \\scorep to trace Python applications. We finish with a study of the overhead that users can expect for instrumenting their applications.", "venue": "Tools for High Performance Computing 2018 / 2019", "authors": ["Andreas  Gocht", "Robert  Sch\u00f6ne", "Jan  Frenzel"], "year": 2021, "n_citations": 2}
{"id": 1537677, "s2_id": "f7708b29ca4a209896fa437e1d15e3306b165ac9", "title": "Multithreaded Input-Sensitive Profiling", "abstract": "Input-sensitive profiling is a recent performance analysis technique that makes it possible to estimate the empirical cost function of individual routines of a program, helping developers understand how performance scales to larger inputs and pinpoint asymptotic bottlenecks in the code. A current limitation of input-sensitive profilers is that they specifi cally target sequential computations, ignoring any communication between threads. In this paper we show how to overcome this limitation, extending the range of applicability of the ori ginal approach to multithreaded applications and to applications that operate on I/O streams. We develop new metrics for automatically estimating the size of the input given to each routine activation, addressing input produced by nondeterministic memory stores performed by other threads as well as by the OS kernel (e.g., in response to I/O or network operations). We provide real case studies, showing that our extension allows it to characterize the behavior of complex applications more precisely than previous approaches. An extensive experimental investigation on a variety of benchmark suites (including the SPEC OMP2012 and the PARSEC benchmarks) shows that our Valgrind-based input-sensitive profiler incurs an overhead comparable to other prominent heavyweight analysis tools, while collecting significantly more performance points from each profiling session and correctly characterizing both thread-indu ced and external input.", "venue": "ArXiv", "authors": ["Emilio  Coppa", "Camil  Demetrescu", "Irene  Finocchi", "Romolo  Marotta"], "year": 2013, "n_citations": 3}
{"id": 1539399, "s2_id": "67d237e3cf207e9ecfe1e484ebfb6d2dea7cc0ac", "title": "Improving DRAM Performance, Security, and Reliability by Understanding and Exploiting DRAM Timing Parameter Margins", "abstract": "Characterization of real DRAM devices has enabled findings in DRAM device properties, which has led to proposals that significantly improve overall system performance by reducing DRAM access latency and power consumption. In addition to improving system performance, a deeper understanding of DRAM technology via characterization can also improve device reliability and security. These can beseen with the recent discoveries of 1) DRAM-based true random number generators (TRNGs), a method for generating true random numbers using DRAM devices which can be used in many applications, 2) DRAM-based physical unclonable functions (PUFs), a method for generating unique device-dependent keys for identification and authentication, and 3) the RowHammer vulnerability, a phenomenon where repeatedly accessing a DRAM row can cause failures in unaccessed neighboring DRAM rows.To advance DRAM-based discoveries and mechanisms, this dissertation rigorously characterizes many modern commodity DRAM devices and shows that by exploitingDRAM access timing margins within manufacturer-recommended DRAM timing specifications, we can significantly improve system performance, reduce power consumption, and improve device reliability and security. First, we characterize DRAM timing parameter margins and find that certain regions of DRAM can be accessed faster than other regions due to DRAM cell process manufacturing variation. We exploit this by enabling variable access times depending on the DRAM cells being accessed, whichnot only improves overall system performance, but also decreases power consumption. Second, we find that we can uniquely identify DRAM devices by the locations offailures that result when we access DRAM with timing parameters reduced below specification values. Because we induce these failures with DRAM accesses, we cangenerate these unique identifiers significantly more quickly than prior work. Third, we propose a random number generator that is based on our observation that timingfailures in certain DRAM cells are randomly induced and can thus be repeatedly polled to very quickly generate true random values. Finally, we characterize the RowHammersecurity vulnerability on a wide range of modern DRAM chips while violating the DRAM refresh requirement in order to directly characterize the underlying DRAM technology without the interference of refresh commands. We demonstrate with our characterization of real chips, that existing RowHammer mitigation mechanisms eitherare not scalable or suffer from prohibitively large performance overheads in projected future devices and it is critical to research more effective solutions to RowHammer.Overall, our studies build a new understanding of modern DRAM devices to improve computing system performance, reliability and security all at the same time.", "venue": "ArXiv", "authors": ["Jeremie  Kim"], "year": 2021, "n_citations": 1}
{"id": 1543883, "s2_id": "c9afbed960f5e619807a095a66c7c87fe3ba10f2", "title": "Stability of a Peer-to-Peer Communication System", "abstract": "This paper focuses on the stationary portion of file download in an unstructured peer-to-peer network, which typically follows for many hours after a flash crowd initiation. The model includes the case that peers can have some pieces at the time of arrival. The contribution of this paper is to identify how much help is needed from the seeds, either fixed seeds or peer seeds (which are peers remaining in the system after obtaining a complete collection), to stabilize the system. The dominant cause for instability is the missing piece syndrome, whereby one piece becomes very rare in the network. It is shown that stability can be achieved with only a small amount of help from peer seeds-even with very little help from a fixed seed, peers need dwell as peer seeds on average only long enough to upload one additional piece. The region of stability is insensitive to the piece selection policy. Network coding can substantially increase the region of stability in case a portion of the new peers arrive with randomly coded pieces.", "venue": "IEEE Transactions on Information Theory", "authors": ["Ji  Zhu", "Bruce E. Hajek"], "year": 2012, "n_citations": 27}
{"id": 1544847, "s2_id": "87eacd05c95bac1adc4df045463ad434e515c09c", "title": "Architectural Middleware that Supports Building High-performance, Scalable, Ubiquitous, Intelligent Personal Assistants", "abstract": "Intelligent Personal Assistants (IPAs) are software agents that can perform tasks on behalf of individuals and assist them on many of their daily activities. IPAs capabilities are expanding rapidly due to the recent advances on areas such as natural language processing, machine learning, artificial cognition, and ubiquitous computing, which equip the agents with competences to understand what users say, collect information from everyday ubiquitous devices (e.g., smartphones, wearables, tablets, laptops, cars, household appliances, etc.), learn user preferences, deliver data-driven search results, and make decisions based on user's context. Apart from the inherent complexity of building such IPAs, developers and researchers have to address many critical architectural challenges (e.g., low-latency, scalability, concurrency, ubiquity, code mobility, interoperability, support to cognitive services and reasoning, to name a few.), thereby diverting them from their main goal: building IPAs. Thus, our contribution in this paper is twofold: 1) we propose an architecture for a platform-agnostic, high-performance, ubiquitous, and distributed middleware that alleviates the burdensome task of dealing with low-level implementation details when building IPAs by adding multiple abstraction layers that hide the underlying complexity; and 2) we present an implementation of the middleware that concretizes the aforementioned architecture and allows the development of high-level capabilities while scaling the system up to hundreds of thousands of IPAs with no extra effort. We demonstrate the powerfulness of our middleware by analyzing software metrics for complexity, effort, performance, cohesion and coupling when developing a conversational IPA.", "venue": "ArXiv", "authors": ["Oscar J. Romero"], "year": 2019, "n_citations": 0}
{"id": 1547550, "s2_id": "5b2c794a5442b2b0e582e87d00905783f1a8ffc3", "title": "Realistic, Extensible DNS and mDNS Models for INET/OMNeT++", "abstract": "The domain name system (DNS) is one of the core services in today's network structures. In local and ad-hoc networks DNS is often enhanced or replaced by mDNS. As of yet, no simulation models for DNS and mDNS have been developed for INET/OMNeT++. We introduce DNS and mDNS simulation models for OMNeT++, which allow researchers to easily prototype and evaluate extensions for these protocols. In addition, we present models for our own experimental extensions, namely Stateless DNS and Privacy-Enhanced mDNS, that are based on the aforementioned models. Using our models we were able to further improve the efficiency of our protocol extensions.", "venue": "ArXiv", "authors": ["Andreas  Rain", "Daniel  Kaiser", "Marcel  Waldvogel"], "year": 2015, "n_citations": 1}
{"id": 1549811, "s2_id": "1c9bc872b565ccdf7df4a10f566837b0542b0543", "title": "Kafka versus RabbitMQ", "abstract": "Publish/subscribe is a distributed interaction paradigm well adapted to the deployment of scalable and loosely coupled systems. \nApache Kafka and RabbitMQ are two popular open-source and commercially-supported pub/sub systems that have been around for almost a decade and have seen wide adoption. Given the popularity of these two systems and the fact that both are branded as pub/sub systems, two frequently asked questions in the relevant online forums are: how do they compare against each other and which one to use? \nIn this paper, we frame the arguments in a holistic approach by establishing a common comparison framework based on the core functionalities of pub/sub systems. Using this framework, we then venture into a qualitative and quantitative (i.e. empirical) comparison of the common features of the two systems. Additionally, we also highlight the distinct features that each of these systems has. After enumerating a set of use cases that are best suited for RabbitMQ or Kafka, we try to guide the reader through a determination table to choose the best architecture given his/her particular set of requirements.", "venue": "ArXiv", "authors": ["Philippe  Dobbelaere", "Kyumars Sheykh Esmaili"], "year": 2017, "n_citations": 5}
{"id": 1550481, "s2_id": "9a016a6199803da324f00b7dafb81ef8b133983c", "title": "Better Algorithms for Hybrid Circuit and Packet Switching in Data Centers", "abstract": "Hybrid circuit and packet switching for data center networking (DCN) has received considerable research attention recently. A hybrid-switched DCN employs a much faster circuit switch that is reconfigurable with a nontrivial cost, and a much slower packet switch that is reconfigurable with no cost, to interconnect its racks of servers. The research problem is, given a traffic demand matrix (between the racks), how to compute a good circuit switch configuration schedule so that the vast majority of the traffic demand is removed by the circuit switch, leaving a remaining demand matrix that contains only small elements for the packet switch to handle. In this paper, we propose two new hybrid switch scheduling algorithms under two different scheduling constraints. Our first algorithm, called 2-hop Eclipse, strikes a much better tradeoff between the resulting performance (of the hybrid switch) and the computational complexity (of the algorithm) than the state of the art solution Eclipse/Eclipse++. Our second algorithm, called BFF (best first fit), is the first hybrid switching solution that exploits the potential partial reconfiguration capability of the circuit switch for performance gains.", "venue": "ArXiv", "authors": ["Liang  Liu", "Long  Gong", "Sen  Yang", "Jun  Xu", "Lance  Fortnow"], "year": 2017, "n_citations": 3}
{"id": 1554305, "s2_id": "31feaa7ed2f37e021021c8c2725586fe067837fe", "title": "GPTPU: Accelerating Applications using Edge Tensor Processing Units", "abstract": "Neural network (NN) accelerators have been integrated into a widespectrum of computer systems to accommodate the rapidly growing demands for artificial intelligence (AI) and machine learning (ML) applications. NN accelerators share the idea of providing native hardware support for operations on multidimensional tensor data. Therefore, NN accelerators are theoretically tensor processors that can improve system performance for any problem that uses tensors as inputs/outputs. Unfortunately, commercially available NN accelerators only expose computation capabilities through AI/MLspecific interfaces. Furthermore, NN accelerators reveal very few hardware design details, so applications cannot easily leverage the tensor operations NN accelerators provide. This paper introduces General-Purpose Computing on Edge Tensor Processing Units (GPETPU), an open-source, open-architecture framework that allows the developer and research communities to discover opportunities that NN accelerators enable for applications. GPETPU includes a powerful programming interface with efficient runtime system-level support\u2014similar to that of CUDA/OpenCL in GPGPU computing\u2014to bridge the gap between application demands and mismatched hardware/software interfaces. We built GPETPU machine uses Edge Tensor Processing Units (Edge TPUs), which are widely available and representative of many commercial NN accelerators. We identified several novel use cases and revisited the algorithms. By leveraging the underlying Edge TPUs to perform tensor-algorithm-based compute kernels, our results reveal that GPETPU can achieve a 2.46\u00d7 speedup over highend CPUs and reduce energy consumption by 40%.", "venue": "ArXiv", "authors": ["Kuan-Chieh  Hsu", "Hung-Wei  Tseng"], "year": 2021, "n_citations": 0}
{"id": 1556386, "s2_id": "382aba541c46a1e916cbe57deaa9abb167c64db9", "title": "A Processor-Sharing Model for the Performance of Virtualized Network Functions", "abstract": "The parallel execution of requests in a Cloud Computing platform, as for Virtualized Network Functions, is modeled by an M^[X]/M/1 Processor-Sharing (PS) system, where each request is seen as a batch of unit jobs. The performance of such paralleled system can then be measured by the quantiles of the batch sojourn time distribution. In this paper, we address the evaluation of this distribution for the M^[X]/M/1-PS queue with batch arrivals and geometrically distributed batch size. General results on the residual busy period (after a tagged batch arrival time) and the number of unit jobs served during this residual busy period are first derived. This enables us to provide an approximation for the distribution tail of the batch sojourn time whose accuracy is confirmed by simulation.", "venue": "2019 31st International Teletraffic Congress (ITC 31)", "authors": ["Fabrice  Guillemin", "Veronica Quintuna Rodriguez", "Alain  Simonian"], "year": 2019, "n_citations": 5}
{"id": 1557343, "s2_id": "bf3d89e4bbe7a3eed2f9667aa125ebc17c76f7d9", "title": "A Hermite-like basis for faster matrix-free evaluation of interior penalty discontinuous Galerkin operators", "abstract": "This work proposes a basis for improved throughput of matrix-free evaluation of discontinuous Galerkin symmetric interior penalty discretizations on hexahedral elements. The basis relies on ideas of Hermite polynomials. It is used in a fully discontinuous setting not for higher order continuity but to minimize the effective stencil width, namely to limit the neighbor access of an element to one data point for the function value and one for the derivative. The basis is extended to higher orders with nodal contributions derived from roots of Jacobi polynomials and extended to multiple dimensions with tensor products, which enable the use of sum factorization. The beneficial effect of the reduced data access on modern processors is shown. Furthermore, the viability of the basis in the context of multigrid solvers is analyzed. While a plain point-Jacobi approach is less efficient than with the best nodal polynomials, a basis change via sum-factorization techniques enables the combination of the fast matrix-vector products with effective multigrid constituents. The basis change is essentially for free on modern hardware because these computations can be hidden behind the cost of the data access.", "venue": "ArXiv", "authors": ["Martin  Kronbichler", "Katharina  Kormann", "Niklas  Fehn", "Peter  Munch", "Julius  Witte"], "year": 2019, "n_citations": 6}
{"id": 1558559, "s2_id": "c68c7b33931581ebefac5d8a1c423067486c1b6b", "title": "Toward accurate platform-aware performance modeling for deep neural networks", "abstract": "In this paper, we provide a fine-grain machine learning-based method, PerfNetV2, which improves the accuracy of our previous work for modeling the neural network performance on a variety of GPU accelerators. Given an application, the proposed method can be used to predict the inference time and training time of the convolutional neural networks used in the application, which enables the system developer to optimize the performance by choosing the neural networks and/or incorporating the hardware accelerators to deliver satisfactory results in time. Furthermore, the proposed method is capable of predicting the performance of an unseen or non-existing device, e.g. a new GPU which has a higher operating frequency with less processor cores, but more memory capacity. This allows a system developer to quickly search the hardware design space and/or fine-tune the system configuration. Compared to the previous works, PerfNetV2 delivers more accurate results by modeling detailed host-accelerator interactions in executing the full neural networks and improving the architecture of the machine learning model used in the predictor. Our case studies show that PerfNetV2 yields a mean absolute percentage error within 13.1% on LeNet, AlexNet, and VGG16 on NVIDIA GTX-1080Ti, while the error rate on a previous work published in ICBD 2018 could be as large as 200%.", "venue": "ArXiv", "authors": ["Chuan-Chi  Wang", "Ying-Chiao  Liao", "Ming-Chang  Kao", "Wen-Yew  Liang", "Shih-Hao  Hung"], "year": 2020, "n_citations": 0}
{"id": 1560999, "s2_id": "bcc219852aefc27ef644aca9ed3fbf07202baec4", "title": "Stationary Distribution of a Generalized LRU-MRU Content Cache", "abstract": "Many different caching mechanisms have been previously proposed, exploring different insertion and eviction policies and their performance individually and as part of caching networks. We obtain a novel closed-form stationary invariant distribution for a generalization of Least Recently Used (LRU) and Most Recently Used (MRU) eviction for single caching nodes under a reference Markov model. Numerical comparisons are made with an \u201cIncremental Rank Progress\u201d (IRP a.k.a. CLIMB) and random eviction (RE a.k.a. random replacement, RANDOM) methods under a steady-state Zipf popularity distribution. The range of cache hit probabilities is smaller under MRU and larger under IRP compared to LRU. We conclude with the invariant distribution for a special case of a RE caching tree-network.", "venue": "2018 International Conference on Computing, Networking and Communications (ICNC)", "authors": ["George  Kesidis"], "year": 2018, "n_citations": 1}
{"id": 1566100, "s2_id": "7e8a8bea81898ed3d511c21c10380109563722e3", "title": "Cost-Efficient Storage for On-Demand Video Streaming on Cloud", "abstract": "Video stream is converted to several formats to support the user\u2019s device, this conversion process is called video transcoding, which imposes high storage and powerful resources. With emerging of cloud technology, video stream companies adopted to process video on the cloud. Generally, many formats of the same video are made (pre-transcoded) and streamed to the adequate user\u2019s device. However, pre-transcoding demands huge storage space and incurs a high-cost to the video stream companies. More importantly, the pre-transcoding of video streams could be hierarchy carried out through different storage types in the cloud. To minimize the storage cost, in this paper, we propose a method to store video streams in the hierarchical storage of the cloud. Particularly, we develop a method to decide which video stream should be pre-transcoded in its suitable cloud storage to minimize the overall cost. Experimental simulation and results show the effectiveness of our approach, specifically, when the percentage of frequently accessed videos is high in repositories, the proposed approach minimizes the overall cost by up to 40%.", "venue": "2020 IEEE 6th World Forum on Internet of Things (WF-IoT)", "authors": ["Mahmoud  Darwich", "Yasser  Ismail", "Talal  Darwich", "Magdy  Bayoumi"], "year": 2020, "n_citations": 3}
{"id": 1566983, "s2_id": "b74480f57c2bf7cfa503fd546608ec7bcbc00f1d", "title": "Stabilizing the virtual response time in single-server processor sharing queues with slowly time-varying arrival rates", "abstract": "Motivated by the work of Whitt, who studied stabilization of the mean virtual waiting time (excluding service time) in a $GI_t/GI_t/1/FCFS$ queue, this paper investigates the stabilization of the mean virtual response time in a single-server processor sharing (PS) queueing system with a time-varying arrival rate and a service rate control (a $GI_t/GI_t/1/PS$ queue). We propose and compare a modified square-root (SR) control and a difference-matching (DM) control to stabilize the mean virtual response time of a $GI_t/GI_t/1/PS$ queue. Extensive simulation studies with various settings of arrival processes and service times show that the DM control outperforms the SR control for heavy-traffic conditions, and that the SR control performs better for light-traffic conditions.", "venue": "Ann. Oper. Res.", "authors": ["Yongkyu  Cho", "Young Myoung Ko"], "year": 2020, "n_citations": 1}
{"id": 1567295, "s2_id": "147e1e8222b20bcd3f6920e67df470a27c4c1a57", "title": "Comparative performance analysis of neural networks architectures on H2O platform for various activation functions", "abstract": "Deep learning (deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers with complex structures or otherwise composed of multiple non-linear transformations. In this paper, we present the results of testing neural networks architectures on H2O platform for various activation functions, stopping metrics, and other parameters of machine learning algorithm. It was demonstrated for the use case of MNIST database of handwritten digits in single-threaded mode that blind selection of these parameters can hugely increase (by 2\u20133 orders) the runtime without the significant increase of precision. This result can have crucial influence for opitmization of available and new machine learning methods, especially for image recognition problems.", "venue": "2017 IEEE International Young Scientists Forum on Applied Physics and Engineering (YSF)", "authors": ["Yuriy  Kochura", "Sergii  Stirenko", "Yuri  Gordienko"], "year": 2017, "n_citations": 9}
{"id": 1571480, "s2_id": "629ae83d63f558e16b530441d765dc822d2949e1", "title": "AI and the Everything in the Whole Wide World Benchmark", "abstract": "There is a tendency across different subfields in AI to valorize a small collection of influential benchmarks. These benchmarks operate as stand-ins for a range of anointed common problems that are frequently framed as foundational milestones on the path towards flexible and generalizable AI systems. State-of-the-art performance on these benchmarks is widely understood as indicative of progress towards these long-term goals. In this position paper, we explore the limits of such benchmarks in order to reveal the construct validity issues in their framing as the functionally \u201cgeneral\u201d broad measures of progress they are set up to be.", "venue": "ArXiv", "authors": ["Inioluwa Deborah Raji", "Emily M. Bender", "Amandalynne  Paullada", "Emily  Denton", "Alex  Hanna"], "year": 2021, "n_citations": 5}
{"id": 1571704, "s2_id": "6c196e7280e022c81a2a1bf0405951142a9a5ffd", "title": "A Comparative Study of Relaying Schemes with Decode and Forward over Nakagami-m Fading Channels", "abstract": "Although relaying can be very beneficial for wireless systems, understanding which relaying schemes can achieve specific performance objectives under realistic fading is crucial. In this paper we present a general framework for modeling and evaluating the performance of dual-hop decode-and-forward (DF) relaying schemes over independent and not necessarily identically distributed (INID) Nakagami-m fading channels. We obtain closed-form expressions for the statistics of the instantaneous output signal-to-noise ratio of repetitive transmission with selection diversity. Furthermore, we present a unified statistical overview of other three significant relaying schemes with DF, one based on repetitive transmission with maximal-ratio diversity and the other two based on relay selection (RS). To compare the considered schemes, we present closed-form and analytical expressions for the outage probability and the average symbol error probability under various modulation methods, respectively. Importantly, it is shown that when the channel state information for RS is perfect, RS-based schemes always outperform repetitive ones. Furthermore, when the direct link between the source and the destination nodes is sufficiently strong, relaying may not result in any gains, and it should be switched off.", "venue": "J. Comput. Networks Commun.", "authors": ["George C. Alexandropoulos", "Agisilaos  Papadogiannis", "Paschalis C. Sofotasios"], "year": 2011, "n_citations": 30}
{"id": 1574020, "s2_id": "cf08496286767749c30b0a2b7a9ecced0faf6378", "title": "Getting in the Zone for Successful Scalability", "abstract": "The universal scalability law (USL) is an analytic model used to quantify application scaling. It is universal because it subsumes Amdahl's law and Gustafson linearized scaling as special cases. Using simulation, we show: (i) that the USL is equivalent to synchronous queueing in a load-dependent machine repairman model and (ii) how USL, Amdahl's law, and Gustafson scaling can be regarded as boundaries defining three scalability zones. Typical throughput measurements lie across all three zones. Simulation scenarios provide deeper insight into queueing effects and thus provide a clearer indication of which application features should be tuned to get into the optimal performance zone.", "venue": "Int. CMG Conference", "authors": ["James  Holtman", "Neil J. Gunther"], "year": 2008, "n_citations": 6}
{"id": 1576274, "s2_id": "1cfb5e92c00628df4a87e5ec36545023e50e1986", "title": "Personalized QoS Prediction of Cloud Services via Learning Neighborhood-Based Model", "abstract": "This paper proposes neighborhood-based approach for QoS-prediction of cloud services by taking advantages of collaborative intelligence. Different from heuristic collaborative filtering and matrix-factorization, we set a formal neighborhood-based prediction framework which allows an efficient global optimization scheme, and then exploits different baseline estimate components to improve predictive performance. To validate our methods, a large-scale QoS-specific dataset which consists of invocation records from 339 service users on 5,825 web services on a world-scale distributed network is used. Experimental results show that the learned neighborhood-based models can overcome existing difficulties of heuristic collaborative filtering methods and achieve superior performance than state-of-the-art prediction methods.", "venue": "CollaborateCom", "authors": ["Hao  Wu", "Jun  He", "Bo  Li", "Yijian  Pei"], "year": 2015, "n_citations": 8}
{"id": 1577054, "s2_id": "f2e2d0d50b805c966f8194a0ab7cd4ce9477ccac", "title": "On the Aloha Throughput-Fairness Tradeoff", "abstract": "A well-known inner bound of the stability region of the finite-user slotted Aloha protocol (with fixed contention probabilities) on the collision channel with <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users assumes worst case service rates (all user queues non-empty). Using this inner bound as a feasible set of achievable rates, a characterization of the throughput\u2013fairness tradeoff over this set is obtained, where the throughput is defined as the sum of the individual user rates, and two definitions of fairness are considered: the Jain\u2013Chiu\u2013Hawe function and the sum-user <inline-formula> <tex-math notation=\"LaTeX\">$\\alpha $ </tex-math></inline-formula>-fair (isoelastic) utility function. This characterization is obtained using both an equality constraint and an inequality constraint on the throughput, and properties of the optimal controls, the optimal rates, and the maximum fairness as a function of the target throughput are established. A key structural property underpinning all theorems is the observation that the vector of contention probabilities that extremizes both fairness objectives has its nonzero components taking at most two distinct values.", "venue": "IEEE Transactions on Information Theory", "authors": ["Nan  Xie", "Steven  Weber"], "year": 2017, "n_citations": 2}
{"id": 1577358, "s2_id": "e31612790e3ea5d29372a93582633234dc72cfeb", "title": "Modeling and Design of Millimeter-Wave Networks for Highway Vehicular Communication", "abstract": "Connected and autonomous vehicles will play a pivotal role in future intelligent transportation systems and smart cities, in general. High-speed and low-latency wireless communication links will allow municipalities to warn vehicles against safety hazards, as well as support cloud-driving solutions to drastically reduce traffic jams and air pollution. To achieve these goals, vehicles need to be equipped with a wide range of sensors generating and exchanging high rate data streams. Recently, millimeter wave (mmWave) techniques have been introduced as a means of fulfilling such high data rate requirements. In this paper, we model a highway communication network and characterize its fundamental link budget metrics. In particular, we specifically consider a network where vehicles are served by mmWave base stations (BSs) deployed alongside the road. To evaluate our highway network, we develop a new theoretical model that accounts for a typical scenario where heavy vehicles (such as buses and lorries) in slow lanes obstruct line-of-sight (LOS) paths of vehicles in fast lanes and, hence, act as blockages. Using tools from stochastic geometry, we derive approximations for the signal-to-interference-plus-noise Ratio (SINR) outage probability, as well as the probability that a user achieves a target communication rate (rate coverage probability). Our analysis provides new design insights for mmWave highway communication networks. In considered highway scenarios, we show that reducing the horizontal beamwidth from <inline-formula><tex-math notation=\"LaTeX\">$90^\\circ$</tex-math></inline-formula> to <inline-formula><tex-math notation=\"LaTeX\">$30^\\circ$</tex-math></inline-formula> determines a minimal reduction in the SINR outage probability (namely <inline-formula><tex-math notation=\"LaTeX\">$4 \\cdot 10^{-2}$</tex-math> </inline-formula> at maximum). Also, unlike bidimensional mmWave cellular networks, for small BS densities (namely one BS every <inline-formula><tex-math notation=\"LaTeX\">$500\\,\\text{m}$</tex-math></inline-formula>) it is still possible to achieve an SINR outage probability smaller than 0.2.", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Andrea  Tassi", "Malcolm  Egan", "Robert J. Piechocki", "Andrew  Nix"], "year": 2017, "n_citations": 97}
{"id": 1580516, "s2_id": "e2382f280c296ef97e5af2054576b6c99614be64", "title": "Multidimensional Intratile Parallelization for Memory-Starved Stencil Computations", "abstract": "Optimizing the performance of stencil algorithms has been the subject of intense research over the last two decades. Since many stencil schemes have low arithmetic intensity, most optimizations focus on increasing the temporal data access locality, thus reducing the data traffic through the main memory interface with the ultimate goal of decoupling from this bottleneck. There are, however, only a few approaches that explicitly leverage the shared cache feature of modern multicore chips. If every thread works on its private, separate cache block, the available cache space can become too small, and sufficient temporal locality may not be achieved. We propose a flexible multidimensional intratile parallelization method for stencil algorithms on multicore CPUs with a shared outer-level cache. This method leads to a significant reduction in the required cache space without adverse effects from hardware prefetching or TLB shortage. Our Girih framework includes an autotuner to select optimal parameter configurations on the target hardware. We conduct performance experiments on two contemporary Intel processors and compare with the state-of-the-art stencil frameworks Pluto and Pochoir, using four corner-case stencil schemes and a wide range of problem sizes. Girih shows substantial performance advantages and best arithmetic intensity at almost all problem sizes, especially on low-intensity stencils with variable coefficients. We study in detail the performance behavior at varying grid sizes using phenomenological performance modeling. Our analysis of energy consumption reveals that our method can save energy through reduced DRAM bandwidth usage even at a marginal performance gain. It is thus well suited for future architectures that will be strongly challenged by the cost of data movement, be it in terms of performance or energy consumption.", "venue": "ACM Trans. Parallel Comput.", "authors": ["Tareq M. Malas", "Georg  Hager", "Hatem  Ltaief", "David E. Keyes"], "year": 2018, "n_citations": 28}
{"id": 1583741, "s2_id": "53a0af6620472b5b36ad4eb0bcda7e4259820334", "title": "Superrecursive Features of Interactive Computation", "abstract": "Functioning and interaction of distributed devices and concurrent algorithms are analyzed in the context of the theory of algorithms. Our main concern here is how and under what conditions algorithmic interactive devices can be more powerful than the recursive models of computation, such as Turing machines. Realization of such a higher computing power makes these systems superrecursive. We find here five sources for superrecursiveness in interaction. In addition, we prove that when all of these sources are excluded, the algorithmic interactive system in question is able to perform only recursive computations. These results provide computer scientists with necessary and sufficient conditions for achieving superrecursiveness by algorithmic interactive devices.", "venue": "ArXiv", "authors": ["Mark  Burgin"], "year": 2007, "n_citations": 1}
{"id": 1584772, "s2_id": "128bb015a3d1259896e8f7f132d570c0e53a01ee", "title": "On allocation policies for power and performance", "abstract": "With the increasing popularity of Internet-based services and applications, power efficiency is becoming a major concern for data center operators, as high electricity consumption not only increases greenhouse gas emissions, but also increases the cost of running the server farm itself. In this paper we address the problem of maximizing the revenue of a service provider by means of dynamic allocation policies that run the minimum amount of servers necessary to meet user's requirements in terms of performance. The results of several experiments executed using Wikipedia traces are described, showing that the proposed schemes work well, even if the workload is non-stationary. Since any resource allocation policy requires the use of forecasting mechanisms, various schemes allowing compensating errors in the load forecasts are presented and evaluated.", "venue": "2010 11th IEEE/ACM International Conference on Grid Computing", "authors": ["Dmytro  Dyachuk", "Michele  Mazzucco"], "year": 2010, "n_citations": 16}
{"id": 1589411, "s2_id": "a4ee38b3add3fceb8c4a5512942f059cfe0028e1", "title": "Scalable Reliability Modelling of RAID Storage Subsystems", "abstract": "Reliability modelling of RAID storage systems with its various components such as RAID controllers, enclosures, expanders, interconnects and disks is important from a storage system designer's point of view. A model that can express all the failure characteristics of the whole RAID storage system can be used to evaluate design choices, perform cost reliability trade-offs and conduct sensitivity analyses. However, including such details makes the computational models of reliability quickly infeasible. \nWe present a CTMC reliability model for RAID storage systems that scales to much larger systems than heretofore reported and we try to model all the components as accurately as possible. We use several state-space reduction techniques at the user level, such as aggregating all in-series components and hierarchical decomposition, to reduce the size of our model. To automate computation of reliability, we use the PRISM model checker as a CTMC solver where appropriate. Our modelling techniques using PRISM are more practical (in both time and effort) compared to previously reported Monte-Carlo simulation techniques. \nOur model for RAID storage systems (that includes, for example, disks, expanders, enclosures) uses Weibull distributions for disks and, where appropriate, correlated failure modes for disks, while we use exponential distributions with independent failure modes for all other components. To use the CTMC solver, we approximate the Weibull distribution for a disk using sum of exponentials and we confirm that this model gives results that are in reasonably good agreement with those from the sequential Monte Carlo simulation methods for RAID disk subsystems reported in literature earlier. Using a combination of scalable techniques, we are able to model and compute reliability for fairly large configurations with upto 600 disks using this model.", "venue": "ArXiv", "authors": ["Prasenjit  Karmakar", "K.  Gopinath"], "year": 2015, "n_citations": 0}
{"id": 1589848, "s2_id": "2c3617358fd6149e2d2b1ca785d4c2ec95ba9d49", "title": "Analytical Performance Modeling of NoCs under Priority Arbitration and Bursty Traffic", "abstract": "Networks-on-Chip (NoCs) used in commercial many-core processors typically incorporate priority arbitration. Moreover, they experience bursty traffic due to application workloads. However, most state-of-the-art NoC analytical performance analysis techniques assume fair arbitration and simple traffic models. To address these limitations, we propose an analytical modeling technique for priority-aware NoCs under bursty traffic. Experimental evaluations with synthetic and bursty traffic show that the proposed approach has less than 10% modeling error with respect to cycle-accurate NoC simulator.", "venue": "IEEE Embedded Systems Letters", "authors": ["Sumit K. Mandal", "Raid  Ayoub", "Micahel  Kishinevsky", "Mohammad M. Islam", "Umit Y. Ogras"], "year": 2021, "n_citations": 4}
{"id": 1592439, "s2_id": "298ced6d80cdf50a7c9ca40c5bcf30f64be29aba", "title": "Monocular Real-Time Volumetric Performance Capture", "abstract": "We present the first approach to volumetric performance capture and novel-view rendering at real-time speed from monocular video, eliminating the need for expensive multi-view systems or cumbersome pre-acquisition of a personalized template model. Our system reconstructs a fully textured 3D human from each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While PIFu achieves high-resolution reconstruction in a memory-efficient manner, its computationally expensive inference prevents us from deploying such a system for real-time applications. To this end, we propose a novel hierarchical surface localization algorithm and a direct rendering method without explicitly extracting surface meshes. By culling unnecessary regions for evaluation in a coarse-to-fine manner, we successfully accelerate the reconstruction by two orders of magnitude from the baseline without compromising the quality. Furthermore, we introduce an Online Hard Example Mining (OHEM) technique that effectively suppresses failure modes due to the rare occurrence of challenging examples. We adaptively update the sampling probability of the training data based on the current reconstruction accuracy, which effectively alleviates reconstruction artifacts. Our experiments and evaluations demonstrate the robustness of our system to various challenging angles, illuminations, poses, and clothing styles. We also show that our approach compares favorably with the state-of-the-art monocular performance capture. Our proposed approach removes the need for multi-view studio settings and enables a consumer-accessible solution for volumetric capture.", "venue": "ECCV", "authors": ["Ruilong  Li", "Yuliang  Xiu", "Shunsuke  Saito", "Zeng  Huang", "Kyle  Olszewski", "Hao  Li"], "year": 2020, "n_citations": 19}
{"id": 1593318, "s2_id": "21ed890bc93e8af0f488c1f79f9bbde6b0b33e16", "title": "Effects of Non-Identical Rayleigh Fading on Differential Unitary Space-Time Modulation", "abstract": "Non-identical fading distribution in a multiple-input multiple-output (MIMO) channel, including unequal average channel gains and fade rates, often occurs when antennas are not co-located. In this paper, we present an analytical study of the effects of non-identical Rayleigh fading on the error performance of differential unitary space-time modulation (DUSTM). The fading processes for different transmit-receive antenna pairs are assumed to be independent and time-variant. We find that the maximum-likelihood (ML) differential detector of DUSTM over such channels is involved except for differential cyclic group codes. The conventional detector is proved to be asymptotically optimal in the limit of high signal-to-noise ratio (SNR) over static fading channels. Applying the distribution of quadratic forms of Gaussian vectors, we then derive closed-form expressions for the exact error probabilities of two specific unitary classes, namely, cyclic group codes and orthogonal codes. Simple and useful asymptotic bounds on error probabilities are also obtained. Our analysis leads to the following general findings: (1) equal power allocation is asymptotically optimal, and (2) non-identical channel gain distribution degrades the error performance. Finally, we also introduce a water-filling based power allocation to exploit the transmit non-identical fading statistics.", "venue": "IEEE Transactions on Communications", "authors": ["Meixia  Tao"], "year": 2009, "n_citations": 6}
{"id": 1593838, "s2_id": "417e1fa13e974c493d35945ea4592b45bf4318f6", "title": "Perfect simulation of M/G/c queues", "abstract": "In this paper we describe a perfect simulation algorithm for the stable M/G/c queue. Sigman (2011) showed how to build a dominated coupling-from-the-past algorithm for perfect simulation of the super-stable M/G/c queue operating under first-come-first-served discipline. Sigman's method used a dominating process provided by the corresponding M/G/1 queue (using Wolff's sample path monotonicity, which applies when service durations are coupled in order of initiation of service). The method exploited the fact that the workload process for the M/G/1 queue remains the same under different queueing disciplines, in particular under the processor sharing discipline, for which a dynamic reversibility property holds. We generalise Sigman's construction to the stable case by comparing the M/G/c queue to a copy run under random assignment. This allows us to produce a na\u00efve perfect simulation algorithm based on running the dominating process back to the time it first empties. We also construct a more efficient algorithm that uses sandwiching by lower and upper processes constructed as coupled M/G/c queues started respectively from the empty state and the state of the M/G/c queue under random assignment. A careful analysis shows that appropriate ordering relationships can still be maintained, so long as service durations continue to be coupled in order of initiation of service. We summarise statistical checks of simulation output, and demonstrate that the mean run-time is finite so long as the second moment of the service duration distribution is finite.", "venue": "Advances in Applied Probability", "authors": ["Stephen B. Connor", "Wilfrid S. Kendall"], "year": 2015, "n_citations": 11}
{"id": 1595458, "s2_id": "70e1699e9d93a73b9eb2c26725d50f537da5d01b", "title": "Characteristics of multithreading models for high-performance IO driven network applications", "abstract": "In a technological landscape that is quickly moving toward dense multi-CPU and multi-core computer systems, where using multithreading is an increasingly popular application design decision, it is important to choose a proper model for distributing tasks across multiple threads that will result in the best efficiency for the application and the system as a whole. The work described in this paper creates, implements and evaluates various models of distributing tasks to CPU threads and investigates their characteristics for use in modern high-performance network servers. The results presented here comprise a roadmap of models for building multithreaded server applications for modern server hardware and Unix-like operating systems.", "venue": "AFRICON 2009", "authors": ["Ivan  Voras", "Mario  Zagar"], "year": 2009, "n_citations": 3}
{"id": 1601175, "s2_id": "78c2e04a0d62479b7af7a6d8f9218a7bfb7b0fcd", "title": "Group-Server Queues", "abstract": "By analyzing energy-efficient management of data centers, this paper proposes and develops a class of interesting Group-Server Queues, and establishes two representative group-server queues through loss networks and impatient customers, respectively. Furthermore, such two group-server queues are given model descriptions and necessary interpretation. Also, simple mathematical discussion is provided, and simulations are made to study the expected queue lengths, the expected sojourn times and the expected virtual service times. In addition, this paper also shows that this class of group-server queues are often encountered in many other practical areas including communication networks, manufacturing systems, transportation networks, financial networks and healthcare systems. Note that the group-server queues are always used to design effectively dynamic control mechanisms through regrouping and recombining such many servers in a large-scale service system by means of, for example, bilateral threshold control, and customers transfer to the buffer or server groups. This leads to the large-scale service system that is divided into several adaptive and self-organizing subsystems through scheduling of batch customers and regrouping of service resources, which make the middle layer of this service system more effectively managed and strengthened under a dynamic, real-time and even reward optimal framework. Based on this, performance of such a large-scale service system may be improved greatly in terms of introducing and analyzing such group-server queues. Therefore, not only analysis of group-server queues is regarded as a new interesting research direction, but there also exist many theoretical challenges, basic difficulties and open problems in the area of queueing networks.", "venue": "QTNA", "authors": ["Quan-Lin  Li", "Jing-Yu  Ma", "Mingzhou  Xie", "Li  Xia"], "year": 2017, "n_citations": 4}
{"id": 1601682, "s2_id": "3882b5adac4f4e2f7236b07e3fa10dddc6044de1", "title": "Interference Analysis in Dynamic TDD System Combined or not with Cell Clustering Scheme", "abstract": "Dynamic Time Division Duplex (TDD) has been introduced as a solution to deal with the uplink and downlink traffic asymmetry, mainly observed for dense heterogeneous network deployments. However, the use of this feature requires new interference mitigation schemes capable to handle two additional types of interferences between cells in opposite transmission cycle: downlink to uplink and uplink to downlink interferences. Among them, Cell clustering has been proposed as an efficient solution to minimize inter-cell interferences in opposite transmission directions and somehow responds to the requirements of enhanced Interference Mitigation and Traffic Adaptation (eIMTA) problem. This work is devoted to provide a new analytical approach to model inter-cell interferences and quantify performances of Dynamic TDD system in terms of SINR (Signal to Interferences plus Noise Ratio) distribution. Analytical system performance investigation concerns two scenarios: i) basic Dynamic TDD without any other feature and ii) Dynamic TDD with interference mitigation schemes.", "venue": "2018 IEEE 87th Vehicular Technology Conference (VTC Spring)", "authors": ["Jalal  Rachad", "Ridha  Nasri", "Laurent  Decreusefond"], "year": 2018, "n_citations": 9}
{"id": 1605496, "s2_id": "6a9bc5ec2521131241498eac00849627d7d5f227", "title": "R-Storm: Resource-Aware Scheduling in Storm", "abstract": "The era of big data has led to the emergence of new systems for real-time distributed stream processing, e.g., Apache Storm is one of the most popular stream processing systems in industry today. However, Storm, like many other stream processing systems lacks an intelligent scheduling mechanism. The default round-robin scheduling currently deployed in Storm disregards resource demands and availability, and can therefore be inefficient at times. We present R-Storm (Resource-Aware Storm), a system that implements resource-aware scheduling within Storm. R-Storm is designed to increase overall throughput by maximizing resource utilization while minimizing network latency. When scheduling tasks, R-Storm can satisfy both soft and hard resource constraints as well as minimizing network distance between components that communicate with each other. We evaluate R-Storm on set of micro-benchmark Storm applications as well as Storm applications used in production at Yahoo! Inc. From our experimental results we conclude that R-Storm achieves 30-47% higher throughput and 69-350% better CPU utilization than default Storm for the micro-benchmarks. For the Yahoo! Storm applications, R-Storm outperforms default Storm by around 50% based on overall throughput. We also demonstrate that R-Storm performs much better when scheduling multiple Storm applications than default Storm.", "venue": "Middleware", "authors": ["Boyang  Peng", "Mohammad  Hosseini", "Zhihao  Hong", "Reza  Farivar", "Roy H. Campbell"], "year": 2015, "n_citations": 151}
{"id": 1606733, "s2_id": "91aa3bca4e62b416e194f2c826aa124bf0e819a8", "title": "Constraint solvers: An empirical evaluation of design decisions", "abstract": "This paper presents an evaluation of the design decisions made in four state-of-the-art constraint solvers; Choco, ECLiPSe, Gecode, and Minion. To assess the impact of design decisions, instances of the five problem classes n-Queens, Golomb Ruler, Magic Square, Social Golfers, and Balanced Incomplete Block Design are modelled and solved with each solver. The results of the experiments are not meant to give an indication of the performance of a solver, but rather investigate what influence the choice of algorithms and data structures has. The analysis of the impact of the design decisions focuses on the dierent ways of memory management, behaviour with increasing problem size, and specialised algorithms for specific types of variables. It also briefly considers other, less significant decisions.", "venue": "ArXiv", "authors": ["Lars  Kotthoff"], "year": 2010, "n_citations": 10}
{"id": 1609979, "s2_id": "bda29a3ccfc5afe15ed9b676d12482e29cac3a8a", "title": "Experimental Analysis of Communication Relaying Delay in Low-Energy Ad-hoc Networks", "abstract": "In this paper, we planned and carried out a delay measurement experiment using Raspberry Pi Zero W. The results demonstrated that, in low-energy ad-hoc networks, processing delay of the application is always too large to ignore; it is at least ten times greater than the kernel routing and corresponds to 30 % of the transmission delay. Furthermore, if the task is CPU-intensive, such as packet encryption, the processing delay can be greater than the transmission delay and its behavior is represented by a simple linear model.", "venue": "2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC)", "authors": ["Taichi  Miya", "Kohta  Ohshima", "Yoshiaki  Kitaguchi", "Katsunori  Yamaoka"], "year": 2021, "n_citations": 2}
{"id": 1612972, "s2_id": "50866a3ede329afd087e0bb9ac991a2260d579b0", "title": "To Compress, or Not to Compress: Characterizing Deep Learning Model Compression for Embedded Inference", "abstract": "The recent advances in deep neural networks (DNNs) make them attractive for embedded systems. However, it can take a long time for DNNs to make an inference on resource constrained computing devices. Model compression techniques can address the computation issue of deep inference on embedded devices. This technique is highly attractive, as it does not rely on specialized hardware, or computation-offloading that is often infeasible due to privacy concerns or high latency. However, it remains unclear how model compression techniques perform across a wide range of DNNs. To design efficient embedded deep learning solutions, we need to understand their behaviors. This work develops a quantitative approach to characterize model compression techniques on a representative embedded deep learning architecture, the NVIDIA Jetson Tx2. We perform extensive experiments by considering 11 influential neural network architectures from the image classification and the natural language processing domains. We experimentally show that how two mainstream compression techniques, data quantization and pruning, perform on these network architectures and the implications of compression techniques to the model storage size, inference time, energy consumption and performance metrics. We demonstrate that there are opportunities to achieve fast deep inference on embedded systems, but one must carefully choose the compression settings. Our results provide insights on when and how to apply model compression techniques and guidelines for designing efficient embedded deep learning systems.", "venue": "2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)", "authors": ["Qing  Qin", "Jie  Ren", "Jialong  Yu", "Ling  Gao", "Hai  Wang", "Jie  Zheng", "Yansong  Feng", "Jianbin  Fang", "Zheng  Wang"], "year": 2018, "n_citations": 7}
{"id": 1613653, "s2_id": "01b0fa40b9f64af8ba171dec445cfed90b7476f0", "title": "Resource Management Schemes for Cloud-Native Platforms with Computing Containers of Docker and Kubernetes", "abstract": "Businesses have made increasing adoption and incorporation of cloud technology into internal processes in the last decade. The cloud-based deployment provides on-demand availability without active management. More recently, the concept of cloud-native application has been proposed and represents an invaluable step toward helping organizations develop software faster and update it more frequently to achieve dramatic business outcomes. Cloud-native is an approach to build and run applications that exploit the cloud computing delivery model's advantages. It is more about how applications are created and deployed than where. The container-based virtualization technology, such as Docker and Kubernetes, serves as the foundation for cloud-native applications. This paper investigates the performance of two popular computational-intensive applications, big data, and deep learning, in a cloud-native environment. We analyze the system overhead and resource usage for these applications. Through extensive experiments, we show that the completion time reduces by up to 79.4% by changing the default setting and increases by up to 96.7% due to different resource management schemes on two platforms. Additionally, the resource release is delayed by up to 116.7% across different systems. Our work can guide developers, administrators, and researchers to better design and deploy their applications by selecting and configuring a hosting platform.", "venue": "ArXiv", "authors": ["Ying  Mao", "Yuqi  Fu", "Suwen  Gu", "Sudip  Vhaduri", "Long  Cheng", "Qingzhi  Liu"], "year": 2020, "n_citations": 10}
{"id": 1614355, "s2_id": "be482101ac21f3425141d062f7d4b6146e208647", "title": "Extended Abstract of Performance Analysis and Prediction of Model Transformation", "abstract": "In the software development process, model transformation is increasingly assimilated. However, systems being developed with model transformation sometimes grow in size and become complex. Meanwhile, the performance of model transformation tends to decrease. Hence, performance is an important quality of model transformation. According to current research model transformation performance focuses on optimising the engines internally. However, there exists no research activities to support transformation engineer to identify performance bottleneck in the transformation rules and hence, to predict the overall performance. In this paper we vision our aim at providing an approach of monitoring and profiling to identify the root cause of performance issues in the transformation rules and to predict the performance of model transformation. This will enable software engineers to systematically identify performance issues as well as predict the performance of model transformation.", "venue": "ICPE", "authors": ["Vijayshree  Vijayshree", "Markus  Frank", "Steffen  Becker"], "year": 2020, "n_citations": 0}
{"id": 1615607, "s2_id": "c2d87218521ffadc51244b670ccceb7613627b70", "title": "Finite Horizon Throughput Maximization for a Wirelessly Powered Device Over a Time Varying Channel", "abstract": "In this work, we consider an energy harvesting device (EHD) served by an access point with a single antenna that is used for both wireless power transfer (WPT) and data transfer. The objective is to maximize the expected throughput of the EHD over a finite horizon when the channel state information is only available causally. The EHD is energized by WPT for a certain duration, which is subject to optimization, and then, EHD transmits its information bits to the AP until the end of the time horizon by employing optimal dynamic power allocation. The joint optimization problem is modeled as a dynamic programming problem. Based on the characteristic of the problem, we prove that a time dependent threshold type structure exists for the optimal WPT duration, and we obtain closed form solution to the dynamic power allocation in the uplink period.", "venue": "2018 IEEE Globecom Workshops (GC Wkshps)", "authors": ["Mehdi Salehi Heydar Abad", "\u00d6zg\u00fcr  Er\u00e7etin"], "year": 2018, "n_citations": 1}
{"id": 1616845, "s2_id": "d0efd4e988b96ee683d2213aa5ae5c13f8ce4fdc", "title": "M/G/c/c state dependent queuing model for a road traffic system of two sections in tandem", "abstract": "We propose in this article a M/G/c/c state dependent queuing model for road traffic flow. The model is based on finite capacity queuing theory which captures the stationary density-flow relationships. It is also inspired from the deterministic Godunov scheme for the road traffic simulation. We first present a reformulation of the existing linear case of M/G/c/c state dependent model, in order to use flow rather than speed variables. We then extend this model in order to consider upstream traffic demand and downstream traffic supply. After that, we propose the model for two road sections in tandem where both sections influence each other. In order to deal with this mutual dependence, we solve an implicit system given by an algebraic equation. Finally, we derive some performance measures (throughput and expected travel time). A comparison with results predicted by the M/G/c/c state dependent queuing networks shows that the model we propose here captures really the dynamics of the road traffic.", "venue": "Comput. Oper. Res.", "authors": ["Nacira  Guerrouahane", "Djamil  A\u00efssani", "Nadir  Farhi", "Louiza  Bouallouche-Medjkoune"], "year": 2017, "n_citations": 13}
{"id": 1618762, "s2_id": "f1e624001f789b6722b791b0ac19831703f54661", "title": "Characterizing and Modeling Distributed Training with Transient Cloud GPU Servers", "abstract": "Cloud GPU servers have become the de facto way for deep learning practitioners to train complex models on large-scale datasets. However, it is challenging to determine the appropriate cluster configuration\u2014e.g., server type and number\u2014for different training workloads while balancing the trade-offs in training time, cost, and model accuracy. Adding to the complexity is the potential to reduce the monetary cost by using cheaper, but revocable, transient GPU servers.In this work, we analyze distributed training performance under diverse cluster configurations using CM-DARE, a cloud-based measurement and training framework. Our empirical datasets include measurements from three GPU types, six geographic regions, twenty convolutional neural networks, and thousands of Google Cloud servers. We also demonstrate the feasibility of predicting training speed and overhead using regression-based models. Finally, we discuss potential use cases of our performance modeling such as detecting and mitigating performance bottlenecks.", "venue": "2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)", "authors": ["Shijian  Li", "Robert J. Walls", "Tian  Guo"], "year": 2020, "n_citations": 8}
{"id": 1619371, "s2_id": "5d122849409aa493a140c2ff5cfb69e0b76df920", "title": "Impact of Pointing Errors on the Performance of Mixed RF/FSO Dual-Hop Transmission Systems", "abstract": "In this work, the performance analysis of a dual-hop relay transmission system composed of asymmetric radio-frequency (RF)/free-space optical (FSO) links with pointing errors is presented. More specifically, we build on the system model presented in to derive new exact closed-form expressions for the cumulative distribution function, probability density function, moment generating function, and moments of the end-to-end signal-to-noise ratio in terms of the Meijer's G function. We then capitalize on these results to offer new exact closed-form expressions for the higher-order amount of fading, average error rate for binary and M-ary modulation schemes, and the ergodic capacity, all in terms of Meijer's G functions. Our new analytical results were also verified via computer-based Monte-Carlo simulation results.", "venue": "IEEE Wireless Communications Letters", "authors": ["Imran Shafique Ansari", "Ferkan  Yilmaz", "Mohamed-Slim  Alouini"], "year": 2013, "n_citations": 224}
{"id": 1622900, "s2_id": "e660c83c5b7009c9261acfe5f7002a816ffadbd3", "title": "On designing and testing distributed virtual environments", "abstract": "Distributed real\u2010time (DRT) systems are among the most complex software systems to design, test, maintain, and evolve. The existence of components distributed over a network often conflicts with real\u2010time requirements, leading to design strategies that depend on domain\u2010specific and even application\u2010specific knowledge. Distributed virtual environment (DVE) systems are DRT systems that connect multiple users instantly with each other and with a shared virtual space over a network. DVE systems deviate from traditional DRT systemsx in the importance of the quality of the end user experience. We present an analysis of important, but challenging, issues in the design, testing, and evaluation of DVE systems through the lens of experiments with a concrete DVE, OpenSimulator. We frame our observations within six dimensions of well\u2010known design concerns: correctness, fault tolerance/prevention, scalability, time sensitivity, consistency, and overhead of distribution. Furthermore, we place our experimental work in a broader historical context, showing that these challenges are intrinsic to DVEs and suggesting lines of future research. Copyright \u00a9 2016 John Wiley & Sons, Ltd.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Arthur  Valadares", "Eugenia  Gabrielova", "Cristina V. Lopes"], "year": 2016, "n_citations": 10}
{"id": 1625145, "s2_id": "8535df955ede1dc248b4295fef9002cf0d748eee", "title": "Transaction Confirmation Time Prediction in Ethereum Blockchain Using Machine Learning", "abstract": "Blockchain offers a decentralized, immutable, transparent system of records. It offers a peer-to-peer network of nodes with no centralised governing entity making it unhackable and therefore, more secure than the traditional paper-based or centralised system of records like banks etc. While there are certain advantages to the paper-based recording approach, it does not work well with digital relationships where the data is in constant flux. Unlike traditional channels, governed by centralized entities, blockchain offers its users a certain level of anonymity by providing capabilities to interact without disclosing their personal identities and allows them to build trust without a third-party governing entity. Due to the aforementioned characteristics of blockchain, more and more users around the globe are inclined towards making a digital transaction via blockchain than via rudimentary channels. Therefore, there is a dire need for us to gain insight on how these transactions are processed by the blockchain and how much time it may take for a peer to confirm a transaction and add it to the blockchain network. This paper presents a novel approach that would allow one to estimate the time, in block time or otherwise, it would take for a mining node to accept and confirm a transaction to a block using machine learning. The paper also aims to compare the predictive accuracy of two machine learning regression models- Random Forest Regressor and Multilayer Perceptron against previously proposed statistical regression model under a set evaluation criterion. The objective is to determine whether machine learning offers a more accurate predictive model than conventional statistical models. The proposed model results in improved accuracy in prediction.", "venue": "ArXiv", "authors": ["Harsh Jot Singh", "Abdelhakim Senhaji Hafid"], "year": 2019, "n_citations": 4}
{"id": 1626020, "s2_id": "547da0273b95c040936fd0f492e6da820ffe7136", "title": "Performance-Driven Internet Path Selection", "abstract": "Internet routing can often be sub-optimal, with the chosen routes providing worse performance than other available policy-compliant routes. This stems from the lack of visibility into route performance at the network layer. While this is an old problem, we argue that recent advances in programmable hardware finally open up the possibility of performance-aware routing in a deployable, BGP-compatible manner. We introduce RouteScout, a hybrid hardware/software system supporting performance-based routing at ISP scale. In the data plane, RouteScoutleverages P4-enabled hardware to monitor performance across policy-compliant route choices for each destination, at line-rate and with a small memory footprint. RouteScout'scontrol plane then asynchronously pulls aggregated performance metrics to synthesize a performance-aware forwarding policy. We show that RouteScoutcan monitor performance across most of an ISP's traffic, using only 4 MB of memory. Further, its control can flexibly satisfy a variety of operator objectives, with sub-second operating times.", "venue": "SOSR", "authors": ["Maria  Apostolaki", "Ankit  Singla", "Laurent  Vanbever"], "year": 2021, "n_citations": 2}
{"id": 1626330, "s2_id": "60fb1d87dbbafca55da8277b62d6fcd6c12eef12", "title": "On the performance of GPU accelerated q-LSKUM based meshfree solvers in Fortran, C++, Python, and Julia", "abstract": "This report presents a comprehensive analysis of the performance of GPU accelerated meshfree CFD solvers for two-dimensional compressible flows in Fortran, C++, Python, and Julia. The programming model CUDA is used to develop the GPU codes. The meshfree solver is based on the least squares kinetic upwind method with entropy variables (q-LSKUM). To assess the computational efficiency of the GPU solvers and to compare their relative performance, benchmark calculations are performed on seven levels of point distribution. To analyse the difference in their run-times, the computationally intensive kernel is profiled. Various performance metrics are investigated from the profiled data to determine the cause of observed variation in run-times. To address some of the performance related issues, various optimisation strategies are employed. The optimised GPU codes are compared with the naive codes, and conclusions are drawn from their performance.", "venue": "ArXiv", "authors": ["Nischay Ram Mamidi", "Kumar  Prasun", "Dhruv  Saxena", "Anil  Nemili", "Bharatkumar  Sharma", "S. M. Deshpande"], "year": 2021, "n_citations": 0}
{"id": 1628073, "s2_id": "8a369963e61ccfc567ff0aa560fca186a1d82c7b", "title": "FastCap: An efficient and fair algorithm for power capping in many-core systems", "abstract": "Future servers will incorporate many active low-power modes for different system components, such as cores and memory. Though these modes provide flexibility for power management via Dynamic Voltage and Frequency Scaling (DVFS), they must be operated in a coordinated manner. Such coordinated control creates a combinatorial space of possible power mode configurations. Given the rapid growth of the number of cores, it is becoming increasingly challenging to quickly select the configuration that maximizes the performance under a given power budget. Prior power capping techniques do not scale well to large numbers of cores, and none of those works has considered memory DVFS. In this paper, we present FastCap, our optimization approach for system-wide power capping, using both CPU and memory DVFS. Based on a queuing model, FastCap formulates power capping as a non-linear optimization problem where we seek to maximize the system performance under a power budget, while promoting fairness across applications. Our FastCap algorithm solves the optimization online and efficiently (low complexity on the number of cores), using a small set of performance counters as input. To evaluate FastCap, we simulate it for a many-core server running different types of workloads. Our results show that FastCap caps power draw accurately, while producing better application performance and fairness than many existing CPU power capping methods (even after they are extended to use of memory DVFS as well).", "venue": "2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Yanpei  Liu", "Guilherme  Cox", "Qingyuan  Deng", "Stark C. Draper", "Ricardo  Bianchini"], "year": 2016, "n_citations": 20}
{"id": 1631478, "s2_id": "d57b8b5d5680ea233937b7c2656338390d926518", "title": "Model Checking Probabilistic Real-Time Properties for Service-Oriented Systems with Service Level Agreements", "abstract": "The assurance of quality of service properties is an important aspect of service-oriented software engineering. Notations for so-called service level agreements (SLAs), such as the Web Service Level Agreement (WSLA) language, provide a formal syntax to specify such assurances in terms of (legally binding) contracts between a service provider and a customer. On the other hand, formal methods for verification of probabilistic real-time behavior have reached a level of expressiveness and efficiency which allows to apply them in real-world scenarios. In this paper, we suggest to employ the recently introduced model of Interval Probabilistic Timed Automata (IPTA) for formal verification of QoS properties of service-oriented systems. Specifically, we show that IPTA in contrast to Probabilistic Timed Automata (PTA) are able to capture the guarantees specified in SLAs directly. A particular challenge in the analysis of IPTA is the fact that their naive semantics usually yields an infinite set of states and infinitely-branching transitions. However, using symbolic representations, IPTA can be analyzed rather efficiently. We have developed the first implementation of an IPTA model checker by extending the PRISM tool and show that model checking IPTA is only slightly more expensive than model checking comparable PTA.", "venue": "INFINITY", "authors": ["Christian  Krause", "Holger  Giese"], "year": 2011, "n_citations": 4}
{"id": 1633751, "s2_id": "ef7db6f1585bfd2a0faded5cf21e3db7f21fba40", "title": "Storage workload modelling by hidden Markov models: Application to Flash memory", "abstract": "A workload analysis technique is presented that processes data from operation type traces and creates a hidden Markov model (HMM) to represent the workload that generated those traces. The HMM can be used to create representative traces for performance models, such as simulators, avoiding the need to repeatedly acquire suitable traces. It can also be used to estimate the transition probabilities and rates of a Markov modulated arrival process directly, for use as input to an analytical performance model of Flash memory. The HMMs obtained from industrial workloads-both synthetic benchmarks, preprocessed by a file translation layer, and real, time-stamped user traces-are validated by comparing their autocorrelation functions and other statistics with those of the corresponding monitored time series. Further, the performance model applications, referred to above, are illustrated by numerical examples.", "venue": "Perform. Evaluation", "authors": ["Peter G. Harrison", "S. K. Harrison", "Naresh M. Patel", "Soraya  Zertal"], "year": 2012, "n_citations": 24}
{"id": 1634197, "s2_id": "c0510152252f3f363791a71b00ef261630034bd6", "title": "Securing of Unmanned Aerial Systems (UAS) against security threats using human immune system", "abstract": "Abstract UASs form a large part of the fighting ability of the advanced military forces. In particular, these systems that carry confidential information are subject to security attacks. Accordingly, an Intrusion Detection System (IDS) has been proposed in the proposed design to protect against the security problems using the human immune system (HIS). The IDSs are used to detect and respond to attempts to compromise the target system. Since the UASs operate in the real world, the testing and validation of these systems with a variety of sensors is confronted with problems. This design is inspired by HIS. In the mapping, insecure signals are equivalent to an antigen that are detected by antibody- based training patterns and removed from the operation cycle. Among the main uses of the proposed design are the quick detection of intrusive signals and quarantining their activity. Moreover, SUAS-HIS method is evaluated here via extensive simulations carried out in NS-3 environment. The simulation results indicate that the UAS network performance metrics are improved in terms of false positive rate, false negative rate, detection rate, and packet delivery rate.", "venue": "Reliab. Eng. Syst. Saf.", "authors": ["Reza  Fotohi"], "year": 2020, "n_citations": 38}
{"id": 1634869, "s2_id": "d5db2b406383231f5456286ccb1efe15a605d0b2", "title": "RCanopus: Making Canopus Resilient to Failures and Byzantine Faults", "abstract": "Distributed consensus is a key enabler for many distributed systems including distributed databases and blockchains. Canopus is a scalable distributed consensus protocol that ensures that live nodes in a system agree on an ordered sequence of operations (called transactions). Unlike most prior consensus protocols, Canopus does not rely on a single leader. Instead, it uses a virtual tree overlay for message dissemination to limit network traffic across oversubscribed links. It leverages hardware redundancies, both within a rack and inside the network fabric, to reduce both protocol complexity and communication overhead. These design decisions enable Canopus to support large deployments without significant performance degradation. \nThe existing Canopus protocol is resilient in the face of node and communication failures, but its focus is primarily on performance, so does not respond well to other types of failures. For example, the failure of a single rack of servers causes all live nodes to stall. The protocol is also open to attack by Byzantine nodes, which can cause different live nodes to conclude the protocol with different transaction orders. In this paper, we describe RCanopus (`resilent Canopus') which extends Canopus to add liveness, that is, allowing live nodes to make progress, when possible, despite many types of failures. This requires RCanopus to accurately detect and recover from failure despite using unreliable failure detectors, and tolerance of Byzantine attacks. Second, RCanopus guarantees safety, that is, agreement amongst live nodes of transaction order, in the presence of Byzantine attacks and network partitioning.", "venue": "ArXiv", "authors": ["Srinivasan  Keshav", "Wojciech M. Golab", "Bernard  Wong", "Sajjad  Rizvi", "Sergey  Gorbunov"], "year": 2018, "n_citations": 4}
{"id": 1641615, "s2_id": "93991d9e9cde0a06758d63d7febe2e87182766d6", "title": "Helenos: A realistic benchmark for distributed transactional memory", "abstract": "Transactional memory (TM) is an approach to concurrency control that aims to make writing parallel programs both effective and simple. The approach has been initially proposed for nondistributed multiprocessor systems, but it is gaining popularity in distributed systems to synchronize tasks at large scales. Efficiency and scalability are often the key issues in TM research; thus, performance benchmarks are an important part of it. However, while standard TM benchmarks like the Stanford Transactional Applications for Multi\u2010Processing suite and STMBench7 are available and widely accepted, they do not translate well into distributed systems. Hence, the set of benchmarks usable with distributed TM systems is very limited, and must be padded with microbenchmarks, whose simplicity and artificial nature often makes them uninformative or misleading. Therefore, this paper introduces Helenos, a realistic, complex, and comprehensive distributed TM benchmark based on the problem of the Facebook inbox, an application of the Cassandra distributed store.", "venue": "Softw. Pract. Exp.", "authors": ["Pawel  Kobylinski", "Konrad  Siek", "Jan  Baranowski", "Pawel T. Wojciechowski"], "year": 2018, "n_citations": 0}
{"id": 1642521, "s2_id": "d8c1bd8b242e2bce85ef255574d73f0eb0a833aa", "title": "Web Performance with Android's Battery-Saver Mode", "abstract": "A Web browser utilizes a device's CPU to parse HTML, build a Document Object Model, a Cascading Style Sheets Object Model, and render trees, and parse, compile, and execute computationally-heavy JavaScript. A powerful CPU is required to perform these tasks as quickly as possible and provide the user with a great experience. However, increased CPU performance comes with increased power consumption and reduced battery life on mobile devices. As an option to extend battery life, Android offers a battery-saver mode that when activated, turns off the power-hungry and faster processor cores and turns on the battery-conserving and slower processor cores on the device. The transition from using faster processor cores to using slower processor cores throttles the CPU clock speed on the device, and therefore impacts the webpage load process. We utilize a large-scale data-set collected by a real user monitoring system of a major content delivery network to investigate the impact of Android's battery-saver mode on various mobile Web performance metrics. Our analysis suggests that users of select smartphones of Huawei and Sony experience a sudden or gradual degradation in Web performance when battery-saver mode is active. Battery-saver mode on newer flagship smartphones, however, does not impact the mobile Web performance. Finally, we encourage for new website design goals that treat slow (and throttled-CPU) devices kindly in favor of improving end-user experience and suggest that Web performance measurements should be aware of user device battery charge levels to correctly associate Web performance.", "venue": "ArXiv", "authors": ["Utkarsh  Goel", "Stephen  Ludin", "Moritz  Steiner"], "year": 2020, "n_citations": 0}
{"id": 1644525, "s2_id": "8f426243273d715349d1f0c233c3cee9739d5b94", "title": "Playing with and against Hedge", "abstract": "Hedge has been proposed as an adaptive scheme, which guides an agent's decision in resource selection and distribution problems that can be modeled as a multi-armed bandit full information game. Such problems are encountered in the areas of computer and communication networks, e.g. network path selection, load distribution, network interdiction, and also in problems in the area of transportation. We study Hedge under the assumption that the total loss that can be suffered by the player in each round is upper bounded. In this paper, we study the worst performance of Hedge.", "venue": "ArXiv", "authors": ["Miltiades E. Anagnostou", "Maria A. Lambrou"], "year": 2018, "n_citations": 0}
{"id": 1649238, "s2_id": "394a3b11dbee1d5ce04e2f3e7f6cdd4ef81e38b1", "title": "Skew-Oblivious Data Routing for Data Intensive Applications on FPGAs with HLS", "abstract": "FPGAs have become emerging computing infrastructures for accelerating applications in datacenters. Meanwhile, high-level synthesis (HLS) tools have been proposed to ease the programming of FPGAs. Even with HLS, irregular data-intensive applications require explicit optimizations, among which multiple processing elements (PEs) with each owning a private BRAM-based buffer are usually adopted to process multiple data per cycle. Data routing, which dynamically dispatches multiple data to designated PEs, avoids data replication in buffers compared to statically assigning data to PEs, hence saving BRAM usage. However, the workload imbalance among PEs vastly diminishes performance when processing skew datasets. In this paper, we propose a skew-oblivious data routing architecture that allocates secondary PEs and schedules them to share the workload of the overloaded PEs at run-time. In addition, we integrate the proposed architecture into a framework called Ditto to minimize the development efforts for applications that require skew handling. We evaluate Ditto on five commonly used applications: histogram building, data partitioning, pagerank, heavy hitter detection and hyperloglog. The results demonstrate that the generated implementations are robust to skew datasets and outperform the state-of-the-art designs in both throughput and BRAM usage efficiency.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Xinyu  Chen", "Hongshi  Tan", "Yao  Chen", "Bingsheng  He", "Weng-Fai  Wong", "Deming  Chen"], "year": 2021, "n_citations": 0}
{"id": 1651600, "s2_id": "152ce4d311465d0038ea72e2184be0f6a1ed28ac", "title": "SAFIUS - A Secure and Accountable Filesystem over Untrusted Storage", "abstract": "We describe SAFIUS, a secure accountable file system that resides over an untrusted storage. SAFIUS provides strong security guarantees like confidentiality, integrity, prevention from rollback attacks, and accountability. SAFIUS also enables read/write sharing of data and provides the standard UNIX-like interface for applications. To achieve accountability with good performance, it uses asynchronous signatures; to reduce the space required for storing these signatures, a novel signature pruning mechanism is used. SAFIUS has been implemented on a GNU/Linux based system modifying OpenGFS. Preliminary performance studies show that SAFIUS has a tolerable overhead for providing secure storage: while it has an overhead of about 50% of OpenGFS in data intensive workloads, it is comparable (or better in some cases) to OpenGFS in metadata intensive workloads.", "venue": "Fourth International IEEE Security in Storage Workshop", "authors": ["V.  Sriram", "Ganesh M. Narayan", "K.  Gopinath"], "year": 2007, "n_citations": 4}
{"id": 1652734, "s2_id": "4bec1d7c6cdb649024bdab8bff21a238d4260f43", "title": "Comparison of the Performance of Two Service Disciplines for a Shared Bus Multiprocessor with Private Caches", "abstract": "In this paper, we compare two analytical models for evaluation of cache coherence overhead of a shared bus multiprocessor with private caches. The models are based on a closed queuing network with different service disciplines. We find that the priority discipline can be used as a lower-level bound. Some numerical results are shown graphically.", "venue": "ArXiv", "authors": ["Angel Vassilev Nikolov", "Lerato  Lerato"], "year": 2010, "n_citations": 0}
{"id": 1652859, "s2_id": "6ce9cf5f0b1d6a884212ab71798f530626246819", "title": "Eigenvalue-based cyclostationary spectrum sensing using multiple antennas", "abstract": "In this paper, we propose a signal-selective spectrum sensing method for cognitive radio networks and specifically targeted for receivers with multiple-antenna capability. This method is used for detecting the presence or absence of primary users based on the eigenvalues of the cyclic covariance matrix of received signals. In particular, the cyclic correlation significance test is used to detect a specific signal-of-interest by exploiting knowledge of its cyclic frequencies. The analytical threshold for achieving constant false alarm rate using this detection method is presented, verified through simulations, and shown to be independent of both the number of samples used and the noise variance, effectively eliminating the dependence on accurate noise estimation. The proposed method is also shown, through numerical simulations, to outperform existing multiple-antenna cyclostationary-based spectrum sensing algorithms under a quasi-static Rayleigh fading channel, in both spatially correlated and uncorrelated noise environments. The algorithm also has significantly lower computational complexity than these other approaches.", "venue": "2012 IEEE Global Communications Conference (GLOBECOM)", "authors": ["Paulo  Urriza", "Eric  Rebeiz", "Danijela  Cabric"], "year": 2012, "n_citations": 4}
{"id": 1654606, "s2_id": "1e68bd3d5fc94567fcab013933e3a340f514b783", "title": "Decentralized Fair Scheduling in Two-Hop Relay-Assisted Cognitive OFDMA Systems", "abstract": "In this paper, we consider a two-hop relay-assisted cognitive downlink Orthogonal frequency-division multiple access (OFDMA) system (named as secondary system) dynamically accessing a spectrum licensed to a primary network, thereby improving the efficiency of spectrum usage. A cluster-based relay-assisted architecture is proposed for the secondary system, where relay stations are employed for minimizing the interference to the users in the primary network and achieving fairness for cell-edge users in the secondary system. Based on this architecture, an asymptotically optimal solution is derived for jointly controlling data rates, transmission power, and subchannel allocation to optimize the average weighted sum goodput where the proportional fair scheduling (PFS) is included as a special case. This solution supports decentralized implementation, requires small communication overhead, and is robust against imperfect channel state information at the transmitter (CSIT) and imperfect sensing measurement. The proposed solution achieves significant throughput gain and better user-fairness compared with the existing designs. Finally, we derive a simple and asymptotically optimal scheduling solution as well as the associated closed-form performance under the proportional fair scheduling for a large number of users. The system throughput is shown to be O(N(1-qp)(1-qpN)lnlnKc), where Kc is the number of users in one cluster, N is the number of subchannels, and qp is the active probability of primary users.", "venue": "IEEE Journal of Selected Topics in Signal Processing", "authors": ["Rui  Wang", "Vincent K. N. Lau", "Ying  Cui"], "year": 2011, "n_citations": 28}
{"id": 1657178, "s2_id": "c8959248c1f70739dee1c7fe43519975551af389", "title": "Array program transformation with Loo.py by example: high-order finite elements", "abstract": "To concisely and effectively demonstrate the capabilities of our program transformation system Loo.py, we examine a transformation path from two real-world Fortran subroutines as found in a weather model to a single high-performance computational kernel suitable for execution on modern GPU hardware. Along the transformation path, we encounter kernel fusion, vectorization, prefetching, parallelization, and algorithmic changes achieved by mechanized conversion between imperative and functional/substitution-based code, among a number more. We conclude with performance results that demonstrate the effects and support the effectiveness of the applied transformations.", "venue": "ARRAY@PLDI", "authors": ["Andreas  Kl\u00f6ckner", "Lucas C. Wilcox", "Timothy C. Warburton"], "year": 2016, "n_citations": 4}
{"id": 1657473, "s2_id": "20384b6d47461c7db8a3e4d08ed7d4afa98f8f6d", "title": "Temporal blocking of finite-difference stencil operators with sparse \u201coff-the-grid\u201d sources", "abstract": "Stencil kernels dominate a range of scientific applications, including seismic and medical imaging, image processing, and neural networks. Temporal blocking is a performance optimization that aims to reduce the required memory bandwidth of stencil computations by re-using data from the cache for multiple time steps. It has already been shown to be beneficial for this class of algorithms. However, applying temporal blocking to practical applications\u2019 stencils remains challenging. These computations often consist of sparsely located operators not aligned with the computational grid (\u201coff-the-grid\u201d). Our work is motivated by modelling problems in which source injections result in wavefields that must then be measured at receivers by interpolation from the grided wavefield. The resulting data dependencies make the adoption of temporal blocking much more challenging. We propose a methodology to inspect these data dependencies and reorder the computation, leading to performance gains in stencil codes where temporal blocking has not been applicable. We implement this novel scheme in the Devito domain-specific compiler toolchain. Devito implements a domain-specific language embedded in Python to generate optimized partial differential equation solvers using the finite-difference method from high-level symbolic problem definitions. We evaluate our scheme using isotropic acoustic, anisotropic acoustic, and isotropic elastic wave propagators of industrial significance. After auto-tuning, performance evaluation shows that this enables substantial performance improvement through temporal blocking over highly-optimized vectorized spatially-blocked code of up to 1.6x.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["George  Bisbas", "Fabio  Luporini", "Mathias  Louboutin", "Rhodri  Nelson", "Gerard  Gorman", "Paul H. J. Kelly"], "year": 2021, "n_citations": 0}
{"id": 1657954, "s2_id": "e7850e5f460362dd55e2e2cade317a458e4f0e50", "title": "Erlang Redux: An Ansatz Method for Solving the M/M/m Queue", "abstract": "This exposition presents a novel approach to solving an M/M/m queue for the waiting time and the residence time. The motivation comes from an algebraic solution for the residence time of the M/M/1 queue. The key idea is the introduction of an ansatz transformation, defined in terms of the Erlang B function, that avoids the more opaque derivation based on applied probability theory. The only prerequisite is an elementary knowledge of the Poisson distribution, which is already necessary for understanding the M/M/1 queue. The approach described here supersedes our earlier approximate morphing transformation.", "venue": "ArXiv", "authors": ["Neil J. Gunther"], "year": 2020, "n_citations": 0}
{"id": 1661370, "s2_id": "e4cf621e3d841e948b32d5d0c8a62c1033a9c80d", "title": "Low Delay Random Linear Coding and Scheduling Over Multiple Interfaces", "abstract": "High-performance real-time applications, expected to be of importance in the upcoming 5G era, such as virtual and augmented reality or tele-presence, have stringent requirements on throughput and per-packet in-order delivery delay. Use of multipath transport is gaining momentum for supporting these applications. However, building an efficient, low latency multipath transfer mechanism remains highly challenging. The primary reason for this is that the delivery delay along each path is typically uncertain and time-varying. When the transmitter ignores the stochastic nature of the path delays, then packets sent along different paths frequently arrive out of order and need to be buffered at the receiver to allow in-order delivery to the application. In this paper, we propose Stochastic Earliest Delivery Path First (S-EDPF), a generalization of EDPF which takes into account uncertainty and time-variation in path delays yet has low-complexity suited to practical implementation. Moreover, we integrate a novel low-delay Forward Error Correction (FEC) scheme into S-EDPF in a principled manner by deriving the optimal schedule for coded packets across multiple paths. Finally, we demonstrate, both analytically and empirically, that S-EDPF is effective at mitigating the delay impact of reordering and loss in multipath transport protocols, offering substantial performance gains over the state of the art.", "venue": "IEEE Transactions on Mobile Computing", "authors": ["Andres  Garcia-Saavedra", "Mohammad  Karzand", "Douglas J. Leith"], "year": 2017, "n_citations": 37}
{"id": 1665590, "s2_id": "8647db784771e30dd2718a45aae9509c9bca04de", "title": "Accelerating unstructured finite volume computations on field\u2010programmable gate arrays", "abstract": "In the paper, an field\u2010programmable gate array (FPGA)\u2010based framework is described to efficiently accelerate unstructured finite volume computations where the same mathematical expression has to be evaluated at every point of the mesh. The irregular memory access patterns caused by the unstructured spatial discretization are eliminated by a novel mesh node reordering technique, and a special architecture is designed to fully utilize the benefits of the predictable memory access patterns. In the proposed architecture, a fixed\u2010size moving window of the input stream of the reordered state variables is cached into the on\u2010chip memory and a pipelined chain of processing elements, which gets input only from the fast on\u2010chip memory, is used to carry out the computations. The arithmetic unit (AU) of the processing elements is generated from the data flow graph extracted from the given mathematical expression. The data flow graph is partitioned with a novel graph partitioning algorithm to break up the AU into smaller locally controlled parts, which can be more efficiently implemented in FPGA than the globally controlled AU. The proposed architecture and algorithms are presented via a case study solving the Euler equations on an unstructured mesh. On the currently available largest FPGA, the generated architecture contains three processing elements working in a pipelined fashion to provide one order of magnitude speedup compared with a high performance microprocessor and three times speedup compared with a high performance graphics processing unit. Copyright \u00a9 2013 John Wiley & Sons, Ltd.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Zolt\u00e1n  Nagy", "Csaba  Nemes", "Antal  Hiba", "\u00c1rp\u00e1d  Cs\u00edk", "Andr\u00e1s  Kiss", "Mikl\u00f3s  Ruszink\u00f3", "P\u00e9ter  Szolgay"], "year": 2014, "n_citations": 9}
{"id": 1667186, "s2_id": "e0254891abcc919f3f949a7fadd30ed04cf1cea8", "title": "Modeling the resilience of large and evolving systems", "abstract": "This paper summarizes the state of knowledge and ongoing research on methods and techniques for resilience evaluation, taking into account the resilience-scaling challenges and properties related to the ubiquitous computerized systems. We mainly focus on quantitative evaluation approaches and, in particular, on model-based evaluation techniques that are commonly used to evaluate and compare, from the dependability point of view, different architecture alternatives at the design stage. We outline some of the main modeling techniques aiming at mastering the largeness of analytical dependability models at the construction level. Actually, addressing the model largeness problem is important with respect to the investigation of the scalability of current techniques to meet the complexity challenges of ubiquitous systems. Finally we present two case studies in which some of the presented techniques are applied for modeling web services and General Packet Radio Service (GPRS) mobile telephone networks, as prominent examples of large and evolving systems.", "venue": "ArXiv", "authors": ["Mohamed  Ka\u00e2niche", "Paolo  Lollini", "Andrea  Bondavalli", "Karama  Kanoun"], "year": 2012, "n_citations": 12}
{"id": 1667429, "s2_id": "c6f89f02330976d504e04d3b47721ca92b026a82", "title": "Performance Modeling of Streaming Kernels and Sparse Matrix-Vector Multiplication on A64FX", "abstract": "The A64FX CPU powers the current #1 supercomputer on the Top500 list. Although it is a traditional cache-based multicore processor, its peak performance and memory bandwidth rival accelerator devices. Generating efficient code for such a new architecture requires a good understanding of its performance features. Using these features, we construct the Execution-Cache-Memory (ECM) performance model for the A64FX processor in the FX700 supercomputer and validate it using streaming loops. We also identify architectural peculiarities and derive optimization hints. Applying the ECM model to sparse matrix-vector multiplication (SpMV), we motivate why the CRS matrix storage format is inappropriate and how the SELL-C-\u03c3 format with suitable code optimizations can achieve bandwidth saturation for SpMV.", "venue": "2020 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)", "authors": ["Christie L. Alappat", "Jan  Laukemann", "Thomas  Gruber", "Georg  Hager", "Gerhard  Wellein", "Nils  Meyer", "Tilo  Wettig"], "year": 2020, "n_citations": 5}
{"id": 1667478, "s2_id": "f4a6ac670235694ec4fc19f42a2a09a718463572", "title": "A Unified Framework for Analyzing Closed Queueing Networks in Bike Sharing Systems", "abstract": "During the last decade bike sharing systems have emerged as a public transport mode in urban short trips in more than 500 major cities around the world. For the mobility service mode, many challenges from its operations are not well addressed yet, for example, how to develop the bike sharing systems to be able to effectively satisfy the fluctuating demands both for bikes and for vacant lockers. To this end, it is a key to give performance analysis of the bike sharing systems. This paper first describes a large-scale bike sharing system. Then the bike sharing system is abstracted as a closed queueing network with multi-class customers, where the virtual customers and the virtual nodes are set up, and the service rates as well as the relative arrival rates are established. Finally, this paper gives a product-form solution to the steady state joint probabilities of queue lengths, and gives performance analysis of the bike sharing system. Therefore, this paper provides a unified framework for analyzing closed queueing networks in the study of bike sharing systems. We hope the methodology and results of this paper can be applicable in the study of more general bike sharing systems.", "venue": "ArXiv", "authors": ["Quan-Lin  Li", "Rui-Na  Fan", "Jing-Yu  Ma"], "year": 2016, "n_citations": 9}
{"id": 1668369, "s2_id": "3f1712b4a71f0a072f3996fb98b731a7e5965ef3", "title": "PerfSim: A Performance Simulator for Cloud Native Microservice Chains", "abstract": "Cloud native computing paradigm allows microservice-based applications to take advantage of cloud infrastructure in a scalable, reusable, and interoperable way. However, in a cloud native system, the vast number of configuration parameters and highly granular resource allocation policies can significantly impact the performance and deployment cost. For understanding and analyzing these implications in an easy, quick, and cost-effective way, we present PerfSim, a discrete-event simulator for approximating and predicting the performance of cloud native service chains in user-defined scenarios. To this end, we proposed a systematic approach for modeling the performance of microservices endpoint functions by collecting and analyzing their performance and network traces. With a combination of the extracted models and user-defined scenarios, PerfSim can then simulate the performance behavior of all services over a given period and provide an approximation for system KPIs, such as requests\u2019 average response time. Using the processing power of a single laptop, we evaluated both simulation accuracy and speed of PerfSim in 104 prevalent scenarios and compared the simulation results with the identical deployment in a real Kubernetes cluster. We achieved \u223c81-99% simulation accuracy in approximating the average response time of incoming requests and \u223c16-1200 times speed-up factor for the simulation.", "venue": "IEEE Transactions on Cloud Computing", "authors": ["Michel Gokan Khan", "Javid  Taheri", "Auday  Al-Dulaimy", "Andreas  Kassler"], "year": 2021, "n_citations": 0}
{"id": 1670115, "s2_id": "a0865ec46d9c3282e6743d02d114385c1c68af49", "title": "Exact solutions for M/M/c/Setup queues", "abstract": "Recently multiserver queues with setup times have been extensively studied because they have applications in power-saving data centers. A challenging model is the M/M/c/Setup queue where a server is turned off when it is idle and is turned on if there are some waiting jobs. Recently, Gandhi et al. (in: Proceedings of the ACM SIGMETRICS, pp. 153\u2013166, ACM,\u00a02013; Queueing Syst. 77(2):177\u2013209,\u00a02014) obtain the generating function of the number of jobs in the system, as well as the Laplace transform of the response time using the recursive renewal reward approach and the distributional Little\u2019s law (Keilson and Servi in Oper Res Lett 7(5):223\u2013 227,\u00a01988). In this paper, we derive exact solutions for the joint stationary queue length distribution of the same model using two alternative methodologies: generating function approach and matrix analytic method. The generating function approach yields exact closed form expressions for the joint stationary queue length distribution and the conditional decomposition formula. On the other hand, the matrix analytic approach leads to an exact recursive algorithm to calculate the joint stationary distribution and performance measures so as to provide some application insights.", "venue": "Telecommun. Syst.", "authors": ["Tuan  Phung-Duc"], "year": 2017, "n_citations": 40}
{"id": 1670310, "s2_id": "61423f44a0f592b9920a91a642f380a6c25d8e6a", "title": "Quiescence of Self-stabilizing Gossiping among Mobile Agents in Graphs", "abstract": "This paper considers gossiping among mobile agents in graphs: agents move on the graph and have to disseminate their initial information to every other agent. We focus on self-stabilizing solutions for the gossip problem, where agents may start from arbitrary locations in arbitrary states. Self-stabilization requires (some of the) participating agents to keep moving forever, hinting at maximizing the number of agents that could be allowed to stop moving eventually. \n \nThis paper formalizes the self-stabilizing agent gossip problem, introduces the quiescence number(i.e., the maximum number of eventually stopping agents) of self-stabilizing solutions and investigates the quiescence number with respect to several assumptions related to agent anonymity, synchrony, link duplex capacity, and whiteboard capacity.", "venue": "SIROCCO", "authors": ["Toshimitsu  Masuzawa", "S\u00e9bastien  Tixeuil"], "year": 2008, "n_citations": 2}
{"id": 1670823, "s2_id": "82dba43899b5dcef15edb6290f844488a3a676a8", "title": "Bolt: Accelerated Data Mining with Fast Vector Compression", "abstract": "Vectors of data are at the heart of machine learning and data mining. Recently, vector quantization methods have shown great promise in reducing both the time and space costs of operating on vectors. We introduce a vector quantization algorithm that can compress vectors over 12x faster than existing techniques while also accelerating approximate vector operations such as distance and dot product computations by up to 10x. Because it can encode over 2GB of vectors per second, it makes vector quantization cheap enough to employ in many more circumstances. For example, using our technique to compute approximate dot products in a nested loop can multiply matrices faster than a state-of-the-art BLAS implementation, even when our algorithm must first compress the matrices. In addition to showing the above speedups, we demonstrate that our approach can accelerate nearest neighbor search and maximum inner product search by over 100x compared to floating point operations and 10x compared to other vector quantization methods. Our approximate Euclidean distance and dot product computations are not only faster than those of related algorithms with slower encodings, but also faster than Hamming distance computations, which have direct hardware support on the tested platforms. We also assess the errors of our algorithm's approximate distances and dot products, and find that it is competitive with existing, slower vector quantization algorithms.", "venue": "KDD", "authors": ["Davis W. Blalock", "John V. Guttag"], "year": 2017, "n_citations": 15}
{"id": 1678115, "s2_id": "b7e0b26542e6fc4d8f5dd1568384efbe9c833723", "title": "Queues with Updating Information: Finding the Amplitude of Oscillations", "abstract": "Many service systems provide customers with information about the system so that customers can make an informed decision about whether to join or not. Many of these systems provide information in the form of an update. Thus, the information about the system is updated periodically in increments of size \u2206. It is known that these updates can cause oscillations in the resulting dynamics. However, it is an open problem to explicitly characterize the size of these oscillations when they occur. In this paper, we solve this open problem and show how to exactly calculate the amplitude of these oscillations via a fixed point equation. We also calculate closed form approximations via Taylor expansions of the fixed point equation and show that these approximations are very accurate, especially when \u2206 is large. Our analysis provides new insight for systems that use updates as a way of disseminating information to customers.", "venue": "ArXiv", "authors": ["Philip  Doldo", "Jamol  Pender"], "year": 2021, "n_citations": 0}
{"id": 1679299, "s2_id": "c7712925b4dad7b53d5d91211ea06ba933d81d41", "title": "A System-Theoretic Approach to Bandwidth Estimation", "abstract": "This paper presents a new foundational approach to reason about available bandwidth estimation as the analysis of a min-plus linear system. The available bandwidth of a link or complete path is expressed in terms of a service curve, which is a function that appears in the network calculus to express the service available to a traffic flow. The service curve is estimated based on measurements of a sequence of probing packets or passive measurements of a sample path of arrivals. It is shown that existing bandwidth estimation methods can be derived in the min-plus algebra of the network calculus, thus providing further mathematical justification for these methods. Principal difficulties of estimating available bandwidth from measurements of network probes are related to potential nonlinearities of the underlying network. When networks are viewed as systems that operate either in a linear or in a nonlinear regime, it is argued that probing schemes extract the most information at a point when the network crosses from a linear to a nonlinear regime. Experiments on the Emulab testbed at the University of Utah, Salt Lake City, evaluate the robustness of the system-theoretic interpretation of networks in practice. Multinode experiments evaluate how well the convolution operation of the min-plus algebra provides estimates for the available bandwidth of a path from estimates of individual links.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["J\u00f6rg  Liebeherr", "Markus  Fidler", "Shahrokh  Valaee"], "year": 2010, "n_citations": 61}
{"id": 1681837, "s2_id": "8b82a2dfa007ad20064a632cfeaf6e835e90ac0e", "title": "Pyxis: An Open-Source Performance Dataset of Sparse Accelerators", "abstract": "Specialized accelerators provide gains of performance and efficiency in specific domains of applications. Sparse data structures or/and representations exist in a wide range of applications. However, it is challenging to design accelerators for sparse applications because no analytic architecture or performance-level models are able to fully capture the spectrum of the sparse data. Accelerator researchers rely on real execution to get precise feedback for their designs. In this work, we present PYXIS, a performance dataset for specialized accelerators on sparse data. PYXIS collects accelerator designs and real execution performance statistics. Currently, there are 73.8 K instances in PYXIS. PYXIS is open-source, and we are constantly growing PYXIS with new accelerator designs and performance statistics. PYXIS can benefit researchers in the fields of accelerator, architecture, performance, algorithm and many related topics.", "venue": "ArXiv", "authors": ["Linghao  Song", "Yuze  Chi", "Jason  Cong"], "year": 2021, "n_citations": 0}
{"id": 1681988, "s2_id": "f59598f6c7d42460f325f5750544014b033be202", "title": "Virtual memory partitioning for enhancing application performance in mobile platforms", "abstract": "Recently, the amount of running software on smart mobile devices is gradually increasing due to the introduction of application stores. The application store is a type of digital distribution platform for application software, which is provided as a component of an operating system on a smartphone or tablet. Mobile devices have limited memory capacity and, unlike server and desktop systems, due to their mobility they do not have a memory slot that can expand the memory capacity. Low memory killer (LMK) and out-of-memory killer (OOMK) are widely used memory management solutions in mobile systems. They forcibly terminate applications when the available physical memory becomes insufficient. In addition, before the forced termination, the memory shortage incurs thrashing and fragmentation, thus slowing down application performance. Although the existing page reclamation mechanism is designed to secure available memory, it could seriously degrade user responsiveness due to the thrashing. Memory management is therefore still important especially in mobile devices with small memory capacity. This paper presents a new memory partitioning technique that resolves the deterioration of the existing application life cycle induced by LMK and OOMK. It provides a completely isolated virtual memory node at the operating system level. Evaluation results demonstrate that the proposed method improves application execution time under memory shortage, compared with methods in previous studies.", "venue": "IEEE Transactions on Consumer Electronics", "authors": ["Geunsik  Lim", "Changwoo  Min", "Young Ik Eom"], "year": 2013, "n_citations": 12}
{"id": 1682801, "s2_id": "706e7b81ee7fa07c1f67d433febd754c344293ef", "title": "Efficient Similarity-aware Compression to Reduce Bit-writes in Non-Volatile Main Memory for Image-based Applications", "abstract": "Image bitmaps have been widely used in in-memory applications, which consume lots of storage space and energy. Compared with legacy DRAM, non-volatile memories (NVMs) are suitable for bitmap storage due to the salient features in capacity and power savings. However, NVMs suffer from higher latency and energy consumption in writes compared with reads. Although compressing data in write accesses to NVMs on-the-fly reduces the bit-writes in NVMs, existing precise or approximate compression schemes show limited performance improvements for data of bitmaps, due to the irregular data patterns and variance in data. We observe that the data containing bitmaps show the pixel-level similarity due to the analogous contents in adjacent pixels. By exploiting the pixel-level similarity, we propose SimCom, an efficient similarity-aware compression scheme in hardware layer, to compress data for each write access on-the-fly. The idea behind SimCom is to compress continuous similar words into the pairs of base words with runs. With the aid of domain knowledge of images, SimCom adaptively selects an appropriate compression mode to achieve an efficient trade-off between image quality and memory performance. We implement SimCom on GEM5 with NVMain and evaluate the performance with real-world workloads. Our results demonstrate that SimCom reduces 33.0%, 34.8% write latency and saves 28.3%, 29.0% energy than state-of-the-art FPC and BDI with minor quality loss of 3%.", "venue": "ArXiv", "authors": ["Zhangyu  Chen", "Yu  Hua", "Pengfei  Zuo", "Yuanyuan  Sun", "Yuncheng  Guo"], "year": 2019, "n_citations": 0}
{"id": 1683111, "s2_id": "0068aa2fb735142c06c559eec453136c9cdd334c", "title": "Multi-core architectures: Complexities of performance prediction and the impact of cache topology", "abstract": "The balance metric is a simple approach to estimate the performance of bandwidth-limited loop kernels. However, applying the method to in-cache situations and modern multi-core architectures yields unsatisfactory results. This paper analyzes the in uence of cache hierarchy design on performance predictions for bandwidth-limited loop kernels on current mainstream processors. We present a diagnostic model with improved predictive power, correcting the limitations of the simple balance metric. The importance of code execution overhead even in bandwidth-bound situations is emphasized. Finally we analyze the impact of synchronization overhead on multi-threaded performance with a special emphasis on the in uence of cache topology.", "venue": "ArXiv", "authors": ["Jan  Treibig", "Georg  Hager", "Gerhard  Wellein"], "year": 2009, "n_citations": 13}
{"id": 1683543, "s2_id": "f2842821c73ef3e6603538a9eff9f3943ade6b6c", "title": "Re-evaluating scaling methods for distributed parallel systems", "abstract": "The paper explains why Amdahl's Law shall be interpreted specifically for distributed parallel systems and why it generated so many debates, discussions, and abuses. We set up a general model and list many of the terms affecting parallel processing. We scrutinize the validity of neglecting certain terms in different approximations, with special emphasis on the famous scaling laws of parallel processing. We clarify that when using the right interpretation of terms, Amdahl's Law is the governing law of all kinds of parallel processing. Amdahl's Law describes among others the history of supercomputing, the inherent performance limitation of the different kinds of parallel processing and it is the basic Law of the 'modern computing' paradigm, that the computing systems working under extreme computing conditions are desperately needed.", "venue": "ArXiv", "authors": ["J'anos  V'egh"], "year": 2020, "n_citations": 5}
{"id": 1684240, "s2_id": "1ee425664ecc6ea7c5fc50de9b04fb714115a25a", "title": "Blockumulus: A Scalable Framework for Smart Contracts on the Cloud", "abstract": "Public blockchains have spurred the growing popularity of decentralized transactions and smart contracts, especially on the financial market. However, public blockchains exhibit their limitations on the transaction throughput, storage availability, and compute capacity. To avoid transaction gridlock, public blockchains impose large fees and per-block resource limits, making it difficult to accommodate the ever-growing high transaction demand. Previous research endeavors to improve the scalability and performance of blockchain through various technologies, such as side-chaining, sharding, secured off-chain computation, communication network optimizations, and efficient consensus protocols. However, these approaches have not attained a widespread adoption due to their inability in delivering a cloud-like performance, in terms of the scalability in transaction throughput, storage, and compute capacity. In this work, we determine that the major obstacle to public blockchain scalability is their underlying unstructured P2P networks. We further show that a centralized network can support the deployment of decentralized smart contracts. We propose a novel approach for achieving scalable decentralization: instead of trying to make blockchain scalable, we deliver decentralization to already scalable cloud by using an Ethereum smart contract. We introduce Blockumulus, a framework that can deploy decentralized cloud smart contract environments using a novel technique called overlay consensus. Through experiments, we demonstrate that Blockumulus is scalable in all three dimensions: computation, data storage, and transaction throughput. Besides eliminating the current code execution and storage restrictions, Blockumulus delivers a transaction latency between 2 and 5 seconds under normal load. Moreover, the stress test of our prototype reveals the ability to execute 20,000 simultaneous transactions under 26 seconds, which is on par with the average throughput of worldwide credit card transactions.", "venue": "2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)", "authors": ["Nikolay  Ivanov", "Qiben  Yan", "Qingyang  Wang"], "year": 2021, "n_citations": 1}
{"id": 1686906, "s2_id": "4bb3ce3f54d3fb52c0c0561d6c35f30bb7ab64fb", "title": "PZnet: Efficient 3D ConvNet Inference on Manycore CPUs", "abstract": "Convolutional nets have been shown to achieve state-of-the-art accuracy in many biomedical image analysis tasks. Many tasks within biomedical analysis domain involve analyzing volumetric (3D) data acquired by CT, MRI and Microscopy acquisition methods. To deploy convolutional nets in practical working systems, it is important to solve the efficient inference problem. Namely, one should be able to apply an already-trained convolutional network to many large images using limited computational resources. In this paper we present PZnet, a CPU-only engine that can be used to perform inference for a variety of 3D convolutional net architectures. PZNet outperforms MKL-based CPU implementations of PyTorch and Tensorflow by more than 3.5x for the popular U-net architecture. Moreover, for 3D convolutions with low featuremap numbers, cloud CPU inference with PZnet outperforms cloud GPU inference in terms of cost efficiency.", "venue": "CVC", "authors": ["Sergiy  Popovych", "Davit  Buniatyan", "Aleksandar  Zlateski", "Kai  Li", "H. Sebastian Seung"], "year": 2019, "n_citations": 4}
{"id": 1691670, "s2_id": "00231a1bc48a1a3087168667681e7c9bd204fe61", "title": "Mesh: compacting memory management for C/C++ applications", "abstract": "Programs written in C/C++ can suffer from serious memory fragmentation, leading to low utilization of memory, degraded performance, and application failure due to memory exhaustion. This paper introduces Mesh, a plug-in replacement for malloc that, for the first time, eliminates fragmentation in unmodified C/C++ applications. Mesh combines novel randomized algorithms with widely-supported virtual memory operations to provably reduce fragmentation, breaking the classical Robson bounds with high probability. Mesh generally matches the runtime performance of state-of-the-art memory allocators while reducing memory consumption; in particular, it reduces the memory of consumption of Firefox by 16% and Redis by 39%.", "venue": "PLDI", "authors": ["Bobby  Powers", "David  Tench", "Emery D.  Berger", "Andrew  McGregor"], "year": 2019, "n_citations": 11}
{"id": 1695459, "s2_id": "3bb24ae1a063476484ee2d22a66d33a0517d484a", "title": "Throughput Optimal Decentralized Scheduling with Single-bit State Feedback for a Class of Queueing Systems", "abstract": "Motivated by medium access control for resource-challenged wireless Internet of Things (IoT), we consider the problem of queue scheduling with reduced queue state information. In particular, we consider a time-slotted scheduling model with $N$ sensor nodes, with pair-wise dependence, such that Nodes $i$ and $i + 1,~0 4,$ there is no QNB policy that is sum-queue length optimal over all arrival rate vectors in the capacity region. \nWe then extend our results to a more general class of interference constraints that we call cluster-of-cliques (CoC) conflict graphs. We consider two types of CoC networks, namely, Linear Arrays of Cliques (LAoC) and Star-of-Cliques (SoC) networks. We develop QNB policies for these classes of networks, study their stability and delay properties, and propose and analyze techniques to reduce the amount of state information to be disseminated across the network for scheduling. In the SoC setting, we propose a throughput-optimal policy that only uses information that nodes in the network can glean by sensing activity (or lack thereof) on the channel. Our throughput-optimality results rely on two new arguments: a Lyapunov drift lemma specially adapted to policies that are queue length-agnostic, and a priority queueing analysis for showing strong stability.", "venue": "ArXiv", "authors": ["Avinash  Mohan", "Aditya  Gopalan", "Anurag  Kumar"], "year": 2020, "n_citations": 2}
{"id": 1696835, "s2_id": "02aa75e8c9c8b62785ae38ceb081110e606f8e1b", "title": "Revisiting the issues on netflow sample and export performance", "abstract": "The high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. Sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. The sampling process in nearly all cases is a deterministic process of choosing 1 in every N packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. Even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. However, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. In this paper, we assess the performance of the sampling process as used in NetFlow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "venue": "2008 Third International Conference on Communications and Networking in China", "authors": ["Hamed  Haddadi", "Raul  Landa", "Miguel  Rio", "Saleem N. Bhatti"], "year": 2008, "n_citations": 7}
{"id": 1698034, "s2_id": "fd6a5fd4aa43d416ebd5ff85cfd10467c65cefd2", "title": "Vertical, Temporal, and Horizontal Scaling of Hierarchical Hypersparse GraphBLAS Matrices", "abstract": "Hypersparse matrices are a powerful enabler for a variety of network, health, finance, and social applications. Hierarchical hypersparse GraphBLAS matrices enable rapid streaming updates while preserving algebraic analytic power and convenience. In many contexts, the rate of these updates sets the bounds on performance. This paper explores hierarchical hypersparse update performance on a variety of hardware with identical software configurations. The high-level language bindings of the GraphBLAS readily enable performance experiments on simultaneous diverse hardware. The best single process performance measured was 4,000,000 updates per second. The best single node performance measured was 170,000,000 updates per second. The hardware used spans nearly a decade and allows a direct comparison of hardware improvements for this computation over this time range; showing a 2x increase in single-core performance, a 3x increase in single process performance, and a 5x increase in single node performance. Running on nearly 2,000 MIT SuperCloud nodes simultaneously achieved a sustained update rate of over 200,000,000,000 updates per second. Hierarchical hypersparse GraphBLAS allows the MIT SuperCloud to analyze extremely large streaming network data sets.", "venue": "2021 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Jeremy  Kepner", "Tim  Davis", "Chansup  Byun", "William  Arcand", "David  Bestor", "William  Bergeron", "Vijay  Gadepally", "Matthew  Hubbell", "Michael  Houle", "Michael  Jones", "Anna  Klein", "Lauren  Milechin", "Julie  Mullen", "Andrew  Prout", "Albert  Reuther", "Antonio  Rosa", "Siddharth  Samsi", "Charles  Yee", "Peter  Michaleas"], "year": 2021, "n_citations": 0}
{"id": 1699273, "s2_id": "836112889f4ad78886081b9e96dd40678cd376d5", "title": "Redundant Loads: A Software Inefficiency Indicator", "abstract": "Modern software packages have become increasingly complex with millions of lines of code and references to many external libraries. Redundant operations are a common performance limiter in these code bases. Missed compiler optimization opportunities, inappropriate data structure and algorithm choices, and developers' inattention to performance are some common reasons for the existence of redundant operations. Developers mainly depend on compilers to eliminate redundant operations. However, compilers' static analysis often misses optimization opportunities due to ambiguities and limited analysis scope; automatic optimizations to algorithmic and data structural problems are out of scope. We develop LoadSpy, a whole-program profiler to pinpoint redundant memory load operations, which are often a symptom of many redundant operations. The strength of LoadSpy exists in identifying and quantifying redundant load operations in programs and associating the redundancies with program execution contexts and scopes to focus developers' attention on problematic code. LoadSpy works on fully optimized binaries, adopts various optimization techniques to reduce its overhead, and provides a rich graphic user interface, which make it a complete developer tool. Applying LoadSpy showed that a large fraction of redundant loads is common in modern software packages despite highest levels of automatic compiler optimizations. Guided by LoadSpy, we optimize several well-known benchmarks and real-world applications, yielding significant speedups.", "venue": "2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)", "authors": ["Pengfei  Su", "Shasha  Wen", "Hailong  Yang", "Milind  Chabbi", "Xu  Liu"], "year": 2019, "n_citations": 13}
{"id": 1699294, "s2_id": "dad1c38e7700251d12b4ed3c729f847e5dfcedb0", "title": "DoKnowMe: Towards a Domain Knowledge-driven Methodology for Performance Evaluation", "abstract": "Software engineering considers performance evaluation to be one of the key portions of software quality assurance. Unfortunately, there seems to be a lack of standard methodologies for performance evaluation even in the scope of experimental computer science. Inspired by the concept of \"instantiation\" in object-oriented programming, we distinguish the generic performance evaluation logic from the distributed and ad-hoc relevant studies, and develop an abstract evaluation methodology (by analogy of \"class\") we name Domain Knowledge-driven Methodology (DoKnowMe). By replacing five predefined domain-specific knowledge artefacts, DoKnowMe could be instantiated into specific methodologies (by analogy of \"object\") to guide evaluators in performance evaluation of different software and even computing systems. We also propose a generic validation framework with four indicators (i.e. usefulness, feasibility, effectiveness and repeatability), and use it to validate DoKnowMe in the Cloud services evaluation domain. Given the positive and promising validation result, we plan to integrate more common evaluation strategies to improve DoKnowMe and further focus on the performance evaluation of Cloud autoscaler systems.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Zheng  Li", "Liam  O'Brien", "Maria  Kihl"], "year": 2016, "n_citations": 7}
{"id": 1704217, "s2_id": "ca9c323b2f38937ede06f94afed9631f409897e9", "title": "Approximate Solution Approach and Performability Evaluation of Large Scale Beowulf Clusters", "abstract": "Beowulf clusters are very popular and deployed worldwide in support of scientific computing, because of the high computational power and performance. However, they also pose several challenges, and yet they need to provide high availability. The practical large-scale Beowulf clusters result in unpredictable, fault-tolerant, often detrimental outcomes. Successful development of high performance in storing and processing huge amounts of data in large-scale clusters necessitates accurate quality of service (QoS) evaluation. This leads to develop as well as design, analytical models to understand and predict of complex system behaviour in order to ensure availability of large-scale systems. Exact modelling of such clusters is not feasible due to the nature of the large scale nodes and the diversity of user requests. An analytical model for QoS of large-scale server farms and solution approaches are necessary. In this paper, analytical modelling of large-scale Beowulf clusters is considered together with availability issues. A generic and flexible approximate solution approach is developed to handle large number of nodes for performability evaluation. The proposed analytical model and the approximate solution approach provide flexibility to evaluate the QoS measurements for such systems. In order to show the efficacy and the accuracy of the proposed approach, the results obtained from the analytical model are validated with the results obtained from the discrete event simulations.", "venue": "ArXiv", "authors": ["Yonal  Kirsal", "Yoney Kirsal Ever"], "year": 2019, "n_citations": 0}
{"id": 1705496, "s2_id": "1af0dfbdc5832adf3866c8bd1eb0b33ed16d1b5d", "title": "ROSA: R Optimizations with Static Analysis", "abstract": "R is a popular language and programming environment for data scientists. It is increasingly co-packaged with both relational and Hadoop-based data platforms and can often be the most dominant computational component in data analytics pipelines. Recent work has highlighted inefficiencies in executing R programs, both in terms of execution time and memory requirements, which in practice limit the size of data that can be analyzed by R. This paper presents ROSA, a static analysis framework to improve the performance and space efficiency of R programs. ROSA analyzes input programs to determine program properties such as reaching definitions, live variables, aliased variables, and types of variables. These inferred properties enable program transformations such as C++ code translation, strength reduction, vectorization, code motion, in addition to interpretive optimizations such as avoiding redundant object copies and performing in-place evaluations. An empirical evaluation shows substantial reductions by ROSA in execution time and memory consumption over both CRAN R and Microsoft R Open.", "venue": "ArXiv", "authors": ["Rathijit  Sen", "Jianqiao  Zhu", "Jignesh M. Patel", "Somesh  Jha"], "year": 2017, "n_citations": 0}
{"id": 1709065, "s2_id": "98e9e8c99b8d0e487e3f4930109b795a58b597df", "title": "On the Stochastic Analysis of a Quantum Entanglement Switch", "abstract": "We study a quantum entanglement switch that serves k users in a star topology. We model variants of the system using continuous-time Markov chains (CTMCs) and obtain expressions for switch capacity and the expected number of qubits stored in memory at the switch. Using CTMCs allows us to obtain a number of analytic results for systems in which the links are homogeneous or heterogeneous and for switches that have infinite or finite buffer sizes. In addition, we can easily model the effects of decoherence of quantum states using this technique. From numerical observations, we discover that decoherence has little effect on capacity and expected number of stored qubits for homogeneous systems. For heterogeneous systems, especially those operating close to stability constraints, buffer size and decoherence can significantly affect performance. We also learn that, in general, increasing the buffer size from one to two qubits per link is advantageous to most systems, while increasing the buffer size further yields diminishing returns.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Gayane  Vardoyan", "Saikat  Guha", "Philippe  Nain", "Don  Towsley"], "year": 2019, "n_citations": 21}
{"id": 1710388, "s2_id": "9676e293b813226af5dbc15061e0f4dc1b87d0ac", "title": "Methods for Partitioning Data to Improve Parallel Execution Time for Sorting on Heterogeneous Clusters", "abstract": "The aim of the paper is to introduce general techniques in order to optimize the parallel execution time of sorting on a distributed architectures with processors of various speeds. Such an application requires a partitioning step. For uniformly related processors (processors speeds are related by a constant factor), we develop a constant time technique for mastering processor load and execution time in an heterogeneous environment and also a technique to deal with unknown cost functions. For non uniformly related processors, we use a technique based on dynamic programming. Most of the time, the solutions are in ${\\mathcal O}$(p) (p is the number of processors), independent of the problem size n. Consequently, there is a small overhead regarding the problem we deal with but it is inherently limited by the knowing of time complexity of the portion of code following the partitioning.", "venue": "GPC", "authors": ["Christophe  C\u00e9rin", "Jean-Christophe  Dubacq", "Jean-Louis  Roch"], "year": 2006, "n_citations": 4}
{"id": 1717695, "s2_id": "a1c3d2e5c2d8be00149fa1f3fd3aa32b57351dd3", "title": "Memory and compiler optimizations for low-power and -energy", "abstract": "Embedded systems become more and more widespread, especially autonomous ones, and clearly tend to be ubiquitous. In such systems, low-power and low-energy usage get ever more crucial. Furthermore, these issues also become paramount in (massively) multi-processors systems, either in one machine or more widely in a grid. The various problems faced pertain to autonomy, power supply possibilities, thermal dissipation, or even sheer energy cost. Although it has since long been studied in harware, energy optimization is more recent in software. In this paper, we thus aim at raising awareness to low-power and low-energy issues in the language and compilation community. We thus broadly but briefly survey techniques and solutions to this energy issue, focusing on a few specific aspects in the context of compiler optimizations and memory management.", "venue": "ArXiv", "authors": ["Olivier  Zendra"], "year": 2006, "n_citations": 13}
{"id": 1719081, "s2_id": "13d580f4b960c8e2881ff413989957e5dbd87704", "title": "Twine: An Embedded Trusted Runtime for WebAssembly", "abstract": "WebAssembly is an Increasingly popular lightweight binary instruction format, which can be efficiently embedded and sandboxed. Languages like C, C++, Rust, Go, and many others can be compiled into WebAssembly. This paper describes Twine, a WebAssembly trusted runtime designed to execute unmodified, language-independent applications. We leverage Intel SGX to build the runtime environment without dealing with language-specific, complex APIs. While SGX hardware provides secure execution within the processor, Twine provides a secure, sandboxed software runtime nested within an SGX enclave, featuring a WebAssembly system interface (WASI) for compatibility with unmodified WebAssembly applications. We evaluate Twine with a large set of general-purpose benchmarks and real-world applications. In particular, we used Twine to implement a secure, trusted version of SQLite, a well-known full-fledged embeddable database. We believe that such a trusted database would be a reasonable component to build many larger application services. Our evaluation shows that SQLite can be fully executed inside an SGX enclave via WebAssembly and existing system interface, with similar average performance overheads. We estimate that the performance penalties measured are largely compensated by the additional security guarantees and its full compatibility with standard WebAssembly. An indepth analysis of our results indicates that performance can be greatly improved by modifying some of the underlying libraries. We describe and implement one such modification in the paper, showing up to 4.1 \u00d7 speedup. Twine is open-source, available at GitHub along with instructions to reproduce our experiments.", "venue": "2021 IEEE 37th International Conference on Data Engineering (ICDE)", "authors": ["James  M'en'etrey", "Marcelo  Pasin", "Pascal  Felber", "Valerio  Schiavoni"], "year": 2021, "n_citations": 4}
{"id": 1726136, "s2_id": "c62af9d36f3501ce499f6517aeef51fc0aac4321", "title": "On CSI-Free Multiantenna Schemes for Massive RF Wireless Energy Transfer", "abstract": "Radio-frequency wireless energy transfer (RF-WET) is emerging as a potential green enabler for massive Internet of Things (IoT). Herein, we analyze channel state information (CSI)-free multiantenna strategies for powering wirelessly a large set of single-antenna IoT devices. The CSI-free schemes are AA-SS (AA-IS), where all antennas transmit the same (independent) signal(s), and SA, where just one antenna transmits at a time such that all antennas are utilized during the coherence block. We characterize the distribution of the provided energy under correlated Rician fading for each scheme and find out that while AA-IS and SA cannot take advantage of the multiple antennas to improve the average provided energy, its dispersion can be significantly reduced. Meanwhile, AA-SS provides the greatest average energy, but also the greatest energy dispersion, and the gains depend critically on the mean phase shifts between the antenna elements. We find that consecutive antennas must be $\\pi $ -phase shifted for optimum average energy performance under AA-SS. Our numerical results evidence that correlation is beneficial under AA-SS, while a greater line of sight (LOS) and/or the number of antennas is not always beneficial under such a scheme. Meanwhile, both AA-IS and SA schemes benefit from small correlation, large LOS, and/or a large number of antennas. Finally, AA-SS (SA and AA-IS) is (are) preferable when devices are (are not) clustered in specific spatial directions.", "venue": "IEEE Internet of Things Journal", "authors": ["Onel L. A. L\u00f3pez", "Samuel  Montejo-S\u00e1nchez", "Richard D. Souza", "Constantinos B. Papadias", "Hirley  Alves"], "year": 2021, "n_citations": 6}
{"id": 1726592, "s2_id": "aee1cbffab9d69f3ac93917200c6a46df823a70e", "title": "BEEBS: Open Benchmarks for Energy Measurements on Embedded Platforms", "abstract": "This paper presents and justifies an open benchmark suite named BEEBS, targeted at evaluating the energy consumption of embedded processors. \nWe explore the possible sources of energy consumption, then select individual benchmarks from contemporary suites to cover these areas. Version one of BEEBS is presented here and contains 10 benchmarks that cover a wide range of typical embedded applications. The benchmark suite is portable across diverse architectures and is freely available. \nThe benchmark suite is extensively evaluated, and the properties of its constituent programs are analysed. Using real hardware platforms we show case examples which illustrate the difference in power dissipation between three processor architectures and their related ISAs. We observe significant differences in the average instruction dissipation between the architectures of 4.4x, specifically 170uW/MHz (ARM Cortex-M0), 65uW/MHz (Adapteva Epiphany) and 88uW/MHz (XMOS XS1-L1).", "venue": "ArXiv", "authors": ["James  Pallister", "Simon J. Hollis", "Jeremy  Bennett"], "year": 2013, "n_citations": 69}
{"id": 1727343, "s2_id": "3c3f290866c43fa94befb0e87a8c283f1afe7ada", "title": "Optimization of Discrete-parameter Multiprocessor Systems using a Novel Ergodic Interpolation Technique", "abstract": "Modern multi-core systems have a large number of design parameters, most of which are discrete-valued, and this number is likely to keep increasing as chip complexity rises. Further, the accurate evaluation of a potential design choice is computationally expensive because it requires detailed cycle-accurate system simulation. If the discrete parameter space can be embedded into a larger continuous parameter space, then continuous space techniques can, in principle, be applied to the system optimization problem. Such continuous space techniques often scale well with the number of parameters. \nWe propose a novel technique for embedding the discrete parameter space into an extended continuous space so that continuous space techniques can be applied to the embedded problem using cycle accurate simulation for evaluating the objective function. This embedding is implemented using simulation-based ergodic interpolation, which, unlike spatial interpolation, produces the interpolated value within a single simulation run irrespective of the number of parameters. We have implemented this interpolation scheme in a cycle-based system simulator. In a characterization study, we observe that the interpolated performance curves are continuous, piece-wise smooth, and have low statistical error. We use the ergodic interpolation-based approach to solve a large multi-core design optimization problem with 31 design parameters. Our results indicate that continuous space optimization using ergodic interpolation-based embedding can be a viable approach for large multi-core design optimization problems.", "venue": "ArXiv", "authors": ["Neha V. Karanjkar", "Madhav P. Desai"], "year": 2014, "n_citations": 0}
{"id": 1729612, "s2_id": "3aedc5997c36befdbecd4b225588e7f404638b45", "title": "LBICA: A Load Balancer for I/O Cache Architectures", "abstract": "In recent years, enterprise Solid-State Drives (SSDs) are used in the caching layer of high-performance servers to close the growing performance gap between processing units and storage subsystem. SSD-based I/O caching is typically not effective in workloads with burst accesses in which the caching layer itself becomes the performance bottleneck because of the large number of accesses. Existing I/O cache architectures mainly focus on maximizing the cache hit ratio while they neglect the average queue time of accesses. Previous studies suggested bypassing the cache when burst accesses are identified. These schemes, however, are not applicable to a general cache configuration and also result in significant performance degradation on burst accesses.In this paper, we propose a novel I/O cache load balancing scheme (LBICA) with adaptive write policy management to prevent the I/O cache from becoming performance bottleneck in burst accesses. Our proposal, unlike previous schemes, which disable the I/O cache or bypass the requests into the disk subsystem in burst accesses, selectively reduces the number of waiting accesses in the SSD queue and balances the load between the I/O cache and the disk subsystem while providing the maximum performance. The proposed scheme characterizes the workload based on the type of in-queue requests and assigns an effective cache write policy. We aim to bypass the accesses which 1) are served faster by the disk subsystem or 2) cannot be merged with other accesses in the I/O cache queue. Doing so, the selected requests are responded by the disk layer, preventing from overloading the I/O cache. Our evaluations on a physical system shows that LBICA reduces the load on the I/O cache by 48% and improves the performance of burst workloads by 30% compared to the latest state-of-the-art load balancing scheme.", "venue": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Saba  Ahmadian", "Reza  Salkhordeh", "Hossein  Asadi"], "year": 2019, "n_citations": 3}
{"id": 1733366, "s2_id": "b51cca6fcf42d6e56c967144cf54a8151a3958d6", "title": "Quantitative Impact Evaluation of an Abstraction Layer for Data Stream Processing Systems", "abstract": "With the demand to process ever-growing data volumes, a variety of new data stream processing frameworks have been developed. Moving an implementation from one such system to another, e.g., for performance reasons, requires adapting existing applications to new interfaces. Apache Beam addresses these high substitution costs by providing an abstraction layer that enables executing programs on any of the supported streaming frameworks. In this paper, we present a novel benchmark architecture for comparing the performance impact of using Apache Beam on three streaming frameworks: Apache Spark Streaming, Apache Flink, and Apache Apex. We find significant performance penalties when using Apache Beam for application development in the surveyed systems. Overall, usage of Apache Beam for the examined streaming applications caused a high variance of query execution times with a slowdown of up to a factor of 58 compared to queries developed without the abstraction layer. All developed benchmark artifacts are publicly available to ensure reproducible results.", "venue": "ICDCS 2019", "authors": ["Guenter  Hesse", "Christoph  Matthies", "Kelvin  Glass", "Johannes  Huegle", "Matthias  Uflacker"], "year": 2019, "n_citations": 5}
{"id": 1739870, "s2_id": "1474766cdc3b7a5f0a6b03bca414e177f82f3bfa", "title": "Performance Modelling of Deep Learning on Intel Many Integrated Core Architectures", "abstract": "Many complex problems, such as natural language processing or visual object detection, are solved using deep learning. However, efficient training of complex deep convolutional neural networks for large data sets is computationally demanding and requires parallel computing resources. In this paper, we present two parameterized performance models for estimation of execution time of training convolutional neural networks on the Intel many integrated core architecture. While for the first performance model we minimally use measurement techniques for parameter value estimation, in the second model we estimate more parameters based on measurements. We evaluate the prediction accuracy of performance models in the context of training three different convolutional neural network architectures on the Intel Xeon Phi. The achieved average performance prediction accuracy is about 15% for the first model and 11% for second model.", "venue": "2019 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Andre  Viebke", "Sabri  Pllana", "Suejb  Memeti", "Joanna  Kolodziej"], "year": 2019, "n_citations": 4}
{"id": 1740601, "s2_id": "cfaaace7f75c394dbf7af51e603016c9ccd432f9", "title": "A flexible Patch-based lattice Boltzmann parallelization approach for heterogeneous GPU-CPU clusters", "abstract": "Sustaining a large fraction of single GPU performance in parallel computations is considered to be the major problem of GPU-based clusters. We address this issue in the context of a lattice Boltzma...", "venue": "ParCo 2011", "authors": ["FeichtingerChristian", "HabichJohannes", "K\u00f6StlerHarald", "HagerGeorg", "R\u00fcDeUlrich", "WelleinGerhard"], "year": 2011, "n_citations": 0}
{"id": 1741127, "s2_id": "0441890cfc45ff8cb6e2bd755ef28a0beb031084", "title": "Comparing the costs of abstraction for DL frameworks", "abstract": "High level abstractions for implementing, training, and testing Deep Learning (DL) models abound. Such frameworks function primarily by abstracting away the implementation details of arbitrary neural architectures, thereby enabling researchers and engineers to focus on design. In principle, such frameworks could be \"zero-cost abstractions\"; in practice, they incur translation and indirection overheads. We study at which points exactly in the engineering life-cycle of a DL model the highest costs are paid and whether they can be mitigated. We train, test, and evaluate a representative DL model using PyTorch, LibTorch, TorchScript, and cuDNN on representative datasets, comparing accuracy, execution time and memory efficiency.", "venue": "ArXiv", "authors": ["Maksim  Levental", "Elena  Orlova"], "year": 2020, "n_citations": 0}
{"id": 1744343, "s2_id": "f7d0b2e1b7cc1d0988e1acbb384ad89b36e6f365", "title": "Experiences & Challenges with Server-Side WiFi Indoor Localization Using Existing Infrastructure", "abstract": "Real-world deployments of WiFi-based indoor localization in large public venues are few and far between as most state-of-the-art solutions require either client or infrastructure-side changes. Hence, even though high location accuracy is possible with these solutions, they are not practical due to cost and/or client adoption reasons. Majority of the public venues use commercial controller-managed WLAN solutions, that neither allow client changes nor infrastructure changes. In fact, for such venues we have observed highly heterogeneous devices with very low adoption rates for client-side apps. In this paper, we present our experiences in deploying a scalable location system for such venues. We show that server-side localization is not trivial and present two unique challenges associated with this approach, namely Cardinality Mismatch and High Client Scan Latency. The \"Mismatch\" challenge results in a significant mismatch between the set of access points (APs) reporting a client in the offline and online phases, while the \"Latency\" challenge results in a low number of APs reporting data for any particular client. We collect three weeks of detailed ground truth data (\u2248 200 landmarks), from a WiFi setup that has been deployed for more than four years, to provide evidences for the extent and understanding the impact of these problems. We propose heuristics to alleviate them. We also summarize the challenges and pitfalls of real deployments which hamper the localization accuracy.", "venue": "MobiQuitous", "authors": ["Dheryta  Jaisinghani", "Rajesh Krishna Balan", "Vinayak S. Naik", "Archan  Misra", "Youngki  Lee"], "year": 2018, "n_citations": 8}
{"id": 1744717, "s2_id": "9e5fbc73d15edb87c32558e4094fddf60efbd279", "title": "Causal datasheet: An approximate guide to practically assess Bayesian networks in the real world", "abstract": "In solving real-world problems like changing healthcare-seeking behaviors, designing interventions to improve downstream outcomes requires an understanding of the causal links within the system. Causal Bayesian Networks (BN) have been proposed as one such powerful method. In real-world applications, however, confidence in the results of BNs are often moderate at best. This is due in part to the inability to validate against some ground truth, as the DAG is not available. This is especially problematic if the learned DAG conflicts with pre-existing domain doctrine. At the policy level, one must justify insights generated by such analysis, preferably accompanying them with uncertainty estimation. Here we propose a causal extension to the datasheet concept proposed by Gebru et al (2018) to include approximate BN performance expectations for any given dataset. To generate the results for a prototype Causal Datasheet, we constructed over 30,000 synthetic datasets with properties mirroring characteristics of real data. We then recorded the results given by state-of-the-art structure learning algorithms. These results were used to populate the Causal Datasheet, and recommendations were automatically generated dependent on expected performance. As a proof of concept, we used our Causal Datasheet Generation Tool (CDG-T) to assign expected performance expectations to a maternal health survey we conducted in Uttar Pradesh, India.", "venue": "ArXiv", "authors": ["Bradley  Butcher", "Vincent S. Huang", "Jeremy  Reffin", "Sema K. Sgaier", "Grace  Charles", "Novi  Quadrianto"], "year": 2020, "n_citations": 1}
{"id": 1749645, "s2_id": "69d950d1b8a10ad5c7562ca134b09ab2da400aea", "title": "Montage: A General System for Buffered Durably Linearizable Data Structures", "abstract": "The recent emergence of fast, dense, nonvolatile main memory suggests that certain long-lived data might remain in its natural pointer-rich format across program runs and hardware reboots. Operations on such data must be instrumented with explicit write-back and fence instructions to ensure consistency in the wake of a crash. Techniques to minimize the cost of this instrumentation are an active topic of research. \nWe present what we believe to be the first general-purpose approach to building buffered durably linearizable persistent data structures, and a system, Montage, to support that approach. Montage is built on top of the Ralloc nonblocking persistent allocator. It employs a slow-ticking epoch clock, and ensures that no operation appears to span an epoch boundary. It also arranges to persist only that data minimally required to reconstruct the structure after a crash. If a crash occurs in epoch $e$, all work performed in epochs $e$ and $e-1$ is lost, but work from prior epochs is preserved. \nWe describe the implementation of Montage, argue its correctness, and report unprecedented throughput for persistent queues, sets/mappings, and general graphs.", "venue": "ArXiv", "authors": ["Haosen  Wen", "Wentao  Cai", "Mingzhe  Du", "Louis  Jenkins", "Benjamin  Valpey", "Michael L. Scott"], "year": 2020, "n_citations": 2}
{"id": 1758900, "s2_id": "35176e33061f59e99c7b29c0d78a7931811673e5", "title": "Fixed-Delay Events in Generalized Semi-Markov Processes Revisited", "abstract": "We study long run average behavior of generalized semi-Markov processes with both fixed-delay events as well as variable-delay events. We show that allowing two fixed-delay events and one variable-delay event may cause an unstable behavior of a GSMP. In particular, we show that a frequency of a given state may not be defined for almost all runs (or more generally, an invariant measure may not exist). We use this observation to disprove several results from literature. Next we study GSMP with at most one fixed-delay event combined with an arbitrary number of variable-delay events. We prove that such a GSMP always possesses an invariant measure which means that the frequencies of states are always well defined and we provide algorithms for approximation of these frequencies. Additionally, we show that the positive results remain valid even if we allow an arbitrary number of reasonably restricted fixed-delay events.", "venue": "CONCUR", "authors": ["Tom\u00e1s  Br\u00e1zdil", "Jan  Krc\u00e1l", "Jan  Kret\u00ednsk\u00fd", "Vojtech  Reh\u00e1k"], "year": 2011, "n_citations": 14}
{"id": 1763323, "s2_id": "55ea9bb2c02522fe632c105a7ea7437c53bd1c32", "title": "Fully Distributed and Asynchronized Stochastic Gradient Descent for Networked Systems", "abstract": "This paper considers a general data-fitting problem over a networked system, in which many computing nodes are connected by an undirected graph. This kind of problem can find many real-world applications and has been studied extensively in the literature. However, existing solutions either need a central controller for information sharing or requires slot synchronization among different nodes, which increases the difficulty of practical implementations, especially for a very large and heterogeneous system. \nAs a contrast, in this paper, we treat the data-fitting problem over the network as a stochastic programming problem with many constraints. By adapting the results in a recent paper, we design a fully distributed and asynchronized stochastic gradient descent (SGD) algorithm. We show that our algorithm can achieve global optimality and consensus asymptotically by only local computations and communications. Additionally, we provide a sharp lower bound for the convergence speed in the regular graph case. This result fits the intuition and provides guidance to design a `good' network topology to speed up the convergence. Also, the merit of our design is validated by experiments on both synthetic and real-world datasets.", "venue": "ArXiv", "authors": ["Ying  Zhang"], "year": 2017, "n_citations": 0}
{"id": 1768804, "s2_id": "502bf5cb0f4b27565ab5f4c09474e4ce71b8176f", "title": "Read Operators and their Expressiveness in Process Algebras", "abstract": "We study two different ways to enhance PAFAS, a process algebra for modelling asynchronous timed concurrent systems, with non-blocking reading actions. We first add reading in the form of a readaction prefix operator. This operator is very flexible, but it s somewhat complex semantics requires two types of transition relations. We also present a read-se t prefix operator with a simpler semantics, but with syntactic restrictions. We discuss the expressive ness of read prefixes; in particular, we compare them to read-arcs in Petri nets and justify the simple semantics of the second variant by showing that its processes can be translated into processes of the first with timed-bisimilar behaviour. It is still an open problem whether the first algebra is more ex pressive than the second; we give a number of laws that are interesting in their own right, and can help to find a backward translation.", "venue": "EXPRESS", "authors": ["Flavio  Corradini", "Maria Rita Di Berardini", "Walter  Vogler"], "year": 2011, "n_citations": 1}
{"id": 1772816, "s2_id": "9ef9860c1d8a897b4533785b1ec78fd0fa9e589f", "title": "A New Upper Bound on Cache Hit Probability for Non-anticipative Caching Policies", "abstract": "Caching systems have long been crucial for improving the performance of a wide variety of network and web based online applications. In such systems, end-to-end application performance heavily depends on the fraction of objects transfered from the cache, also known as the cache hit probability. Many cache eviction policies have been proposed and implemented to improve the hit probability. In this work, we propose a new method to compute an upper bound on hit probability for all non-anticipative caching policies, i.e. for policies that have no knowledge of future requests. At each object request arrival, we use hazard rate (HR) function based ordering to classify the request as a hit or not. Under some statistical assumptions, we prove that our proposed HR based ordering model computes the maximum achievable hit probability and serves as an upper bound for all non-anticipative caching policies. We also provide simulation results to validate its correctness and to compare it to Belady's upper bound. We find it to almost always be tighter than Belady's bound.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Nitish K. Panigrahy", "Philippe  Nain", "Giovanni  Neglia", "Don  Towsley"], "year": 2020, "n_citations": 1}
{"id": 1777235, "s2_id": "4897ec1ba470f44e3b72e5ba2dde45f15e7dcf1a", "title": "Analysis of a non-work conserving Generalized Processor Sharing queue", "abstract": "We consider in this paper a non work-conserving Generalized Processor Sharing (GPS) system composed of two queues with Poisson arrivals and exponential service times. Using general results due to Fayolle \\emph{et al}, we first establish the stability condition for this system. We then determine the functional equation satisfied by the generating function of the numbers of jobs in both queues and the associated Riemann-Hilbert problem. We prove the existence and the uniqueness of the solution. This allows us to completely characterize the system, in particular to compute the empty queue probability. We finally derive the tail asymptotics of the number of jobs in one queue.", "venue": "ArXiv", "authors": ["Fabrice  Guillemin"], "year": 2013, "n_citations": 1}
{"id": 1783240, "s2_id": "452f737bd08089e346b31ef13b09868ae837feb3", "title": "Dynamic Bandwidth-Efficient BCube Topologies for Virtualized Data Center Networks", "abstract": "Network virtualization enables computing networks and data center (DC) providers to manage their networking resources in a flexible manner using software running on physical computers. In this paper, we address the existing issues with the classic DC network topologies in virtualized environment, and investigate a set of DC network topologies with the capability of providing dynamic structures according to the service-level required by the active traffic in a virtual DC network. In particular, we propose three main approaches to modify the structure of a classic BCube topology as a topology benchmark, and investigate their associated structural features and maximum achievable interconnected bandwidth for different routing scenarios. Finally, we run an extensive simulation program to check the performance of the proposed modified topologies in a simulation environment which considers failure analysis and also traffic congestion. Our simulation experiments, which are consistent to our design goals, show the efficiency of the proposed modified topologies comparing to the classic BCube in terms of bandwidth availability and failure resiliency.", "venue": "ArXiv", "authors": ["Vahid  Asghari", "Reza Farrahi Moghaddam", "Fereydoun Farrahi Moghaddam", "Mohamed  Cheriet"], "year": 2015, "n_citations": 0}
{"id": 1783578, "s2_id": "e16ae95deae0c144baffabdf8e382531e50fd6ed", "title": "EXtensible animator for mobile simulations: EXAMS", "abstract": "One of the most widely used simulation environments for mobile wireless networks is the Network Simulator 2 (NS-2). However NS-2 stores its outcome in a text file, so there is a need for a visualization tool to animate the simulation of the wireless network. The purpose of this tool is to help the researcher examine in detail how the wireless protocol works both on a network and a node basis. It is clear that much of this information is protocol dependent and cannot be depicted properly by a general purpose animation process. Existing animation tools do not provide this level of information nor permit the specific protocol to control the animation at all. EXAMS is an NS-2 visualization tool for mobile simulations which makes possible the portrayal of NS-2's internal information like transmission properties and node's data structures. This is mainly possible due to EXAMS extensible architecture which separates the animation process into a general and a protocol specific part. The latter can be developed independently by the protocol designer and loaded on demand. These and other useful characteristics of the EXAMS tool can be an invaluable help for a researcher in order to investigate and debug a mobile networking protocol.", "venue": "2009 IEEE International Symposium on Modeling, Analysis & Simulation of Computer and Telecommunication Systems", "authors": ["Livathinos S. Nikolaos"], "year": 2009, "n_citations": 2}
{"id": 1784272, "s2_id": "83504752f7b1d8591a0d2b7feae5c3f275d8a6e2", "title": "Optimal Service Elasticity in Large-Scale Distributed Systems", "abstract": "A fundamental challenge in large-scale cloud networks and data centers is to achieve highly efficient server utilization and limit energy consumption, while providing excellent user-perceived performance in the presence of uncertain and time-varying demand patterns. Auto-scaling provides a popular paradigm for automatically adjusting service capacity in response to demand while meeting performance targets, and queue-driven auto-scaling techniques have been widely investigated in the literature. In typical data center architectures and cloud environments however, no centralized queue is maintained, and load balancing algorithms immediately distribute incoming tasks among parallel queues. In these distributed settings with vast numbers of servers, centralized queue-driven auto-scaling techniques involve a substantial communication overhead and major implementation burden, or may not even be viable at all. Motivated by the above issues, we propose a joint auto-scaling and load balancing scheme which does not require any global queue length information or explicit knowledge of system parameters, and yet provides provably near-optimal service elasticity. We establish the fluid-level dynamics for the proposed scheme in a regime where the total traffic volume and nominal service capacity grow large in proportion. The fluid-limit results show that the proposed scheme achieves asymptotic optimality in terms of user-perceived delay performance as well as energy consumption. Specifically, we prove that both the waiting time of tasks and the relative energy portion consumed by idle servers vanish in the limit. At the same time, the proposed scheme operates in a distributed fashion and involves only constant communication overhead per task, thus ensuring scalability in massive data center operations. Extensive simulation experiments corroborate the fluid-limit results, and demonstrate that the proposed scheme can match the user performance and energy consumption of state-of-the-art approaches that do take full advantage of a centralized queue.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Debankur  Mukherjee", "Souvik  Dhara", "Sem C. Borst", "Johan van Leeuwaarden"], "year": 2017, "n_citations": 20}
{"id": 1788007, "s2_id": "efe19a1c935e490eaa48487a77b8653a5e10f791", "title": "Cost-Effective Energy Monitoring of a Zynq-Based Real-Time System Including Dual Gigabit Ethernet", "abstract": "Recent FPGA architectures integrate various power management features already established in CPU-driven SoCs to reach more energy-sensitive application domains such as, e.g., automotive and robotics. This also qualifies hybrid Programmable SoCs (pSoCs) that combine fixed-function SoCs with configurable FPGA fabric for heterogeneous Real-time Systems (RTSs), which operate under predefined latency and power constraints in safety-critical environments. Their complex application-specific computation and communication (incl. I/O) architectures result in highly varying power consumption, which requires precise voltage and current sensing on all relevant supply rails to enable dependable evaluation of available and novel power management techniques. In this paper, we propose a low-cost 18-channel 16-bit-resolution measurement system capable of over 200 kSPS (kilo-samples per second) for instrumentation of current pSoC development boards. In addition, we propose to include crucial I/O components such as Ethernet PHYs into the power monitoring to gain a holistic view on the RTS's temporal behavior covering not only computation on FPGA and CPUs, but also communication in terms of, e.g., reception of sensor values and transmission of actuation signals. We present an FMC-sized implementation of our measurement system combined with two Gigabit Ethernet PHYs and one HDMI input. Paired with Xilinx' ZC702 development board, we are able to synchronously acquire power traces of a Zynq pSoC and the two PHYs precise enough to identify individual Ethernet frames.", "venue": "2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Martin  Geier", "Dominik  Faller", "Marian  Br\u00e4ndle", "Samarjit  Chakraborty"], "year": 2019, "n_citations": 4}
{"id": 1788528, "s2_id": "d47f22d9280fbf143dc538d6e553572295c8e012", "title": "Adroitness: An Android-based Middleware for Fast Development of High-performance Apps", "abstract": "As smartphones become increasingly more powerful, a new generation of highly interactive user-centric mobile apps emerge to make user's life simpler and more productive. Mobile phones applications have to sustain limited resource availability on mobile devices such as battery life, network connectivity while also providing better responsiveness, lightweight interactions within the application. Developers end up spending a considerable amount of time dealing with the architecture constraints imposed by the wide variety of platforms, tools, and devices offered by the mobile ecosystem, thereby diverting them from their main goal of building such apps. Therefore, we propose a mobile-based middleware architecture that alleviates the burdensome task of dealing with low-level architectural decisions and fine-grained implementation details. We achieve such a goal by focusing on the separation of concerns and abstracting away the complexity of orchestrating device sensors and effectors, decision-making processes, and connection to remote services, while providing scaffolding for the development of higher-level functional features of interactive high-performance mobile apps. We demonstrate the powerfulness of our approach vs. Android's conventional framework by comparing different software metric", "venue": "ArXiv", "authors": ["Oscar J. Romero", "Sushma A. Akoju"], "year": 2019, "n_citations": 1}
{"id": 1791794, "s2_id": "23a8dac66ed5d5272545d6a4f1a1b05532e15cc0", "title": "Spatial Temporal Analysis of 40,000,000,000,000 Internet Darkspace Packets", "abstract": "The Internet has never been more important to our society, and understanding the behavior of the Internet is essential. The Center for Applied Internet Data Analysis (CAIDA) Telescope observes a continuous stream of packets from an unsolicited darkspace representing 1/256 of the Internet. During 2019 and 2020 over 40,000,000,000,000 unique packets were collected representing the largest ever assembled public corpus of Internet traffic. Using the combined resources of the Supercomputing Centers at UC San Diego, Lawrence Berkeley National Laboratory, and MIT, the spatial temporal structure of anonymized source-destination pairs from the CAIDA Telescope data has been analyzed with GraphBLAS hierarchical hyper-sparse matrices. These analyses provide unique insight on this unsolicited Internet darkspace traffic with the discovery of many previously unseen scaling relations. The data show a significant sustained increase in unsolicited traffic corresponding to the start of the COVID19 pandemic, but relatively little change in the underlying scaling relations associated with unique sources, source fan-outs, unique links, destination fan-ins, and unique destinations. This work provides a demonstration of the practical feasibility and benefit of the safe collection and analysis of significant quantities of anonymized Internet traffic.", "venue": "2021 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Jeremy  Kepner", "Michael  Jones", "Daniel  Andersen", "Aydin  Buluc", "Chansup  Byun", "K  Claffy", "Timothy  Davis", "William  Arcand", "Jonathan  Bernays", "David  Bestor", "William  Bergeron", "Vijay  Gadepally", "Micheal  Houle", "Matthew  Hubbell", "Anna  Klein", "Chad  Meiners", "Lauren  Milechin", "Julie  Mullen", "Sandeep  Pisharody", "Andrew  Prout", "Albert  Reuther", "Antonio  Rosa", "Siddharth  Samsi", "Doug  Stetson", "Adam  Tse", "Charles  Yee", "Peter  Michaleas"], "year": 2021, "n_citations": 0}
{"id": 1793784, "s2_id": "182a10c8a826fa41f719f88f31f1deb45c730d91", "title": "Estimation of available bandwidth and measurement infrastructure for Russian segment of Internet", "abstract": "In paper the method for estimation of available bandwidth is supposed which does not demand the advanced utilities. Our method is based on the measurement of network delay $D$ for packets of different sizes $W$. The simple expression for available bandwidth $B_{av} =(W_2-W_1)/(D_2-D_1)$ is substantiated. For the experimental testing the measurement infrastructure for Russian segment of Internet was installed in framework of RFBR grant 06-07-89074.", "venue": "ArXiv", "authors": ["A. P. Platonov", "D. I. Sidelnikov", "M. V. Strizhov", "Andrei M. Sukhov"], "year": 2008, "n_citations": 9}
{"id": 1794190, "s2_id": "ffbba54c9a3ae0842de259b0b42ea658495e4e39", "title": "On a Catalogue of Metrics for Evaluating Commercial Cloud Services", "abstract": "Given the continually increasing amount of commercial Cloud services in the market, evaluation of different services plays a significant role in cost-benefit analysis or decision making for choosing Cloud Computing. In particular, employing suitable metrics is essential in evaluation implementations. However, to the best of our knowledge, there is not any systematic discussion about metrics for evaluating Cloud services. By using the method of Systematic Literature Review (SLR), we have collected the de facto metrics adopted in the existing Cloud services evaluation work. The collected metrics were arranged following different Cloud service features to be evaluated, which essentially constructed an evaluation metrics catalogue, as shown in this paper. This metrics catalogue can be used to facilitate the future practice and research in the area of Cloud services evaluation. Moreover, considering metrics selection is a prerequisite of benchmark selection in evaluation implementations, this work also supplements the existing research in benchmarking the commercial Cloud services.", "venue": "2012 ACM/IEEE 13th International Conference on Grid Computing", "authors": ["Zheng  Li", "Liam  O'Brien", "He  Zhang", "Rainbow  Cai"], "year": 2012, "n_citations": 100}
{"id": 1795563, "s2_id": "2e62c01456d60873ad716e73a8a5f77ae62fdce1", "title": "An ECM-based Energy-Efficiency Optimization Approach for Bandwidth-Limited Streaming Kernels on Recent Intel Xeon Processors", "abstract": "We investigate an approach that uses low-level analysis and the execution-cache-memory (ECM) performance model in combination with tuning of hardware parameters to lower energy requirements of memory-bound applications. The ECM model is extended appropriately to deal with software optimizations such as non-temporal stores. Using incremental steps and the ECM model, we analytically quantify the impact of various single-core optimizations and pinpoint microarchitectural improvements that are relevant to energy consumption. Using a 2D Jacobi solver as example that can serve as a blueprint for other memory-bound applications, we evaluate our approach on the four most recent Intel Xeon E5 processors (Sandy Bridge-EP, Ivy Bridge-EP, Haswell-EP, and Broadwell-EP). We find that chip energy consumption can be reduced in the range of 2.0\u20132.4\u00d7 on the examined processors.", "venue": "2016 4th International Workshop on Energy Efficient Supercomputing (E2SC)", "authors": ["Johannes  Hofmann", "Dietmar  Fey"], "year": 2016, "n_citations": 3}
{"id": 1796890, "s2_id": "ffe2f4f919106676442227206b89ac87ccb16d53", "title": "Experimental Verification and Analysis of Dynamic Loop Scheduling in Scientific Applications", "abstract": "Scientific applications are often irregular and characterized by large computationally-intensive parallel loops. Dynamic loop scheduling (DLS) techniques improve the performance of computationally-intensive scientific applications via load balancing of their execution on high-performance computing (HPC) systems. Identifying the most suitable choices of data distribution strategies, system sizes, and DLS techniques which improve the performance of a given application, requires intensive assessment and a large number of exploratory native experiments (using real applications on real systems), which may not always be feasible or practical due to associated time and costs. In such cases, simulative experiments are more appropriate for studying the performance of applications. This motivates the question of 'How realistic are the simulations of executions of scientific applications using DLS on HPC platforms?' In the present work, a methodology is devised to answer this question. It involves the experimental verification and analysis of the performance of DLS in scientific applications. The proposed methodology is employed for a computer vision application executing using four DLS techniques on two different HPC platforms, both via native and simulative experiments. The evaluation and analysis of the native and simulative results indicate that the accuracy of the simulative experiments is strongly influenced by the approach used to extract the computational effort of the application (FLOP-or time-based), the choice of application model representation into simulation (data or task parallel), and the available HPC subsystem models in the simulator (multi-core CPUs, memory hierarchy, and network topology). The minimum and the maximum percent errors achieved between the native and the simulative experiments are 0.95% and 8.03%, respectively.", "venue": "2018 17th International Symposium on Parallel and Distributed Computing (ISPDC)", "authors": ["Ali  Mohammed", "Ahmed  Eleliemy", "Florina M. Ciorba", "Franziska  Kasielke", "Ioana  Banicescu"], "year": 2018, "n_citations": 7}
{"id": 1801536, "s2_id": "b426284e9ab8f42f73c2ccc560b7758f269fc0e5", "title": "Performance Modeling and Prediction for Dense Linear Algebra", "abstract": "Countless applications cast their computational core in terms of dense linear algebra operations. These operations can usually be implemented by combining the routines offered by standard linear algebra libraries such as BLAS and LAPACK, and typically each operation can be obtained in many alternative ways. Interestingly, identifying the fastest implementation -- without executing it -- is a challenging task even for experts. An equally challenging task is that of tuning each routine to performance-optimal configurations. Indeed, the problem is so difficult that even the default values provided by the libraries are often considerably suboptimal; as a solution, normally one has to resort to executing and timing the routines, driven by some form of parameter search. In this paper, we discuss a methodology to solve both problems: identifying the best performing algorithm within a family of alternatives, and tuning algorithmic parameters for maximum performance; in both cases, we do not execute the algorithms themselves. Instead, our methodology relies on timing and modeling the computational kernels underlying the algorithms, and on a technique for tracking the contents of the CPU cache. In general, our performance predictions allow us to tune dense linear algebra algorithms within few percents from the best attainable results, thus allowing computational scientists and code developers alike to efficiently optimize their linear algebra routines and codes.", "venue": "ArXiv", "authors": ["Elmar  Peise"], "year": 2017, "n_citations": 3}
{"id": 1802823, "s2_id": "a832ee87ff826d24572d0e4968fcfde410d108de", "title": "A Parallel Linear Temporal Logic Tableau", "abstract": "For many applications, we are unable to take full advantage of the potential massive parallelisation offered by supercomputers or cloud computing because it is too hard to work out how to divide up the computation task between processors in such a way to minimise the need for communication. However, a recently developed branch-independent tableaux for the common LTL temporal logic should intuitively be easy to parallelise as each branch can be developed independently. Here we describe a simple technique for partitioning such a tableau such that each partition can be processed independently without need for interprocess communication. We investigate the extent to which this technique improves the performance of the LTL tableau on standard benchmarks and random formulas.", "venue": "GandALF", "authors": ["John Christopher McCabe-Dansted", "Mark  Reynolds"], "year": 2017, "n_citations": 4}
{"id": 1808034, "s2_id": "25209a29cbe0b0ce7f9518d31579d4274f6e38e5", "title": "Modeling and predicting measured response time of cloud-based web services using long-memory time series", "abstract": "Predicting cloud performance from user\u2019s perspective is a complex task, because of several factors involved in providing the service to the consumer. In this work, the response time of 10 real-world services is analyzed. We have observed long memory in terms of the measured response time of the CPU-intensive services and statistically verified this observation using estimators of the Hurst exponent. Then, na\u00efve, mean, autoregressive integrated moving average (ARIMA) and autoregressive fractionally integrated moving average (ARFIMA) methods are used to forecast the future values of quality of service (QoS) at runtime. Results of the cross-validation over the 10 datasets show that the long-memory ARFIMA model provides the mean of 37.5\u00a0% and the maximum of 57.8\u00a0% reduction in the forecast error when compared to the short-memory ARIMA model according to the standard error measure of mean absolute percentage error. Our work implies that consideration of the long-range dependence in QoS data can help to improve the selection of services according to their possible future QoS values.", "venue": "The Journal of Supercomputing", "authors": ["Hossein  Nourikhah", "Mohammad Kazem Akbari", "Mohammad  Kalantari"], "year": 2014, "n_citations": 18}
{"id": 1808149, "s2_id": "4e13620a45ec7b49ad4e7c58e0cf59d0a19b295a", "title": "On the Convergence of the TTL Approximation for an LRU Cache under Independent Stationary Request Processes", "abstract": "The modeling and analysis of an LRU cache is extremely challenging as exact results for the main performance metrics (e.g., hit rate) are either lacking or cannot be used because of their high computational complexity for large caches. As a result, various approximations have been proposed. The state-of-the-art method is the so-called TTL approximation, first proposed and shown to be asymptotically exact for IRM requests by Fagin [13]. It has been applied to various other workload models and numerically demonstrated to be accurate but without theoretical justification. In this article, we provide theoretical justification for the approximation in the case where distinct contents are described by independent stationary and ergodic processes. We show that this approximation is exact as the cache size and the number of contents go to infinity. This extends earlier results for the independent reference model. Moreover, we establish results not only for the aggregate cache hit probability but also for every individual content. Last, we obtain bounds on the rate of convergence.", "venue": "ACM Trans. Model. Perform. Evaluation Comput. Syst.", "authors": ["Bo  Jiang", "Philippe  Nain", "Donald F. Towsley"], "year": 2018, "n_citations": 19}
{"id": 1808760, "s2_id": "e8c334df36db33c1c31c14ac8379f138cea16801", "title": "Using performance analysis tools for parallel-in-time integrators - Does my time-parallel code do what I think it does?", "abstract": "While many ideas and proofs of concept for parallel-in-time integration methods exists, the number of large-scale, accessible time-parallel codes is rather small. This is often due to the apparent or subtle complexity of the algorithms and the many pitfalls awaiting developers of parallel numerical software. One example of such a time-parallel code is pySDC, which implements, among others, the parallel full approximation scheme in space and time (PFASST). Inspired by nonlinear multigrid ideas, PFASST allows to integrate multiple time-steps simultaneously using a space-time hierarchy of spectral deferred corrections. In this paper we demonstrate the application of performance analysis tools to the PFASST implementation pySDC. Tracing the path we took for this work, we highlight the obstacles encountered, describe remedies and explain the sometimes surprising findings made possible by the tools. Although focusing only on a single implementation of a particular parallel-in-time integrator, we hope that our results and in particular the way we obtained them are a blueprint for other time-parallel codes.", "venue": "ArXiv", "authors": ["Robert  Speck", "Michael  Knobloch", "Andreas  Gocht", "Sebastian  L\u00fchrs"], "year": 2019, "n_citations": 0}
{"id": 1810581, "s2_id": "139700d30575f11bc4fae336b43b7b16962da506", "title": "MCDS: AI Augmented Workflow Scheduling in Mobile Edge Cloud Computing Systems", "abstract": "Workflow scheduling is a long-studied problem in parallel and distributed computing (PDC), aiming to efficiently utilize compute resources to meet user\u2019s service requirements. Recently proposed scheduling methods leverage the low response times of edge computing platforms to optimize application Quality of Service (QoS). However, scheduling workflow applications in mobile edge-cloud systems is challenging due to computational heterogeneity, changing latencies of mobile devices and the volatile nature of workload resource requirements. To overcome these difficulties, it is essential, but at the same time challenging, to develop a long-sighted optimization scheme that efficiently models the QoS objectives. In this work, we propose MCDS: Monte Carlo Learning using Deep Surrogate Models to efficiently schedule workflow applications in mobile edge-cloud computing systems. MCDS is an Artificial Intelligence (AI) based scheduling approach that uses a tree-based search strategy and a deep neural network-based surrogate model to estimate the long-term QoS impact of immediate actions for robust optimization of scheduling decisions. Experiments on physical and simulated edge-cloud testbeds show that MCDS can improve over the state-of-the-art methods in terms of energy consumption, response time, SLA violations and cost by at least 6.13, 4.56, 45.09 and 30.71 percent respectively.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Shreshth  Tuli", "Giuliano  Casale", "Nicholas R. Jennings"], "year": 2021, "n_citations": 1}
{"id": 1810703, "s2_id": "5f6d2b0341d658d17b2e0514b05b76f1b8c94c81", "title": "Scylla: A Mesos Framework for Container Based MPI Jobs", "abstract": "Open source cloud technologies provide a wide range of support for creating customized compute node clusters to schedule tasks and managing resources. In cloud infrastructures such as Jetstream and Chameleon, which are used for scientific research, users receive complete control of the Virtual Machines (VM) that are allocated to them. Importantly, users get root access to the VMs. This provides an opportunity for HPC users to experiment with new resource management technologies such as Apache Mesos that have proven scalability, flexibility, and fault tolerance. To ease the development and deployment of HPC tools on the cloud, the containerization technology has matured and is gaining interest in the scientific community. In particular, several well known scientific code bases now have publicly available Docker containers. While Mesos provides support for Docker containers to execute individually, it does not provide support for container inter-communication or orchestration of the containers for a parallel or distributed application. In this paper, we present the design, implementation, and performance analysis of a Mesos framework, Scylla, which integrates Mesos with Docker Swarm to enable orchestration of MPI jobs on a cluster of VMs acquired from the Chameleon cloud [1]. Scylla uses Docker Swarm for communication between containerized tasks (MPI processes) and Apache Mesos for resource pooling and allocation. Scylla allows a policy-driven approach to determine how the containers should be distributed across the nodes depending on the CPU, memory, and network throughput requirement for each application.", "venue": "ArXiv", "authors": ["Pankaj  Saha", "Angel  Beltre", "Madhusudhan  Govindaraju"], "year": 2019, "n_citations": 10}
{"id": 1812263, "s2_id": "03dbcb01974913e81d35b8bcc8ffd62b5f8d1fbc", "title": "Performance comparison between Java and JNI for optimal implementation of computational micro-kernels", "abstract": "General purpose CPUs used in high performance computing (HPC) support a vector instruction set and an out-of-order engine dedicated to increase the instruction level parallelism. Hence, related optimizations are currently critical to improve the performance of applications requiring numerical computation. Moreover, the use of a Java run-time environment such as the HotSpot Java Virtual Machine (JVM) in high performance computing is a promising alternative. It benefits from its programming flexibility, productivity and the performance is ensured by the Just-In-Time (JIT) compiler. Though, the JIT compiler suffers from two main drawbacks. First, the JIT is a black box for developers. We have no control over the generated code nor any feedback from its optimization phases like vectorization. Secondly, the time constraint narrows down the degree of optimization compared to static compilers like GCC or LLVM. So, it is compelling to use statically compiled code since it benefits from additional optimization reducing performance bottlenecks. Java enables to call native code from dynamic libraries through the Java Native Interface (JNI). Nevertheless, JNI methods are not inlined and require an additional cost to be invoked compared to Java ones. Therefore, to benefit from better static optimization, this call overhead must be leveraged by the amount of computation performed at each JNI invocation. In this paper we tackle this problem and we propose to do this analysis for a set of micro-kernels. Our goal is to select the most efficient implementation considering the amount of computation defined by the calling context. We also investigate the impact on performance of several different optimization schemes which are vectorization, out-of-order optimization, data alignment, method inlining and the use of native memory for JNI methods.", "venue": "ArXiv", "authors": ["Nassim A. Halli", "Henri-Pierre  Charles", "Jean-Fran\u00e7ois  M\u00e9haut"], "year": 2014, "n_citations": 9}
{"id": 1816945, "s2_id": "b27e44f265ad985f20ee83e7520cc19d49d93f80", "title": "Implementation and Analysis of QUIC for MQTT", "abstract": "Transport and security protocols are essential to ensure reliable and secure communication between two parties. For IoT applications, these protocols must be lightweight, since IoT devices are usually resource constrained. Unfortunately, the existing transport and security protocols -- namely TCP/TLS and UDP/DTLS -- fall short in terms of connection overhead, latency, and connection migration when used in IoT applications. In this paper, after studying the root causes of these shortcomings, we show how utilizing QUIC in IoT scenarios results in a higher performance. Based on these observations, and given the popularity of MQTT as an IoT application layer protocol, we integrate MQTT with QUIC. By presenting the main APIs and functions developed, we explain how connection establishment and message exchange functionalities work. We evaluate the performance of MQTTw/QUIC versus MQTTw/TCP using wired, wireless, and long-distance testbeds. Our results show that MQTTw/QUIC reduces connection overhead in terms of the number of packets exchanged with the broker by up to 56%. In addition, by eliminating half-open connections, MQTTw/QUIC reduces processor and memory usage by up to 83% and 50%, respectively. Furthermore, by removing the head-of-line blocking problem, delivery latency is reduced by up to 55%. We also show that the throughput drops experienced by MQTTw/QUIC when a connection migration happens is considerably lower than that of MQTTw/TCP.", "venue": "Comput. Networks", "authors": ["Puneet  Kumar", "Behnam  Dezfouli"], "year": 2019, "n_citations": 24}
{"id": 1818846, "s2_id": "e183b8626cc7ce149237c28821a517a0a31cd241", "title": "Hybrid Static/Dynamic Schedules for Tiled Polyhedral Programs", "abstract": "Polyhedral compilers perform optimizations such as tiling and parallelization; when doing both, they usually generate code that executes \"barrier-synchronized wavefronts\" of tiles. We present a system to express and generate code for hybrid schedules, where some constraints are automatically satisfied through the structure of the code, and the remainder are dynamically enforced at run-time with data flow mechanisms. We prove bounds on the added overheads that are better, by at least one polynomial degree, than those of previous techniques. \nWe propose a generic mechanism to implement the needed synchronization, and show it can be easily realized for a variety of targets: OpenMP, Pthreads, GPU (CUDA or OpenCL) code, languages like X10, Habanero, Cilk, as well as data flow platforms like DAGuE, and OpenStream and MPI. We also provide a simple concrete implementation that works without the need of any sophisticated run-time mechanism. \nOur experiments show our simple implementation to be competitive or better than the wavefront-synchronized code generated by other systems. We also show how the proposed mechanism can achieve 24% to 70% reduction in energy.", "venue": "ArXiv", "authors": ["Tian  Jin", "Nirmal  Prajapati", "Waruna  Ranasinghe", "Guillaume  Iooss", "Yun  Zou", "Sanjay V. Rajopadhye", "David G. Wonnacott"], "year": 2016, "n_citations": 3}
{"id": 1820722, "s2_id": "c8e18551887f8a4ab9d310ae38340c9b1246a8fb", "title": "Hierarchical Beamforming: Resource Allocation, Fairness and Flow Level Performance", "abstract": "We consider hierarchical beamforming in wireless networks. For a given population of flows, we propose computationally efficient algorithms for fair rate allocation including proportional fairness and max-min fairness. We next propose closed-form formulas for flow level performance, for both elastic (with either proportional fairness and max-min fairness) and streaming traffic. We further assess the performance of hierarchical beamforming using numerical experiments. Since the proposed solutions have low complexity compared to conventional beamforming, our work suggests that hierarchical beamforming is a promising candidate for the implementation of beamforming in future cellular networks.", "venue": "Perform. Evaluation", "authors": ["Julien  Floquet", "Richard  Combes", "Zwi  Altman"], "year": 2018, "n_citations": 2}
{"id": 1823330, "s2_id": "ef16cfb732304eac420ccb344859dd660ace0b6a", "title": "Performance Analysis of Deep Learning Workloads on Leading-edge Systems", "abstract": "This work examines the performance of leading-edge systems designed for machine learning computing, including the NVIDIA DGX-2, Amazon Web Services (AWS) P3, IBM Power System Accelerated Compute Server AC922, and a consumer-grade Exxact TensorEX TS4 GPU server. Representative deep learning workloads from the fields of computer vision and natural language processing are the focus of the analysis. Performance analysis is performed along with a number of important dimensions. Performance of the communication interconnects and large and high-throughput deep learning models are considered. Different potential use models for the systems as standalone and in the cloud also are examined. The effect of various optimization of the deep learning models and system configurations is included in the analysis.", "venue": "2019 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)", "authors": ["Yihui  Ren", "Shinjae  Yoo", "Adolfy  Hoisie"], "year": 2019, "n_citations": 6}
{"id": 1823512, "s2_id": "161e8b0eb44910edf2f7d09f985893afa3a17838", "title": "Some observations on optimal frequency selection in DVFS-based energy consumption minimization", "abstract": "In recent years, the issue of energy consumption in parallel and distributed computing systems has attracted a great deal of attention. In response to this, many energy-aware scheduling algorithms have been developed primarily using the dynamic voltage-frequency scaling (DVFS) capability which has been incorporated into recent commodity processors. Majority of these algorithms involve two passes: schedule generation and slack reclamation. The former pass involves the redistribution of tasks among DVFS-enabled processors based on a given cost function that includes makespan and energy consumption, while the latter pass is typically achieved by executing individual tasks with slacks at a lower processor frequency. In this paper, a new slack reclamation algorithm is proposed by approaching the energy reduction problem from a different angle. Firstly, the problem of task slack reclamation by using combinations of processors' frequencies is formulated. Secondly, several proofs are provided to show that (1) if the working frequency set of processor is assumed to be continues, the optimal energy will be always achieved by using only one frequency, (2) for real processors with a discrete set of working frequencies, the optimal energy is always achieved by using at most two frequencies, and (3) these two frequencies are adjacent/neighbouring when processor energy consumption is a convex function of frequency. Thirdly, a novel algorithm to find the best combination of frequencies to result the optimal energy is presented. The presented algorithm has been evaluated based on results obtained from experiments with three different sets of task graphs: 3000 randomly generated task graphs, and 600 task graphs for two popular applications (Gauss-Jordan and LU decomposition). The results show the superiority of the proposed algorithm in comparison with other techniques.", "venue": "J. Parallel Distributed Comput.", "authors": ["Nikzad Babaii Rizvandi", "Javid  Taheri", "Albert Y. Zomaya"], "year": 2011, "n_citations": 128}
{"id": 1824050, "s2_id": "e7b85b1c6739068fe0b0dd577b8c2c7023a0b447", "title": "Best bang for your buck: GPU nodes for GROMACS biomolecular simulations", "abstract": "The molecular dynamics simulation package GROMACS runs efficiently on a wide variety of hardware from commodity workstations to high performance computing clusters. Hardware features are well\u2010exploited with a combination of single instruction multiple data, multithreading, and message passing interface (MPI)\u2010based single program multiple data/multiple program multiple data parallelism while graphics processing units (GPUs) can be used as accelerators to compute interactions off\u2010loaded from the CPU. Here, we evaluate which hardware produces trajectories with GROMACS 4.6 or 5.0 in the most economical way. We have assembled and benchmarked compute nodes with various CPU/GPU combinations to identify optimal compositions in terms of raw trajectory production rate, performance\u2010to\u2010price ratio, energy efficiency, and several other criteria. Although hardware prices are naturally subject to trends and fluctuations, general tendencies are clearly visible. Adding any type of GPU significantly boosts a node's simulation performance. For inexpensive consumer\u2010class GPUs this improvement equally reflects in the performance\u2010to\u2010price ratio. Although memory issues in consumer\u2010class GPUs could pass unnoticed as these cards do not support error checking and correction memory, unreliable GPUs can be sorted out with memory checking tools. Apart from the obvious determinants for cost\u2010efficiency like hardware expenses and raw performance, the energy consumption of a node is a major cost factor. Over the typical hardware lifetime until replacement of a few years, the costs for electrical power and cooling can become larger than the costs of the hardware itself. Taking that into account, nodes with a well\u2010balanced ratio of CPU and consumer\u2010class GPU resources produce the maximum amount of GROMACS trajectory over their lifetime. \u00a9 2015 The Authors. Journal of Computational Chemistry Published by Wiley Periodicals, Inc.", "venue": "J. Comput. Chem.", "authors": ["Carsten  Kutzner", "Szil\u00e1rd  P\u00e1ll", "Martin  Fechner", "Ansgar  Esztermann", "Bert L. de Groot", "Helmut  Grubm\u00fcller"], "year": 2015, "n_citations": 138}
{"id": 1829568, "s2_id": "c6af253eda438b791b8cc5e43501dc9a04b2e2de", "title": "A Study of Energy and Locality Effects Using Space-Filling Curves", "abstract": "The cost of energy is becoming an increasingly important driver for the operating cost of HPC systems, adding yet another facet to the challenge of producing efficient code. In this paper, we investigate the energy implications of trading computation for locality by applying Hilbert and Morton space-filling curves to dense matrix-matrix multiplication. The advantage of these curves is that they exhibit an inherent tiling effect without requiring specific architecture tuning. By accessing the matrices in the order determined by the space-filling curves, we can trade computation for locality. The index computation overhead of the Morton curve is found to be balanced against its locality and energy efficiency, while the overhead of the Hilbert curve outweighs its improvements on our test system.", "venue": "2014 IEEE International Parallel & Distributed Processing Symposium Workshops", "authors": ["Nico  Reissmann", "Jan Christian Meyer", "Magnus  Jahre"], "year": 2014, "n_citations": 8}
{"id": 1832603, "s2_id": "05cc2439b55e044af0ec7d3be34845d1ccdfd28a", "title": "A Comparative Study of Containers and Virtual Machines in Big Data Environment", "abstract": "Container technique is gaining increasing attention in recent years and has become an alternative to traditional virtual machines. Some of the primary motivations for the enterprise to adopt the container technology include its conveniency to encapsulate and deploy applications, lightweight operations, as well as efficiency and flexibility in resources sharing. However, there still lacks an in-depth and systematic comparison study on how big data applications, such as Spark jobs, perform between a container environment and a virtual machine environment. In this paper, by running various Spark applications with different configurations, we evaluate the two environments from many interesting aspects, such as how convenient the execution environment can be set up, what are makespans of different workloads running in each setup, how efficient the hardware resources, such as CPU and memory, are utilized, and how well each environment can scale. The results show that compared with virtual machines, containers provide a more easy-to-deploy and scalable environment for big data workloads. The research work in this paper can help practitioners and researchers to make more informed decisions on tuning their cloud environment and configuring the big data applications, so as to achieve better performance and higher resources utilization", "venue": "2018 IEEE 11th International Conference on Cloud Computing (CLOUD)", "authors": ["Qi  Zhang", "Ling  Liu", "Calton  Pu", "Qiwei  Dou", "Liren  Wu", "Wei  Zhou"], "year": 2018, "n_citations": 53}
{"id": 1834585, "s2_id": "b36beda141ac00102ab44185425db37bfb1b1b4f", "title": "AI-oriented Medical Workload Allocation for Hierarchical Cloud/Edge/Device Computing", "abstract": "In a hierarchically-structured cloud/edge/device computing environment, workload allocation can greatly affect the overall system performance. This paper deals with AI-oriented medical workload generated in emergency rooms (ER) or intensive care units (ICU) in metropolitan areas. The goal is to optimize AI-workload allocation to cloud clusters, edge servers, and end devices so that minimum response time can be achieved in life-saving emergency applications. \nIn particular, we developed a new workload allocation method for the AI workload in distributed cloud/edge/device computing systems. An efficient scheduling and allocation strategy is developed in order to reduce the overall response time to satisfy multi-patient demands. We apply several ICU AI workloads from a comprehensive edge computing benchmark Edge AIBench. The healthcare AI applications involved are short-of-breath alerts, patient phenotype classification, and life-death threats. Our experimental results demonstrate the high efficiency and effectiveness in real-life health-care and emergency applications.", "venue": "ArXiv", "authors": ["Tianshu  Hao", "Jianfeng  Zhan", "Kai  Hwang", "Wanling  Gao", "Xu  Wen"], "year": 2020, "n_citations": 1}
{"id": 1839053, "s2_id": "1a9b40ce96dd25fe9d36ad4580218420b4cfc424", "title": "Class-Based Service Connectivity Using Multi-level Bandwidth Adaptation in Multimedia Wireless Networks", "abstract": "Due to the fact that quality of service requirements are not very strict for all traffic types, more calls of higher priority can be accommodated by reducing some bandwidth allocation for the bandwidth adaptive calls. The bandwidth adaptation to accept a higher priority call is more than that of a lower priority call. Therefore, the multi-level bandwidth adaptation technique improves the overall forced call termination probability as well as provides priority of the traffic classes in terms of call blocking probability without reducing the bandwidth utilization. We propose a novel bandwidth adaptation model that releases multi-level of bandwidth from the existing multimedia traffic calls. The amount of released bandwidth is decided based on the priority of the requesting traffic calls and the number of existing bandwidth adaptive calls. This prioritization of traffic classes does not reduce the bandwidth utilization. Moreover, our scheme reduces the overall forced call termination probability significantly. The proposed scheme is modeled using the Markov Chain. The numerical results show that the proposed scheme is able to provide negligible handover call dropping probability as well as significantly reduced new call blocking probability of higher priority calls without increasing the overall forced call termination probability.", "venue": "Wirel. Pers. Commun.", "authors": ["Mostafa Zaman Chowdhury", "Yeong Min Jang"], "year": 2014, "n_citations": 8}
{"id": 1840941, "s2_id": "f387d94e488f6a09d08433d2bdee4b6d2f4876dc", "title": "Improving MATLAB's isprime performance without arbitrary-precision arithmetic", "abstract": "MATLAB is a numerical computing platform used by scientists, engineers, mathematicians, and students which contains many mathematical functions, including isprime. MATLAB\u2019s isprime function determines which elements of an input array are prime. This research details modular arithmetic techniques, the Miller\u2014Rabin primality test, vectorized operations, and division-minimizing strategies which harness the power of MATLAB\u2019s capabilities to improve isprime\u2019s performance. The results are typically 5 to 10 times faster for small integers and many hundreds of times faster for large integers and long arrays.", "venue": "ArXiv", "authors": ["Travis  Near"], "year": 2021, "n_citations": 0}
{"id": 1845529, "s2_id": "bf1d72d0d010a8cd1c50fafa623c1088d1254412", "title": "A generalization of Amdahl's law and relative conditions of parallelism", "abstract": "In this work I present a generalization of Amdahl's law on the limits of a parallel implementation with many processors. In particular I establish some mathematical relations involving the number of processors and the dimension of the treated problem, and with these conditions I define, on the ground of the reachable speedup, some classes of parallelism for the implementations. I also derive a condition for obtaining superlinear speedup. The used mathematical technics are those of differential calculus. I describe some examples from classical problems offered by the specialized literature on the subject.", "venue": "ArXiv", "authors": ["Gianluca  Argentini"], "year": 2002, "n_citations": 4}
{"id": 1846548, "s2_id": "6d1212f2adcc5d43967ab686264907e477b84eed", "title": "Performance Modeling and Analysis of a Hyperledger-based System Using GSPN", "abstract": "Abstract As a highly scalable permissioned blockchain platform, Hyperledger Fabric supports a wide range of industry use cases ranging from governance to finance. In this paper, we propose a model to analyze the performance of a Hyperledger-based system by using Generalized Stochastic Petri Nets (GSPN). This model decomposes a transaction flow into multiple phases and provides a simulation-based approach to obtain the system latency and throughput with a specific arrival rate. Based on this model, we analyze the impact of different configurations of ordering service on system performance to find out the bottleneck. Moreover, a mathematical configuration selection approach is proposed to determine the best configuration which can maximize the system throughput. Finally, extensive experiments are performed on a real-time system to validate the proposed model and approaches.", "venue": "Comput. Commun.", "authors": ["Pu  Yuan", "Kan  Zheng", "Xiong  Xiong", "Kuan  Zhang", "Lei  Lei"], "year": 2020, "n_citations": 20}
{"id": 1850976, "s2_id": "b9d9887c9a676add04a064166763b5f4292cc2a5", "title": "Gravitational Octree Code Performance Evaluation on Volta GPU", "abstract": "In this study, the gravitational octree code originally optimized for the Fermi, Kepler, and Maxwell GPU architectures is adapted to the Volta architecture. The Volta architecture introduces independent thread scheduling requiring either the insertion of the explicit synchronizations at appropriate locations or the enforcement of the same implicit synchronizations as do the Pascal or earlier architectures by specifying -gencode arch=compute_60,code=sm_70. The performance measurements on Tesla V100, the current flagship GPU by NVIDIA, revealed that the N-body simulations of the Andromeda galaxy model with 223 = 8 388 608 particles took 3.8 \u00d7 10-2 s or 3.3 \u00d7 10-2 s per step for cases without or with the implicit synchronizations, respectively. Tesla V100 achieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the flagship GPU in the previous generation. The observed speed-up of 2.2 is greater than 1.5, which is the ratio of the theoretical peak performance of the two GPUs. The independence of the units for integer operations from those for floating-point number operations enables the overlapped execution of integer and floating-point number operations. It hides the execution time of the integer operations leading to the speed-up rate above the theoretical peak performance ratio. Tesla V100 can execute N-body simulation with up to 25 \u00d7 220 = 26 214 400 particles, and it took 2.0 \u00d7 10-1 s per step. It corresponds to 3.5 TFlop/s, which is 22% of the single-precision theoretical peak performance.", "venue": "ICPP", "authors": ["Yohei  Miki"], "year": 2019, "n_citations": 0}
{"id": 1855521, "s2_id": "2314b2fd006002f1be45a597daeafc9fbbf1c8f7", "title": "On Pollaczek-Khinchine Formula for Peer-to-Peer Networks", "abstract": "The performance analysis of peer-to-peer (P2P) networks calls for a new kind of queueing model, in which jobs and service stations arrive randomly. Except in some simple special cases, in general, the queueing model with varying service rate is mathematically intractable. Motivated by the P-K formula for M/G/1 queue, we developed a limiting analysis approach based on the connection between the fluctuation of service rate and the mean queue length. Considering the two extreme service rates, we proved the conjecture on the lower bound and upper bound of mean queue length previously postulated. Furthermore, an approximate P-K formula to estimate the mean queue length is derived from the convex combination of these two bounds and the conditional mean queue length under the overload condition. We confirmed the accuracy of our approximation by extensive simulation studies with different system parameters. We also verified that all limiting cases of the system behavior are consistent with the predictions of our formula.", "venue": "ArXiv", "authors": ["Jian  Zhang", "Tony T. Lee", "Tong  Ye", "Weisheng  Hu"], "year": 2016, "n_citations": 3}
{"id": 1857141, "s2_id": "9d167eb24b4e5013e97d2fa9048f3fa3cc284144", "title": "Exploiting Network Cooperation in Green Wireless Communication", "abstract": "There is a growing interest in energy efficient or so-called \"green\" wireless communication to reduce the energy consumption in cellular networks. Since today's wireless terminals are typically equipped with multiple network access interfaces such as Bluetooth, Wi-Fi, and cellular networks, this paper investigates user terminals cooperating with each other in transmitting their data packets to the base station (BS), by exploiting the multiple network access interfaces, called inter-network cooperation. We also examine the conventional schemes without user cooperation and with intra-network cooperation for comparison. Given target outage probability and data rate requirements, we analyze the energy consumption of conventional schemes as compared to the proposed inter-network cooperation by taking into account both physical-layer channel impairments and upper-layer protocol overheads. It is shown that distances between different network entities (i.e., user terminals and BS) have a significant influence on the energy efficiency of proposed inter-network cooperation scheme. Specifically, when the cooperating users are close to BS or the users are far away from each other, the inter-network cooperation may consume more energy than conventional schemes without user cooperation or with intra-network cooperation. However, as the cooperating users move away from BS and the inter-user distance is not too large, the inter-network cooperation significantly reduces the energy consumption over conventional schemes.", "venue": "IEEE Transactions on Communications", "authors": ["YuLong  Zou", "Jia  Zhu", "Rui  Zhang"], "year": 2013, "n_citations": 55}
{"id": 1859216, "s2_id": "4dddf286970abbc033908cd11a85bb36a6692b72", "title": "Interface Modeling for Quality and Resource Management", "abstract": "We develop an interface-modeling framework for quality and resource management that captures configurable working points of hardware and software components in terms of functionality, resource usage and provision, and quality indicators such as performance and energy consumption. We base these aspects on partially-ordered sets to capture quality levels, budget sizes, and functional compatibility. This makes the framework widely applicable and domain independent (although we aim for embedded and cyber-physical systems). The framework paves the way for dynamic (re-)configuration and multi-objective optimization of component-based systems for quality- and resource-management purposes.", "venue": "Log. Methods Comput. Sci.", "authors": ["Martijn  Hendriks", "Marc  Geilen", "Kees  Goossens", "Rob de Jong", "Twan  Basten"], "year": 2021, "n_citations": 6}
{"id": 1860000, "s2_id": "91e028dd30c114f1a0af539cf6ac86c20eda9f73", "title": "In-DRAM Bulk Bitwise Execution Engine", "abstract": "Many applications heavily use bitwise operations on large bitvectors as part of their computation. In existing systems, performing such bulk bitwise operations requires the processor to transfer a large amount of data on the memory channel, thereby consuming high latency, memory bandwidth, and energy. In this paper, we describe Ambit, a recently-proposed mechanism to perform bulk bitwise operations completely inside main memory. Ambit exploits the internal organization and analog operation of DRAM-based memory to achieve low cost, high performance, and low energy. Ambit exposes a new bulk bitwise execution model to the host processor. Evaluations show that Ambit significantly improves the performance of several applications that use bulk bitwise operations, including databases.", "venue": "ArXiv", "authors": ["Vivek  Seshadri", "Onur  Mutlu"], "year": 2019, "n_citations": 38}
{"id": 1860570, "s2_id": "5319fd6817eef36567c316049fff58c820ce3fe3", "title": "TFix+: Self-configuring Hybrid Timeout Bug Fixing for Cloud Systems", "abstract": "Timeout bugs can cause serious availability and performance issues which are often difficult to fix due to the lack of diagnostic information. Previous work proposed solutions for fixing specific type of timeout-related performance bugs. In this paper, we present TFix, a self-configuring timeout bug fixing framework for automatically correcting two major kinds of timeout bugs (i.e., misused timeout bugs and missing timeout bugs) with dynamic timeout value predictions. TFix provides two new hybrid schemes for fixing misused and missing timeout bugs, respectively. TFix further provides prediction-driven timeout variable configuration based on runtime function tracing. We have implemented a prototype of TFix and conducted experiments on 16 real world timeout bugs. Our experimental results show that TFix can effectively fix 15 out of tested 16 timeout bugs.", "venue": "ArXiv", "authors": ["Jingzhu  He", "Ting  Dai", "Xiaohui  Gu"], "year": 2021, "n_citations": 0}
{"id": 1862106, "s2_id": "9c531c01f2d4a1e51d1b11cd36f1bbb8747fa72a", "title": "Network Cache Design Under Stationary Requests: Exact Analysis and Poisson Approximation", "abstract": "The design of caching algorithms to maximize hit probability has been extensively studied. In this paper, we associate each content with a utility, which is a function of either corresponding content hit rate or hit probability. We formulate a cache optimization problem to maximize the sum of utilities over all contents under stationary and ergodic request process. This problem is non-convex in general but we reformulate it as a convex optimization problem when the inter-request time (irt) distribution has a non-increasing hazard rate function. We provide explicit optimal solutions for some irt distributions, and compare the solutions of the hit-rate based (HRB) and hit probability based (HPB) problems. We also propose decentralized algorithms that can be implemented using limited information and are guaranteed to provide optimal solutions. We find that decentralized algorithms that solve HRB are more robust than decentralized HPB algorithms. Informed by these results, we further propose lightweight Poisson approximate decentralized and online algorithms that are accurate and efficient in achieving optimal hit rates and hit probabilities.", "venue": "2018 IEEE 26th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)", "authors": ["Nitish  Panigrahy", "Jian  Li", "Donald F. Towsley"], "year": 2018, "n_citations": 9}
{"id": 1866852, "s2_id": "672d828f9fc2939222d1cd3eec79c9f9a7079593", "title": "An Efficient Load Balancing Method for Tree Algorithms", "abstract": "Nowadays, multiprocessing is mainstream with exponentially increasing number of processors. Load balancing is, therefore, a critical operation for the efficient execution of parallel algorithms. In this paper we consider the fundamental class of tree-based algorithms that are notoriously irregular,, hard to load-balance with existing static techniques. We propose a hybrid load balancing method using the utility of statistical random sampling in estimating the tree depth, node count distributions to uniformly partition an input tree. To conduct an initial performance study, we implemented the method on an Intel Xeon Phi accelerator system. We considered the tree traversal operation on both regular, irregular unbalanced trees manifested by Fibonacci, unbalanced (biased) randomly generated trees, respectively. The results show scalable performance for up to the 60 physical processors of the accelerator, as well as an extrapolated 128 processors case.", "venue": "2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)", "authors": ["Osama Talaat Ibrahim", "Ahmed  El-Mahdy"], "year": 2016, "n_citations": 0}
{"id": 1867993, "s2_id": "8a431e9875a72e78c2b517dd4b8943c5f2778436", "title": "Mathematical Modeling of Competitive Group Recommendation Systems with Application to Peer Review Systems", "abstract": "In this paper, we present a mathematical model to capture various factors which may influence the accuracy of a competitive group recommendation system. We apply this model to peer review systems, i.e., conference or research grants review, which is an essential component in our scientific community. We explore number of important questions, i.e., how will the number of reviews per paper affect the accuracy of the overall recommendation? Will the score aggregation policy influence the final recommendation? How reviewers' preference may affect the accuracy of the final recommendation? To answer these important questions, we formally analyze our model. Through this analysis, we obtain the insight on how to design a randomized algorithm which is both computationally efficient and asymptotically accurate in evaluating the accuracy of a competitive group recommendation system. We obtain number of interesting observations: i.e., for a medium tier conference, three reviews per paper is sufficient for a high accuracy recommendation. For prestigious conferences, one may need at least seven reviews per paper to achieve high accuracy. We also propose a heterogeneous review strategy which requires equal or less reviewing workload, but can improve over a homogeneous review strategy in recommendation accuracy by as much as 30% . We believe our models and methodology are important building blocks to study competitive group recommendation systems.", "venue": "ArXiv", "authors": ["Hong  Xie", "John C. S. Lui"], "year": 2012, "n_citations": 1}
{"id": 1872295, "s2_id": "09dad6c1d72e53d2da1cee9a60c111e084472847", "title": "The Deep Learning Compiler: A Comprehensive Survey", "abstract": "The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this article, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. We present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey article focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Mingzhen  Li", "Yi  Liu", "Xiaoyan  Liu", "Qingxiao  Sun", "Xin  You", "Hailong  Yang", "Zhongzhi  Luan", "Depei  Qian"], "year": 2021, "n_citations": 22}
{"id": 1874871, "s2_id": "d6ba4c6391d3d6f6d1ef997deb192491e0398192", "title": "Improved spectrum mobility using virtual reservation in collaborative cognitive radio networks", "abstract": "Cognitive radio technology would enable a set of secondary users (SU) to opportunistically use the spectrum licensed to a primary user (PU). On the appearance of this PU on a specific frequency band, any SU occupying this band should free it for PUs. Typically, SUs may collaborate to reduce the impact of cognitive users on the primary network and to improve the performance of the SUs. In this paper, we propose and analyze the performance of virtual reservation in collaborative cognitive networks. Virtual reservation is a novel link maintenance strategy that aims to maximize the throughput of the cognitive network through full spectrum utilization. Our performance evaluation shows significant improvements not only in the SUs blocking and forced termination probabilities but also in the throughput of cognitive users.", "venue": "2013 IEEE Symposium on Computers and Communications (ISCC)", "authors": ["Ayman T. Abdel-Hamid", "Ahmed H. Zahran", "Tamer A. ElBatt"], "year": 2013, "n_citations": 14}
{"id": 1878610, "s2_id": "1fcd4f33c175c352477b07b15e9d1c6e30aaa93f", "title": "Optimizing JPEG Quantization for Classification Networks", "abstract": "Deep learning for computer vision depends on lossy image compression: it reduces the storage required for training and test data and lowers transfer costs in deployment. Mainstream datasets and imaging pipelines all rely on standard JPEG compression. In JPEG, the degree of quantization of frequency coefficients controls the lossiness: an 8 by 8 quantization table (Q-table) decides both the quality of the encoded image and the compression ratio. While a long history of work has sought better Q-tables, existing work either seeks to minimize image distortion or to optimize for models of the human visual system. This work asks whether JPEG Q-tables exist that are \"better\" for specific vision networks and can offer better quality--size trade-offs than ones designed for human perception or minimal distortion. We reconstruct an ImageNet test set with higher resolution to explore the effect of JPEG compression under novel Q-tables. We attempt several approaches to tune a Q-table for a vision task. We find that a simple sorted random sampling method can exceed the performance of the standard JPEG Q-table. We also use hyper-parameter tuning techniques including bounded random search, Bayesian optimization, and composite heuristic optimization methods. The new Q-tables we obtained can improve the compression rate by 10% to 200% when the accuracy is fixed, or improve accuracy up to $2\\%$ at the same compression rate.", "venue": "ArXiv", "authors": ["Zhijing  Li", "Christopher De Sa", "Adrian  Sampson"], "year": 2020, "n_citations": 4}
{"id": 1881237, "s2_id": "8a295b72e112b814e8e9d3105c866f9ba01f0133", "title": "Energy Management in Data Centers with Server Setup Delay: A Semi-MDP Approximation", "abstract": "The energy management schemes in multi-server data centers with setup time mostly consider thresholds on the number of idle servers or waiting jobs to switch servers on or off. An optimal energy management policy can be characterized as a Markov decision process (MDP) at large, given that the system parameters evolve Markovian. The resulting optimal reward can be defined as the weighted sum of mean power usage and mean delay of requested jobs. For large-scale data centers however, these models become intractable due to the colossal state-action space, thus making conventional algorithms inefficient in finding the optimal policy. In this paper, we propose an approximate semi-MDP (SMDP) approach, known as \u2018multi-level SMDP \u2019, based on state aggregation and Markovian analysis of the system behavior. Rather than averaging the transition probabilities of aggregated states as in typical methods, we introduce an approximate Markovian framework for calculating the transition probabilities of the proposed multi-level SMDP accurately. Moreover, near-optimal performance can be attained at the expense of increased state-space dimensionality by tuning the number of levels in the multi-level approach. Simulation results show that the proposed approach reduces the SMDP size while yielding better rewards as against existing fixed threshold-based policies and aggregation methods.", "venue": "ArXiv", "authors": ["Behzad  Chitsaz", "Ahmad  Khonsari", "Masoumeh  Moradian", "Aresh  Dadlani"], "year": 2021, "n_citations": 0}
{"id": 1882026, "s2_id": "9e5024eca72fb74e1ef814ce56594fdedd722b8d", "title": "Understanding and taming SSD read performance variability: HDFS case study", "abstract": "In this paper we analyze the influence that lower layers (file system, OS, SSD) have on HDFS' ability to extract maximum performance from SSDs on the read path. We uncover and analyze three surprising performance slowdowns induced by lower layers that result in HDFS read throughput loss. First, intrinsic slowdown affects reads from every new file system extent for a variable amount of time. Second, temporal slowdown appears temporarily and periodically and is workload-agnostic. Third, in permanent slowdown, some files can individually and permanently become slower after a period of time. We analyze the impact of these slowdowns on HDFS and show significant throughput loss. Individually, each of the slowdowns can cause a read throughput loss of 10-15%. However, their effect is cumulative. When all slowdowns happen concurrently, read throughput drops by as much as 30%. We further analyze mitigation techniques and show that two of the three slowdowns could be addressed via increased IO request parallelism in the lower layers. Unfortunately, HDFS cannot automatically adapt to use such additional parallelism. Our results point to a need for adaptability in storage stacks. The reason is that an access pattern that maximizes performance in the common case is not necessarily the same one that can mask performance fluctuations.", "venue": "ArXiv", "authors": ["Mar\u00eda F. Borge", "Florin  Dinu", "Willy  Zwaenepoel"], "year": 2019, "n_citations": 2}
{"id": 1887555, "s2_id": "e5b76e11df3febcc9e436fc001a22b0d2b9732e1", "title": "Buffer-aware Worst Case Timing Analysis of Wormhole Network On Chip", "abstract": "A buffer-aware worst-case timing analysis of wormhole NoC is proposed in this paper to integrate the impact of buffer size on the different dependencies relationship between flows, i.e. direct and indirect blocking flows, and consequently the timing performance. First, more accurate definitions of direct and indirect blocking flows sets have been introduced to take into account the buffer size impact. Then, the modeling and worst-case timing analysis of wormhole NoC have been detailed, based on Network Calculus formalism and the newly defined blocking flows sets. This introduced approach has been illustrated in the case of a realistic NoC case study to show the trade off between latency and buffer size. The comparative analysis of our proposed Buffer-aware timing analysis with conventional approaches is conducted and noticeable enhancements in terms of maximum latency have been proved.", "venue": "ArXiv", "authors": ["Ahlem  Mifdaoui", "Hamid  Ayed"], "year": 2016, "n_citations": 4}
{"id": 1889425, "s2_id": "2825b053aec6777175a432cf1618be5c42b39e2f", "title": "A comprehensive study on software aging across android versions and vendors", "abstract": "This paper analyzes the phenomenon of software aging \u2013 namely, the gradual performance degradation and resource exhaustion in the long run \u2013 in the Android OS. The study intends to highlight if, and to what extent, devices from different vendors, under various usage conditions and configurations, are affected by software aging and which parts of the system are the main contributors. The results demonstrate that software aging systematically determines a gradual loss of responsiveness perceived by the user, and an unjustified depletion of physical memory. The analysis reveals differences in the aging trends due to the workload factors and to the type of running applications, as well as differences due to vendors\u2019 customization. Moreover, we analyze several system-level metrics to trace back the software aging effects to their main causes. We show that bloated Java containers are a significant contributor to software aging, and that it is feasible to mitigate aging through a micro-rejuvenation solution at the container level.", "venue": "Empirical Software Engineering", "authors": ["Domenico  Cotroneo", "Antonio Ken Iannillo", "Roberto  Natella", "Roberto  Pietrantuono"], "year": 2020, "n_citations": 2}
{"id": 1891791, "s2_id": "aac95d44cf39e38d8cf70c9ccc5eec032bc24c54", "title": "Accelerating Geometric Multigrid Preconditioning with Half-Precision Arithmetic on GPUs", "abstract": "With the hardware support for half-precision arithmetic on NVIDIA V100 GPUs, high-performance computing applications can benefit from lower precision at appropriate spots to speed up the overall execution time. In this paper, we investigate a mixed-precision geometric multigrid method to solve large sparse systems of equations stemming from discretization of elliptic PDEs. While the final solution is always computed with high-precision accuracy, an iterative refinement approach with multigrid preconditioning in lower precision and residuum scaling is employed. We compare the FP64 baseline for Poisson's equation to purely FP16 multigrid preconditioning and to the employment of FP16-FP32-FP64 combinations within a mesh hierarchy. While the iteration count is almost not affected by using lower accuracy, the solver runtime is considerably decreased due to the reduced memory transfer and a speedup of up to 2.5x is gained for the overall solver. We investigate the performance of selected kernels with the hierarchical Roofline model.", "venue": "ArXiv", "authors": ["Kyaw L. Oo", "Andreas  Vogel"], "year": 2020, "n_citations": 2}
{"id": 1898992, "s2_id": "c79053febf7efdd967ac6a9cb4f7aed2f774c8fd", "title": "A Multi-State Power Model for Adequacy Assessment of Distributed Generation via Universal Generating Function", "abstract": "The current and future developments of electric power systems are pushing the boundaries of reliability assessment to consider distribution networks with renewable generators. Given the stochastic features of these elements, most modeling approaches rely on Monte Carlo simulation. The computational costs associated to the simulation approach force to treating mostly small-sized systems, i.e. with a limited number of lumped components of a given renewable technology (e.g. wind or solar, etc.) whose behavior is described by a binary state, working or failed. In this paper, we propose an analytical multi-state modeling approach for the reliability assessment of distributed generation (DG). The approach allows looking to a number of diverse energy generation technologies distributed on the system. Multiple states are used to describe the randomness in the generation units, due to the stochastic nature of the generation sources and of the mechanical degradation/failure behavior of the generation systems. The universal generating function (UGF) technique is used for the individual component multi-state modeling. A multiplication-type composition operator is introduced to combine the UGFs for the mechanical degradation and renewable generation source states into the UGF of the renewable generator power output. The overall multi-state DG system UGF is then constructed and classical reliability indices (e.g. loss of load expectation (LOLE), expected energy not supplied (EENS)) are computed from the DG system generation and load UGFs. An application of the model is shown on a DG system adapted from the IEEE 34 nodes distribution test feeder.", "venue": "ArXiv", "authors": ["Yanfu  Li", "Enrico  Zio"], "year": 2012, "n_citations": 7}
{"id": 1904164, "s2_id": "f6a32eb3c31f1b85bdb12320a25e4a0ce8bef31c", "title": "The Preliminary Evaluation of a Hypervisor-based Virtualization Mechanism for Intel Optane DC Persistent Memory Module", "abstract": "Non-volatile memory (NVM) technologies, being accessible in the same manner as DRAM, are considered indispensable for expanding main memory capacities. Intel Optane DCPMM is a long-awaited product that drastically increases main memory capacities. However, a substantial performance gap exists between DRAM and DCPMM. In our experiments, the read/write latencies of DCPMM were 400% and 407% higher than those of DRAM, respectively. The read/write bandwidths were 37% and 8% of those of DRAM. This performance gap in main memory presents a new challenge to researchers; we need a new system software technology supporting emerging hybrid memory architecture. In this paper, we present RAMinate, a hypervisor-based virtualization mechanism for hybrid memory systems, and a key technology to address the performance gap in main memory systems. It provides great flexibility in memory management and maximizes the performance of virtual machines (VMs) by dynamically optimizing memory mappings. Through experiments, we confirmed that even though a VM has only 1% of DRAM in its RAM, the performance degradation of the VM was drastically alleviated by memory mapping optimization. The elapsed time to finish the build of Linux Kernel in the VM was 557 seconds, which was only 13% increase from the 100% DRAM case (i.e., 495 seconds). When the optimization mechanism was disabled, the elapsed time increased to 624 seconds (i.e. 26% increase from the 100% DRAM case).", "venue": "ArXiv", "authors": ["Takahiro  Hirofuchi", "Ryousei  Takano"], "year": 2019, "n_citations": 12}
{"id": 1912059, "s2_id": "1a06223da90ec4024b89000477cd6feabacb1095", "title": "Cache Optimization Models and Algorithms", "abstract": "Storage resources and caching techniques permeate almost every area of communication networks today. In the near future, caching is set to play an important role in storage-assisted Internet architectures, information-centric networks, and wireless systems, reducing operating and capital expenditures and improving the services offered to users. In light of the remarkable data traffic growth and the increasing number of rich-media applications, the impact of caching is expected to become even more profound than it is today. Therefore, it is crucial to design these systems in an optimal fashion, ensuring the maximum possible performance and economic benefits from their deployment. To this end, this article presents a collection of detailed models and algorithms, which are synthesized to build a powerful analytical framework for caching optimization.", "venue": "Found. Trends Commun. Inf. Theory", "authors": ["Georgios  Paschos", "George  Iosifidis", "Giuseppe  Caire"], "year": 2019, "n_citations": 10}
{"id": 1912845, "s2_id": "786cb3adf7e2eb1b061651c3a6c0f46d1a6ca093", "title": "Passive Optical Networking for 5G and Beyond 5G Low-Latency Mobile Fronthauling Services", "abstract": "Passive optical network (PON) technology offers an attractive cost-efficient alternative to support 5G and Beyond 5G mobile network fronthauling (MFH). However, MFH for such networks is challenging given its high bandwidth and strict latency requirements. To reduce these requirements, radio access network (RAN) functional splitting has been introduced in 5G networks; this provides more flexibility in resource allocation since the protocol stack is distributed between the centralized and the distributed units. In contrast to the conventional MFH requirement of the RF-PHY splitting, the MFH traffic produced by higher-layer splittings becomes more dependent on the actual user traffic load. By capitalizing on the new characteristics of the MFH traffic with RAN functional splitting, this article introduces a resource allocation mechanism to improve the performance of PONs serving MFH.", "venue": "ArXiv", "authors": ["Oscar J. Ciceri", "Carlos A. Astudillo", "Gustavo B. Figueiredo", "Zunqing  Zhu", "Nelson L. S. da Fonseca"], "year": 2021, "n_citations": 1}
{"id": 1914550, "s2_id": "980f2311733dce0ff0bf239f992dbc178792c3f0", "title": "Optimizing Deep Learning Recommender Systems' Training On CPU Cluster Architectures", "abstract": "During the last two years, the goal of many researchers has been to squeeze the last bit of performance out of HPC system for AI tasks. Often this discussion is held in the context of how fast ResNet50 can be trained. Unfortunately, ResNet50 is no longer a representative workload in 2020. Thus, we focus on Recommender Systems which account for most of the AI cycles in cloud computing centers. More specifically, we focus on Facebook's DLRM benchmark. By enabling it to run on latest CPU hardware and software tailored for HPC, we are able to achieve more than two-orders of magnitude improvement in performance (110x) on a single socket compared to the reference CPU implementation, and high scaling efficiency up to 64 sockets, while fitting ultra-large datasets. This paper discusses the optimization techniques for the various operators in DLRM and which component of the systems are stressed by these different operators. The presented techniques are applicable to a broader set of DL workloads that pose the same scaling challenges/characteristics as DLRM.", "venue": "SC", "authors": ["Dhiraj  Kalamkar", "Evangelos  Georganas", "Sudarshan  Srinivasan", "Jianping  Chen", "Mikhail  Shiryaev", "Alexander  Heinecke"], "year": 2020, "n_citations": 12}
{"id": 1917043, "s2_id": "c72fdb560bdeaeb1c8cc5ebb4c12fcc4a6cb1851", "title": "Security and performance comparison of different secure channel protocols for Avionics Wireless Networks", "abstract": "The notion of Integrated Modular Avionics (IMA) refers to inter-connected pieces of avionics equipment supported by a wired technology, with stringent reliability and safety requirements. If the inter-connecting wires are physically secured so that a malicious user cannot access them directly, then this enforces (at least partially) the security of the network. However, substituting the wired network with a wireless network - which in this context is referred to as an Avionics Wireless Network (AWN) - brings a number of new challenges related to assurance, reliability, and security. The AWN thus has to ensure that it provides at least the required security and safety levels offered by the equivalent wired network. Providing a wired-equivalent security for a communication channel requires the setting up of a strong, secure (encrypted) channel between the entities that are connected to the AWN. In this paper, we propose three approaches to establish such a secure channel based on (i) pre-shared keys, (ii) trusted key distribution, and (iii) key-sharing protocols. For each of these approaches, we present at least two representative protocol variants. These protocols are then implemented as part of a demo AWN and they are then compared based on performance measurements. Most importantly, we have evaluated these protocols based on security and operational requirements that we define in this paper for an AWN.", "venue": "2016 IEEE/AIAA 35th Digital Avionics Systems Conference (DASC)", "authors": ["Raja Naeem Akram", "Konstantinos  Markantonakis", "Keith  Mayes", "Pierre-Fran\u00e7ois  Bonnefoi", "Damien  Sauveron", "Serge  Chaumette"], "year": 2016, "n_citations": 8}
{"id": 1919734, "s2_id": "6cf9eb5a8f91694f1c12572eb688fd71e451989d", "title": "Data-Unit-Size Distribution Model with Retransmitted Packet Size Preservation Property and Its Application to Goodput Analysis for Stop-and-Wait Protocol: Case of Independent Packet Losses", "abstract": "This paper proposes a data-unit-size distribution model to represent the retransmitted packet size preservation (RPSP) property in a scenario where independently lost packets are retransmitted by a stop-and-wait protocol. RPSP means that retransmitted packets with the same sequence number are equal in size to the packet of the original transmission, which is identical to the packet generated from a message through the segmentation function, namely, generated packet. Furthermore, we derive goodput formula using an approach to derive the data-unit-size distribution. We investigate the effect of RPSP on frame size distributions and goodput in a simple case when no collision happens over the bit-error prone wireless network equipped with IEEE 802.11 Distributed Coordination Function, which is a typical example of the stop-and-wait protocol. Numerical results show that the effect gets stronger as bit error rate increases and the maximum size of the generated packets is larger than the mean size for large enough packet retry limits because longer packets will be repeatedly corrupted and retransmitted more times as a result of RPSP.", "venue": "ArXiv", "authors": ["Takashi  Ikegawa"], "year": 2016, "n_citations": 1}
{"id": 1921300, "s2_id": "fa2c223b7ff1ac23a0bad4ddee6dc8ce358aa80d", "title": "Optimization of Location Management for PCS Networks with CTRW Mobility Model", "abstract": "This paper considers the design of the optimal locationupdate area (LA) of the distance-based scheme for personal communication service (PCS) networks. We focus on the optimization of two design parameters associated with the LA: 1) initial position upon LA update; 2) distance threshold for triggering of LA update. Based on the popular continuous-time random walk (CTRW) mobility model, we propose a novel analytical framework that uses a diffusion equation to minimize the location management cost. In this framework, a number of measurable physical parameters, such as length of road section, angle between road sections, and road section crossing time, can be integrated into the system design. This framework allows us to easily evaluate the total cost under general call arrival distributions and LA of different shapes. For the particular case of circular LA and small Poisson call-arrival rate, we prove the following: (1) When the drift is weak, the optimal initial position approaches the center of the LA; when the drift is strong, it approaches the boundary of the LA. (2) Comparing the optimal initial-position and center-initial-position solutions (which is assumed in most prior work), when the drift is weak, the optimal distance threshold and the minimum total cost are roughly equal; when the drift is strong, the optimal distance threshold in the later is about 1.260 times that in the former, and the minimum total cost in the later is about 1.587 times that in the former. That is, optimizing on initial position, which previous work did not consider, has the potential of reducing the cost measure by 37%.", "venue": "ArXiv", "authors": ["Qinglin  Zhao", "Soung Chang Liew"], "year": 2008, "n_citations": 0}
{"id": 1928537, "s2_id": "99364c3c17e5e799894e1d0f9b5e5c7fde93567e", "title": "Adaptive Kernel Value Caching for SVM Training", "abstract": "Support vector machines (SVMs) can solve structured multioutput learning problems such as multilabel classification, multiclass classification, and vector regression. SVM training is expensive, especially for large and high-dimensional data sets. The bottleneck of the SVM training often lies in the kernel value computation. In many real-world problems, the same kernel values are used in many iterations during the training, which makes the caching of kernel values potentially useful. The majority of the existing studies simply adopt the least recently used (LRU) replacement strategy for caching kernel values. However, as we analyze in this article, the LRU strategy generally achieves high hit ratio near the final stage of the training but does not work well in the whole training process. Therefore, we propose a new caching strategy called EFU (less frequently used), which replaces the EFU kernel values that enhance least frequently used (LFU). Our experimental results show that EFU often has 20% higher hit ratio than LRU in the training with the Gaussian kernel. To further optimize the strategy, we propose a caching strategy called hybrid caching for the SVM training (HCST), which has a novel mechanism to automatically adapt the better caching strategy in different stages of the training. We have integrated the caching strategy into ThunderSVM, a recent SVM library on many-core processors. Our experiments show that HCST adaptively achieves high hit ratios with little runtime overhead among different problems including multilabel classification, multiclass classification, and regression problems. Compared with other existing caching strategies, HCST achieves 20% more reduction in training time on average.", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "authors": ["Qinbin  Li", "Zeyi  Wen", "Bingsheng  He"], "year": 2020, "n_citations": 6}
{"id": 1931611, "s2_id": "8d2f143a72b096797c3e063943fc14d5d3c8d351", "title": "Performance monitoring for multicore embedded computing systems on FPGAs", "abstract": "When designing modern embedded computing systems, most software programmers choose to use multicore processors, possibly in combination with general-purpose graphics processing units (GPGPUs) and/or hardware accelerators. They also often use an embedded Linux O/S and run multi-application workloads that may even be multi-threaded. Modern FPGAs are large enough to combine multicore hard/soft processors with multiple hardware accelerators as custom compute units, enabling entire embedded compute systems to be implemented on a single FPGA. Furthermore, the large FPGA vendors also support embedded Linux kernels for both their soft and embedded processors. When combined with high-level synthesis to generate hardware accelerators using a C-to-gates flows, the necessary primitives for a framework that can enable software designers to use FPGAs as their custom compute platform now exist. However, in order to ensure that computing resources are integrated and shared effectively, software developers need to be able to monitor and debug the runtime performance of the applications in their workload. This paper describes ABACUS, a performance-monitoring framework that can be used to debug the execution behaviours and interactions of multi-application workloads on multicore systems. We also discuss how this framework is extensible for use with hardware accelerators in heterogeneous systems.", "venue": "ArXiv", "authors": ["Lesley  Shannon", "Eric  Matthews", "Nicholas C. Doyle", "Alexandra  Fedorova"], "year": 2015, "n_citations": 6}
{"id": 1931644, "s2_id": "73c9a5beceea745330d7e9d952d13233389c453d", "title": "Fault-tolerant linear solvers via selective reliability", "abstract": "Energy increasingly constrains modern computer hardware, yet protecting computations and data against errors costs energy. This holds at all scales, but especially for the largest parallel computers being built and planned today. As processor counts continue to grow, the cost of ensuring reliability consistently throughout an application will become unbearable. However, many algorithms only need reliability for certain data and phases of computation. This suggests an algorithm and system codesign approach. We show that if the system lets applications apply reliability selectively, we can develop algorithms that compute the right answer despite faults. These \"fault-tolerant\" iterative methods either converge eventually, at a rate that degrades gracefully with increased fault rate, or return a clear failure indication in the rare case that they cannot converge. Furthermore, they store most of their data unreliably, and spend most of their time in unreliable mode. \nWe demonstrate this for the specific case of detected but uncorrectable memory faults, which we argue are representative of all kinds of faults. We developed a cross-layer application / operating system framework that intercepts and reports uncorrectable memory faults to the application, rather than killing the application, as current operating systems do. The application in turn can mark memory allocations as subject to such faults. Using this framework, we wrote a fault-tolerant iterative linear solver using components from the Trilinos solvers library. Our solver exploits hybrid parallelism (MPI and threads). It performs just as well as other solvers if no faults occur, and converges where other solvers do not in the presence of faults. We show convergence results for representative test problems. Near-term future work will include performance tests.", "venue": "ArXiv", "authors": ["Patrick G. Bridges", "Kurt B. Ferreira", "Michael A. Heroux", "Mark  Hoemmen"], "year": 2012, "n_citations": 67}
{"id": 1932539, "s2_id": "2e917f4fa65e0668b1827178b064fc35bbf51708", "title": "Detecting and understanding real-world differential performance bugs in machine learning libraries", "abstract": "Programming errors that degrade the performance of systems are widespread, yet there is very little tool support for finding and diagnosing these bugs. We present a method and a tool based on differential performance analysis---we find inputs for which the performance varies widely, despite having the same size. To ensure that the differences in the performance are robust (i.e. hold also for large inputs), we compare the performance of not only single inputs, but of classes of inputs, where each class has similar inputs parameterized by their size. Thus, each class is represented by a performance function from the input size to performance. Importantly, we also provide an explanation for why the performance differs in a form that can be readily used to fix a performance bug. The two main phases in our method are discovery with fuzzing and explanation with decision tree classifiers, each of which is supported by clustering. First, we propose an evolutionary fuzzing algorithm to generate inputs that characterize different performance functions. For this fuzzing task, the unique challenge is that we not only need the input class with the worst performance, but rather a set of classes exhibiting differential performance. We use clustering to merge similar input classes which significantly improves the efficiency of our fuzzer. Second, we explain the differential performance in terms of program inputs and internals (e.g., methods and conditions). We adapt discriminant learning approaches with clustering and decision trees to localize suspicious code regions. We applied our techniques on a set of micro-benchmarks and real-world machine learning libraries. On a set of micro-benchmarks, we show that our approach outperforms state-of-the-art fuzzers in finding inputs to characterize differential performance. On a set of case-studies, we discover and explain multiple performance bugs in popular machine learning frameworks, for instance in implementations of logistic regression in scikit-learn. Four of these bugs, reported first in this paper, have since been fixed by the developers.", "venue": "ISSTA", "authors": ["Saeid  Tizpaz-Niari", "Pavol  Cern'y", "Ashutosh  Trivedi"], "year": 2020, "n_citations": 5}
{"id": 1935254, "s2_id": "0da6b36e341ee009cd1f124eb7b991706b39b2bf", "title": "A Basic Result on the Superposition of Arrival Processes in Deterministic Networks", "abstract": "Time-Sensitive Networking (TSN) and Deterministic Networking (DetNet) are emerging standards to enable deterministic, delay-critical communication in such networks. This naturally (re-)calls attention to the network calculus theory (NC), since a rich set of results for delay guarantee analysis have already been developed there. One could anticipate an immediate adoption of those existing network calculus results to TSN and DetNet. However, the fundamental difference between the traffic specification adopted in TSN and DetNet and those traffic models in NC makes this difficult, let alone that there is a long-standing open challenge in NC. To address them, this paper considers an arrival time function based max-plus NC traffic model. In particular, the mapping between the TSN / DetNet and the NC traffic model is proved. In addition, the superposition property of the arrival time function based NC traffic model is found and proved. Appealingly, the proved superposition property shows a clear analogy with that of a well-known counterpart traffic model in NC. These results help make an important step forward towards the development of a system theory for delay guarantee analysis of TSN / DetNet networks.", "venue": "2018 IEEE Global Communications Conference (GLOBECOM)", "authors": ["Yuming  Jiang"], "year": 2018, "n_citations": 2}
{"id": 1936980, "s2_id": "c2da7703fe449c8e1ce85c80f587eefc4f459cd7", "title": "Optimal power cost management using stored energy in data centers", "abstract": "Since the electricity bill of a data center constitutes a significant portion of its overall operational costs, reducing this has become important. We investigate cost reduction opportunities that arise by the use of uninterrupted power supply (UPS) units as energy storage devices. This represents a deviation from the usual use of these devices as mere transitional fail-over mechanisms between utility and captive sources such as diesel generators. We consider the problem of opportunistically using these devices to reduce the time average electric utility bill in a data center. Using the technique of Lyapunov optimization, we develop an online control algorithm that can optimally exploit these devices to minimize the time average cost. This algorithm operates without any knowledge of the statistics of the workload or electricity cost processes, making it attractive in the presence of workload and pricing uncertainties. An interesting feature of our algorithm is that its deviation from optimality reduces as the storage capacity is increased. Our work opens up a new area in data center power management.", "venue": "SIGMETRICS 2011", "authors": ["Rahul  Urgaonkar", "Bhuvan  Urgaonkar", "Michael J. Neely", "Anand  Sivasubramaniam"], "year": 2011, "n_citations": 95}
{"id": 1939345, "s2_id": "62cb7f7b0f245883d54da733cb2900f177f6e8d3", "title": "Differentiate Quality of Experience Scheduling for Deep Learning Applications with Docker Containers in the Cloud", "abstract": "With the prevalence of big-data-driven applications, such as face recognition on smartphones and tailored recommendations from Google Ads, we are on the road to a lifestyle with significantly more intelligence than ever before. For example, Aipoly Vision [1] is an object and color recognizer that helps the blind, visually impaired, and color blind understand their surroundings. At the back end side of their intelligence, various neural networks powered models are running to enable quick responses to users. Supporting those models requires lots of cloud-based computational resources, e.g. CPUs and GPUs. The cloud providers charge their clients by the amount of resources that they occupied. From clients' perspective, they have to balance the budget and quality of experiences (e.g. response time). The budget leans on individual business owners and the required Quality of Experience (QoE) depends on usage scenarios of different applications, for instance, an autonomous vehicle requires realtime response, but, unlocking your smartphone can tolerate delays. However, cloud providers fail to offer a QoE based option to their clients. In this paper, we propose DQoES, a differentiate quality of experience scheduler for deep learning applications. DQoES accepts client's specification on targeted QoEs, and dynamically adjust resources to approach their targets. Through extensive, cloud-based experiments, DQoES demonstrates that it can schedule multiple concurrent jobs with respect to various QoEs and achieve up to 8x times more satisfied models compared to the existing system.", "venue": "ArXiv", "authors": ["Ying  Mao", "Weifeng  Yan", "Yun  Song", "Yue  Zeng", "Ming  Chen", "Long  Cheng", "Qingzhi  Liu"], "year": 2020, "n_citations": 2}
{"id": 1945409, "s2_id": "6ce438339ea66d3c4acdeb5d5601c6661f2ab2b2", "title": "Implementation of a parallel tree method on a GPU", "abstract": "Abstract The k d-tree is a fundamental tool in computer science. Among other applications, the application of k d-tree search (by the tree method) to the fast evaluation of particle interactions and neighbor search is highly important, since the computational complexity of these problems is reduced from O ( N 2 ) for a brute force method to O ( N \u00a0log\u00a0\u00a0 N ) for the tree method, where N is the number of particles. In this paper, we present a parallel implementation of the tree method running on a graphics processing unit (GPU). We present a detailed description of how we have implemented the tree method on a Cypress GPU. An optimization that we found important is localized particle ordering to effectively utilize cache memory. We present a number of test results and performance measurements. Our results show that the execution of the tree traversal in a force calculation on a GPU is practical and efficient.", "venue": "J. Comput. Sci.", "authors": ["Naohito  Nakasato"], "year": 2012, "n_citations": 34}
{"id": 1946351, "s2_id": "da64c69112925698cfd15d8ada5d47f4c418e104", "title": "A General Theory of Computational Scalability Based on Rational Functions", "abstract": "The universal scalability law of computational capacity is a rational function Cp = P (p)=Q(p) with P (p) a linear polynomial and Q(p) a second-degree polynomial in the number of physical processors p, that has been long used for statistical modeling and prediction of computer system performance. We prove that Cp is equivalent to the synchronous throughput bound for a machinerepairman with state-dependent service rate. Simpler rational functions, such as Amdahl\u2019s law and Gustafson speedup, are corollaries of this queue-theoretic bound. Cp is further shown to be both necessary and sucient for modeling all practical characteristics of computational scalability.", "venue": "ArXiv", "authors": ["Neil J. Gunther"], "year": 2008, "n_citations": 38}
{"id": 1947239, "s2_id": "05c2677c5dc2ca9b846183343159193a666ce324", "title": "Proceedings of the 2nd OMNeT++ Community Summit, IBM Research - Zurich, Switzerland, September 3-4, 2015", "abstract": "This is the Proceedings of the 2nd OMNeT++ Community Summit, which was held at IBM Research - Zurich, Switzerland on September 3-4, 2015.", "venue": "ArXiv", "authors": ["Anna  F\u00f6rster", "Cyriel  Minkenberg", "Germ\u00e1n  Rodr\u00edguez", "Michael  Kirsche"], "year": 2015, "n_citations": 0}
{"id": 1948413, "s2_id": "694d336358762c1c7b9b39784dbf2de1beb9c602", "title": "Optimizing mission critical data dissemination in massive IoT networks", "abstract": "Mission critical data dissemination in massive Internet of things (IoT) networks imposes constraints on the message transfer delay between devices. Due to low power and communication range of IoT devices, data is foreseen to be relayed over multiple device-to-device (D2D) links before reaching the destination. The coexistence of a massive number of IoT devices poses a challenge in maximizing the successful transmission capacity of the overall network alongside reducing the multi-hop transmission delay in order to support mission critical applications. There is a delicate interplay between the carrier sensing threshold of the contention based medium access protocol and the choice of packet forwarding strategy selected at each hop by the devices. The fundamental problem in optimizing the performance of such networks is to balance the tradeoff between conflicting performance objectives such as the spatial frequency reuse, transmission quality, and packet progress towards the destination. In this paper, we use a stochastic geometry approach to quantify the performance of multi-hop massive IoT networks in terms of the spatial frequency reuse and the transmission quality under different packet forwarding schemes. We also develop a comprehensive performance metric that can be used to optimize the system to achieve the best performance. The results can be used to select the best forwarding scheme and tune the carrier sensing threshold to optimize the performance of the network according to the delay constraints and transmission quality requirements.", "venue": "2017 15th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)", "authors": ["Muhammad Junaid Farooq", "Quanyan  Zhu"], "year": 2017, "n_citations": 20}
{"id": 1951126, "s2_id": "d0ff421e3bd5558c5196d005a0b533b811c45be3", "title": "Asymptotic efficiency of restart and checkpointing", "abstract": "Many tasks are subject to failure before completion. Two of the most common failure recovery strategies are restart and checkpointing. Under restart, once a failure occurs, it is restarted from the beginning. Under checkpointing, the task is resumed from the preceding checkpoint after the failure. We study asymptotic efficiency of restart for an infinite sequence of tasks, whose sizes form a stationary sequence. We define asymptotic efficiency as the limit of the ratio of the total time to completion in the absence of failures over the total time to completion when failures take place. Whether the asymptotic efficiency is positive or not depends on the comparison of the tail of the distributions of the task size and the random variables governing failures. Our framework allows for variations in the failure rates and dependencies between task sizes. We also study a similar notion of asymptotic efficiency for checkpointing when the task is infinite a.s. and the inter-checkpoint times are i.i.d.. Moreover, in checkpointing, when the failures are exponentially distributed, we prove the existence of an infinite sequence of universal checkpoints, which are always used whenever the system starts from any checkpoint that precedes them.", "venue": "ArXiv", "authors": ["Antonio  Sodre"], "year": 2018, "n_citations": 0}
{"id": 1954949, "s2_id": "e322ce945e1fcf0809394131838004613b9024e2", "title": "Accurate performance analysis of opportunistic decode-and-forward relaying", "abstract": "In this paper, we investigate an opportunistic relaying scheme where the selected relay assists the source-destination (direct) communication. In our study, we consider a regenerative opportunistic relaying scheme in which the direct path may be considered unusable, and the destination may use a selection combining technique. We first derive the exact statistics of each hop, in terms of probability density function (PDF). Then, the PDFs are used to determine accurate closed form expressions for end-to-end outage probability for a transmission rate R. Furthermore, we evaluate the asymptotical performance analysis and the diversity order is deduced. Finally, we validate our analysis by showing that performance simulation results coincide with our analytical results over different network architectures.", "venue": "2011 7th International Wireless Communications and Mobile Computing Conference", "authors": ["Kamel  Tourki", "Hong-Chuan  Yang", "Mohamed-Slim  Alouini"], "year": 2011, "n_citations": 2}
{"id": 1957769, "s2_id": "62fa4c9cadd0ab34d633129c456fd7700a5b1047", "title": "Multi-Level Analysis of Compiler-Induced Variability and Performance Tradeoffs", "abstract": "Successful HPC software applications are long-lived. When ported across machines and their compilers, these applications often produce different numerical results, many of which are unacceptable. Such variability is also a concern while optimizing the code more aggressively to gain performance. Efficient tools that help locate the program units (files and functions) within which most of the variability occurs are badly needed, both to plan for code ports and to root-cause errors due to variability when they happen in the field. In this work, we offer an enhanced version of the open-source testing framework FLiT to serve these roles. Key new features of FLiT include a suite of bisection algorithms that help locate the root causes of variability. Another added feature allows an analysis of the tradeoffs between performance and the degree of variability. Our new contributions also include a collection of case studies. Results on the MFEM finite-element library include variability/performance tradeoffs, and the identification of a (hitherto unknown) abnormal level of result-variability even under mild compiler optimizations. Results from studying the Laghos proxy application include identifying a significantly divergent floating-point result-variability and successful root-causing down to the problematic function over as little as 14 program executions. Finally, in an evaluation of 4,376 controlled injections of floating-point perturbations on the LULESH proxy application, we showed that the FLiT framework has 100% precision and recall in discovering the file and function locations of the injections all within an average of only 15 program executions.", "venue": "HPDC", "authors": ["Michael  Bentley", "Ian  Briggs", "Ganesh  Gopalakrishnan", "Dong H. Ahn", "Ignacio  Laguna", "Gregory L. Lee", "Holger E. Jones"], "year": 2019, "n_citations": 6}
{"id": 1961365, "s2_id": "9371e618d3cc275bf623189abcfd85f2644ae690", "title": "Measuring independence of datasets", "abstract": "Approximating pairwise, or k-wise, independence with sublinear memory is of considerable importance in the data stream model. In the streaming model the joint distribution is given by a stream of k-tuples, with the goal of testing correlations among the components measured over the entire stream. Indyk and McGregor (SODA 08) recently gave exciting new results for measuring pairwise independence in this model.\n Statistical distance is one of the most fundamental metrics for measuring the similarity of two distributions, and it has been a metric of choice in many papers that discuss distribution closeness. For pairwise independence, the Indyk and McGregor methods provide log{n}-approximation under statistical distance between the joint and product distributions in the streaming model. Indyk and McGregor leave, as their main open question, the problem of improving their log n-approximation for the statistical distance metric.\n In this paper we solve the main open problem posed by Indyk and McGregor for the statistical distance for pairwise independence and extend this result to any constant k. In particular, we present an algorithm that computes an (\u03b5, \u03b4)-approximation of the statistical distance between the joint and product distributions defined by a stream of k-tuples. Our algorithm requires O((1/\u03b5 log(nm/\u03b4))(30+k)k) memory and a single pass over the data stream.", "venue": "STOC '10", "authors": ["Vladimir  Braverman", "Rafail  Ostrovsky"], "year": 2010, "n_citations": 27}
{"id": 1964235, "s2_id": "00a7577173c6e8447a139293ccdd023e44d3b41f", "title": "Fault-Tolerant Adaptive Parallel and Distributed Simulation", "abstract": "Discrete Event Simulation is a widely used technique that is used to model and analyze complex systems in many fields of science and engineering. The increasingly large size of simulation models poses a serious computational challenge, since the time needed to run a simulation can be prohibitively large. For this reason, Parallel and Distributes Simulation techniques have been proposed to take advantage of multiple execution units which are found in multicore processors, cluster of workstations or HPC systems. The current generation of HPC systems includes hundreds of thousands of computing nodes and a vast amount of ancillary components. Despite improvements in manufacturing processes, failures of some components are frequent, and the situation will get worse as larger systems are built. In this paper we describe FT-GAIA, a software-based fault-tolerant extension of the GAIA/ARTIS parallel simulation middleware. FT-GAIA transparently replicates simulation entities and distributes them on multiple execution nodes. This allows the simulation to tolerate crash-failures of computing nodes, furthermore, FT-GAIA offers some protection against Byzantine failures since synchronization messages are replicated as well, so that the receiving entity can identify and discard corrupted messages. We provide an experimental evaluation of FT-GAIA on a running prototype. Results show that a high degree of fault tolerance can be achieved, at the cost of a moderate increase in the computational load of the execution units.", "venue": "2016 IEEE/ACM 20th International Symposium on Distributed Simulation and Real Time Applications (DS-RT)", "authors": ["Gabriele  D'Angelo", "Stefano  Ferretti", "Moreno  Marzolla", "Lorenzo  Armaroli"], "year": 2016, "n_citations": 5}
{"id": 1964567, "s2_id": "9353b888a442b5324e091e4cf76d0a0393f1ab88", "title": "A fully pipelined FPGA accelerator for scale invariant feature transform keypoint descriptor matching", "abstract": "Abstract The scale invariant feature transform (SIFT) algorithm is considered a classical feature extraction algorithm within the field of computer vision. SIFT keypoint descriptor matching is a computationally intensive process due to the amount of data consumed. In this work, we designed a novel fully pipelined hardware accelerator architecture for SIFT keypoint descriptor matching. The accelerator core was implemented and tested on a field programmable gate array (FPGA). The proposed hardware architecture is able to properly handle the memory bandwidth necessary for a fully-pipelined implementation and hits the roofline performance model, achieving the potential maximum throughput. The fully pipelined matching architecture was designed based on the consine angle distance method. Our architecture was optimized for 16-bit fixed-point operations and implemented on hardware using a Xilinx Zynq-based FPGA development board. Our proposed architecture shows a noticeable reduction of area resources compared with its counterparts in literature, while maintaining high throughput by alleviating memory bandwidth restrictions. The results show a reduction in consumed device resources of up to 91% in LUTs and 79% of BRAMs. Our hardware implementation is 15.7\u202f\u00d7\u202f faster than the comparable software approach.", "venue": "Microprocess. Microsystems", "authors": ["Luka  Daoud", "Muhammad Kamran Latif", "H S. Jacinto", "Nader  Rafla"], "year": 2020, "n_citations": 3}
{"id": 1966640, "s2_id": "4d137d45aa245b9fb772c64ba179e4975c10dc2c", "title": "Effects of Diversity and Procrastination in Priority Queuing Theory: the Different Power Law Regimes", "abstract": "Empirical analyses show that after the update of a browser, or the publication of the vulnerability of a software, or the discovery of a cyber worm, the fraction of computers still using the older browser or software version, or not yet patched, or exhibiting worm activity decays as a power law approximately 1/t(alpha) with 0<alpha<or=1 over a time scale of years. We present a simple model for this persistence phenomenon, framed within the standard priority queuing theory, of a target task which has the lowest priority compared to all other tasks that flow on the computer of an individual. We identify a \"time deficit\" control parameter beta and a bifurcation to a regime where there is a nonzero probability for the target task to never be completed. The distribution of waiting time T until the completion of the target task has the power law tail approximately 1/t(1/2), resulting from a first-passage solution of an equivalent Wiener process. Taking into account a diversity of time deficit parameters in a population of individuals, the power law tail is changed into 1/t(alpha), with alpha is an element of (0.5,infinity), including the well-known case 1/t. We also study the effect of \"procrastination,\" defined as the situation in which the target task may be postponed or delayed even after the individual has solved all other pending tasks. This regime provides an explanation for even slower apparent decay and longer persistence.", "venue": "Physical review. E, Statistical, nonlinear, and soft matter physics", "authors": ["A.  Saichev", "Didier  Sornette"], "year": 2010, "n_citations": 15}
{"id": 1972374, "s2_id": "e196e912df9513576a4718f7249ad4b68c7c9269", "title": "Rank-Aware Dynamic Migrations and Adaptive Demotions for DRAM Power Management", "abstract": "Modern DRAM architectures allow a number of low-power states on individual memory ranks for advanced power management. Many previous studies have taken advantage of demotions on low-power states for energy saving. However, most of the demotion schemes are statically performed on a limited number of pre-selected low-power states, and are suboptimal for different workloads and memory architectures. Even worse, the idle periods are often too short for effective power state transitions, especially for memory intensive applications. Wrong decisions on power state transition incur significant energy and delay penalties. In this paper, we propose a novel memory system design named RAMZzz with rank-aware energy saving optimizations including dynamic page migrations and adaptive demotions. Specifically, we group the pages with similar access locality into the same rank with dynamic page migrations. Ranks have their hotness: hot ranks are kept busy for high utilization and cold ranks can have more lengthy idle periods for power state transitions. We further develop adaptive state demotions by considering all low-power states for each rank and a prediction model to estimate the power-down timeout among states. We experimentally compare our algorithm with other energy saving policies with cycle-accurate simulation. Experiments with benchmark workloads show that RAMZzz achieves significant improvement on energy-delay2 and energy consumption over other energy saving techniques.", "venue": "IEEE Transactions on Computers", "authors": ["Yanchao  Lu", "Donghong  Wu", "Bingsheng  He", "Xueyan  Tang", "Jianliang  Xu", "Minyi  Guo"], "year": 2016, "n_citations": 9}
{"id": 1973589, "s2_id": "7c973bb2debc7fa4ffd2bfe4308b2374861eeb9a", "title": "OWL: a reliable online watcher for LTE control channel measurements", "abstract": "Reliable network measurements are a fundamental component of networking research as they enable network analysis, system debugging, performance evaluation and optimization. In particular, decoding the LTE control channel would give access to the full base station traffic at a 1 ms granularity, thus allowing for traffic profiling and accurate measurements. Although a few open-source implementations of LTE are available, they do not provide tools to reliably decoding the LTE control channel and, thus, accessing the scheduling information. In this paper, we present OWL, an Online Watcher for LTE that is able to decode all the resource blocks in more than 99% of the system frames, significantly outperforming existing non-commercial prior decoders. Compared to previous attempts, OWL grounds the decoding procedure on information obtained from the LTE random access mechanism. This makes it possible to run our software on inexpensive hardware coupled with almost any software defined radio capable of sampling the LTE signal with sufficient accuracy.", "venue": "ATC@MobiCom", "authors": ["Nicola  Bui", "J\u00f6rg  Widmer"], "year": 2016, "n_citations": 61}
{"id": 1979875, "s2_id": "a95455efdbfeee6753352be066800fe38af94ea8", "title": "ICE: An Interactive Configuration Explorer for High Dimensional Categorical Parameter Spaces", "abstract": "There are many applications where users seek to explore the impact of the settings of several categorical variables with respect to one dependent numerical variable. For example, a computer systems analyst might want to study how the type of file system or storage device affects system performance. A usual choice is the method of Parallel Sets designed to visualize multivariate categorical variables, However, we found that the magnitude of the parameter impacts on the numerical variable cannot be easily observed here. We also attempted a dimension reduction approach based on Multiple Correspondence Analysis but found that the SVD-generated 2D layout resulted in a loss of information. We hence propose a novel approach, the Interactive Configuration Explorer (ICE), which directly addresses the need of analysts to learn how the dependent numerical variable is affected by the parameter settings given multiple optimization objectives. No information is lost as ICE shows the complete distribution and statistics of the dependent variable in context with each categorical variable. Analysts can interactively filter the variables to optimize for certain goals such as achieving a system with maximum performance, low variance, etc. Our system was developed in tight collaboration with a group of systems performance researchers and its final effectiveness was evaluated with expert interviews, a comparative user study, and two case studies.", "venue": "2019 IEEE Conference on Visual Analytics Science and Technology (VAST)", "authors": ["Anjul  Tyagi", "Zhen  Cao", "Tyler  Estro", "Erez  Zadok", "Klaus  Mueller"], "year": 2019, "n_citations": 2}
{"id": 1983992, "s2_id": "8b984d607fd92b4cdf41cb8e97f1e93c04c7a98e", "title": "Power aware wireless file downloading: A constrained restless bandit approach", "abstract": "This paper treats power-aware throughput maximization in a multi-user file downloading system. Each user can receive a new file only after its previous file is finished. The file state processes for each user act as coupled Markov chains that form a generalized restless bandit system. First, an optimal algorithm is derived for the case of one user. The algorithm maximizes throughput subject to an average power constraint. Next, the one-user algorithm is extended to a low complexity heuristic for the multi-user problem. The heuristic uses a simple online index policy and its effectiveness is shown via simulation. For simple 3-user cases where the optimal solution can be computed offline, the heuristic is shown to be near-optimal for a wide range of parameters.", "venue": "2014 12th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)", "authors": ["Xiaohan  Wei", "Michael J. Neely"], "year": 2014, "n_citations": 3}
{"id": 1988621, "s2_id": "ed673e612bec2f8f458d584add83332f0c4ca411", "title": "Mixed polling with rerouting and applications", "abstract": "Queueing systems with a single server in which customers wait to be served at a finite number of distinct locations (buffers/queues) are called discrete polling systems. Polling systems in which arrivals of users occur anywhere in a continuum are called continuous polling systems. Often one encounters a combination of the two systems: the users can either arrive in a continuum or wait in a finite set (i.e. wait at a finite number of queues). We call these systems mixed polling systems. Also, in some applications, customers are rerouted to a new location (for another service) after their service is completed. In this work, we study mixed polling systems with rerouting. We obtain their steady state performance by discretization using the known pseudo conservation laws of discrete polling systems. Their stationary expected workload is obtained as a limit of the stationary expected workload of a discrete system. The main tools for our analysis are: (a) the fixed point analysis of infinite dimensional operators and; (b) the convergence of Riemann sums to an integral. We analyze two applications using our results on mixed polling systems and discuss the optimal system design. We consider a local area network, in which a moving ferry facilitates communication (data transfer) using a wireless link. We also consider a distributed waste collection system and derive the optimal collection point. In both examples, the service requests can arrive anywhere in a subset of the two dimensional plane. Namely, some users arrive in a continuous set while others wait for their service in a finite set. The only polling systems that can model these applications are mixed systems with rerouting as introduced in this manuscript.", "venue": "Perform. Evaluation", "authors": ["Veeraruna  Kavitha", "Richard  Combes"], "year": 2013, "n_citations": 4}
{"id": 1989184, "s2_id": "a5a195068af98760c6123d19f8314cd571f41f08", "title": "Performance Testing Using a Smart Reinforcement Learning-Driven Test Agent", "abstract": "Performance testing with the aim of generating an efficient and effective workload to identify performance issues is challenging. Many of the automated approaches mainly rely on analyzing system models, source code, or extracting the usage pattern of the system during the execution. However, such information and artifacts are not always available. Moreover, all the transactions within a generated workload do not impact the performance of the system the same way, a finely tuned workload could accomplish the test objective in an efficient way. Model-free reinforcement learning is widely used for finding the optimal behavior to accomplish an objective in many decision-making problems without relying on a model of the system. This paper proposes that if the optimal policy (way) for generating test workload to meet a test objective can be learned by a test agent, then efficient test automation would be possible without relying on system models or source code. We present a self-adaptive reinforcement learning-driven load testing agent, RELOAD, that learns the optimal policy for test workload generation and generates an effective workload efficiently to meet the test objective. Once the agent learns the optimal policy, it can reuse the learned policy in subsequent testing activities. Our experiments show that the proposed intelligent load test agent can accomplish the test objective with lower test cost compared to common load testing procedures, and results in higher test efficiency.", "venue": "2021 IEEE Congress on Evolutionary Computation (CEC)", "authors": ["Mahshid Helali Moghadam", "Golrokh  Hamidi", "Markus  Borg", "Mehrdad  Saadatmand", "Markus  Bohlin", "Bjorn  Lisper", "Pasqualina  Potena"], "year": 2021, "n_citations": 0}
{"id": 1991893, "s2_id": "ab7b5d224449cd2c556628c09f74e439c976b362", "title": "Exact analysis of TTL cache networks: the case of caching policies driven by stopping times", "abstract": "TTL caching models have recently regained significant research interest, largely due to their ability to fit popular caching policies such as LRU. In this extended abstract we briefly describe our recent work on two exact methods to analyze TTL cache networks. The first method generalizes existing results for line networks under renewal requests to the broad class of caching policies whereby evictions are driven by stopping times. The obtained results are further generalized, using the second method, to feedforward networks with Markov arrival processes (MAP) requests. MAPs are particularly suitable for non-line networks because they are closed not only under superposition and splitting, as known, but also under input-output caching operations as proven herein for phase-type TTL distributions. The crucial benefit of the two closure properties is that they jointly enable the first exact analysis of feedforward networks of TTL caches in great generality.", "venue": "SIGMETRICS '14", "authors": ["Daniel S. Berger", "Philipp  Gland", "Sahil  Singla", "Florin  Ciucu"], "year": 2014, "n_citations": 42}
{"id": 1996050, "s2_id": "fd4d6529991ab7b209acc7be46f4abca11f27884", "title": "Automatic Microprocessor Performance Bug Detection", "abstract": "Processor design validation and debug is a difficult and complex task, which consumes the lion\u2019s share of the design process. Design bugs that affect processor performance rather than its functionality are especially difficult to catch, particularly in new microarchitectures. This is because, unlike functional bugs, the correct processor performance of new microarchitectures on complex, long-running benchmarks is typically not deterministically known. Thus, when performance benchmarking new microarchitectures, performance teams may assume that the design is correct when the performance of the new microarchitecture exceeds that of the previous generation, despite significant performance regressions existing in the design. In this work we present a two-stage, machine learning-based methodology that is able to detect the existence of performance bugs in microprocessors. Our results show that our best technique detects 91.5% of microprocessor core performance bugs whose average IPC impact across the studied applications is greater than 1% versus a bug-free design with zero false positives. When evaluated on memory system bugs, our technique achieves 100% detection with zero false positives. Moreover, the detection is automatic, requiring very little performance engineer time.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Erick Carvajal Barboza", "Sara  Jacob", "Mahesh  Ketkar", "Michael  Kishinevsky", "Paul  Gratz", "Jiang  Hu"], "year": 2021, "n_citations": 0}
{"id": 2000410, "s2_id": "10e4d154b3b36cd83021853f81ce9850a5b868c4", "title": "On the energy efficiency of client-centric data consistency management under random read/write access to Big Data with Apache HBase", "abstract": "The total estimated energy bill for data centers in 2010 was \\$11.5 billion, and experts estimate that the energy cost of a typical data center doubles every five years. On the other hand, computational developments have started to lag behind storage advancements, therein becoming a future bottleneck for the ongoing data growth which already approaches Exascale levels. We investigate the relationship among data throughput and energy footprint on a large storage cluster, with the goal of formalizing it as a metric that reflects the trading among consistency and energy. Employing a client-centric consistency approach, and while honouring ACID properties of the chosen columnar store for the case study (Apache HBase), we present the factors involved in the energy consumption of the system as well as lessons learned to underpin further design of energy-efficient cluster scale storage systems.", "venue": "ArXiv", "authors": ["\u00c1lvaro  Garc\u00eda-Recuero"], "year": 2015, "n_citations": 0}
{"id": 2001059, "s2_id": "88761ac37505aa538e87aa26dcb18419202337bf", "title": "Mean-Payoff Optimization in Continuous-Time Markov Chains with Parametric Alarms", "abstract": "Continuous-time Markov chains with alarms (ACTMCs) allow for alarm events that can be non-exponentially distributed. Within parametric ACTMCs, the parameters of alarm-event distributions are not given explicitly and can be subject of parameter synthesis. An algorithm solving the $\\varepsilon$-optimal parameter synthesis problem for parametric ACTMCs with long-run average optimization objectives is presented. Our approach is based on reduction of the problem to finding long-run average optimal strategies in semi-Markov decision processes (semi-MDPs) and sufficient discretization of parameter (i.e., action) space. Since the set of actions in the discretized semi-MDP can be very large, a straightforward approach based on explicit action-space construction fails to solve even simple instances of the problem. The presented algorithm uses an enhanced policy iteration on symbolic representations of the action space. The soundness of the algorithm is established for parametric ACTMCs with alarm-event distributions satisfying four mild assumptions that are shown to hold for uniform, Dirac and Weibull distributions in particular, but are satisfied for many other distributions as well. An experimental implementation shows that the symbolic technique substantially improves the efficiency of the synthesis algorithm and allows to solve instances of realistic size.", "venue": "QEST", "authors": ["Christel  Baier", "Clemens  Dubslaff", "Lubos  Korenciak", "Anton\u00edn  Kucera", "Vojtech  Reh\u00e1k"], "year": 2017, "n_citations": 4}
{"id": 2001575, "s2_id": "8708039c0d1158e445913e15c5e3fc10cdb81bfc", "title": "Rule Designs for Optimal Online Game Matchmaking", "abstract": "Online games are the most popular form of entertainment among youngsters as well as elders. Recognized as e-Sports, they may become an official part of the Olympic Games by 2020. However, a long waiting time for matchmaking will largely affect players\u2019 experiences. We examine different matchmaking mechanisms for 2v2 games. By casting the mechanisms into a queueing theoretic framework, we decompose the rule design process into a sequence of decision making problems, and derive the optimal mechanism with minimum expected waiting time. We further the result by exploring additional static as well as dynamic rule designs\u2019 impacts. In the static setting, we consider the game allows players to choose sides before the battle. In the dynamic setting, we consider the game offers multiple zones for players of different skill levels. In both settings, we examine the value of choice-free players. Closed form expressions for the expected waiting time in different settings illuminate the guidelines for online game rule designs.", "venue": "2021 40th Chinese Control Conference (CCC)", "authors": ["Mingkuan  Xu", "Yang  Yu", "Chenye  Wu"], "year": 2021, "n_citations": 0}
{"id": 2001710, "s2_id": "8af40d7dee045ca2deaaa5d33922db541a693963", "title": "Energy Efficient Task Assignment in Virtualized Wireless Sensor Networks", "abstract": "Wireless Sensor Networks (WSNs) are being used extensively today in various domains. However, they are traditionally deployed with applications embedded in them which precludes their re-use for new applications. Nowadays, virtualization enables several applications on a same WSN by abstracting the physical resources (i.e. sensing capabilities) into logical ones. However, this comes at a cost, including an energy cost. It is therefore critical to ensure the efficient allocation of these resources. In this paper, we study the problem of assigning application sensing tasks to sensor devices, in virtualized WSNs. Our goal is to minimize the overall energy consumption resulting from the assignment. We focus on the static version of the problem and formulate it using Integer Linear Programming (ILP), while accounting for sensor nodes\u2019 available energy and virtualization overhead. We solve the problem over different scenarios and compare the obtained solution to the case of a traditional WSN, i.e. one with no support for virtualization. Our results show that significant energy can be saved when tasks are appropriately assigned in a WSN that supports virtualization.", "venue": "2018 IEEE Symposium on Computers and Communications (ISCC)", "authors": ["Vahid Maleki Raee", "Diala  Naboulsi", "Roch H. Glitho"], "year": 2018, "n_citations": 7}
{"id": 2003852, "s2_id": "2573e3b0651b28d17f89d3e545ba74ec2867217e", "title": "On evaluating commercial Cloud services: A systematic review", "abstract": "HighlightsWe review and synthesize the existing knowledge and experiences of evaluating commercial Cloud services.The findings identify several research gaps in the Cloud services evaluation domain.A dictionary-like reference is provided for future Cloud services evaluation work.IaaS and PaaS would serve different types of customers, and they cannot be replaced with each other.The Elasticity and Security evaluation of Cloud services could be long-term research challenges. BackgroundCloud Computing is increasingly booming in industry with many competing providers and services. Accordingly, evaluation of commercial Cloud services is necessary. However, the existing evaluation studies are relatively chaotic. There exists tremendous confusion and gap between practices and theory about Cloud services evaluation. AimTo facilitate relieving the aforementioned chaos, this work aims to synthesize the existing evaluation implementations to outline the state-of-the-practice and also identify research opportunities in Cloud services evaluation. MethodBased on a conceptual evaluation model comprising six steps, the systematic literature review (SLR) method was employed to collect relevant evidence to investigate the Cloud services evaluation step by step. ResultsThis SLR identified 82 relevant evaluation studies. The overall data collected from these studies essentially depicts the current practical landscape of implementing Cloud services evaluation, and in turn can be reused to facilitate future evaluation work. ConclusionsEvaluation of commercial Cloud services has become a world-wide research topic. Some of the findings of this SLR identify several research gaps in the area of Cloud services evaluation (e.g., Elasticity and Security evaluation of commercial Cloud services could be a long-term challenge), while some other findings suggest the trend of applying commercial Cloud services (e.g.,?compared with PaaS, IaaS seems more suitable for customers and is particularly important in industry). This SLR study itself also confirms some previous experiences and records new evidence-based software engineering (EBSE) lessons.", "venue": "J. Syst. Softw.", "authors": ["Zheng  Li", "He  Zhang", "Liam  O'Brien", "Rainbow  Cai", "Shayne  Flint"], "year": 2013, "n_citations": 62}
{"id": 2004618, "s2_id": "5ba9707af58fcf672d1d9f69056e899efa8f13d7", "title": "Evaluating impact of human errors on the availability of data storage systems", "abstract": "In this paper, we investigate the effect of incorrect disk replacement service on the availability of data storage systems. To this end, we first conduct Monte Carlo simulations to evaluate the availability of disk subsystem by considering disk failures and incorrect disk replacement service. We also propose a Markov model that corroborates the Monte Carlo simulation results. We further extend the proposed model to consider the effect of automatic disk fail-over policy. The results obtained by the proposed model show that overlooking the impact of incorrect disk replacement can result up to three orders of magnitude unavailability underestimation. Moreover, this study suggests that by considering the effect of human errors, the conventional believes about the dependability of different RAID mechanisms should be revised. The results show that in the presence of human errors, RAID1 can result in lower availability compared to RAID5.", "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017", "authors": ["Mostafa  Kishani", "Reza  Eftekhari", "Hossein  Asadi"], "year": 2017, "n_citations": 7}
{"id": 2004621, "s2_id": "c2c63b03526b558d9f9a1d5c711f3354b376eb7b", "title": "NeuroVectorizer: end-to-end vectorization with deep reinforcement learning", "abstract": "One of the key challenges arising when compilers vectorize loops for today\u2019s SIMD-compatible architectures is to decide if vectorization or interleaving is beneficial. Then, the compiler has to determine the number of instructions to pack together and the interleaving level (stride). Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions. Alternatively, software engineers often hand-write the vectorization factors of every loop. This, however, places a huge burden on them, since it requires prior experience and significantly increases the development time. In this work, we explore a novel approach for handling loop vectorization and propose an end-to-end solution using deep reinforcement learning (RL). We conjecture that deep RL can capture different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine the optimal vectorization factors. We develop an end-to-end framework, from code to vectorization, that integrates deep RL in the LLVM compiler. Our proposed framework takes benchmark codes as input and extracts the loop codes. These loop codes are then fed to a loop embedding generator that learns an embedding for these loops. Finally, the learned embeddings are used as input to a Deep RL agent, which dynamically determines the vectorization factors for all the loops. We further extend our framework to support random search, decision trees, supervised neural networks, and nearest-neighbor search. We evaluate our approaches against the currently used LLVM vectorizer and loop polyhedral optimization techniques. Our experiments show 1.29\u00d7\u22124.73\u00d7 performance speedup compared to baseline and only 3% worse than the brute-force search on a wide range of benchmarks.", "venue": "CGO", "authors": ["Ameer  Haj-Ali", "Nesreen K. Ahmed", "Theodore L. Willke", "Sophia  Shao", "Krste  Asanovic", "Ion  Stoica"], "year": 2020, "n_citations": 30}
{"id": 2007556, "s2_id": "944c78fc724d40975a7ff40a1b8368ec2445009c", "title": "Symbol Detection of Ambient Backscatter Systems With Manchester Coding", "abstract": "Ambient backscatter communication is a newly emerged paradigm, which utilizes the ambient radio frequency signal as the carrier to reduce the system battery requirement, and is regarded as a promising solution for enabling large-scale deployment of future Internet of Things networks. The key issue of ambient backscatter communication systems is how to perform reliable detection. In this paper, we propose novel encoding methods at the information tag and devise the corresponding symbol detection methods at the reader. In particular, Manchester coding and differential Manchester coding are adopted at the information tag, and the corresponding semi-coherent Manchester (SeCoMC) and non-coherent Manchester (NoCoMC) detectors are developed. In addition, analytical bit-error-rate (BER) expressions are characterized for both detectors assuming either complex Gaussian or unknown deterministic ambient signal. Simulation results show that the BER performance of unknown deterministic ambient signal is better, and the SeCoMC detector outperforms the NoCoMC detector. Finally, compared with the prior detectors for ambient backscatter communications, the proposed detectors have the advantages of achieving superior BER performance with lower communication delay.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Qin  Tao", "Caijun  Zhong", "Hai  Lin", "Zhaoyang  Zhang"], "year": 2018, "n_citations": 51}
{"id": 2007627, "s2_id": "6ed100ac87367fcaf1285c496575b77fddbe98b5", "title": "Super-Exponential Solution in Markovian Supermarket Models: Framework and Challenge", "abstract": "Marcel F. Neuts opened a key door in numerical computation of stochastic models by means of phase-type (PH) distributions and Markovian arrival processes (MAPs). To celebrate his 75th birthday, this paper reports a more general framework of Markovian supermarket models, including a system of differential equations for the fraction measure and a system of nonlinear equations for the fixed point. To understand this framework heuristically, this paper gives a detailed analysis for three important supermarket examples: M/G/1 type, GI/M/1 type and multiple choices, explains how to derive the system of differential equations by means of density-dependent jump Markov processes, and shows that the fixed point may be simply super-exponential through solving the system of nonlinear equations. Note that supermarket models are a class of complicated queueing systems and their analysis can not apply popular queueing theory, it is necessary in the study of supermarket models to summarize such a more general framework which enables us to focus on important research issues. On this line, this paper develops matrix-analytical methods of Markovian supermarket models. We hope this will be able to open a new avenue in performance evaluation of supermarket models by means of matrix-analytical methods.", "venue": "ArXiv", "authors": ["Quan-Lin  Li"], "year": 2011, "n_citations": 5}
{"id": 2008367, "s2_id": "7eec96cff72fb7ab821ea7c7a4e8ae315c487b21", "title": "Optimizing Graph Processing and Preprocessing with Hardware Assisted Propagation Blocking", "abstract": "Extensive prior research has focused on alleviating the characteristic poor cache locality of graph analytics workloads. However, graph pre-processing tasks remain relatively unexplored. In many important scenarios, graph pre-processing tasks can be as expensive as the downstream graph analytics kernel. We observe that Propagation Blocking (PB), a software optimization designed for SpMV kernels, generalizes to many graph analytics kernels as well as common pre-processing tasks. In this work, we identify the lingering inefficiencies of a PB execution on conventional multicores and propose architecture support to eliminate PB's bottlenecks, further improving the performance gains from PB. Our proposed architecture -- COBRA -- optimizes the PB execution of both graph processing and pre-processing alike to provide end-to-end speedups of up to 4.6x (3.5x on average).", "venue": "ArXiv", "authors": ["Vignesh  Balaji", "Brandon  Lucia"], "year": 2020, "n_citations": 0}
{"id": 2012183, "s2_id": "961cb0721fb7fe99bdef515d23443291136637a9", "title": "Accelerating Data Loading in Deep Neural Network Training", "abstract": "Data loading can dominate deep neural network training time on large-scale systems. We present a comprehensive study on accelerating data loading performance in large-scale distributed training. We first identify performance and scalability issues in current data loading implementations. We then propose optimizations that utilize CPU resources to the data loader design. We use an analytical model to characterize the impact of data loading on the overall training time and establish the performance trend as we scale up distributed training. Our model suggests that I/O rate limits the scalability of distributed training, which inspires us to design a locality-aware data loading method. By utilizing software caches, our method can drastically reduce the data loading communication volume in comparison with the original data loading implementation. Finally, we evaluate the proposed optimizations with various experiments. We achieved more than 30x speedup in data loading using 256 nodes with 1,024 learners.", "venue": "2019 IEEE 26th International Conference on High Performance Computing, Data, and Analytics (HiPC)", "authors": ["Chih-Chieh  Yang", "Guojing  Cong"], "year": 2019, "n_citations": 9}
{"id": 2015236, "s2_id": "b1a3a8f8bc8a29cde21ce409ca5e03edea1a675a", "title": "Analysis and Design of Distributed MIMO Networks with a Wireless Fronthaul", "abstract": "We consider the analysis and design of distributed wireless networks wherein remote radio heads (RRHs) coordinate transmissions to serve multiple users on the same resource block (RB). Specifically, we analyze two possible multiple-input multiple-output wireless fronthaul solutions: multicast and zero forcing (ZF) beamforming. We develop a statistical model for the fronthaul rate and, coupled with an analysis of the user access rate, we optimize the placement of the RRHs. This model allows us to formulate the location optimization problem with a statistical constraint on fronthaul outage. Our results are cautionary, showing that the fronthaul requires considerable bandwidth to enable joint service to users. This requirement can be relaxed by serving a low number of users on the same RB. Additionally, we show that, with a fixed number of antennas, for the multicast fronthaul, it is prudent to concentrate these antennas on a few RRHs. However, for the ZF beamforming fronthaul, it is better to distribute the antennas on more RRHs. For the parameters chosen, using a ZF beamforming fronthaul improves the typical access rate by approximately 8% compared to multicast. Crucially, our work quantifies the effect of these fronthaul solutions and provides an effective tool for the design of distributed networks.", "venue": "IEEE Transactions on Communications", "authors": ["Hussein A. Ammar", "Raviraj  Adve", "Shahram  Shahbazpanahi", "Gary  Boudreau"], "year": 2021, "n_citations": 1}
{"id": 2016031, "s2_id": "89bd25ef3eaa8b1eaabee0816a70b930720db953", "title": "Cross-Platform Performance Portability Using Highly Parametrized SYCL Kernels", "abstract": "Over recent years heterogeneous systems have become more prevalent across HPC systems, with over 100 supercomputers in the TOP500 incorporating GPUs or other accelerators. These hardware platforms have different performance characteristics and optimization requirements. In order to make the most of multiple accelerators a developer has to provide implementations of their algorithms tuned for each device. Hardware vendors provide libraries targeting their devices specifically, which provide good performance but frequently have different API designs, hampering portability. \nThe SYCL programming model allows users to write heterogeneous programs using completely standard C++, and so developers have access to the power of C++ templates when developing compute kernels. In this paper we show that by writing highly parameterized kernels for matrix multiplies and convolutions we achieve performance competitive with vendor implementations across different architectures. Furthermore, tuning for new devices amounts to choosing the combinations of kernel parameters that perform best on the hardware.", "venue": "ArXiv", "authors": ["John  Lawson", "Mehdi  Goli", "Duncan  McBain", "Daniel  Soutar", "Louis  Sugy"], "year": 2019, "n_citations": 4}
{"id": 2017465, "s2_id": "448285842dadcc3689d1741f388d7eb6cef94aec", "title": "A prediction packetizing scheme for reducing channel traffic in transaction-level hardware/software co-emulation", "abstract": "The paper presents a scheme for efficient channel usage between simulator and accelerator, where the accelerator models some RTL sub-blocks in the accelerator-based hardware/software co-simulation while the simulator runs a transaction-level model of the remaining part of the whole chip being verified. With conventional simulation, accelerator evaluations of simulator and accelerator alternate at every valid simulation time, which results in poor simulation performance due to the startup overhead of simulator-accelerator channel access. The startup overhead can be reduced by merging multiple transactions on the channel into a single burst traffic. We propose a predictive packetizing scheme for reducing channel traffic by merging as many transactions into a burst traffic as possible based on 'prediction and rollback'. Under ideal conditions with 100% prediction accuracy, the proposed method shows a performance gain of 1500% compared to the conventional one.", "venue": "Design, Automation and Test in Europe", "authors": ["Jae-Gon  Lee", "Moo-Kyoung  Chung", "Ki-Yong  Ahn", "Sang-Heon  Lee", "Chong-Min  Kyung"], "year": 2005, "n_citations": 7}
{"id": 2018408, "s2_id": "51dd36629ca04a53d995f964f257937a43aa063c", "title": "TOFEC: Achieving optimal throughput-delay trade-off of cloud storage using erasure codes", "abstract": "Our paper presents solutions using erasure coding, parallel connections to storage cloud and limited chunking (i.e., dividing the object into a few smaller segments) together to significantly improve the delay performance of uploading and downloading data in and out of cloud storage. TOFEC is a strategy that helps front-end proxy adapt to level of workload by treating scalable cloud storage (e.g. Amazon S3) as a shared resource requiring admission control. Under light workloads, TOFEC creates more smaller chunks and uses more parallel connections per file, minimizing service delay. Under heavy workloads, TOFEC automatically reduces the level of chunking (fewer chunks with increased size) and uses fewer parallel connections to reduce overhead, resulting in higher throughput and preventing queueing delay. Our trace-driven simulation results show that TOFEC's adaptation mechanism converges to an appropriate code that provides the optimal delay-throughput trade-off without reducing system capacity. Compared to a non-adaptive strategy optimized for throughput, TOFEC delivers 2.5\u00d7 lower latency under light workloads; compared to a non-adaptive strategy optimized for latency, TOFEC can scale to support over 3\u00d7 as many requests.", "venue": "IEEE INFOCOM 2014 - IEEE Conference on Computer Communications", "authors": ["Guanfeng  Liang", "Ulas C. Kozat"], "year": 2014, "n_citations": 68}
{"id": 2021773, "s2_id": "6faa80cae754cc3a57dbde127a83d5987104fb7e", "title": "Towards Million-Server Network Simulations on Just a Laptop", "abstract": "The growing size of data center and HPC networks pose unprecedented requirements on the scalability of simulation infrastructure. The ability to simulate such large-scale interconnects on a simple PC would facilitate research efforts. Unfortunately, as we first show in this work, existing shared-memory packet-level simulators do not scale to the sizes of the largest networks considered today. We then illustrate a feasibility analysis and a set of enhancements that enable a simple packet-level htsim simulator to scale to the unprecedented simulation sizes on a single PC. Our code is available online and can be used to design novel schemes in the coming era of omnipresent data centers and HPC clusters.", "venue": "ArXiv", "authors": ["Maciej  Besta", "Marcel  Schneider", "Salvatore Di Girolamo", "Ankit  Singla", "Torsten  Hoefler"], "year": 2021, "n_citations": 0}
{"id": 2023263, "s2_id": "8a5fe6b6ed9f0e3c19ad2550b6b15864a01917d6", "title": "Patterns and Rewrite Rules for Systematic Code Generation (From High-Level Functional Patterns to High-Performance OpenCL Code)", "abstract": "Computing systems have become increasingly complex with the emergence of heterogeneous hardware combining multicore CPUs and GPUs. These parallel systems exhibit tremendous computational power at the cost of increased programming effort. This results in a tension between achieving performance and code portability. Code is either tuned using device-specific optimizations to achieve maximum performance or is written in a high-level language to achieve portability at the expense of performance. \nWe propose a novel approach that offers high-level programming, code portability and high-performance. It is based on algorithmic pattern composition coupled with a powerful, yet simple, set of rewrite rules. This enables systematic transformation and optimization of a high-level program into a low-level hardware specific representation which leads to high performance code. \nWe test our design in practice by describing a subset of the OpenCL programming model with low-level patterns and by implementing a compiler which generates high performance OpenCL code. Our experiments show that we can systematically derive high-performance device-specific implementations from simple high-level algorithmic expressions. The performance of the generated OpenCL code is on par with highly tuned implementations for multicore CPUs and GPUs written by experts", "venue": "ArXiv", "authors": ["Michel  Steuwer", "Christian  Fensch", "Christophe  Dubach"], "year": 2015, "n_citations": 3}
{"id": 2023461, "s2_id": "c0058a86d8c613a9340720ff59db7a4b8793d077", "title": "Highly efficient lattice Boltzmann multiphase simulations of immiscible fluids at high-density ratios on CPUs and GPUs through code generation", "abstract": "A high-performance implementation of a multiphase lattice Boltzmann method based on the conservative Allen-Cahn model supporting high-density ratios and high Reynolds numbers is presented. Meta-programming techniques are used to generate optimized code for CPUs and GPUs automatically. The coupled model is specified in a high-level symbolic description and optimized through automatic transformations. The memory footprint of the resulting algorithm is reduced through the fusion of compute kernels. A roofline analysis demonstrates the excellent efficiency of the generated code on a single GPU. The resulting single GPU code has been integrated into the multiphysics framework waLBerla to run massively parallel simulations on large domains. Communication hiding and GPUDirect-enabled MPI yield near-perfect scaling behavior. Scaling experiments are conducted on the Piz Daint supercomputer with up to 2048 GPUs, simulating several hundred fully resolved bubbles. Further, validation of the implementation is shown in a physically relevant scenario\u2014a three-dimensional rising air bubble in water.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Markus  Holzer", "Martin  Bauer", "Harald  K\u00f6stler", "Ulrich  R\u00fcde"], "year": 2021, "n_citations": 4}
{"id": 2029028, "s2_id": "eccf781fca0f1b8bc415a0c71bff14fcdc13df39", "title": "Performance Analysis of Single-Cell Adaptive Data Rate-Enabled LoRaWAN", "abstract": "LoRaWAN enables massive connectivity for Internet-of-Things applications. Many published works employ stochastic geometry to derive outage models of LoRaWAN over fading channels assuming fixed transmit power and distance-based spreading factor (SF) allocation. However, in practice, LoRaWAN employs the Adaptive Data Rate (ADR) mechanism, which dynamically adjusts SF and transmit power of nodes based on channel state. The community addressed the performance of ADR using simulations, but analytical models have not been introduced. In this letter, we seek to close this gap. We build over an analytical LoRaWAN model to consider the performance of steady-state ADR-enabled LoRaWAN. We derive outage expressions and an optimization procedure to maximize the number of users under reliability constraints. Results show that power allocation reduces interference and improves network capacity while reducing average power.", "venue": "IEEE Wireless Communications Letters", "authors": ["Arliones  Hoeller", "Richard Demo Souza", "Samuel  Montejo-S\u00e1nchez", "Hirley  Alves"], "year": 2020, "n_citations": 3}
{"id": 2033021, "s2_id": "4b89ef9f818c37cdc77f26458fe3a9481f3d21b4", "title": "Accelerating a Cloud-Based Software GNSS Receiver", "abstract": "This paper discusses ways to reduce the execution time of a software Global Navigation Satellite System (GNSS) receiver that is meant for offline operation in a cloud environment. Client devices register satellite signals they receive, and send them to the cloud, to be processed by this software. The goal of this project is for each client request to be processed as fast as possible, but also to increase total system throughput by making sure as many requests as possible are processed within a unit of time. The characteristics of the application provided both opportunities and challenges for increasing performance. This paper describes the speedups we obtained by enabling the software to exploit multi-core CPUs and GPGPUs. It mentions which techniques worked and which did not. To increase throughput, it describes how to control the resources allocated for each invocation of the software to process a client request, such that multiple copies of the application can run at the same time. It uses the notion of the effective running time to measure the system's throughput when running multiple instances at the same time, and show how to determine when the system's computing resources have been saturated.", "venue": "Int. J. Grid High Perform. Comput.", "authors": ["Kamran  Karimi", "Aleks G. Pamir", "M. Haris Afzal"], "year": 2014, "n_citations": 5}
{"id": 2035179, "s2_id": "d4030eab27b46ebce7d96e92df35562b36f30c14", "title": "Machine Learning Based Routing Congestion Prediction in FPGA High-Level Synthesis", "abstract": "High-level synthesis (HLS) shortens the development time of hardware designs and enables faster design space exploration at a higher abstraction level. Optimization of complex applications in HLS is challenging due to the effects of implementation issues such as routing congestion. Routing congestion estimation is absent or inaccurate in existing HLS design methods and tools. Early and accurate congestion estimation is of great benefit to guide the optimization in HLS and improve the efficiency of implementation. However, routability, a serious concern in FPGA designs, has been difficult to evaluate in HLS without analyzing post-implementation details after Place and Route. To this end, we propose a novel method to predict routing congestion in HLS using machine learning and map the expected congested regions in the design to the relevant high-level source code. This is greatly beneficial in early identification of routability oriented bottlenecks in the high-level source code without running time-consuming register-transfer level (RTL) implementation flow. Experiments demonstrate that our approach accurately estimates vertical and horizontal routing congestion with errors of 6.71% and 10.05% respectively. By presenting Face Detection application as a case study, we show that by discovering the bottlenecks in high-level source code, routing congestion can be easily and quickly resolved compared to the efforts involved in RTL level implementation and design feedback.", "venue": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Jieru  Zhao", "Tingyuan  Liang", "Sharad  Sinha", "Wei  Zhang"], "year": 2019, "n_citations": 12}
{"id": 2035208, "s2_id": "4796c13f152251f962f1edae41ea53372e079221", "title": "nanoBench: A Low-Overhead Tool for Running Microbenchmarks on x86 Systems", "abstract": "We present nanoBench, a tool for evaluating small microbenchmarks using hardware performance counters on Intel and AMD x86 systems. Most existing tools and libraries are intended to either benchmark entire programs, or program segments in the context of their execution within a larger program. In contrast, nanoBench is specifically designed to evaluate small, isolated pieces of code. Such code is common in microbenchmark-based hardware analysis techniques. Unlike previous tools, nanoBench can execute microbenchmarks directly in kernel space. This allows to benchmark privileged instructions, and it enables more accurate measurements. The reading of the performance counters is implemented with minimal overhead avoiding functions calls and branches. As a consequence, nanoBench is precise enough to measure individual memory accesses. We illustrate the utility of nanoBench at the hand of two case studies. First, we briefly discuss how nanoBench has been used to determine the latency, throughput, and port usage of more than 13,000 instruction variants on recent x86 processors. Second, we show how to generate microbenchmarks to precisely characterize the cache architectures of eleven Intel Core microarchitectures. This includes the most comprehensive analysis of the employed cache replacement policies to date.", "venue": "2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Andreas  Abel", "Jan  Reineke"], "year": 2020, "n_citations": 21}
{"id": 2036283, "s2_id": "0858e5e7aac3bae09149a501d2e7ba45942f776b", "title": "A Unified, Hardware-Fitted, Cross-GPU Performance Model", "abstract": "We present a mechanism to symbolically gather performance-relevant operation counts from numerically-oriented subprograms (`kernels') expressed in the Loopy programming system, and apply these counts in a simple, linear model of kernel run time. We use a series of `performance-instructive' kernels to fit the parameters of a unified model to the performance characteristics of GPU hardware from multiple hardware generations and vendors. We evaluate the predictive power of the model on a broad array of computational kernels relevant to scientific computing. In terms of the geometric mean, our simple, vendor- and GPU-type-independent model achieves relative accuracy comparable to that of previously published work using hardware specific models.", "venue": "ArXiv", "authors": ["James  Stevens", "Andreas  Kl\u00f6ckner"], "year": 2016, "n_citations": 1}
{"id": 2038896, "s2_id": "60810570c3d7d7d3fa460260ebe7482a41557489", "title": "AnySeq: A High Performance Sequence Alignment Library based on Partial Evaluation", "abstract": "Sequence alignments are fundamental to bioinformatics which has resulted in a variety of optimized implementations. Unfortunately, the vast majority of them are hand-tuned and specific to certain architectures and execution models. This not only makes them challenging to understand and extend, but also difficult to port to other platforms. We present AnySeq\u2014a novel library for computing different types of pairwise alignments of DNA sequences. Our approach combines high performance with an intuitively understandable implementation, which is achieved through the concept of partial evaluation. Using the AnyDSL compiler framework, AnySeq enables the compilation of algorithmic variants that are highly optimized for specific usage scenarios and hardware targets with a single, uniform codebase. The resulting domain-specific library thus allows the variation of alignment parameters (such as alignment type, scoring scheme, and traceback vs. plain score) by simple function composition rather than metaprogramming techniques which are often hard to understand. Our implementation supports multithreading and SIMD vectorization on CPUs, CUDA-enabled GPUs, and FPGAs. AnySeq is at most 7% slower and in many cases faster (up to 12%) than state-of-the art manually optimized alignment libraries on CPUs (SeqAn) and on GPUs (NVBio).", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Andr\u00e9  M\u00fcller", "Bertil  Schmidt", "Andreas  Hildebrandt", "Richard  Membarth", "Roland  Lei\u00dfa", "Matthis  Kruse", "Sebastian  Hack"], "year": 2020, "n_citations": 2}
{"id": 2040199, "s2_id": "eb1f10994c559cfa9073a2e46f3eaa4b99d73bd9", "title": "On the Performance of Network Parallel Training in Artificial Neural Networks", "abstract": "Artificial Neural Networks (ANNs) have received increasing attention in recent years with applications that span a wide range of disciplines including vital domains such as medicine, network security and autonomous transportation. However, neural network architectures are becoming increasingly complex and with an increasing need to obtain real-time results from such models, it has become pivotal to use parallelization as a mechanism for speeding up network training and deployment. In this work we propose an implementation of Network Parallel Training through Cannon's Algorithm for matrix multiplication. We show that increasing the number of processes speeds up training until the point where process communication costs become prohibitive; this point varies by network complexity. We also show through empirical efficiency calculations that the speedup obtained is superlinear.", "venue": "ArXiv", "authors": ["Ludvig  Ericson", "Rendani  Mbuvha"], "year": 2017, "n_citations": 8}
{"id": 2043338, "s2_id": "9df70e6f3d578c17ea95c33f47932ab676dcb5c2", "title": "To use or not to use: CPUs' cache optimization techniques on GPGPUs", "abstract": "General Purpose Graphic Processing Unit(GPGPU) is used widely for achieving high performance or high throughput in parallel programming. This capability of GPGPUs is very famous in the new era and mostly used for scientific computing which requires more processing power than normal personal computers. Therefore, most of the programmers, researchers and industry use this new concept for their work. However, achieving high-performance or high-throughput using GPGPUs are not an easy task compared with conventional programming concepts in the CPU side. In this research, the CPUs cache memory optimization techniques have been adopted to the GPGPUs cache memory to identify rare performance improvement techniques compared to GPGPU's best practices. The cache optimization techniques of blocking, loop fusion, array merging and array transpose were tested on GPGPUs for finding suitability of these techniques. Finally, we identified that some of the CPU cache optimization techniques go well with the cache memory system of the GPGPU and shows performance improvements while some others show the opposite effect on the GPGPUs compared with the CPUs.", "venue": "2016 IEEE International Conference on Information and Automation for Sustainability (ICIAfS)", "authors": ["Vajira  Thambawita", "Roshan G. Ragel", "Dhammika  Elkaduwe"], "year": 2016, "n_citations": 0}
{"id": 2044303, "s2_id": "29995068ad72bb20f2af2392aae43e745db8cb60", "title": "An Overview of Low latency for Wireless Communications: an Evolutionary Perspective", "abstract": "Ultra-low latency supported by the fifth generation (5G) give impetus to the prosperity of many wireless network applications, such as autonomous driving, robotics, telepresence, virtual reality and so on. Ultra-low latency is not achieved in a moment, but requires long-term evolution of network structure and key enabling communication technologies. In this paper, we provide an evolutionary overview of low latency in mobile communication systems, including two different evolutionary perspectives: 1) network architecture; 2) physical layer air interface technologies. We firstly describe in detail the evolution of communication network architecture from the second generation (2G) to 5G, highlighting the key points reducing latency. Moreover, we review the evolution of key enabling technologies in the physical layer from 2G to 5G, which is also aimed at reducing latency. We also discussed the challenges and future research directions for low latency in network architecture and physical layer.", "venue": "ArXiv", "authors": ["Xin  Fan", "Yan  Huo"], "year": 2021, "n_citations": 0}
{"id": 2044656, "s2_id": "08937c92f31895e16af48de1c7d18eeceef11f6f", "title": "Enabling highly-scalable remote memory access programming with MPI-3 one sided", "abstract": "Modern interconnects offer remote direct memory access (RDMA) features. Yet, most applications rely on explicit message passing for communications albeit their unwanted overheads. The MPI-3.0 standard defines a programming interface for exploiting RDMA networks directly, however, it's scalability and practicability has to be demonstrated in practice. In this work, we develop scalable bufferless protocols that implement the MPI-3.0 specification. Our protocols support scaling to millions of cores with negligible memory consumption while providing highest performance and minimal overheads. To arm programmers, we provide a spectrum of performance models for all critical functions and demonstrate the usability of our library and models with several application studies with up to half a million processes. We show that our design is comparable to, or better than UPC and Fortran Coarrays in terms of latency, bandwidth, and message rate. We also demonstrate application performance improvements with comparable programming complexity.", "venue": "2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)", "authors": ["Robert  Gerstenberger", "Maciej  Besta", "Torsten  Hoefler"], "year": 2013, "n_citations": 88}
{"id": 2045063, "s2_id": "49c575a7a26020cb5c22be4d116ed4d234963662", "title": "Reliable access to massive restricted texts: Experience\u2010based evaluation", "abstract": "Libraries are seeing growing numbers of digitized textual corpora that frequently come with restrictions on their content. Computational analysis corpora that are large, while of interest to scholars, can be cumbersome because of the combination of size, granularity of access, and access restrictions. Efficient management of such a collection for general access especially under failures depends on the primary storage system. In this paper, we identify the requirements of managing for computational analysis a massive text corpus and use it as basis to evaluate candidate storage solutions. The study based on the 5.9 billion page collection of the HathiTrust digital library. Our findings led to the choice of Cassandra 3.x for the primary back end store, which is currently in deployment in the HathiTrust Research Center.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Zong  Peng", "Beth  Plale"], "year": 2020, "n_citations": 1}
{"id": 2046074, "s2_id": "1d2316cfcc9f62f0c70171978d629b26d27eff4e", "title": "Automated system performance testing at MongoDB", "abstract": "Distributed Systems Infrastructure (DSI) is MongoDB's framework for running fully automated system performance tests in our Continuous Integration (CI) environment. To run in CI it needs to automate everything end-to-end: provisioning and deploying multinode clusters, executing tests, tuning the system for repeatable results, and collecting and analyzing the results. Today DSI is MongoDB's most used and most useful performance testing tool. It runs almost 200 different benchmarks in daily CI, and we also use it for manual performance investigations. As we can alert the responsible engineer in a timely fashion, all but one of the major regressions were fixed before the 4.2.0 release. We are also able to catch net new improvements, of which DSI caught 17. We open sourced DSI in March 2020.", "venue": "DBTest@SIGMOD", "authors": ["Henrik  Ingo", "David  Daly"], "year": 2020, "n_citations": 3}
{"id": 2046383, "s2_id": "4bfdf10c62489338aecb87af790b87b8ad9b068c", "title": "Performance of Devito on HPC-Optimised ARM Processors", "abstract": "We evaluate the performance of Devito, a domain specific language (DSL) for finite differences on Arm ThunderX2 processors. Experiments with two common seismic computational kernels demonstrate that Arm processors can deliver competitive performance compared to other Intel Xeon processors.", "venue": "ArXiv", "authors": ["Hermes  Senger", "Jaime F. de Souza", "Edson S. Gomi", "Fabio  Luporini", "Gerard J. Gorman"], "year": 2019, "n_citations": 0}
{"id": 2047454, "s2_id": "8b1f000673f473316cc1db91416dae1b16a73990", "title": "Barriers towards no-reference metrics application to compressed video quality analysis: on the example of no-reference metric NIQE", "abstract": "This paper analyses the application of no-reference metric NIQE to the task of video-codec comparison. A number of issues in the metric behavior on videos was detected and described. The metric has outlying scores on black and solid-colored frames. The proposed averaging technique for metric quality scores helped to improve the results in some cases. Also, NIQE has low-quality scores for videos with detailed textures and higher scores for videos of lower bit rates due to the blurring of these textures after compression. Although NIQE showed natural results for many tested videos, it is not universal and currently can\u2019t be used for video-codec comparisons.", "venue": "GraphiCon'2019 Proceedings. Volume 2", "authors": ["Anastasia  Antsiferova", "Dmitriy  Kulikov", "Denis  Kondranin", "Dmitriy  Vatolin"], "year": 2019, "n_citations": 2}
{"id": 2048712, "s2_id": "ef567ab5d454ed3d22ffe209b99ea2e704ac5ae9", "title": "A comparison of five probabilistic view-size estimation techniques in OLAP", "abstract": "A data warehouse cannot materialize all possible views, hence we must estimate quickly, accurately, and reliably the size of views to determine the best candidates for materialization. Many available techniques for view-size estimation make particular statistical assumptions and their error can be large. Comparatively, unassuming probabilistic techniques are slower, but they estimate accurately and reliability very large view sizes using little memory. We compare five unassuming hashing-based view-size estimation techniques including Stochastic Probabilistic Counting and LogLog Probabilistic Counting. Our experiments show that only Generalized Counting, Gibbons-Tirthapura, and Adaptive Counting provide universally tight estimates irrespective of the sizeof the view; of those, only Adaptive Counting remains constantly fast as we increasethe memory budget.", "venue": "DOLAP '07", "authors": ["Kamel  Aouiche", "Daniel  Lemire"], "year": 2007, "n_citations": 24}
{"id": 2055355, "s2_id": "6ffeeb508dc2797f601d2c6d815f164a9e78952d", "title": "On Time Synchronization Issues in Time-Sensitive Networks with Regulators and Nonideal Clocks", "abstract": "Flow reshaping is used in time-sensitive networks (as in the context of IEEE TSN and IETF Detnet) in order to reduce burstiness inside the network and to support the computation of guaranteed latency bounds. This is performed using per-flow regulators (such as the Token Bucket Filter) or interleaved regulators (as with IEEE TSN Asynchronous Traffic Shaping, ATS). The former use one FIFO queue per flow, whereas the latter use one FIFO queue per input port. Both types of regulators are beneficial as they cancel the increase of burstiness due to multiplexing inside the network. It was demonstrated, by using network calculus, that they do not increase the worst-case latency. However, the properties of regulators were established assuming that time is perfect in all network nodes. In reality, nodes use local, imperfect clocks. Time-sensitive networks exist in two flavours: (1) in non-synchronized networks, local clocks run independently at every node and their deviations are not controlled and (2) in synchronized networks, the deviations of local clocks are kept within very small bounds using for example a synchronization protocol (such as PTP) or a satellite based geo-positioning system (such as GPS). We revisit the properties of regulators in both cases. In non-synchronized networks, we show that ignoring the timing inaccuracies can lead to network instability due to unbounded delay in per-flow or interleaved regulators. We propose and analyze two methods (rate and burst cascade, and asynchronous dual arrival-curve method) for avoiding this problem. In synchronized networks, we show that there is no instability with per-flow regulators but, surprisingly, interleaved regulators can lead to instability. To establish these results, we develop a new framework that captures industrial requirements on clocks in both non-synchronized and synchronized networks, and we develop a toolbox that extends network calculus to account for clock imperfections.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Ludovic  Thomas", "Jean-Yves Le Boudec"], "year": 2020, "n_citations": 4}
{"id": 2055556, "s2_id": "2147fc73dc3672cb7c47ec704c0c4894d688fc1a", "title": "A Combined LIFO-Priority Scheme for Overload Control of E-commerce Web Servers", "abstract": "E-commerce Web-servers often face overload conditions during which revenue-generating requests may be dropped or abandoned due to an increase in the browsing requests. In this paper we present a simple, yet effective, mechanism for overload control of E-commerce Web-servers. We develop an E-commerce workload model that separates the browsing requests from revenue-generating transaction requests. During overload, we apply LIFO discipline in the browsing queues and use a dynamic priority model to service them. The transaction queues are given absolute priority over the browsing queues. This is called the LIFO-Pri scheduling discipline. Experimental results show that LIFO-Pri dramatically improves the overall Web-server throughput while also increasing the completion rate of revenue-generating requests. The Web-server was able to operate at nearly 60% of its maximum capacity even when offered load was 1.5 times its capacity. Further, when compared to a single queue FIFO system, there was a seven-fold increase in the number of completed revenue-generating requests during overload.", "venue": "ArXiv", "authors": ["Naresh  Singhmar", "Vipul  Mathur", "Varsha  Apte", "D.  Manjunath"], "year": 2006, "n_citations": 29}
{"id": 2058291, "s2_id": "fcc6e40a55656015dfaaa08d8dd8cac8d7da9120", "title": "Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients", "abstract": "Applying differentiable programming techniques and machine learning algorithms to foreign programs requires developers to either rewrite their code in a machine learning framework, or otherwise provide derivatives of the foreign code. This paper presents Enzyme, a high-performance automatic differentiation (AD) compiler plugin for the LLVM compiler framework capable of synthesizing gradients of statically analyzable programs expressed in the LLVM intermediate representation (IR). Enzyme synthesizes gradients for programs written in any language whose compiler targets LLVM IR including C, C++, Fortran, Julia, Rust, Swift, MLIR, etc., thereby providing native AD capabilities in these languages. Unlike traditional source-to-source and operator-overloading tools, Enzyme performs AD on optimized IR. On a machine-learning focused benchmark suite including Microsoft's ADBench, AD on optimized IR achieves a geometric mean speedup of 4.5x over AD on IR before optimization allowing Enzyme to achieve state-of-the-art performance. Packaging Enzyme for PyTorch and TensorFlow provides convenient access to gradients of foreign code with state-of-the art performance, enabling foreign code to be directly incorporated into existing machine learning workflows.", "venue": "NeurIPS", "authors": ["William S. Moses", "Valentin  Churavy"], "year": 2020, "n_citations": 6}
{"id": 2060203, "s2_id": "7948ef2dd24aac811f3177eb3b6bad2de6ba8703", "title": "Low-Precision Hardware Architectures Meet Recommendation Model Inference at Scale", "abstract": "Tremendous success of machine learning (ML) and the unabated growth in model complexity motivated many ML-specific designs in hardware architectures to speed up the model inference. While these architectures are diverse, highly optimized low-precision arithmetic is a component shared by most. Nevertheless, recommender systems important to Facebook\u2019s personalization services are demanding and complex: They must serve billions of users per month responsively with low latency while maintaining high prediction accuracy. Do these low-precision architectures work well with our production recommendation systems? They do. But not without significant effort. In this article, we share our search strategies to adapt reference recommendation models to low-precision hardware, our optimization of low-precision compute kernels, and the tool chain to maintain our models\u2019 accuracy throughout their lifespan. We believe our lessons from the trenches can promote better codesign between hardware architecture and software engineering, and advance the state of the art of ML in industry.", "venue": "IEEE Micro", "authors": ["Zhaoxia  Deng", "Jongsoo  Park", "Ping Tak Peter Tang", "Haixin  Liu", "Jie  Yang", "Hector  Yuen", "Jianyu  Huang", "Daya  Khudia", "Xiaohan  Wei", "Ellie  Wen", "Dhruv  Choudhary", "Raghuraman  Krishnamoorthi", "Carole-Jean  Wu", "Satish  Nadathur", "Changkyu  Kim", "Maxim  Naumov", "Sam  Naghshineh", "Mikhail  Smelyanskiy"], "year": 2021, "n_citations": 1}
{"id": 2062014, "s2_id": "8c263fa2c38e26e83306842b984803907b3b900c", "title": "Computing the k-coverage of a wireless network", "abstract": "Coverage is one of the main quality of service of a wireless network. k-coverage, that is to be covered simultaneously by k network nodes, is synonym of reliability and numerous applications such as multiple site MIMO features, or handovers. We introduce here a new algorithm for computing the k-coverage of a wireless network. Our method is based on the observation that k-coverage can be interpreted as k layers of 1-coverage, or simply coverage. We use simplicial homology to compute the network's topology and a reduction algorithm to indentify the layers of 1-coverage. We provide figures and simulation results to illustrate our algorithm.", "venue": "VALUETOOLS", "authors": ["Ana\u00efs  Vergne", "Laurent  Decreusefond", "Philippe  Martins"], "year": 2019, "n_citations": 1}
{"id": 2063427, "s2_id": "0823fd577c503166606fee8d96224e7a437a747b", "title": "Stability of Finite Population ALOHA with Variable Packets", "abstract": "ALOHA is one of the most basic Medium Access Control (MAC) protocols and represents a foundation for other more sophisticated distributed and asynchronous MAC protocols, e.g., CSMA. In this paper, unlike in the traditional work that focused on mean value analysis, we study the distributional properties of packet transmission delays over an ALOHA channel. We discover a new phenomenon showing that a basic finite population ALOHA model with variable size (exponential) packets is characterized by power law transmission delays, possibly even resulting in zero throughput. These results are in contrast to the classical work that shows exponential delays and positive throughput for finite population ALOHA with fixed packets. Furthermore, we characterize a new stability condition that is entirely derived from the tail behavior of the packet and backoff distributions that may not be determined by mean values. The power law effects and the possible instability might be diminished, or perhaps eliminated, by reducing the variability of packets. However, we show that even a slotted (synchronized) ALOHA with packets of constant size can exhibit power law delays when the number of active users is random. From an engineering perspective, our results imply that the variability of packet sizes and number of active users need to be taken into consideration when designing robust MAC protocols, especially for ad-hoc/sensor networks where other factors, such as link failures and mobility, might further compound the problem.", "venue": "ArXiv", "authors": ["Predrag R. Jelenkovic", "Jian  Tan"], "year": 2009, "n_citations": 9}
{"id": 2064555, "s2_id": "aab63e64a3ffa2b31a9b9c4bb045d53fb9d0d411", "title": "Fast Online 'Next Best Offers' using Deep Learning", "abstract": "In this paper we present iPrescribe, a scalable low-latency architecture for recommending 'next-best-offers' in an online setting. The paper presents the design of iPrescribe and compares its performance for implementations using different real-time streaming technology stacks. iPrescribe uses ensemble of deep learning and machine learning algorithms for prediction. We describe the scalable real-time streaming technology stack and optimised machine-learning implementations to achieve a 90th percentile recommendation latency of 38 milliseconds. Optimizations include a novel mechanism to deploy recurrent Long Short Term Memory (LSTM) deep learning networks efficiently.", "venue": "COMAD/CODS", "authors": ["Rekha  Singhal", "Gautam  Shroff", "Mukund  Kumar", "Sharod Roy Choudhury", "Sanket  Kadarkar", "Rupinder  Virk", "Siddharth  Verma", "Vartika  Tewari"], "year": 2019, "n_citations": 6}
{"id": 2074666, "s2_id": "1d22a4e8e62ee58cf87f1bb56333735bfbf3ddfc", "title": "Stochastic Modeling of Hybrid Cache Systems", "abstract": "In recent years, there is an increasing demand of big memory systems so to perform large scale data analytics. Since DRAM memories are expensive, some researchers are suggesting to use other memory systems such as non-volatile memory (NVM) technology to build large-memory computing systems. However, whether the NVM technology can be a viable alternative (either economically and technically) to DRAM remains an open question. To answer this question, it is important to consider how to design a memory system from a \"system perspective\",that is, incorporating different performance characteristics andprice ratios from hybrid memory devices. This paper presents an analytical model of a \"hybrid page cache system\" so to understand the diverse design space and performance impact of a hybrid cache system. We consider (1) various architectural choices, (2) design strategies, and (3) configuration of different memory devices. Using this model, we provide guidelines on how to design hybrid page cache to reach a good trade-off between high system throughput (in I/O per sec or IOPS) and fast cache reactivity which is defined by the time to fill the cache. We also show how one can configure the DRAM capacity and NVM capacity under a fixed budget. We pick PCM as an example for NVM and conduct numerical analysis. Our analysis indicates that incorporating PCM in a page cache system significantly improves the system performance, and it also shows larger benefit to allocate more PCM in page cache in some cases. Besides, for the common setting of performance-price ratio of PCM, \"flat architecture\" offers as a better choice, but \"layered architecture\" outperforms if PCM write performance can be significantly improved in the future.", "venue": "2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)", "authors": ["Gaoying  Ju", "Yongkun  Li", "Yinlong  Xu", "Jiqiang  Chen", "John C. S. Lui"], "year": 2016, "n_citations": 4}
{"id": 2077204, "s2_id": "58d9bbba5111e92316f63cefd32e8c4ca02807f9", "title": "T-SNE-CUDA: GPU-Accelerated T-SNE and its Applications to Modern Data", "abstract": "Modern datasets and models are notoriously difficult to explore and analyze due to their inherent high dimensionality and massive numbers of samples. Existing visualization methods which employ dimensionality reduction to two or three dimensions are often inefficient and/or ineffective for these datasets. This paper introduces T-SNE-CUDA, a GPU-accelerated implementation of t-distributed Symmetric Neighbour Embedding (t-SNE) for visualizing datasets and models. T-SNE-CUDA significantly outperforms current implementations with 50-700x speedups on the CIFAR-10 and MNIST datasets. These speedups enable, for the first time, visualization of the neural network activations on the entire ImageNet dataset - a feat that was previously computationally intractable. We also demonstrate visualization performance in the NLP domain by visualizing the GloVe embedding vectors. From these visualizations, we can draw interesting conclusions about using the L2 metric in these embedding spaces. T-SNE-CUDA is publicly available at https://github.com/CannyLab/tsne-cuda.", "venue": "2018 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)", "authors": ["David M. Chan", "Roshan  Rao", "Forrest  Huang", "John F. Canny"], "year": 2018, "n_citations": 51}
{"id": 2077563, "s2_id": "ec7f2ee4d6444827f0733db60bb249aef6f6cc50", "title": "Snap-stabilization in message-passing systems", "abstract": "In this announcement, we report recent results where we address the open problem of snap-stabilization in message-passing systems.", "venue": "PODC '08", "authors": ["Sylvie  Dela\u00ebt", "St\u00e9phane  Devismes", "Mikhail  Nesterenko", "S\u00e9bastien  Tixeuil"], "year": 2008, "n_citations": 10}
{"id": 2078151, "s2_id": "3858ccebd64b63e27a8cbc2d4d4498e063c05688", "title": "Comparison and Benchmarking of AI Models and Frameworks on Mobile Devices", "abstract": "Due to increasing amounts of data and compute resources, deep learning achieves many successes in various domains. The application of deep learning on the mobile and embedded devices is taken more and more attentions, benchmarking and ranking the AI abilities of mobile and embedded devices becomes an urgent problem to be solved. Considering the model diversity and framework diversity, we propose a benchmark suite, AIoTBench, which focuses on the evaluation of the inference abilities of mobile and embedded devices. AIoTBench covers three typical heavy-weight networks: ResNet50, InceptionV3, DenseNet121, as well as three light-weight networks: SqueezeNet, MobileNetV2, MnasNet. Each network is implemented by three frameworks which are designed for mobile and embedded devices: Tensorflow Lite, Caffe2, Pytorch Mobile. To compare and rank the AI capabilities of the devices, we propose two unified metrics as the AI scores: Valid Images Per Second (VIPS) and Valid FLOPs Per Second (VOPS). Currently, we have compared and ranked 5 mobile devices using our benchmark. This list will be extended and updated soon after.", "venue": "ArXiv", "authors": ["Chunjie  Luo", "Xiwen  He", "Jianfeng  Zhan", "Lei  Wang", "Wanling  Gao", "Jiahui  Dai"], "year": 2020, "n_citations": 12}
{"id": 2078917, "s2_id": "0da5c63abfe6bc6fe24c864af90a8ef62d5e8834", "title": "A framework to experiment optimizations for real-time and embedded software", "abstract": "Typical constraints on embedded systems include code size limits, upper bounds on energy consumption and hard or soft deadlines. To meet these requirements, it may be necessary to improve the software by applying various kinds of transformations like compiler optimizations, specific mapping of code and data in the available memories, code compression, etc. However, a transformation that aims at improving the software with respect to a given criterion might engender side effects on other criteria and these effects must be carefully analyzed. For this purpose, we have developed a common framework that makes it possible to experiment various code transfor-mations and to evaluate their impact of various criteria. This work has been carried out within the French ANR MORE project.", "venue": "ArXiv", "authors": ["Hugues  Cass\u00e9", "Karine  Heydemann", "Haluk  Ozaktas", "Jonathan  Ponroy", "Christine  Rochange", "Olivier  Zendra"], "year": 2010, "n_citations": 1}
{"id": 2078943, "s2_id": "59afa29f39306ad7ed1e1785a932a192aa48c315", "title": "A Framework for Allocating Server Time to Spot and On-Demand Services in Cloud Computing", "abstract": "Cloud computing delivers value to users by facilitating their access to servers at any time period needed. An approach is to provide both on-demand and spot services on shared servers. The former allows users to access servers on demand at a fixed price and users occupy different time periods on servers. The latter allows users to bid for the remaining unoccupied time periods via dynamic pricing; however, without appropriate design, such time periods may be arbitrarily short since on-demand users arrive randomly. This is also the current service model adopted by Amazon Elastic Cloud Compute. In this article, we provide the first integral framework for sharing time on servers between on-demand and spot services while optimally pricing spot service. It guarantees that on-demand users can get served quickly while spot users can stably use servers for a properly long period once accepted, which is a key feature in making both on-demand and spot services accessible. Simulation results show that, by complementing the on-demand market with a spot market, a cloud provider can improve revenue by up to 461.5%. The framework is designed under assumptions that are met in real environments. It is a new tool that other cloud operators can use to quantify the advantage of a hybrid spot and on-demand service, making the case for eventually integrating this service model into their own infrastructures.", "venue": "ACM Trans. Model. Perform. Evaluation Comput. Syst.", "authors": ["Xiaohu  Wu", "Francesco De Pellegrini", "Guanyu  Gao", "Giuliano  Casale"], "year": 2019, "n_citations": 5}
{"id": 2084973, "s2_id": "dc2bfb5f968745736501f79f78e17bc3fa8c84a5", "title": "Comparative benchmarking of cloud computing vendors with high performance linpack", "abstract": "We present a comparative analysis of the maximum performance achieved by the Linpack benchmark on compute intensive hardware publicly available from multiple cloud providers. We study both performance within a single compute node, and speedup for distributed memory calculations with up to 32 nodes or at least 512 computing cores. We distinguish between hyper-threaded and non-hyper-threaded scenarios and estimate the performance per single computing core. We also compare results with a traditional supercomputing system for reference. Our findings provide a way to rank the cloud providers and demonstrate the viability of the cloud for high performance computing applications.", "venue": "HP3C", "authors": ["Mohammad  Mohammadi", "Timur  Bazhirov"], "year": 2018, "n_citations": 22}
{"id": 2092502, "s2_id": "5ee7bb8123c89090eb010a370724f7f57759231e", "title": "On performance of PBFT for IoT-applications with constrained devices", "abstract": "Cyber-physical systems and the Internet of things (IoT) is becoming an integral part of the digital society. The use of IoT services improves human life in many ways. Protection against cyber threats is an important aspect of the functioning of IoT devices. Malicious activities lead to confidential data leakages and incorrect performance of devices are becoming critical. Therefore, development of effective solutions that can protect both IoT devices data and data exchange networks turns in to a real challenge. This study provides a critical analysis of the feasibility of using blockchain technology to protect constrained IoT devices data, justifies the choice of Practical Byzantine Fault Tolerance (PBFT) consensus algorithm for implementation on such devices, and simulates the main distributed ledger scenarios using PBFT. The simulation results demonstrate the efficiency of the blockchain technology for constrained devices and make it possible to evaluate the applicability limits of the chosen consensus algorithm. INDEX TERMS Blockchain, Consensus algorithm, Constrained devices, Internet of things, Practical Byzantine Fault Tolerance", "venue": "ArXiv", "authors": ["Yaroslav  Meshcheryakov", "Anna  Melman", "Oleg  Evsutin", "Vladimir  Morozov", "Yevgeni  Koucheryavy"], "year": 2021, "n_citations": 1}
{"id": 2093463, "s2_id": "66113d42aa3325e4ade373111587f77a834b405e", "title": "ScalAna: Automating Scaling Loss Detection with Graph Analysis", "abstract": "Scaling a parallel program to modern supercomputers is challenging due to inter-process communication, Amdahl's law, and resource contention. Performance analysis tools for finding such scaling bottlenecks either base on profiling or tracing. Profiling incurs low overheads but does not capture detailed dependencies needed for root-cause analysis. Tracing collects all information at prohibitive overheads. In this work, we design ScalAna that uses static analysis techniques to achieve the best of both worlds - it enables the analyzability of traces at a cost similar to profiling. ScalAna first leverages static compiler techniques to build a Program Structure Graph, which records the main computation and communication patterns as well as the program's control structures. At runtime, we adopt lightweight techniques to collect performance data according to the graph structure and generate a Program Performance Graph. With this graph, we propose a novel approach, called backtracking root cause detection, which can automatically and efficiently detect the root cause of scaling loss. We evaluate ScalAna with real applications. Results show that our approach can effectively locate the root cause of scaling loss for real applications and incurs 1.73% overhead on average for up to 2,048 processes. We achieve up to 11.11% performance improvement by fixing the root causes detected by ScalAna on 2,048 processes.", "venue": "SC", "authors": ["Yuyang  Jin", "Haojie  Wang", "Teng  Yu", "Xiongchao  Tang", "Torsten  Hoefler", "Xu  Liu", "Jidong  Zhai"], "year": 2020, "n_citations": 0}
{"id": 2095743, "s2_id": "9e6b5a0d062fbe8d9a9a58115c93471eb8cf6005", "title": "Reinforced Wasserstein Training for Severity-Aware Semantic Segmentation in Autonomous Driving", "abstract": "Semantic segmentation is important for many real-world systems, e.g., autonomous vehicles, which predict the class of each pixel. Recently, deep networks achieved significant progress w.r.t. the mean Intersection-over Union (mIoU) with the cross-entropy loss. However, the cross-entropy loss can essentially ignore the difference of severity for an autonomous car with different wrong prediction mistakes. For example, predicting the car to the road is much more servery than recognize it as the bus. Targeting for this difficulty, we develop a Wasserstein training framework to explore the inter-class correlation by defining its ground metric as misclassification severity. The ground metric of Wasserstein distance can be pre-defined following the experience on a specific task. From the optimization perspective, we further propose to set the ground metric as an increasing function of the pre-defined ground metric. Furthermore, an adaptively learning scheme of the ground matrix is proposed to utilize the high-fidelity CARLA simulator. Specifically, we follow a reinforcement alternative learning scheme. The experiments on both CamVid and Cityscapes datasets evidenced the effectiveness of our Wasserstein loss. The SegNet, ENet, FCN and Deeplab networks can be adapted following a plug-in manner. We achieve significant improvements on the predefined important classes, and much longer continuous playtime in our simulator.", "venue": "ArXiv", "authors": ["Xiaofeng  Liu", "Yimeng  Zhang", "Xiongchang  Liu", "Song  Bai", "Site  Li", "Jane  You"], "year": 2020, "n_citations": 1}
{"id": 2101256, "s2_id": "8cc842c8fd167c1985d2f7660946f3a6a4aa9c36", "title": "The hidden cost of the edge: a performance comparison of edge and cloud latencies", "abstract": "Edge computing has emerged as a popular paradigm for running latency-sensitive applications due to its ability to offer lower network latencies to end-users. In this paper, we argue that despite its lower network latency, the resource-constrained nature of the edge can result in higher end-to-end latency, especially at higher utilizations, when compared to cloud data centers. We study this edge performance inversion problem through an analytic comparison of edge and cloud latencies and analyze conditions under which the edge can yield worse performance than the cloud. To verify our analytic results, we conduct a detailed experimental comparison of the edge and the cloud latencies using a realistic application and real cloud workloads. Both our analytical and experimental results show that even at moderate utilizations, the edge queuing delays can offset the benefits of lower network latencies, and even result in performance inversion where running in the cloud would provide superior latencies. We finally discuss practical implications of our results and provide insights into how application designers and service providers should design edge applications and systems to avoid these pitfalls.", "venue": "SC", "authors": ["Ahmed  Ali-Eldin", "Bin  Wang", "Prashant  Shenoy"], "year": 2021, "n_citations": 0}
{"id": 2102861, "s2_id": "f850d7e656650c441a487f1e75735cacedbcccbb", "title": "Beyond 5G Low-Power Wide-Area Networks: A LoRaWAN Suitability Study", "abstract": "In this paper, we deliver a discussion regarding the role of Low-Power Wide-Area Networks (LPWAN) in the cellular Internet-of-Things (IoT) infrastructure to support massive Machine-Type Communications (mMTC) in next-generation wireless systems beyond 5G. We commence by presenting a performance analysis of current LPWAN systems, specifically LoRaWAN, in terms of coverage and throughput. The results obtained using analytic methods and network simulations are combined in the paper for getting a more comprehensive vision. Next, we identify possible performance bottlenecks, speculate on the characteristics of coming IoT applications, and seek to identify potential enhancements to the current technologies that may overcome the identified shortcomings.", "venue": "2020 2nd 6G Wireless Summit (6G SUMMIT)", "authors": ["Arliones  Hoeller", "Jean  Sant'Ana", "Juho  Markkula", "Konstantin  Mikhaylov", "Richard  Souza", "Hirley  Alves"], "year": 2020, "n_citations": 3}
{"id": 2106333, "s2_id": "81dd34f0591a8067180cdbe1f38e60cb6fac7eb7", "title": "Infinite-server queueing model with MAPkGk Markov arrival streams, random volume of customers in random environment subject to catastrophe", "abstract": "In this paper the infinite server queue model in semi-Markov random environment with k Markov arrival streams, random resources of customers, and catastrophes is considered. After catastrophes occur, all customers in the model are flashed out and the system jumps into recovery station. After the recovery time the model works from the empty state. The transient and stationary joint distributions of numbers of different types of customers in the model at moment t, numbers of different types of served in some interval customers, volume of accumulated resources in the model at moment t, and total volume of served resources in an interval for the model without catastrophes are found. The transient and stationary joint distributions of numbers of different types of customers in the model at moment t, and volume of accumulated resources in the model at moment t and their moments for the model with catastrophes are obtained. All results are obtained using Danzig collective marks method and renewal theory methods.", "venue": "ArXiv", "authors": ["Khanik  Kerobyan", "Ruben  Kerobyan", "Koffi  Enakoutsa"], "year": 2018, "n_citations": 0}
{"id": 2108875, "s2_id": "ace98a1af49a2db288be000a7584d57f3278a504", "title": "Performance limitations for sparse matrix-vector multiplications on current multicore environments", "abstract": "The increasing importance of multi-core processors calls for a reevaluation of established numerical algorithms in view of their ability to profit from this new hardware concept. In order to optimize the existent algorithms, a detailed knowledge of the different performance-limiting factors is mandatory. In this contribution we investigate sparse matrix-vector multiplications, which are the dominant operation in many sparse eigenvalue solvers. Two conceptually different storage schemes and computational kernels have been conceived in the past to target cache-based and vector architectures, respectively: compressed row and jagged diagonal storage. Starting from a series of microbenchmarks to single out performance limitations, we apply the gained insight to optimize sparse MVM implementations, reviewing serial and OpenMP-parallel performance on state-of-the-art multi-core systems.", "venue": "ArXiv", "authors": ["Gerald  Schubert", "Georg  Hager", "Holger  Fehske"], "year": 2009, "n_citations": 8}
{"id": 2112452, "s2_id": "1b22bde6108ea4b840f0fecb96164ad15123a917", "title": "Characterizing the impact of the workload on the value of dynamic resizing in data centers", "abstract": "Energy consumption imposes a significant cost for data centers; yet much of that energy is used to maintain excess service capacity during periods of predictably low load. Resultantly, there has recently been interest in developing designs that allow the service capacity to be dynamically resized to match the current workload. However, there is still much debate about the value of such approaches in real settings. In this paper, we show that the value of dynamic resizing is highly dependent on statistics of the workload process. In particular, both slow timescale non-stationarities of the workload (e.g., the peak-to-mean ratio) and the fast time-scale stochasticity (e.g., the burstiness of arrivals) play key roles. To illustrate the impact of these factors, we combine optimization-based modeling of the slow time-scale with stochastic modeling of the fast time scale.", "venue": "2013 Proceedings IEEE INFOCOM", "authors": ["Kai  Wang", "Minghong  Lin", "Florin  Ciucu", "Adam  Wierman", "Chuang  Lin"], "year": 2013, "n_citations": 25}
{"id": 2114772, "s2_id": "acbeffc2e8620ee4bce8d66748a63580600fc7a4", "title": "Control of parallel non-observable queues: asymptotic equivalence and optimality of periodic policies", "abstract": "We consider a queueing system composed of a dispatcher that routes deterministically jobs to a set of non-observable queues working in parallel. In this setting, the fundamental problem is which policy should the dispatcher implement to minimize the stationary mean waiting time of the incoming jobs. We present a structural property that holds in the classic scaling of the system where the network demand (arrival rate of jobs) grows proportionally with the number of queues. Assuming that each queue of type $r$ is replicated $k$ times, we consider a set of policies that are periodic with period $k \\sum_r p_r$ and such that exactly $p_r$ jobs are sent in a period to each queue of type $r$. When $k\\to\\infty$, our main result shows that all the policies in this set are equivalent, in the sense that they yield the same mean stationary waiting time, and optimal, in the sense that no other policy having the same aggregate arrival rate to \\emph{all} queues of a given type can do better in minimizing the stationary mean waiting time. This property holds in a strong probabilistic sense. Furthermore, the limiting mean waiting time achieved by our policies is a convex function of the arrival rate in each queue, which facilitates the development of a further optimization aimed at solving the fundamental problem above for large systems.", "venue": "ArXiv", "authors": ["Jonatha  Anselmi", "Bruno  Gaujal", "Tommaso  Nesti"], "year": 2014, "n_citations": 6}
{"id": 2117427, "s2_id": "d4d34eafb7e4aa336b8c6ded01a4b6c6e21a79a4", "title": "Grand Challenge: Optimized Stage Processing for Anomaly Detection on Numerical Data Streams", "abstract": "The 2017 Grand Challenge focused on the problem of automatic detection of anomalies for manufacturing equipment. This paper reports the technical details of a solution focused on particular optimizations of the processing stages. These included customized input parsing, fine tuning of a k-means clustering algorithm and probability analysis using a lazy flavor of a Markov chain. We have observed in our custom implementation that carefully tweaking these processing stages at single node level by leveraging various data stream characteristics can yield good performance results. We start the paper with several observations concerning the input data stream, following with our solution description with details on particular optimizations, and we conclude with evaluation and a discussion of obtained results.", "venue": "ArXiv", "authors": ["Ciprian  Amariei", "Paul  Diac", "Emanuel  Onica"], "year": 2017, "n_citations": 1}
{"id": 2119868, "s2_id": "05233cf6194ddee6427f0bb76cb8749cc220d2bb", "title": "Stochastic Gradient Descent on Highly-Parallel Architectures", "abstract": "There is an increased interest in building data analytics frameworks with advanced algebraic capabilities both in industry and academia. Many of these frameworks, e.g., TensorFlow and BIDMach, implement their compute-intensive primitives in two flavors---as multi-thread routines for multi-core CPUs and as highly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is the most popular optimization method for model training implemented extensively on modern data analytics platforms. While the data-intensive properties of SGD are well-known, there is an intense debate on which of the many SGD variants is better in practice. In this paper, we perform a comprehensive study of parallel SGD for training generalized linear models. We consider the impact of three factors -- computing architecture (multi-core CPU or GPU), synchronous or asynchronous model updates, and data sparsity -- on three measures---hardware efficiency, statistical efficiency, and time to convergence. In the process, we design an optimized asynchronous SGD algorithm for GPU that leverages warp shuffling and cache coalescing for data and model access. We draw several interesting findings from our extensive experiments with logistic regression (LR) and support vector machines (SVM) on five real datasets. For synchronous SGD, GPU always outperforms parallel CPU---they both outperform a sequential CPU solution by more than 400X. For asynchronous SGD, parallel CPU is the safest choice while GPU with data replication is better in certain situations. The choice between synchronous GPU and asynchronous CPU depends on the task and the characteristics of the data. As a reference, our best implementation outperforms TensorFlow and BIDMach consistently. We hope that our insights provide a useful guide for applying parallel SGD to generalized linear models.", "venue": "ArXiv", "authors": ["Yujing  Ma", "Florin  Rusu", "Martin  Torres"], "year": 2018, "n_citations": 7}
{"id": 2121378, "s2_id": "bcb0943f864958bc9821a71b12f3cbfff1d44a40", "title": "Aggregate Cyber-Risk Management in the IoT Age Cautionary Statistics for (Re)Insurers and Likes", "abstract": "IoT-driven smart societies are modern service-networked ecosystems, whose proper functioning is hugely based on the success of supply chain relationships. Robust security is still a big challenge in such ecosystems, catalyzed primarily by naive cyber-security practices (e.g., setting default IoT device passwords) on behalf of the ecosystem managers, i.e., users and organizations. This has recently led to some catastrophic malware-driven DDoS and ransomware attacks (e.g., the Mirai and WannaCry attacks). Consequently, markets for commercial third-party cyber-risk management (CRM) services (e.g., cyber-insurance) are steadily but sluggishly gaining traction with the rapid increase of IoT deployment in society, and provides a channel for ecosystem managers to transfer residual cyber-risk post attack events. Current empirical studies have shown that such residual cyber-risks affecting smart societies are often heavy-tailed in nature and exhibit tail dependencies. This is both, a major concern for a profit-minded CRM firm that might normally need to cover multiple such dependent cyber-risks from different sectors (e.g., manufacturing and energy) in a service-networked ecosystem, and a good intuition behind the sluggish market growth of CRM products. In this article, we provide: 1) a rigorous general theory to elicit conditions on (tail-dependent) heavy-tailed cyber-risk distributions under which a risk management firm might find it (non)sustainable to provide aggregate cyber-risk coverage services for smart societies and 2) a real-data-driven numerical study to validate claims made in theory assuming boundedly rational cyber-risk managers, alongside providing ideas to boost markets that aggregate dependent cyber-risks with heavy-tails. To the best of our knowledge, this is the only complete general theory till date on the feasibility of aggregate CRM.", "venue": "IEEE Internet of Things Journal", "authors": ["Ranjan  Pal", "Ziyuan  Huang", "Xinlong  Yin", "Sergey  Lototsky", "Swades  De", "Sasu  Tarkoma", "Mingyan  Liu", "Jon  Crowcroft", "Nishanth  Sastry"], "year": 2021, "n_citations": 2}
{"id": 2122018, "s2_id": "1e7eff2cc174d340f7397a2f0c0f30afac356a00", "title": "Estimating Self-Sustainability in Peer-to-Peer Swarming Systems", "abstract": "Peer-to-peer swarming is one of the \\emph{de facto} solutions for distributed content dissemination in today's Internet. By leveraging resources provided by clients, swarming systems reduce the load on and costs to publishers. However, there is a limit to how much cost savings can be gained from swarming; for example, for unpopular content peers will always depend on the publisher in order to complete their downloads. In this paper, we investigate this dependence. For this purpose, we propose a new metric, namely \\emph{swarm self-sustainability}. A swarm is referred to as self-sustaining if all its blocks are collectively held by peers; the self-sustainability of a swarm is the fraction of time in which the swarm is self-sustaining. We pose the following question: how does the self-sustainability of a swarm vary as a function of content popularity, the service capacity of the users, and the size of the file? We present a model to answer the posed question. We then propose efficient solution methods to compute self-sustainability. The accuracy of our estimates is validated against simulation. Finally, we also provide closed-form expressions for the fraction of time that a given number of blocks is collectively held by peers.", "venue": "Perform. Evaluation", "authors": ["Daniel Sadoc Menasch\u00e9", "Ant\u00f4nio Augusto de Arag\u00e3o Rocha", "Edmundo de Souza e Silva", "Rosa Maria Meri Le\u00e3o", "Donald F. Towsley", "Arun  Venkataramani"], "year": 2010, "n_citations": 31}
{"id": 2124810, "s2_id": "5c719583204612598c574b89d73bedfeb6461cc3", "title": "A Note on the Performance of Algorithms for Solving Linear Diophantine Equations in the Naturals", "abstract": "We implement four algorithms for solving linear Diophantine equations in the naturals: a lexicographic enumeration algorithm, a completion procedure, a graph-based algorithm, and the Slopes algorithm. As already known, the lexicographic enumeration algorithm and the completion procedure are slower than the other two algorithms. We compare in more detail the graph-based algorithm and the Slopes algorithm. In contrast to previous comparisons, our work suggests that they are equally fast on small inputs, but the graphbased algorithm gets much faster as the input grows. We conclude that implementations of AC-unification algorithms should use the graph-based algorithm for maximum efficiency.", "venue": "ArXiv", "authors": ["Valeriu  Motroi", "Stefan  Ciobaca"], "year": 2021, "n_citations": 0}
{"id": 2127103, "s2_id": "4007ea090b43d7c269409a0f02b9b1582ad529f9", "title": "The Supermarket Model with Known and Predicted Service Times", "abstract": "The supermarket model refers to a system with a large number of queues, where arriving customers choose $d$ queues at random and join the queue with the fewest customers. The supermarket model demonstrates the power of even small amounts of choice, as compared to simply joining a queue chosen uniformly at random, for load balancing systems. In this work we perform simulation-based studies to consider variations where service times for a customer are predicted, as might be done in modern settings using machine learning techniques or related mechanisms. Our primary takeaway is that using even seemingly weak predictions of service times can yield significant benefits over blind First In First Out queueing in this context. However, some care must be taken when using predicted service time information to both choose a queue and order elements for service within a queue; while in many cases using the information for both choosing and ordering is beneficial, in many of our simulation settings we find that simply using the number of jobs to choose a queue is better when using predicted service times to order jobs in a queue. Although this study is simulation based, our study leaves many natural theoretical open questions for future work.", "venue": "ArXiv", "authors": ["Michael  Mitzenmacher"], "year": 2019, "n_citations": 5}
{"id": 2128444, "s2_id": "33836bddb910b303b942dd9ecae900072b20360e", "title": "Lightweight Task Analysis for Cache-Aware Scheduling on Heterogeneous Clusters", "abstract": "We present a novel characterization of how a program stresses cache. This characterization permits fast performance prediction in order to simulate and assist task scheduling on heterogeneous clusters. It is based on the estimation of stack distance probability distributions. The analysis requires the observation of a very small subset of memory accesses, and yields a reasonable to very accurate prediction in constant time.", "venue": "PDPTA", "authors": ["Xavier  Grehant", "Sverre  Jarp"], "year": 2008, "n_citations": 3}
{"id": 2136697, "s2_id": "b273603c6e8eb8817f332f1713163e6d9752edc0", "title": "Model-Driven Automatic Tiling with Cache Associativity Lattices", "abstract": "Traditional compiler optimization theory distinguishes three separate classes of cache miss -- Cold, Conflict and Capacity. Tiling for cache is typically guided by capacity miss counts. Models of cache function have not been effectively used to guide cache tiling optimizations due to model error and expense. Instead, heuristic or empirical approaches are used to select tilings. We argue that conflict misses, traditionally neglected or seen as a small constant effect, are the only fundamentally important cache miss category, that they form a solid basis by which caches can become modellable, and that models leaning on cache associatvity analysis can be used to generate cache performant tilings. We develop a mathematical framework that expresses potential and actual cache misses in associative caches using Associativity Lattices. We show these lattices to possess two theoretical advantages over rectangular tiles -- volume maximization and miss regularity. We also show that to generate such lattice tiles requires, unlike rectangular tiling, no explicit, expensive lattice point counting. We also describe an implementation of our lattice tiling approach, show that it can be used to give speedups of over 10x versus unoptimized code, and despite currently only tiling for one level of cache, can already be competitive with the aggressive compiler optimizations used in general purposes compares such as GCC and Intel's ICC. We also show that the tiling approach can lead to reasonable automatic parallelism when compared to existing auto-threading compilers.", "venue": "ArXiv", "authors": ["David  Adjiashvili", "Utz-Uwe  Haus", "Adrian  Tate"], "year": 2015, "n_citations": 0}
{"id": 2140042, "s2_id": "0bfb0d357587629c9fdb7429e6ca898168287f89", "title": "Towards Automatic Transformation of Legacy Scientific Code into OpenCL for Optimal Performance on FPGAs", "abstract": "There is a large body of legacy scientific code written in languages like Fortran that is not optimised to get the best performance out of heterogeneous acceleration devices like GPUs and FPGAs, and manually porting such code into parallel languages frameworks like OpenCL requires considerable effort. We are working towards developing a turn-key, self-optimising compiler for accelerating scientific applications, that can automatically transform legacy code into a solution for heterogeneous targets. In this paper we focus on FPGAs as the acceleration devices, and carry out our discussion in the context of the OpenCL programming framework. We show a route to automatic creation of kernels which are optimised for execution in a \"streaming\" fashion, which gives optimal performance on FPGAs. We use a 2D shallow-water model as an illustration; specifically we show how the use of \\emph{channels} to communicate directly between peer kernels and the use of on-chip memory to create stencil buffers can lead to significant performance improvements. Our results show better FPGA performance against a baseline CPU implementation, and better energy-efficiency against both CPU and GPU implementations.", "venue": "ArXiv", "authors": ["Wim  Vanderbauwhede", "Syed Waqar Nabi"], "year": 2019, "n_citations": 1}
{"id": 2141015, "s2_id": "bf7afdf0d2a1047d2d46c3180b719c511deee05b", "title": "Diversity/Parallelism Trade-off in Distributed Systems with Redundancy", "abstract": "As numerous machine learning and other algorithms increase in complexity and data requirements, distributed computing becomes necessary to satisfy the growing computational and storage demands, because it enables parallel execution of smaller tasks that make up a large computing job. However, random fluctuations in task service times lead to straggling tasks with long execution times. Redundancy, in the form of task replication and erasure coding, provides diversity that allows a job to be completed when only a subset of redundant tasks is executed, thus removing the dependency on the straggling tasks. In situations of constrained resources (here a fixed number of parallel servers), increasing redundancy reduces the available resources for parallelism. In this paper, we characterize the diversity vs. parallelism trade-off and identify the optimal strategy, among replication, coding and splitting, which minimizes the expected job completion time. We consider three common service time distributions and establish three models that describe scaling of these distributions with the task size. We find that different distributions with different scaling models operate optimally at different levels of redundancy, and thus may require very different code rates.", "venue": "IEEE Transactions on Information Theory", "authors": ["Pei  Peng", "Emina  Soljanin", "Philip  Whiting"], "year": 2021, "n_citations": 2}
{"id": 2144357, "s2_id": "ad3eb30bc59433e33199b68f1f8b87a0364e2c96", "title": "Network calculus for parallel processing", "abstract": "In this paper, we present preliminary results on the use of \"network calculus\" for parallel processing (fork join) systems, e.g., MapReduce. We derive a probabilistic bound that the delay through a single parallel processing stage exceeds a threshold.", "venue": "PERV", "authors": ["George  Kesidis", "Yuquan  Shan", "Bhuvan  Urgaonkar", "J\u00f6rg  Liebeherr"], "year": 2015, "n_citations": 11}
{"id": 2147273, "s2_id": "c537e76e57726b841d036bf3466f10bffe68b082", "title": "Tuning Crowdsourced Human Computation", "abstract": "As crowdsourcing has been dramatically investigated and utilized to address problems in the real world, it is essential and important to think about performance optimization. Analogous to computer systems with CPUs, treating each worker as a HPU (Human Processing Unit [1]) and studying the performance optimization on top of HPUs are interesting perspectives to resolve crowdsourcing issues. However, as we characterize HPUs in detail for this purpose, we find that there are significant differences between CPUs and HPUs, leading to the need of completely new optimization algorithms. In this paper, we study the specific optimization problem of obtaining results the fastest for a crowdsourced job with a fixed total budget. In crowdsourcing, jobs are usually broken down into sets of small tasks, which are assigned to workers one at a time. We consider three scenarios of increasing complexity: Identical Round Homogeneous Tasks, Multiplex Round Homogeneous Tasks, and Multiple Round Heterogeneous Tasks. For each scenario, we analyze the stochastic behavior of the HPU clock rate as a function of the remuneration offered. After that, we develop an optimum Budget Allocation Strategy to minimize the latency of the job completion. We validate our results through extensive simulations and experiments on Amazon Mechanical Turk.", "venue": "2017 IEEE 33rd International Conference on Data Engineering (ICDE)", "authors": ["Chen  Cao", "Jiayang  Tu", "Zheng  Liu", "Lei  Chen", "H.V.  Jagadish"], "year": 2017, "n_citations": 5}
{"id": 2148544, "s2_id": "9ca25666f1aa9d349b32b8c1b4bb8423182bd484", "title": "Layer-Parallel Training with GPU Concurrency of Deep Residual Neural Networks via Nonlinear Multigrid", "abstract": "A Multigrid Full Approximation Storage algorithm for solving Deep Residual Networks is developed to enable neural network parallelized layer-wise training and concurrent computational kernel execution on GPUs. This work demonstrates a 10.2x speedup over traditional layer-wise model parallelism techniques using the same number of compute units.", "venue": "2020 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Andrew C. Kirby", "Siddharth  Samsi", "Michael  Jones", "Albert  Reuther", "Jeremy  Kepner", "Vijay  Gadepally"], "year": 2020, "n_citations": 5}
{"id": 2148996, "s2_id": "b507a358e1fbf8ee2503bd3660acefc59ddb8e3d", "title": "Honing and proofing Astrophysical codes on the road to Exascale. Experiences from code modernization on many-core systems", "abstract": "The complexity of modern and upcoming computing architectures poses severe challenges for code developers and application specialists, and forces them to expose the highest possible degree of parallelism, in order to make the best use of the available hardware. The Intel$^{(R)}$ Xeon Phi$^{(TM)}$ of second generation (code-named Knights Landing, henceforth KNL) is the latest many-core system, which implements several interesting hardware features like for example a large number of cores per node (up to 72), the 512 bits-wide vector registers and the high-bandwidth memory. The unique features of KNL make this platform a powerful testbed for modern HPC applications. The performance of codes on KNL is therefore a useful proxy of their readiness for future architectures. In this work we describe the lessons learnt during the optimisation of the widely used codes for computational astrophysics P-Gadget-3, Flash and Echo. Moreover, we present results for the visualisation and analysis tools VisIt and yt. These examples show that modern architectures benefit from code optimisation at different levels, even more than traditional multi-core systems. However, the level of modernisation of typical community codes still needs improvements, for them to fully utilise resources of novel architectures.", "venue": "Future Gener. Comput. Syst.", "authors": ["Salvatore  Cielo", "Luigi  Iapichino", "Fabio  Baruffa", "Matteo  Bugli", "Christoph  Federrath"], "year": 2020, "n_citations": 2}
{"id": 2150202, "s2_id": "961df8612e99c21837682cecf1c7a070230b8732", "title": "Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility", "abstract": "Data mining involves the systematic analysis of large data sets, and data mining in agricultural soil datasets is exciting and modern research area. The productive capacity of a soil depends on soil fertility. Achieving and maintaining appropriate levels of soil fertility, is of utmost importance if agricultural land is to remain capable of nourishing crop production. In this research, Steps for building a predictive model of soil fertility have been explained. \nThis paper aims at predicting soil fertility class using decision tree algorithms in data mining . Further, it focuses on performance tuning of J48 decision tree algorithm with the help of meta-techniques such as attribute selection and boosting.", "venue": "ArXiv", "authors": ["Jay  Gholap"], "year": 2012, "n_citations": 52}
{"id": 2157350, "s2_id": "8c7310477fd027193cd040288f0aa9824c80b91f", "title": "Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code", "abstract": "This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel commands to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.", "venue": "2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)", "authors": ["Riyadh  Baghdadi", "Jessica  Ray", "Malek Ben Romdhane", "Emanuele Del Sozzo", "Abdurrahman  Akkas", "Yunming  Zhang", "Patricia  Suriana", "Shoaib  Kamil", "Saman P. Amarasinghe"], "year": 2019, "n_citations": 93}
{"id": 2157754, "s2_id": "01f660b239de2a7d250315053170ee792a91f4b5", "title": "Autotuning GPU Kernels via Static and Predictive Analysis", "abstract": "Optimizing the performance of GPU kernels is challenging for both human programmers and code generators. For example, CUDA programmers must set thread and block parameters for a kernel, but might not have the intuition to make a good choice. Similarly, compilers can generate working code, but may miss tuning opportunities by not targeting GPU models or performing code transformations. Although empirical autotuning addresses some of these challenges, it requires extensive experimentation and search for optimal code variants. This research presents an approach for tuning CUDA kernels based on static analysis that considers fine-grained code structure and the specific GPU architecture features. Notably, our approach does not require any program runs in order to discover near-optimal parameter settings. We demonstrate the applicability of our approach in enabling code autotuners such as Orio to produce competitive code variants comparable with empirical-based methods, without the high cost of experiments.", "venue": "2017 46th International Conference on Parallel Processing (ICPP)", "authors": ["Robert V. Lim", "Boyana  Norris", "Allen D. Malony"], "year": 2017, "n_citations": 18}
{"id": 2160702, "s2_id": "e53be3b7b854ad3c2055461ff1039847c46c4c93", "title": "RapidMind: Portability across Architectures and Its Limitations", "abstract": "Recently, hybrid architectures using accelerators like GP-GPUs or the Cell processor have gained much interest in the HPC community. The \"RapidMind Multi-Core Development Platform\" is a programming environment that allows generating code which is able to seamlessly run on hardware accelerators like GPUs or the Cell processor and multi-core CPUs both from AMD and Intel. This paper describes the ports of three mathematical kernels to RapidMind which have been chosen as synthetic benchmarks and representatives of scientific codes. Performance of these kernels has been measured on various RapidMind backends (cuda, cell and x86) and compared to other hardware-specific implementations (using CUDA, Cell SDK and Intel MKL). The results give an insight into the degree of portability of RapidMind code and code performance across different architectures.", "venue": "Facing the Multicore-Challenge", "authors": ["Iris  Christadler", "Volker  Weinberg"], "year": 2010, "n_citations": 11}
{"id": 2163416, "s2_id": "b8af85e58e4263e054724bf527c4c1c4d4ee36d7", "title": "Categorization of Program Regions for Agile Compilation using Machine Learning and Hardware Support", "abstract": "A compiler processes the code written in a high level language and produces machine executable code. The compiler writers often face the challenge of keeping the compilation times reasonable. That is because aggressive optimization passes which potentially will give rise to high performance are often expensive in terms of running time and memory footprint. Consequently the compiler designers arrive at a compromise where they either simplify the optimization algorithm which may decrease the performance of the produced code, or they will restrict the optimization to the subset of the overall input program in which case large parts of the input application will go un-optimized. \nThe problem we address in this paper is that of keeping the compilation times reasonable, and at the same time optimizing the input program to the fullest extent possible. Consequently, the performance of the produced code will match the performance when all the aggressive optimization passes are applied over the entire input program.", "venue": "ArXiv", "authors": ["Sanket  Tavarageri"], "year": 2019, "n_citations": 0}
{"id": 2163688, "s2_id": "a81b63e0e574a77605f366722cfe56880093db3d", "title": "Intelligent-Unrolling: Exploiting Regular Patterns in Irregular Applications", "abstract": "Modern optimizing compilers are able to exploit memory access or computation patterns to generate vectorization codes. However, such patterns in irregular applications are unknown until runtime due to the input dependence. Thus, either compiler's static optimization or profile-guided optimization based on specific inputs cannot predict the patterns for any common input, which leads to suboptimal code generation. To address this challenge, we develop Intelligent-Unroll, a framework to automatically optimize irregular applications with vectorization. Intelligent-Unroll allows the users to depict the computation task using \\textit{code seed} with the memory access and computation patterns represented in \\textit{feature table} and \\textit{information-code tree}, and generates highly efficient codes. Furthermore, Intelligent-Unroll employs several novel optimization techniques to optimize reduction operations and gather/scatter instructions. We evaluate Intelligent-Unroll with sparse matrix-vector multiplication (SpMV) and graph applications. Experimental results show that Intelligent-Unroll is able to generate more efficient vectorization codes compared to the state-of-the-art implementations.", "venue": "ArXiv", "authors": ["Changxi  Liu", "Hailong  Yang", "Xu  Liu", "Zhongzhi  Luan", "Depei  Qian"], "year": 2019, "n_citations": 0}
{"id": 2164288, "s2_id": "eab52147ff91bf9354b184fdaa28aec64d8e0399", "title": "Optimal Resource Allocation for Elastic and Inelastic Jobs", "abstract": "Modern data centers are tasked with processing heterogeneous workloads consisting of various classes of jobs. These classes differ in their arrival rates, size distributions, and job parallelizability. With respect to parallelizability, some jobs are elastic, meaning they can parallelize linearly across any number of servers. Other jobs are inelastic, meaning they can only run on a single server. Although job classes can differ drastically, they are typically forced to share a single cluster. When sharing a cluster among heterogeneous jobs, one must decide how to allocate servers to each job at every moment in time. In this paper, we design and analyze allocation policies which aim to minimize the mean response time across jobs, where a job's response time is the time from when it arrives until it completes. We model this problem in a stochastic setting where each job may be elastic or inelastic. Job sizes are drawn from exponential distributions, but are unknown to the system. We show that, in the common case where elastic jobs are larger on average than inelastic jobs, the optimal allocation policy is Inelastic-First, giving inelastic jobs preemptive priority over elastic jobs. We obtain this result by introducing a novel sample path argument. We also show that there exist cases where Elastic-First (giving priority to elastic jobs) performs better than Inelastic-First. We provide the first analysis of mean response time under both Elastic-First and Inelastic-First by leveraging techniques for solving high-dimensional Markov chains.", "venue": "SPAA", "authors": ["Benjamin  Berg", "Mor  Harchol-Balter", "Benjamin  Moseley", "Weina  Wang", "Justin  Whitehouse"], "year": 2020, "n_citations": 2}
{"id": 2165057, "s2_id": "f1c6d6bfed3151894538a753f4ce9b7fec63ba90", "title": "COSCO: Container Orchestration Using Co-Simulation and Gradient Based Optimization for Fog Computing Environments", "abstract": "Intelligent task placement and management of tasks in large-scale fog platforms is challenging due to the highly volatile nature of modern workload applications and sensitive user requirements of low energy consumption and response time. Container orchestration platforms have emerged to alleviate this problem with prior art either using heuristics to quickly reach scheduling decisions or AI driven methods like reinforcement learning and evolutionary approaches to adapt to dynamic scenarios. The former often fail to quickly adapt in highly dynamic environments, whereas the latter have run-times that are slow enough to negatively impact response time. Therefore, there is a need for scheduling policies that are both reactive to work efficiently in volatile environments and have low scheduling overheads. To achieve this, we propose a Gradient Based Optimization Strategy using Back-propagation of gradients with respect to Input (GOBI). Further, we leverage the accuracy of predictive digital-twin models and simulation capabilities by developing a Coupled Simulation and Container Orchestration Framework (COSCO). Using this, we create a hybrid simulation driven decision approach, GOBI*, to optimize Quality of Service (QoS) parameters. Co-simulation and the back-propagation approaches allow these methods to adapt quickly in volatile environments. Experiments conducted using real-world data on fog applications using the GOBI and GOBI* methods, show a significant improvement in terms of energy consumption, response time, Service Level Objective and scheduling time by up to 15, 40, 4, and 82 percent respectively when compared to the state-of-the-art algorithms.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Shreshth  Tuli", "Shivananda R. Poojara", "Satish N. Srirama", "Giuliano  Casale", "Nicholas R. Jennings"], "year": 2021, "n_citations": 11}
{"id": 2169434, "s2_id": "429e27acb7e8f44f629c843098d555282c92f360", "title": "Legio: Fault Resiliency for Embarrassingly Parallel MPI Applications", "abstract": "Due to the increasing size of HPC machines, the fault presence is becoming an eventuality that applications must face. Natively, MPI provides no support for the execution past the detection of a fault, and this is becoming more and more constraining. With the introduction of ULFM (User Level Fault Mitigation library), it has been provided with a possible way to overtake a fault during the application execution at the cost of code modifications. ULFM is intrusive in the application and requires also a deep understanding of its recovery procedures. In this paper we propose Legio, a framework that lowers the complexity of introducing resiliency in an embarrassingly parallel MPI application. By hiding ULFM behind the MPI calls, the library is capable to expose resiliency features to the application in a transparent manner thus removing any integration effort. Upon fault, the failed nodes are discarded and the execution continues only with the non-failed ones. A hierarchical implementation of the solution has been also proposed to reduce the overhead of the repair process when scaling towards a large number of nodes. We evaluated our solutions on the Marconi100 cluster at CINECA, showing that the overhead introduced by the library is negligible and it does not limit the scalability properties of MPI. Moreover, we also integrated the solution in real-world applications to further prove its robustness by injecting faults.", "venue": "ArXiv", "authors": ["Roberto  Rocco", "Davide  Gadioli", "Gianluca  Palermo"], "year": 2021, "n_citations": 1}
{"id": 2169650, "s2_id": "3562e3b585eda80839b3c175bb8eef5105e7c2a5", "title": "Optimal Service Elasticity in Large-Scale Distributed Systems", "abstract": "A fundamental challenge in large-scale cloud networks and data centers is to achieve highly efficient server utilization and limit energy consumption, while providing excellent user-perceived performance in the presence of uncertain and time-varying demand patterns. Auto-scaling provides a popular paradigm for automatically adjusting service capacity in response to demand while meeting performance targets, and queue-driven auto-scaling techniques have been widely investigated in the literature. In typical data center architectures and cloud environments however, no centralized queue is maintained, and load balancing algorithms immediately distribute incoming tasks among parallel queues. In these distributed settings with vast numbers of servers, centralized queue-driven auto-scaling techniques involve a substantial communication overhead and major implementation burden, or may not even be viable at all. Motivated by the above issues, we propose a joint auto-scaling and load balancing scheme which does not require any global queue length information or explicit knowledge of system parameters, and yet provides provably near-optimal service elasticity. We establish the fluid-level dynamics for the proposed scheme in a regime where the total traffic volume and nominal service capacity grow large in proportion. The fluid-limit results show that the proposed scheme achieves asymptotic optimality in terms of user-perceived delay performance as well as energy consumption. Specifically, we prove that both the waiting time of tasks and the relative energy portion consumed by idle servers vanish in the limit. At the same time, the proposed scheme operates in a distributed fashion and involves only constant communication overhead per task, thus ensuring scalability in massive data center operations. Extensive simulation experiments corroborate the fluid-limit results, and demonstrate that the proposed scheme can match the user performance and energy consumption of state-of-the-art approaches that do take full advantage of a centralized queue.", "venue": "SIGMETRICS 2017", "authors": ["Debankur  Mukherjee", "Souvik  Dhara", "Sem C. Borst", "Johan S.H. van Leeuwaarden"], "year": 2017, "n_citations": 18}
{"id": 2173012, "s2_id": "f57c706cd20f468cc8c79a2c33da6fae53e4f82c", "title": "Towards quantitative methods to assess network generative models", "abstract": "Assessing generative models is not an easy task. Generative models should synthesize graphs which are not replicates of real networks but show topological features similar to real graphs. We introduce an approach for assessing graph generative models using graph classifiers. The inability of an established graph classifier for distinguishing real and synthesized graphs could be considered as a performance measurement for graph generators.", "venue": "ArXiv", "authors": ["Vahid  Mostofi", "Sadegh  Aliakbary"], "year": 2018, "n_citations": 0}
{"id": 2178329, "s2_id": "2d0b224f94cf5d8d015a0ee5a25285c4da3a4af3", "title": "Optimizing the performance of streaming numerical kernels on the IBM Blue Gene/P PowerPC 450 processor", "abstract": "Several emerging petascale architectures use energy-efficient processors with vectorized computational units and in-order thread processing. On these architectures the sustained performance of streaming numerical kernels, ubiquitous in the solution of partial differential equations, represents a challenge despite the regularity of memory access. Sophisticated optimization techniques are required to fully utilize the CPU. We propose a new method for constructing streaming numerical kernels using a high-level assembly synthesis and optimization framework. We describe an implementation of this method in Python targeting the IBM\u00ae Blue Gene\u00ae/P supercomputer\u2019s PowerPC\u00ae 450 core. This paper details the high-level design, construction, simulation, verification, and analysis of these kernels utilizing a subset of the CPU\u2019s instruction set. We demonstrate the effectiveness of our approach by implementing several three-dimensional stencil kernels over a variety of cached memory scenarios and analyzing the mechanically scheduled variants, including a 27-point stencil achieving a 1.7 \u00d7 speedup over the best previously published results.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Tareq M. Malas", "Aron J. Ahmadia", "Jed  Brown", "John A. Gunnels", "David E. Keyes"], "year": 2013, "n_citations": 7}
{"id": 2180990, "s2_id": "0c020d0be3573757ab778ce3ada0a2b18e2b4cb8", "title": "NMPO: Near-Memory Computing Profiling and Offloading", "abstract": "Real-world applications are now processing big-data sets, often bottlenecked by the data movement between the compute units and the main memory. Near-memory computing (NMC), a modern data-centric computational paradigm, can alleviate these bottlenecks, thereby improving the performance of applications. The lack of NMC system availability makes simulators the primary evaluation tool for performance estimation. However, simulators are usually time-consuming, and methods that can reduce this overhead would accelerate the early-stage design process of NMC systems. This work proposes Near-Memory computing Profiling and Offloading (NMPO), a high-level framework capable of predicting NMC offloading suitability employing an ensemble machine learning model. NMPO predicts NMC suitability with an accuracy of 85.6% and, compared to prior works, can reduce the prediction time by using hardware-dependent applications features by up to 3 order of magnitude.", "venue": "2021 24th Euromicro Conference on Digital System Design (DSD)", "authors": ["Stefano  Corda", "Madhurya  Kumaraswamy", "Ahsan Javed Awan", "Roel  Jordans", "Akash  Kumar", "Henk  Corporaal"], "year": 2021, "n_citations": 0}
{"id": 2183362, "s2_id": "ad88aeadbddcd39b6439b27f42a14d4eb2bb1f02", "title": "\u201cPlease Come Back Later\u201d: Benefiting from Deferrals in Service Systems", "abstract": "The performance evaluation of loss service systems, where customers who cannot be served upon arrival get dropped, has a long history going back to the classical Erlang B model. In this paper, we consider the performance benefits arising from the possibility of deferring customers who cannot be served upon arrival. Specifically, we consider an Erlang B type loss system where the system operator can, subject to certain constraints, ask a customer arriving when all servers are busy, to come back at a specified time in the future. If the system is still fully loaded when the deferred customer returns, she gets dropped for good. For such a system, we ask: How should the system operator determine the \u2018rearrival\u2019 times of the deferred customers based on the state of the system (which includes those customers already deferred and yet to arrive)? How does one quantify the performance benefit of such a deferral policy? Our contributions are as follows. We propose a simple state-dependent policy for determining the rearrival times of deferred customers. For this policy, we characterize the long run fraction of customers dropped. We also analyse a relaxation where the deferral times are bounded in expectation. Via extensive numerical evaluations, we demonstrate the superiority of the proposed state-dependent policies over naive state-independent deferral policies.", "venue": "2020 International Conference on COMmunication Systems & NETworkS (COMSNETS)", "authors": ["Anmol  Kagrecha", "Jayakrishnan  Nair"], "year": 2020, "n_citations": 0}
{"id": 2184768, "s2_id": "fa06c8e798f0585d675ce6b43ada977b7abe6b28", "title": "An Enhanced Buffer Management Scheme for Multimedia Traffic in HSDPA", "abstract": "High Speed Downlink Packet Access (HSDPA) enables higher data rate packet switch services over UMTS, creating opportunity to provide a variety of broadband multimedia applications. In HSDPA, buffering of downlink data in the base station (Node B) is required to fulfill the Medium Access Control (MAC) Packet Scheduling functionality. Additionally, flow control over the Iub interface that connects the Node B to the Radio Network Controller (RNC) is standardized in the 3GPP HSDPA technical specifications. Thus, buffer queue management schemes can be conveniently applied to improve downlink traffic QoS performance in HSDPA. Our previous work focused on a novel priority queuing scheme shown to be effective for QoS management of multimedia traffic with concurrent diverse flows towards a HSDPA end user. This paper presents an enhanced scheme that incorporates Iub flow control. A dynamic HSDPA simulator is developed to study the extended scheme, demonstrating the performance improvement achievable.", "venue": "The 2007 International Conference on Next Generation Mobile Applications, Services and Technologies (NGMAST 2007)", "authors": ["Suleiman Y. Yerima", "Khalid  Al-Begain"], "year": 2007, "n_citations": 6}
{"id": 2189103, "s2_id": "16930f09c600d34a973596c1a1a2ac6cddfaa77b", "title": "Analysis of the Packet Loss Probability in Energy Harvesting Cognitive Radio Networks", "abstract": "A Markovian battery model is proposed to provide the variation of energy states for energy harvesting (EH) secondary users (SUs) in the EH cognitive radio networks (CRN). Based on the proposed battery model, we derive the packet loss probability in the EH SUs due to sensing inaccuracy and energy outage. With the proposed analysis, the packet loss probability can easily be predicted and utilized to optimize the transmission policy (i.e., opportunities for successful transmission and EH) of EH SUs to improve their throughput. Especially, the proposed method can be applied to upper layer (scheduling and routing) optimization. To this end, we validate the proposed analysis through Monte-Carlo simulation and show an agreement between the analysis and simulations results.", "venue": "ArXiv", "authors": ["Shannai  Wu", "Yoan  Shin", "Jin Young Kim", "Dong In Kim"], "year": 2016, "n_citations": 0}
{"id": 2192839, "s2_id": "232e6280ffb773b21a8ea9343fbeaa8302def569", "title": "A Valgrind Tool to Compute the Working Set of a Software Process", "abstract": "This paper introduces a new open-source tool for the dynamic analyzer Valgrind. The tool measures the amount of memory that is actively being used by a process at any given point in time. While there exist numerous tools to measure the memory requirements of a process, the vast majority only focuses on metrics like resident or proportional set sizes, which include memory that was once claimed, but is momentarily disused. Consequently, such tools do not permit drawing conclusions about how much cache or RAM a process actually requires at each point in time, and thus cannot be used for performance debugging. The few tools which do measure only actively used memory, however, have limitations in temporal resolution and introspection. In contrast, our tool offers an easy way to compute the memory that has recently been accessed at any point in time, reflecting how cache and RAM requirements change over time. In particular, this tool computes the set of memory references made within a fixed time interval before any point in time, known as the working set, and captures call stacks for interesting peaks in the working set size. We first introduce the tool, then we run some examples comparing the output from our tool with similar memory tools, and we close with a discussion of limitations", "venue": "ArXiv", "authors": ["Martin  Becker", "Samarjit  Chakraborty"], "year": 2019, "n_citations": 0}
{"id": 2198478, "s2_id": "e2297bc70fac75b208d94b838acf779e13142f66", "title": "Inferring Catchment in Internet Routing", "abstract": "BGP is the de-facto Internet routing protocol for interconnecting Autonomous Systems (AS). Each AS selects its preferred routes based on its routing policies, which are typically not disclosed. Due to the distributed route selection and information hiding, answering questions such as \"what is the expected catchment of the anycast sites of a content provider at the AS-level, if new sites are deployed?\", or \"how will load-balancing behave if an ISP changes its routing policy for a prefix?\", is a hard challenge. In this work, we propose a framework and methodology to infer the routing behavior in existing or hypothetical routing configurations, and provide new capabilities and insights for informative route inference (e.g., isolating the effect of randomness that is present in prior simulation-based approaches). The proposed framework can be useful in a number of applications: measurements/monitoring, traffic engineering, network planning, Internet routing models, etc.", "venue": "Abstracts of the 2019 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems", "authors": ["Pavlos  Sermpezis", "Vasileios  Kotronis"], "year": 2019, "n_citations": 7}
{"id": 2200131, "s2_id": "bae7453162d5a231c6609fa32225140b0549253c", "title": "Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M", "abstract": "The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database implementation of hypersparse arrays that are ideal for analyzing many types of network data. D4M relies on associative arrays which combine properties of spreadsheets, databases, matrices, graphs, and networks, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of D4M associative arrays put enormous pressure on the memory hierarchy. This work describes the design and performance optimization of an implementation of hierarchical associative arrays that reduces memory pressure and dramatically increases the update rate into an associative array. The parameters of hierarchical associative arrays rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve optimal performance for a variety of applications. Hierarchical arrays achieve over 40,000 updates per second in a single instance. Scaling to 34,000 instances of hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.", "venue": "2019 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Jeremy  Kepner", "Vijay  Gadepally", "Lauren  Milechin", "Siddharth  Samsi", "William  Arcand", "David  Bestor", "William  Bergeron", "Chansup  Byun", "Matthew  Hubbell", "Michael  Houle", "Michael  Jones", "Anne  Klein", "Peter  Michaleas", "Julie  Mullen", "Andrew  Prout", "Antonio  Rosa", "Charles  Yee", "Albert  Reuther"], "year": 2019, "n_citations": 7}
{"id": 2201039, "s2_id": "40b3b66225fc03097a55c982c7b91a0f72fc6bea", "title": "Short Note on Costs of Floating Point Operations on current x86-64 Architectures: Denormals, Overflow, Underflow, and Division by Zero", "abstract": "Simple floating point operations like addition or multiplication on normalized floating point values can be computed by current AMD and Intel processors in three to five cycles. This is different for denormalized numbers, which appear when an underflow occurs and the value can no longer be represented as a normalized floating-point value. Here the costs are about two magnitudes higher.", "venue": "ArXiv", "authors": ["Markus  Wittmann", "Thomas  Zeiser", "Georg  Hager", "Gerhard  Wellein"], "year": 2015, "n_citations": 2}
{"id": 2201266, "s2_id": "d84c6dfe028a660811534185b6afcd9d3111a7ec", "title": "Heavy-traffic Delay Optimality in Pull-based Load Balancing Systems: Necessary and Sufficient Conditions", "abstract": "In this paper, we consider a load balancing system under a general pull-based policy. In particular, each arrival is randomly dispatched to any server whose queue length is below a threshold; if no such server exists, then the arrival is randomly assigned to any server. We are interested in the fundamental relationship between the threshold and the delay performance of the system in heavy traffic. To this end, we first establish the following necessary condition to guarantee heavy-traffic delay optimality: the threshold needs to grow to infinity as the exogenous arrival rate approaches the boundary of the capacity region (i.e., the load intensity approaches one) but the growth rate should be slower than a polynomial function of the mean number of tasks in the system. As a special case of this result, we directly show that the delay performance of the popular pull-based policy Join-Idle-Queue (JIQ) is not heavy traffic optimal, but performs strictly better than random routing. We further show that a sufficient condition for heavy-traffic delay optimality is that the threshold grows logarithmically with the mean number of tasks in the system. This result directly resolves a generalized version of the conjecture by Kelly and Laws.", "venue": "Abstracts of the 2019 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems", "authors": ["Xingyu  Zhou", "Jian  Tan", "Ness  Shroff"], "year": 2019, "n_citations": 11}
{"id": 2201891, "s2_id": "dbbbf6977e9f0e6abb052fbcf9f3dfc4637d57d2", "title": "DCRoute: Speeding up Inter-Datacenter Traffic Allocation while Guaranteeing Deadlines", "abstract": "Datacenters provide the infrastructure for cloud computing services used by millions of users everyday. Many such services are distributed over multiple datacenters at geographically distant locations possibly in different continents. These datacenters are then connected through high speed WAN links over private or public networks. To perform data backups or data synchronization operations, many transfers take place over these networks that have to be completed before a deadline in order to provide necessary service guarantees to end users. Upon arrival of a transfer request, we would like the system to be able to decide whether such a request can be guaranteed successful delivery. If yes, it should provide us with transmission schedule in the shortest time possible. In addition, we would like to avoid packet reordering at the destination as it affects TCP performance. Previous work in this area either cannot guarantee that admitted transfers actually finish before the specified deadlines or use techniques that can result in packet reordering. In this paper, we propose DCRoute, a fast and efficient routing and traffic allocation technique that guarantees transfer completion before deadlines for admitted requests. It assigns each transfer a single path to avoid packet reordering. Through simulations, we show that DCRoute is at least 200 times faster than other traffic allocation techniques based on linear programming (LP) while admitting almost the same amount of traffic to the system.", "venue": "2016 IEEE 23rd International Conference on High Performance Computing (HiPC)", "authors": ["Mohammad  Noormohammadpour", "Cauligi S. Raghavendra", "Sriram  Rao"], "year": 2016, "n_citations": 14}
{"id": 2208948, "s2_id": "caf37cf688209280d0cafc1538c889e2b3e4af4c", "title": "A Bounded Multi-Vacation Queue Model for Multi-stage Sleep Control 5G Base station", "abstract": "Modelling and control of energy consumption is an important problem in telecommunication this http URL model such systems, this paper publishes a bounded multi-vacation queue model. The energy consumption predicted by the model shows an average error rate of 0.0177 and the delay predicted by the model shows an average error rate of 0.0655 over 99 test instances.Subsequently, an optimization algorithm is proposed to minimize the energy consumption while not violate the delay bound. Furthermore, given current state of art 5G base station system configuration, numerical results shows that with the increase of traffic load, energy saving rate becomes less.", "venue": "ArXiv", "authors": ["Jie  Chen"], "year": 2020, "n_citations": 0}
{"id": 2219453, "s2_id": "66d6b08cbf221c66a0d0f48dba69df0b6bc434eb", "title": "iPregel: Strategies to Deal with an Extreme Form of Irregularity in Vertex-Centric Graph Processing", "abstract": "Over the last decade, the vertex-centric programming model has attracted significant attention in the world of graph processing, resulting in the emergence of a number of vertex-centric frameworks. Its simple programming interface, where computation is expressed from a vertex point of view, offers both ease of programming to the user and inherent parallelism for the underlying framework to leverage. However, vertex-centric programs represent an extreme form of irregularity, both inter and intra core. This is because they exhibit a variety of challenges from a workload that may greatly vary across supersteps, through fine-grain synchronisations, to memory accesses that are unpredictable both in terms of quantity and location. In this paper, we explore three optimisations which address these irregular challenges; a hybrid combiner carefully coupling lock-free and lock-based combinations, the partial externalisation of vertex structures to improve locality and the shift to an edge-centric representation of the workload. We also assess the suitability of more traditional optimisations such as dynamic load-balancing and software prefetching. The optimisations were integrated into the iPregel vertex-centric framework, enabling the evaluation of each optimisation in the context of graph processing across three general purpose benchmarks common in the vertex-centric community, each run on four publicly available graphs covering all orders of magnitude from a million to a billion edges. The result of this work is a set of techniques which we believe not only provide a significant performance improvement in vertex-centric graph processing, but are also applicable more generally to irregular applications.", "venue": "2019 IEEE/ACM 9th Workshop on Irregular Applications: Architectures and Algorithms (IA3)", "authors": ["Ludovic Anthony Richard Capelli", "Nick  Brown", "Jonathan Mark Bull"], "year": 2019, "n_citations": 1}
{"id": 2220223, "s2_id": "05126f7e17c3b2e04aef2669f8664b82b5521b7c", "title": "A performance study of monitoring and information services for distributed systems", "abstract": "Monitoring and information services form a key component of a distributed system, or Grid. A quantitative study of such services can aid in understanding the performance limitations, advise in the deployment of the monitoring system, and help evaluate future development work. To this end, we study the performance of three monitoring and information services for distributed systems: the Globus Toolkit/spl reg/ Monitoring and Discovery Service (MDS2), the European Data Grid Relational Grid Monitoring Architecture (R-GMA) and Hawkeye, part of the Condor project. We perform experiments to test their scalability with respect to number of users, number of resources and amount of data collected. Our study shows that each approach has different behaviors, often due to their different design goals. In the four sets of experiments we conducted to evaluate the performance of the service components under different circumstances, we found a strong advantage to caching or pre-fetching the data, as well as the need to have primary components at well-connected sites because of the high load seen by all systems.", "venue": "High Performance Distributed Computing, 2003. Proceedings. 12th IEEE International Symposium on", "authors": ["Xuehai  Zhang", "Jeffrey L. Freschl", "Jennifer M. Schopf"], "year": 2003, "n_citations": 237}
{"id": 2222122, "s2_id": "177ae95c22dde720ab47716bf6283ed86cf194d4", "title": "Improving Zero-Day Malware Testing Methodology Using Statistically Significant Time-Lagged Test Samples", "abstract": "Enterprise networks are in constant danger of being breached by cyber-attackers, but making the decision about what security tools to deploy to mitigate this risk requires carefully designed evaluation of security products. One of the most important metrics for a protection product is how well it is able to stop malware, specifically on \"zero\"-day malware that has not been seen by the security community before. However, evaluating zero-day performance is difficult, because of larger number of previously unseen samples that are needed to properly measure the true and false positive rate, and the challenges involved in accurately labeling these samples. This paper addresses these issues from a statistical and practical perspective. Our contributions include first showing that the number of benign files needed for proper evaluation is on the order of a millions, and the number of malware samples needed is on the order of tens of thousands. We then propose and justify a time-delay method for easily collecting large number of previously unseen, but labeled, samples. This enables cheap and accurate evaluation of zero-day true and false positive rates. Finally, we propose a more fine-grain labeling of the malware/benignware in order to better model the heterogeneous distribution of files on various networks.", "venue": "ArXiv", "authors": ["Konstantin  Berlin", "Joshua  Saxe"], "year": 2016, "n_citations": 3}
{"id": 2224392, "s2_id": "747bb4a622a686577e161e924a8f1b33f296887f", "title": "Chip\u2010level and multi\u2010node analysis of energy\u2010optimized lattice Boltzmann CFD simulations", "abstract": "Memory\u2010bound algorithms show complex performance and energy consumption behavior on multicore processors. We choose the lattice Boltzmann method on an Intel Sandy Bridge cluster as a prototype scenario to investigate if and how single\u2010chip performance and power characteristics can be generalized to the highly parallel case. First, we perform an analysis of a sparse\u2010lattice lattice Boltzmann method implementation for complex geometries. Using a single\u2010core performance model, we predict the intra\u2010chip saturation characteristics and the optimal operating point in terms of energy\u2010to\u2010solution as a function of implementation details, clock frequency, vectorization, and number of active cores per chip. We show that high single\u2010core performance and a correct choice of the number of active cores per chip are the essential optimizations for the lowest energy\u2010to\u2010solution at minimal performance degradation. Then we extrapolate to the Message Passing Interface (MPI)\u2010parallel level and quantify the energy\u2010saving potential of various optimizations and execution modes, where we find these guidelines to be even more important, especially when communication overhead is non\u2010negligible. In our setup, we could achieve energy savings of 35% in this case, compared with a naive approach. We also demonstrate that a simple non\u2010reflective reduction of the clock speed leaves most of the energy\u2010saving potential unused. Copyright \u00a9 2015 John Wiley & Sons, Ltd.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Markus  Wittmann", "Georg  Hager", "Thomas  Zeiser", "Jan  Treibig", "Gerhard  Wellein"], "year": 2016, "n_citations": 27}
{"id": 2225578, "s2_id": "515112634c019d0e096e66c3108f65826e47b047", "title": "Interference queueing networks on grids", "abstract": "Consider a countably infinite collection of coupled queues representing a large wireless network with a queue at each point of the $d$-dimensional integer grid. These queues have independent Poisson arrivals, but are coupled through their service rates which is the signal to interference ratio of wireless network theory. More precisely, the service discipline is translation invariant and of the processor sharing type, with the service rate in each queue slowed down, when the neighboring queues have a larger workload. The dynamics is infinite dimensional Markov, with each queue having a non compact state space. It is neither reversible nor asymptotically product form, as in the mean-field setting. Coupling and percolation techniques are first used to show that this dynamics has well defined trajectories. Coupling from the past techniques of the Loynes' type are then proposed to build its minimal stationary regime. This regime is the one obtained when starting from the all empty initial condition in the distant past. The rate conservation principle of Palm calculus is then used to identify the stability condition of this system, namely the condition on the interference sequence and arrival rates guaranteeing the finiteness of this minimal regime. Remarkably, the rate conservation principle also provides a closed form expression for its mean queue size. When the stability condition holds, this minimal solution is the unique stationary regime, provided it has finite second moments, and this is the case if the arrival rate is small enough. In addition, there exists a range of small initial conditions for which the dynamics is attracted to the minimal regime. Surprisingly however, there exists another range of larger though finite initial conditions for which the dynamics diverges, even though stability criterion holds.", "venue": "The Annals of Applied Probability", "authors": ["Abishek  Sankararaman", "Franccois  Baccelli", "Sergey  Foss"], "year": 2019, "n_citations": 11}
{"id": 2229139, "s2_id": "e601cdf0f701258115087c2339566a5ce4c84b70", "title": "Reproducibility Report for the Paper: Modeling of Request Cloning in Cloud Server Systems using Processor Sharing", "abstract": "The authors have uploaded their artifact on Zenodo, which ensures a long-term retention of the artifact. The code is suitably documented, and some examples are given. A minimalistic overall description of the engine is provided. The artifact allows to setup the environment quite quickly, and the dependencies are well documented. The process to regenerate data for the figures in the paper completes, and all results are reproducible. \nThis paper can thus receive the Artifacts Available badge and the Artifacts Evaluated-Functional. Given the high quality of the artifact, also the Artifacts Evaluated-Reusable badge can be assigned.", "venue": "ArXiv", "authors": ["Alessandro  Pellegrini"], "year": 2020, "n_citations": 0}
{"id": 2234055, "s2_id": "fee72aa6b95b11cd2925b0ec4f6c09963ffdf028", "title": "Competitive Online Optimization under Inventory Constraints", "abstract": "This paper studies online optimization under inventory (budget) constraints. While online optimization is a well-studied topic, versions with inventory constraints have proven difficult. We consider a formulation of inventory-constrained optimization that is a generalization of the classic one-way trading problem and has a wide range of applications. We present a new algorithmic framework, CR-Pursuit, and prove that it achieves the optimal competitive ratio among all deterministic algorithms (up to a problem-dependent constant factor) for inventory-constrained online optimization. Our algorithm and its analysis not only simplify and unify the state-of-the-art results for the standard one-way trading problem, but they also establish novel bounds for generalizations including concave revenue functions. For example, for one-way trading with price elasticity, CR-Pursuit achieves a competitive ratio within a small additive constant (i.e., 1/3) to the lower bound of ln\u04e8+1, where \u04e8 is the ratio between the maximum and minimum base prices.", "venue": "Abstracts of the 2019 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems", "authors": ["Qiulin  Lin", "Hanling  Yi", "John  Pang", "Minghua  Chen", "Adam  Wierman", "Michael  Honig", "Yuanzhang  Xiao"], "year": 2019, "n_citations": 4}
{"id": 2235908, "s2_id": "8ff8cba883d0c85b45f720a21127596a98a3d5eb", "title": "A Case Study: Task Scheduling Methodologies for High Speed Computing Systems", "abstract": "High Speed computing meets ever increasing real-time computational demands through the leveraging of flexibility and parallelism. The flexibility is achieved when computing platform designed with heterogeneous resources to support multifarious tasks of an application where as task scheduling brings parallel processing. The efficient task scheduling is critical to obtain optimized performance in heterogeneous computing Systems (HCS). In this paper, we brought a review of various application scheduling models which provide parallelism for homogeneous and heterogeneous computing systems. In this paper, we made a review of various scheduling methodologies targeted to high speed computing systems and also prepared summary chart. The comparative study of scheduling methodologies for high speed computing systems has been carried out based on the attributes of platform & application as well. The attributes are execution time, nature of task, task handling capability, type of host & computing platform. Finally a summary chart has been prepared and it demonstrates that the need of developing scheduling methodologies for Heterogeneous Reconfigurable Computing Systems (HRCS) which is an emerging high speed computing platform for real time applications.", "venue": "ArXiv", "authors": ["Mahendra  Vucha", "Arvind  Rajawat"], "year": 2015, "n_citations": 4}
{"id": 2237535, "s2_id": "de2cec421c7ee3ff60c999d6f76bbb7709fea109", "title": "Network Resilience Assessment via QoS Degradation Metrics: An Algorithmic Approach", "abstract": "This paper focuses on network resilience to perturbation of edge weight. Other than connectivity, many network applications nowadays rely upon some measure of network distance between a pair of connected nodes. In these systems, a metric related to network functionality is associated to each edge. A pair of nodes only being functional if the weighted, shortest-path distance between the pair is below a given threshold T. Consequently, a natural question is on which degree the change of edge weights can damage the network functionality? With this motivation, we study a new problem, Quality of Service Degradation : given a set of pairs, find a minimum budget to increase the edge weights which ensures the distance between each pair exceeds T. We introduce four algorithms with theoretical performance guarantees for this problem. Each of them has its own strength in trade-off between effectiveness and running time, which are illustrated both in theory and comprehensive experimental evaluation.", "venue": "Abstracts of the 2019 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems", "authors": ["Lan N. Nguyen", "My T. Thai"], "year": 2019, "n_citations": 3}
{"id": 2238289, "s2_id": "553852955f37e91e319bd602f73f0aad8afd21ef", "title": "Exploiting Acceleration Features of LabVIEW platform for Real-Time GNSS Software Receiver Optimization", "abstract": "This paper presents the new generation of LabVIEW-based GPS receiver testbed that is based on National Instruments' (NI) LabVIEW (LV) platform in conjunction to C/C++ dynamic link libraries (DLL) used inside the platform for performance execution. This GPS receiver has been optimized for real-time operation and has been developed for fast prototyping and easiness on future additions and implementations to the system. The receiver DLLs are divided into three baseband modules: acquisition, tracking, and navigation. The openness of received baseband modules allows for extensive research topics such as signal quality improvement on GPS-denied areas, signal spoofing, and signal interferences. The hardware used in the system was chosen with an effort to achieve portability and mobility in the SDR receiver. Several acceleration factors that accomplish real-time operation and that are inherent to LabVIEW mechanisms, such as multithreading, parallelization and dedicated loop-structures, are discussed. The proposed SDR also exploits C/C++ optimization techniques for single-instruction multiple-data (SIMD) capable processors in software correlators for real-time operation of GNSS tracking loops. It is demonstrated that LabVIEW-based solutions provide competitive real-time solutions for fast prototyping of receiver algorithms.", "venue": "ArXiv", "authors": ["Erick  Schmidt", "David  Akopian"], "year": 2019, "n_citations": 3}
{"id": 2238783, "s2_id": "cde39ce861e4c7514ee07fd91b6b8aac50cbf01b", "title": "RedisGraph GraphBLAS Enabled Graph Database", "abstract": "RedisGraph is a Redis module developed by Redis Labs to add graph database functionality to the Redis database. RedisGraph represents connected data as adjacency matrices. By representing the data as sparse matrices and employing the power of GraphBLAS (a highly optimized library for sparse matrix operations), RedisGraph delivers a fast and efficient way to store, manage and process graphs. Initial benchmarks indicate that RedisGraph is significantly faster than comparable graph databases.", "venue": "2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Pieter  Cailliau", "Tim  Davis", "Vijay  Gadepally", "Jeremy  Kepner", "Roi  Lipman", "Jeffrey  Lovitz", "Keren  Ouaknine"], "year": 2019, "n_citations": 10}
{"id": 2242328, "s2_id": "061e4339df833cba739abf8c743199af24256451", "title": "Reinforcement Learning-based Admission Control in Delay-sensitive Service Systems", "abstract": "Ensuring quality of service (QoS) guarantees in service systems is a challenging task, particularly when the system is composed of more fine-grained services, such as service function chains. An important QoS metric in service systems is the end-to-end delay, which becomes even more important in delay-sensitive applications, where the jobs must be completed within a time deadline. Admission control is one way of providing end-to-end delay guarantee, where the controller accepts a job only if it has a high probability of meeting the deadline. In this paper, we propose a reinforcement learning-based admission controller that guarantees a probabilistic upper-bound on the end-to-end delay of the service system, while minimizes the probability of unnecessary rejections. Our controller only uses the queue length information of the network and requires no knowledge about the network topology or system parameters. Since long-term performance metrics are of great importance in service systems, we take an average-reward reinforcement learning approach, which is well suited to infinite horizon problems. Our evaluations verify that the proposed RL-based admission controller is capable of providing probabilistic bounds on the end-to-end delay of the network, without using system model information.", "venue": "GLOBECOM 2020 - 2020 IEEE Global Communications Conference", "authors": ["Majid  Raeis", "Ali  Tizghadam", "Alberto  Leon-Garcia"], "year": 2020, "n_citations": 5}
{"id": 2242994, "s2_id": "149225fe54518f88465f368afe35efee12803968", "title": "Towards an Achievable Performance for the Loop Nests", "abstract": "Numerous code optimization techniques, including loop nest optimizations, have been developed over the last four decades. Loop optimization techniques transform loop nests to improve the performance of the code on a target architecture, including exposing parallelism. Finding and evaluating an optimal, semantic-preserving sequence of transformations is a complex problem. The sequence is guided using heuristics and/or analytical models and there is no way of knowing how close it gets to optimal performance or if there is any headroom for improvement. This paper makes two contributions. First, it uses a comparative analysis of loop optimizations/transformations across multiple compilers to determine how much headroom may exist for each compiler. And second, it presents an approach to characterize the loop nests based on their hardware performance counter values and a Machine Learning approach that predicts which compiler will generate the fastest code for a loop nest. The prediction is made for both auto-vectorized, serial compilation and for auto-parallelization. The results show that the headroom for state-of-the-art compilers ranges from 1.10x to 1.42x for the serial code and from 1.30x to 1.71x for the auto-parallelized code. These results are based on the Machine Learning predictions.", "venue": "LCPC", "authors": ["Aniket  Shivam", "Neftali  Watkinson", "Alexandru  Nicolau", "David A. Padua", "Alexander V. Veidenbaum"], "year": 2018, "n_citations": 2}
{"id": 2243813, "s2_id": "98f9343441c0e94b5af2d2a93526491c3d0f8a8f", "title": "Contrasting Effects of Replication in Parallel Systems: From Overload to Underload and Back", "abstract": "Task replication has recently been advocated as a practical solution to reduce latencies in parallel systems. In addition to several convincing empirical studies, analytical results have been provided, yet under some strong assumptions such as independent service times of the replicas, which may lend themselves to some contrasting and perhaps contriving behavior. For instance, under the independence assumption, an overloaded system can be stabilized by a replication factor, but can be sent back in overload through further replication. Motivated by the need to dispense with such common and restricting assumptions, which may cause unexpected behavior, we develop a unified and general theoretical framework to compute tight bounds on the distribution of response times in general replication systems. These results immediately lend themselves to the optimal number of replicas minimizing response time quantiles, depending on the parameters of the system (e.g., the degree of correlation amongst replicas).", "venue": "SIGMETRICS 2016", "authors": ["Felix  Poloczek", "Florin  Ciucu"], "year": 2016, "n_citations": 15}
{"id": 2247683, "s2_id": "0f2d3547f0eead61e4b3ed387d616fad0d5d025c", "title": "Performance Analysis of Sequential Method for HandOver in Cognitive Radio Networks", "abstract": "This paper has been withdrawn by the author due to a crucial problem in Lemma 3. This equation must be changed.", "venue": "ArXiv", "authors": ["Hossein  Shokri", "Mohammad  Mozaffari", "Adnan  Gavili", "Masoumeh  Nasiri-Kenari"], "year": 2011, "n_citations": 0}
{"id": 2249426, "s2_id": "f47647e953159c341c4dfd10eb5ab4f746aff6cd", "title": "kEDM: A Performance-portable Implementation of Empirical Dynamic Modeling using Kokkos", "abstract": "Empirical Dynamic Modeling (EDM) is a state-of-the-art non-linear time-series analysis framework. Despite its wide applicability, EDM was not scalable to large datasets due to its expensive computational cost. To overcome this obstacle, researchers have attempted and succeeded in accelerating EDM from both algorithmic and implementational aspects. In previous work, we developed a massively parallel implementation of EDM targeting HPC systems (mpEDM). However, mpEDM maintains different backends for different architectures. This design becomes a burden in the increasingly diversifying HPC systems, when porting to new hardware. In this paper, we design and develop a performance-portable implementation of EDM based on the Kokkos performance portability framework (kEDM), which runs on both CPUs and GPUs while based on a single codebase. Furthermore, we optimize individual kernels specifically for EDM computation, and use real-world datasets to demonstrate up to 5.5 \u00d7 speedup compared to mpEDM in convergent cross mapping computation.", "venue": "PEARC", "authors": ["Keichi  Takahashi", "Wassapon  Watanakeesuntorn", "Kohei  Ichikawa", "Joseph  Park", "Ryousei  Takano", "Jason  Haga", "George  Sugihara", "Gerald M. Pao"], "year": 2021, "n_citations": 0}
{"id": 2251633, "s2_id": "d65f897b7cea2761f88411e757e9587c0282cb41", "title": "On data skewness, stragglers, and MapReduce progress indicators", "abstract": "We tackle the problem of predicting the performance of MapReduce applications designing accurate progress indicators, which keep programmers informed on the percentage of completed computation time during the execution of a job. This is especially important in pay-as-you-go cloud environments, where slow jobs can be aborted in order to avoid excessive costs. Performance predictions can also serve as a building block for several profile-guided optimizations. By assuming that the running time depends linearly on the input size, state-of-the-art techniques can be seriously harmed by data skewness, load unbalancing, and straggling tasks. We thus design a novel profile-guided progress indicator, called NearestFit, that operates without the linear hypothesis assumption in a fully online way (i.e., without resorting to profile data collected from previous executions). NearestFit exploits a careful combination of nearest neighbor regression and statistical curve fitting techniques. Fine-grained profiles required by our theoretical progress model are approximated through space- and time-efficient data streaming algorithms. We implemented NearestFit on top of Hadoop 2.6.0. An extensive empirical assessment over the Amazon EC2 platform on a variety of benchmarks shows that its accuracy is very good, even when competitors incur non-negligible errors and wide prediction fluctuations.", "venue": "SoCC", "authors": ["Emilio  Coppa", "Irene  Finocchi"], "year": 2015, "n_citations": 28}
{"id": 2255486, "s2_id": "cb1be5fa56480910da062c50a55e15cc7ac9294f", "title": "SDN Architecture and Southbound APIs for IPv6 Segment Routing Enabled Wide Area Networks", "abstract": "The SRv6 architecture (segment routing based on IPv6 data plane) is a promising solution to support services like Traffic Engineering, service function chaining and virtual private networks in IPv6 backbones and datacenters. The SRv6 architecture has interesting scalability properties as it reduces the amount of state information that needs to be configured in the nodes to support the network services. In this paper, we describe the advantages of complementing the SRv6 technology with an software defined networking (SDN) based approach in backbone networks. We discuss the architecture of a SRv6 enabled network based on Linux nodes. In addition, we present the design and implementation of the Southbound API between the SDN controller and the SRv6 device. We have defined a data-model and four different implementations of the API, respectively based on gRPC, REST, NETCONF, and remote command line interface. Since it is important to support both the development and testing aspects we have realized an Intent-based emulation system to build realistic and reproducible experiments. This collection of tools automate most of the configuration aspects relieving the experimenter from a significant effort. Finally, we have realized an evaluation of some performance aspects of our architecture and of the different variants of the Southbound APIs and we have analyzed the effects of the configuration updates in the SRv6 enabled nodes.", "venue": "IEEE Transactions on Network and Service Management", "authors": ["Pier Luigi Ventre", "Mohammad Mahdi Tajiki", "Stefano  Salsano", "Clarence  Filsfils"], "year": 2018, "n_citations": 26}
{"id": 2256406, "s2_id": "8e6c189e5437c43b40c43d62685b71d415f8e430", "title": "Randomized C/C++ dynamic memory allocator", "abstract": "Dynamic memory management requires special attention in programming. It should be fast and secure at the same time. This paper proposes a new randomized dynamic memory management algorithm designed to meet these requirements. Randomization is a key feature intended to protect applications from \u201cuse-after-free\u201d or similar attacks. At the same time, the state in the algorithm consists only of one pointer, so it does not consume extra memory for itself. However, our algorithm is not a universal solution. It does not solve the memory fragmentation problem and it needs further development and testing.", "venue": "ArXiv", "authors": ["Irina Aleksandrovna Astrakhantseva", "Roman Gennadevich Astrakhantsev", "Arseny Viktorovich Mitin"], "year": 2021, "n_citations": 0}
{"id": 2259182, "s2_id": "efed8a634ef2617e270084afe95321500ecb0258", "title": "Minimizing maintenance cost involving flow-time and tardiness penalty with unequal release dates", "abstract": "This paper proposes important and useful results relating to the minimization of the sum of the flow time and the tardiness of tasks or jobs with unequal release dates (occurrence date), with application to maintenance planning and scheduling. First, the policy of real-time maintenance is defined for minimizing the cost of tardiness and critical states. The required local optimality rule (flow time and tardiness rule) is proved, in order to minimize the sum or the linear combination of the tasks' flow time and tardiness costs. This rule has served to design a scheduling algorithm, with O(n3) complexity when it is applied to schedule a set of n tasks on one processor. To evaluate its performance, the results are compared with a lower bound that is provided in a numerical case study. Using this algorithm in combination with the tasks' urgency criterion, a real-time algorithm is developed to schedule the tasks on a parallel processors. This latter algorithm is finally applied to schedule and assign preventive maintenance tasks to processors in the case of a distributed system. Its efficiency enables, as shown in the numerical example, the cost of preventive maintenance tasks expressed as the sum of the tasks' tardiness and flow time to be minimized. This corresponds to the costs of critical states and of tardiness of preventive maintenance.", "venue": "ArXiv", "authors": ["Kondo H. Adjallah", "Kossi P. Adzakpa"], "year": 2020, "n_citations": 6}
{"id": 2259492, "s2_id": "753fa338f64f2ebaf8a5ca1d9d3c84c3dbbd76ef", "title": "Measuring Software Performance on Linux", "abstract": "Measuring and analyzing the performance of software has reached a high complexity, caused by more advanced processor designs and the intricate interaction between user programs, the operating system, and the processor's microarchitecture. In this report, we summarize our experience about how performance characteristics of software should be measured when running on a Linux operating system and a modern processor. In particular, (1) We provide a general overview about hardware and operating system features that may have a significant impact on timing and how they interact, (2) we identify sources of errors that need to be controlled in order to obtain unbiased measurement results, and (3) we propose a measurement setup for Linux to minimize errors. Although not the focus of this report, we describe the measurement process using hardware performance counters, which can faithfully reflect the real bottlenecks on a given processor. Our experiments confirm that our measurement setup has a large impact on the results. More surprisingly, however, they also suggest that the setup can be negligible for certain analysis methods. Furthermore, we found that our setup maintains significantly better performance under background load conditions, which means it can be used to improve software in high-performance applications.", "venue": "ArXiv", "authors": ["Martin  Becker", "Samarjit  Chakraborty"], "year": 2018, "n_citations": 3}
{"id": 2260364, "s2_id": "37942ebb6435eb75df73361fb6fe2c029cb47374", "title": "Collective Mind, Part II: Towards Performance- and Cost-Aware Software Engineering as a Natural Science", "abstract": "Nowadays, engineers have to develop software often without even knowing which hardware it will eventually run on in numerous mobile phones, tablets, desktops, laptops, data centers, supercomputers and cloud services. Unfortunately, optimizing compilers are not keeping pace with ever increasing complexity of computer systems anymore and may produce severely underperforming executable codes while wasting expensive resources and energy. \nWe present our practical and collaborative solution to this problem via light-weight wrappers around any software piece when more than one implementation or optimization choice available. These wrappers are connected with a public Collective Mind autotuning infrastructure and repository of knowledge (c-mind.org/repo) to continuously monitor various important characteristics of these pieces (computational species) across numerous existing hardware configurations together with randomly selected optimizations. Similar to natural sciences, we can now continuously track winning solutions (optimizations for a given hardware) that minimize all costs of a computation (execution time, energy spent, code size, failures, memory and storage footprint, optimization time, faults, contentions, inaccuracy and so on) of a given species on a Pareto frontier along with any unexpected behavior. The community can then collaboratively classify solutions, prune redundant ones, and correlate them with various features of software, its inputs (data sets) and used hardware either manually or using powerful predictive analytics techniques. Our approach can then help create a large, realistic, diverse, representative, and continuously evolving benchmark with related optimization knowledge while gradually covering all possible software and hardware to be able to predict best optimizations and improve compilers and hardware depending on usage scenarios and requirements.", "venue": "ArXiv", "authors": ["Grigori  Fursin", "Abdul Wahid Memon", "Christophe  Guillon", "Anton  Lokhmotov"], "year": 2015, "n_citations": 13}
{"id": 2261083, "s2_id": "dcd0523514682df7c242855a83a03f30e3dffe49", "title": "Plumber: Diagnosing and Removing Performance Bottlenecks in Machine Learning Data Pipelines", "abstract": "Input pipelines, which ingest and transform input data, are an essential part of training Machine Learning (ML) models. However, it is challenging to implement efficient input pipelines, as it requires reasoning about parallelism, asynchrony, and variability in fine-grained profiling information. Our analysis of over 2 million ML jobs in Google datacenters reveals that a significant fraction of model training jobs could benefit from faster input data pipelines. At the same time, our analysis reveals that most jobs do not saturate host hardware, pointing in the direction of software-based bottlenecks. Motivated by these findings, we propose Plumber, a tool for finding bottlenecks in ML input pipelines. Plumber uses an extensible and interprettable operational analysis analytical model to automatically tune parallelism, prefetching, and caching under host resource constraints. Across five representative ML pipelines, Plumber obtains speedups of up to 46\u00d7 for misconfigured pipelines. By automating caching, Plumber obtains end-to-end speedups of over 40% compared to state-of-the-art tuners.", "venue": "ArXiv", "authors": ["Michael  Kuchnik", "Ana  Klimovic", "Jiri  Simsa", "George  Amvrosiadis", "Virginia  Smith"], "year": 2021, "n_citations": 0}
{"id": 2266252, "s2_id": "4de9195fb8a36edba60bd8e2e9da13986721af44", "title": "Direct N-body application on low-power and energy-efficient parallel architectures", "abstract": "The aim of this work is to quantitatively evaluate the impact of computation on the energy consumption on ARM MPSoC platforms, exploiting CPUs, embedded GPUs and FPGAs. One of them possibly represents the future of High Performance Computing systems: a prototype of an Exascale supercomputer. Performance and energy measurements are made using a state-of-the-art direct $N$-body code from the astrophysical domain. We provide a comparison of the time-to-solution and energy delay product metrics, for different software configurations. We have shown that FPGA technologies can be used for application kernel acceleration and are emerging as a promising alternative to \"traditional\" technologies for HPC, which purely focus on peak-performance than on power-efficiency.", "venue": "PARCO", "authors": ["David  Goz", "G.  Ieronymakis", "V.  Papaefstathiou", "N.  Dimou", "S.  Bertocco", "A.  Ragagnin", "Luca  Tornatore", "Giuliano  Taffoni", "I.  Coretti"], "year": 2019, "n_citations": 1}
{"id": 2267954, "s2_id": "ec690fad685ca160ad2745315a69da6ffb75b95c", "title": "Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems", "abstract": "Innovations in Next-Generation Sequencing are enabling generation of DNA sequence data at ever faster rates and at very low cost. For example, the Illumina NovaSeq 6000 sequencer can generate 6 Terabases of data in less than two days, sequencing nearly 20 Billion short DNA fragments called reads at the low cost of $1000 per human genome. Large sequencing centers typically employ hundreds of such systems. Such highthroughput and low-cost generation of data underscores the need for commensurate acceleration in downstream computational analysis of the sequencing data. A fundamental step in downstream analysis is mapping of the reads to a long reference DNA sequence, such as a reference human genome. Sequence mapping is a compute-intensive step that accounts for more than 30% of the overall time of the GATK (Genome Analysis ToolKit) best practices workflow. BWA-MEM is one of the most widely used tools for sequence mapping and has tens of thousands of users. In this work, we focus on accelerating BWA-MEM through an efficient architecture aware implementation, while maintaining identical output. The volume of data requires distributed computing and is usually processed on clusters or cloud deployments with multicore processors usually being the platform of choice. Since the application can be easily parallelized across multiple sockets (even across distributed memory systems) by simply distributing the reads equally, we focus on performance improvements on a single socket multicore processor. BWA-MEM run time is dominated by three kernels, collectively responsible for more than 85% of the overall compute time. We improved the performance of the three kernels by 1) using techniques to improve cache reuse, 2) simplifying the algorithms, 3) replacing many small memory allocations with a few large contiguous ones to improve hardware prefetching of data, 4) software prefetching of data, and 5) utilization of SIMD wherever applicable and massive reorganization of the source code to enable these improvements. As a result, we achieved nearly 2\u00d7, 183\u00d7, and 8\u00d7 speedups on the three kernels, respectively, resulting in up to 3:5\u00d7 and 2:4\u00d7 speedups on end-to-end compute time over the original BWA-MEM on single thread and single socket of Intel Xeon Skylake processor. To the best of our knowledge, this is the highest reported speedup over BWA-MEM (running on a single CPU) while using a single CPU or a single CPU-single GPGPU/FPGA combination. Source-code: https://github.com/bwa-mem2/bwa-mem2", "venue": "2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Md.  Vasimuddin", "Sanchit  Misra", "Heng  Li", "Srinivas  Aluru"], "year": 2019, "n_citations": 57}
{"id": 2270936, "s2_id": "2d5711cc4b0a2aef74a63fd1101a1dcd91c4d043", "title": "A Multi-GPU Programming Library for Real-Time Applications", "abstract": "We present MGPU, a C++ programming library targeted at single-node multi-GPU systems. Such systems combine disproportionate floating point performance with high data locality and are thus well suited to implement real-time algorithms. We describe the library design, programming interface and implementation details in light of this specific problem domain. The core concepts of this work are a novel kind of container abstraction and MPI-like communication methods for intra-system communication. We further demonstrate how MGPU is used as a framework for porting existing GPU libraries to multi-device architectures. Putting our library to the test, we accelerate an iterative non-linear image reconstruction algorithm for real-time magnetic resonance imaging using multiple GPUs. We achieve a speed-up of about 1.7 using 2 GPUs and reach a final speed-up of 2.1 with 4 GPUs. These promising results lead us to conclude that multi-GPU systems are a viable solution for real-time MRI reconstruction as well as signal-processing applications in general.", "venue": "ICA3PP", "authors": ["Sebastian  Schaetz", "Martin  Uecker"], "year": 2012, "n_citations": 44}
{"id": 2271787, "s2_id": "ae7d2022bb2d5426e828396946b54ae63265b15a", "title": "Analysis Framework for Opportunistic Spectrum OFDMA and Its Application to the IEEE 802.22 Standard", "abstract": "We present an analytical model that enables the evaluation of opportunistic spectrum orthogonal frequency division multiple-access (OS-OFDMA) networks using metrics such as blocking probability or, most importantly, throughput. The core feature of the model, based on a discrete-time Markov chain, is the consideration of different channel and subchannel allocation strategies under different primary and secondary user types, traffic, and priority levels. The analytical model also assesses the impact of different spectrum sensing strategies on the throughput of OS-OFDMA network. In addition, we consider studies of cochannel interference. The analysis is applied to the IEEE 802.22 standard to evaluate the impact of the two-stage spectrum sensing strategy and the varying temporal activity of wireless microphones on the system throughput. In addition to the analytical model, we present a set of comprehensive simulation results using NS-2 related to the delay performance of the OS-OFDMA system considered. Our study suggests that OS-OFDMA with subchannel notching and channel bonding could provide almost ten times higher throughput compared with a design without these options when the activity and density of wireless microphones are very high. Furthermore, we confirm that OS-OFDMA implementation without subchannel notching, which is used in the IEEE 802.22, can support the real-time and non-real-time quality of service classes, provided that the temporal activity of wireless microphones is moderate (with sparse wireless microphone distribution, with light urban population density and short duty cycles). Finally, the two-stage spectrum sensing option improves the OS-OFDMA throughput, provided that the length of spectrum sensing at every stage is optimized using our model.", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Jihoon  Park", "Przemyslaw  Pawelczak", "P\u00e5l  Gr\u00f8nsund", "Danijela  Cabric"], "year": 2012, "n_citations": 12}
{"id": 2275695, "s2_id": "a8afd0cc87fac5f125a081e39b6b476295261628", "title": "Faster Base64 Encoding and Decoding Using AVX2 Instructions", "abstract": "Web developers use base64 formats to include images, fonts, sounds, and other resources directly inside HTML, JavaScript, JSON, and XML files. We estimate that billions of base64 messages are decoded every day. We are motivated to improve the efficiency of base64 encoding and decoding. Compared to state-of-the-art implementations, we multiply the speeds of both the encoding (\u2248 10 \u00d70) and the decoding (\u2248 7 \u00d7). We achieve these good results by using the single-instruction-multiple-data instructions available on recent Intel processors (AVX2). Our accelerated software abides by the specification and reports errors when encountering characters outside of the base64 set. It is available online as free software under a liberal license.", "venue": "ACM Trans. Web", "authors": ["Wojciech  Mula", "Daniel  Lemire"], "year": 2018, "n_citations": 8}
{"id": 2276586, "s2_id": "944d9db1b23442f4d6fa90c6272d53c651e089dd", "title": "Dynamically Provisioning Cray DataWarp Storage", "abstract": "Complex applications and workflows needs are often exclusively expressed in terms of computational resources on HPC systems. In many cases, other resources like storage or network are not allocatable and are shared across the entire HPC system. By looking at the storage resource in particular, any workflow or application should be able to select both its preferred data manager and its required storage capability or capacity. To achieve such a goal, new mechanisms should be introduced. In this work, we introduce such a mechanism for dynamically provision a data management system on top of storage devices. We particularly focus our effort on deploying a BeeGFS instance across multiple DataWarp nodes on a Cray XC50 system. However, we also demonstrate that the same mechanism can be used to deploy BeeGFS on non-Cray system.", "venue": "ArXiv", "authors": ["Franccois  Tessier", "Maxime  Martinasso", "Matteo  Chesi", "Mark  Klein", "Miguel  Gila"], "year": 2019, "n_citations": 0}
{"id": 2284651, "s2_id": "f173b8f5b0044b9507729518c232ff7a3a1b7f7d", "title": "Dynamic Partial Cooperative MIMO System for Delay-Sensitive Applications with Limited Backhaul Capacity", "abstract": "Considering backhaul consumption, it may not be the best choice to engage all the time in full cooperative MIMO for interference mitigation. In this paper, we propose a novel downlink partial cooperative MIMO (Pco-MIMO) physical layer (PHY) scheme, which allows flexible tradeoff between the partial data cooperation level and the backhaul consumption. Based on this Pco-MIMO scheme, we consider dynamic transmit power and rate allocation according to the imperfect channel state information at transmitters (CSIT) and the queue state information (QSI) to minimize the average delay cost subject to average backhaul consumption constraints and average power constraints. The delay-optimal control problem is formulated as an infinite horizon average cost constrained partially observed Markov decision process (CPOMDP). By exploiting the special structure in our problem, we derive an equivalent Bellman Equation to solve the CPOMDP. To reduce computational complexity and facilitate distributed implementation, we propose a distributed online learning algorithm to estimate the per-flow potential functions and Lagrange multipliers (LMs) and a distributed online stochastic partial gradient algorithm to obtain the power and rate control policy. We prove the convergence and asymptotic optimality of the proposed solution.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Ying  Cui", "Vincent K. N. Lau", "Huang  Huang"], "year": 2013, "n_citations": 5}
{"id": 2286095, "s2_id": "ed1d9daf0ff645ab6934c2a2c4103eb8f39af064", "title": "Analysis of Load Balancing in Large Heterogeneous Processor Sharing Systems", "abstract": "We analyze randomized dynamic load balancing schemes for multi-server processor sharing systems when the number of servers in the system is large and the servers have heterogeneous service rates. In particular, we focus on the classical power-of-two load balancing scheme and a variant of it in which a newly arrived job is assigned to the server having the least instantaneous Lagrange shadow cost among two randomly chosen servers. The instantaneous Lagrange shadow cost at a server is given by the ratio of the number of unfinished jobs at the server to the capacity of the server. Two different approaches of analysis are presented for each scheme. For exponential job length distribution, the analysis is done using the mean field approach and for more general job length distributions the analysis is carried out assuming an asymptotic independence property. Analytical expressions to compute mean sojourn time of jobs are found for both schemes. Asymptotic insensitivity of the schemes to the type of job length distribution is established. Numerical results are presented to validate the theoretical results and to show that, unlike the homogeneous scenario, the power-of-two type schemes considered in this paper may not always result in better behaviour in terms of the mean sojourn time of jobs.", "venue": "ArXiv", "authors": ["Arpan  Mukhopadhyay", "Ravi  Mazumdar"], "year": 2013, "n_citations": 14}
{"id": 2288614, "s2_id": "28fd5f8829daab351a0cc68fd221c23ecf90b045", "title": "Persistent Spread Measurement for Big Network Data Based on Register Intersection", "abstract": "Persistent spread measurement is to count the number of distinct elements that persist in each network flow for predefined time periods. It has many practical applications, including detecting long-term stealthy network activities in the background of normal-user activities, such as stealthy DDoS attack, stealthy network scan, or faked network trend, which cannot be detected by traditional flow cardinality measurement. With big network data, one challenge is to measure the persistent spreads of a massive number of flows without incurring too much memory overhead as such measurement may be performed at the line speed by network processors with fast but small on-chip memory. We propose a highly compact Virtual Intersection HyperLogLog (VI-HLL) architecture for this purpose. It achieves far better memory efficiency than the best prior work of V-Bitmap, and in the meantime drastically extends the measurement range. Theoretical analysis and extensive experiments demonstrate that VI-HLL provides good measurement accuracy even in very tight memory space of less than 1 bit per flow.", "venue": "SIGMETRICS 2017", "authors": ["You  Zhou", "Yian  Zhou", "Min  Chen", "Shigang  Chen"], "year": 2017, "n_citations": 6}
{"id": 2291232, "s2_id": "ce2960107614495e564342ec36a7eb2c508182c1", "title": "Modeling Shared Cache Performance of OpenMP Programs using Reuse Distance", "abstract": "Performance modeling of parallel applications on multicore computers remains a challenge in computational co-design due to the complex design of multicore processors including private and shared memory hierarchies. We present a Scalable Analytical Shared Memory Model to predict the performance of parallel applications that runs on a multicore computer and shares the same level of cache in the hierarchy. This model uses a computationally efficient, probabilistic method to predict the reuse distance profiles, where reuse distance is a hardware architecture-independent measure of the patterns of virtual memory accesses. It relies on a stochastic, static basic block-level analysis of reuse profiles measured from the memory traces of applications ran sequentially on small instances rather than using a multi-threaded trace. The results indicate that the hit-rate predictions on the shared cache are accurate.", "venue": "ArXiv", "authors": ["Atanu  Barai", "Gopinath  Chennupati", "Nandakishore  Santhi", "Abdel-Hameed A. Badawy", "Stephan  Eidenbenz"], "year": 2019, "n_citations": 1}
{"id": 2293005, "s2_id": "1b070b6dbe7cc720805ef44dc69972f5b1f7969f", "title": "Deep Learning on Operational Facility Data Related to Large-Scale Distributed Area Scientific Workflows", "abstract": "Distributed computing platforms provide a robust mechanism to perform large-scale computations by splitting the task and data among multiple locations, possibly located thousands of miles apart geographically. Although such distribution of resources can lead to benefits, it also comes with its associated problems such as rampant duplication of file transfers increasing congestion, long job completion times, unexpected site crashing, suboptimal data transfer rates, unpredictable reliability in a time range, and suboptimal usage of storage elements. In addition, each sub-system becomes a potential failure node that can trigger system wide disruptions. In this vision paper, we outline our approach to leveraging Deep Learning algorithms to discover solutions to unique problems that arise in a system with computational infrastructure that is spread over a wide area. The presented vision, motivated by a real scientific use case from Belle II experiments, is to develop multilayer neural networks to tackle forecasting, anomaly detection and optimization challenges in a complex and distributed data movement environment. Through this vision based on Deep Learning principles, we aim to achieve reduced congestion events, faster file transfer rates, and enhanced site reliability.", "venue": "2017 IEEE 13th International Conference on e-Science (e-Science)", "authors": ["Alok  Singh", "Eric G. Stephan", "Malachi  Schram", "Ilkay  Altintas"], "year": 2017, "n_citations": 2}
{"id": 2293179, "s2_id": "1b12f7b66522d2d5773a359433c24dcc6baa06df", "title": "A Comparison of Push and Pull Techniques for AJAX", "abstract": "AJAX applications are designed to have high user interactivity and low user-perceived latency. Real-time dynamic Web data such as news headlines, stock tickers, and auction updates need to be propagated to the users as soon as possible. However, AJAX still suffers from the limitations of the Web's request/response architecture which prevents servers from pushing real-time dynamic web data. Such applications usually use a pull style to obtain the latest updates, where the client actively requests the changes based on a predefined interval. It is possible to overcome this limitation by adopting a push style of interaction where the server broadcasts data when a change occurs on the server side. Both these options have their own trade-offs. This paper explores the fundamental limits of browser-based applications and analyzes push solutions for AJAX technology. It also shows the results of an empirical study comparing push and pull.", "venue": "2007 9th IEEE International Workshop on Web Site Evolution", "authors": ["Engin  Bozdag", "Ali  Mesbah", "Arie van Deursen"], "year": 2007, "n_citations": 106}
{"id": 2294599, "s2_id": "131fe05509cf1ea46ef175486463675351cc5bab", "title": "A Note on Using Performance and Data Profiles for Training Algorithms", "abstract": "This article shows how to use performance and data profile benchmarking tools to improve the performance of algorithms. We propose to achieve this goal by defining and approximately solving suitable optimization problems involving the parameters of the algorithm under consideration. Because these problems do not have derivatives and may involve integer variables, we suggest using a mixed-integer derivative-free optimizer for this task. A numerical illustration is presented (using the BFO package), which indicates that the obtained gains are potentially significant.", "venue": "ACM Trans. Math. Softw.", "authors": ["Margherita  Porcelli", "Philippe L. Toint"], "year": 2019, "n_citations": 3}
{"id": 2302330, "s2_id": "1ebb3e4394e6ea17fb0ee438158a9ad89b54cab4", "title": "Modeling memory bandwidth patterns on NUMA machines with performance counters", "abstract": "Computers used for data analytics are often NUMA systems with multiple sockets per machine, multiple cores per socket, and multiple thread contexts per core. To get the peak performance out of these machines requires the correct number of threads to be placed in the correct positions on the machine. One particularly interesting element of the placement of memory and threads is the way it effects the movement of data around the machine, and the increased latency this can introduce to reads and writes. In this paper we describe work on modeling the bandwidth requirements of an application on a NUMA compute node based on the placement of threads. The model is parameterized by sampling performance counters during 2 application runs with carefully chosen thread placements. Evaluating the model with thousands of measurements shows a median difference from predictions of 2.34% of the bandwidth. The results of this modeling can be used in a number of ways varying from: Performance debugging during development where the programmer can be alerted to potentially problematic memory access patterns; To systems such as Pandia which take an application and predict the performance and system load of a proposed thread count and placement; To libraries of data structures such as Parallel Collections and Smart Arrays that can abstract from the user memory placement and thread placement issues when parallelizing code.", "venue": "ArXiv", "authors": ["Daniel  Goodman", "Roni  Haecki", "Tim  Harris"], "year": 2021, "n_citations": 0}
{"id": 2304198, "s2_id": "9524d4a6d4353643d883488b46d264b2d684da4a", "title": "A Fluid-Flow Interpretation of SCED Scheduling", "abstract": "We show that a fluid-flow interpretation of Service Curve Earliest Deadline First (SCED) scheduling simplifies deadline derivations for this scheduler. By exploiting the recently reported isomorphism between min-plus and max-plus network calculus and expressing deadlines in a max-plus algebra, deadline computations no longer require explicit pseudo-inverse computations. SCED deadlines are provided for latency-rate as well as a class of piecewise linear service curves.", "venue": "2018 30th International Teletraffic Congress (ITC 30)", "authors": ["J\u00f6rg  Liebeherr"], "year": 2018, "n_citations": 0}
{"id": 2305996, "s2_id": "dbc0e5e5cda117a89989937fc3406b7d3f1ea9b6", "title": "Benchmarking Big Data Systems: State-of-the-Art and Future Directions", "abstract": "The great prosperity of big data systems such as Hadoop in recent years makes the benchmarking of these systems become crucial for both research and industry communities. The complexity, diversity, and rapid evolution of big data systems gives rise to various new challenges about how we design generators to produce data with the 4V properties (i.e. volume, velocity, variety and veracity), as well as implement application-specific but still comprehensive workloads. However, most of the existing big data benchmarks can be described as attempts to solve specific problems in benchmarking systems. This article investigates the state-of-the-art in benchmarking big data systems along with the future challenges to be addressed to realize a successful and efficient benchmark.", "venue": "ArXiv", "authors": ["Rui  Han", "Zhen  Jia", "Wanling  Gao", "Xinhui  Tian", "Lei  Wang"], "year": 2015, "n_citations": 13}
{"id": 2306621, "s2_id": "4d86e6cdb61b4f5387ef21718a92a730a4846e27", "title": "Markov-modulated on/off processes for long-range dependent internet traffic", "abstract": "The aim of this paper is to use a very simple queuing model to compare a number of models from the literature which have been used to replicate the statistical nature of internet traffic and, in particular, the long-range dependence of this traffic. The four models all have the form of discrete time Markov-modulated processes (two other models are introduced for comparison purposes). \nWhile it is often stated that long-range dependence has a critical effect on queuing performance, it appears that the models used here do not well replicated the queuing performance of real internet traffic. In particular, they fail to replicate the mean queue length (and hence the mean delay) and the probability of the queue length exceeding a given level.", "venue": "ArXiv", "authors": ["Richard G. Clegg"], "year": 2006, "n_citations": 13}
{"id": 2307896, "s2_id": "f3bd30f7b62dc584c9d8a6883f4de9e4c066d6ef", "title": "CAMUS: A Framework to Build Formal Specifications for Deep Perception Systems Using Simulators", "abstract": "The topic of provable deep neural network robustness has raised considerable interest in recent years. Most research has focused on adversarial robustness, which studies the robustness of perceptive models in the neighbourhood of particular samples. However, other works have proved global properties of smaller neural networks. Yet, formally verifying perception remains uncharted. This is due notably to the lack of relevant properties to verify, as the distribution of possible inputs cannot be formally specified. We propose to take advantage of the simulators often used either to train machine learning models or to check them with statistical tests, a growing trend in industry. Our formulation allows us to formally express and verify safety properties on perception units, covering all cases that could ever be generated by the simulator, to the difference of statistical tests which cover only seen examples. Along with this theoretical formulation , we provide a tool to translate deep learning models into standard logical formulae. As a proof of concept, we train a toy example mimicking an autonomous car perceptive unit, and we formally verify that it will never fail to capture the relevant information in the provided inputs.", "venue": "ECAI", "authors": ["Julien  Girard-Satabin", "Guillaume  Charpiat", "Zakaria  Chihani", "Marc  Schoenauer"], "year": 2020, "n_citations": 1}
{"id": 2311217, "s2_id": "653940b861c30ab657fdc232b7f8edbf77893577", "title": "Stateful dataflow multigraphs: a data-centric model for performance portability on heterogeneous architectures", "abstract": "The ubiquity of accelerators in high-performance computing has driven programming complexity beyond the skill-set of the average domain scientist. To maintain performance portability in the future, it is imperative to decouple architecture-specific programming paradigms from the underlying scientific computations. We present the Stateful DataFlow multiGraph (SDFG), a data-centric intermediate representation that enables separating program definition from its optimization. By combining fine-grained data dependencies with high-level control-flow, SDFGs are both expressive and amenable to program transformations, such as tiling and double-buffering. These transformations are applied to the SDFG in an interactive process, using extensible pattern matching, graph rewriting, and a graphical user interface. We demonstrate SDFGs on CPUs, GPUs, and FPGAs over various motifs --- from fundamental computational kernels to graph analytics. We show that SDFGs deliver competitive performance, allowing domain scientists to develop applications naturally and port them to approach peak hardware performance without modifying the original scientific code.", "venue": "SC", "authors": ["Tal  Ben-Nun", "Johannes de Fine Licht", "Alexandros Nikolaos Ziogas", "Timo  Schneider", "Torsten  Hoefler"], "year": 2019, "n_citations": 30}
{"id": 2311480, "s2_id": "713b7a6d3b6c4d7a12d55fa9c74d6e4406286e76", "title": "Control Analysis of Packet Transmission Algorithms: Study on Fairness and Stability", "abstract": "This document is a study of fairness, feedback and stability notions of different packet transmission algorithms. We start the discussion with defining two scalable control algorithms namely primal and dual algorithm. We discuss the dual algorithm model and then understand the fair dual algorithm. Further, we discuss different notions of fairness under fair dual algorithm those correspond to TCP and RCP congestion control protocols. Feedback parameters are analyzed in each of these fairness algorithms and thus their stability is studied.", "venue": "ArXiv", "authors": ["Lokesh  Bommisetty"], "year": 2021, "n_citations": 0}
{"id": 2312123, "s2_id": "c95b7a9184b2edfb8bbb3616c390123bb5c0ad0b", "title": "TEEMon: A continuous performance monitoring framework for TEEs", "abstract": "Trusted Execution Environments (TEEs), such as Intel Software Guard eXtensions (SGX), are considered as a promising approach to resolve security challenges in clouds. TEEs protect the confidentiality and integrity of application code and data even against privileged attackers with root and physical access by providing an isolated secure memory area, i.e., enclaves. The security guarantees are provided by the CPU, thus even if system software is compromised, the attacker can never access the enclave's content. While this approach ensures strong security guarantees for applications, it also introduces a considerable runtime overhead in part by the limited availability of protected memory (enclave page cache). Currently, only a limited number of performance measurement tools for TEE-based applications exist and none offer performance monitoring and analysis during runtime. This paper presents TEEMon, the first continuous performance monitoring and analysis tool for TEE-based applications. TEEMon provides not only fine-grained performance metrics during runtime, but also assists the analysis of identifying causes of performance bottlenecks, e.g., excessive system calls. Our approach smoothly integrates with existing open-source tools (e.g., Prometheus or Grafana) towards a holistic monitoring solution, particularly optimized for systems deployed through Docker containers or Kubernetes and offers several dedicated metrics and visualizations. Our evaluation shows that TEEMon's overhead ranges from 5% to 17%.", "venue": "Middleware", "authors": ["Robert  Krahn", "Donald  Dragoti", "Franz  Gregor", "Do Le Quoc", "Valerio  Schiavoni", "Pascal  Felber", "Clenimar  Souza", "Andrey  Brito", "Christof  Fetzer"], "year": 2020, "n_citations": 3}
{"id": 2314973, "s2_id": "c7e167d136d371ed6899c028adc52934ed78e9a8", "title": "Scientific Computing Using Consumer Video-Gaming Hardware Devices", "abstract": "Commodity video-gaming hardware (consoles, graphics cards, tablets, etc.) performance has been advancing at a rapid pace owing to strong consumer demand and stiff market competition. Gaming hardware devices are currently amongst the most powerful and cost-effective computational technologies available in quantity. In this article, we evaluate a sample of current generation video-gaming hardware devices for scientific computing and compare their performance with specialized supercomputing general purpose graphics processing units (GPGPUs). We use the OpenCL SHOC benchmark suite, which is a measure of the performance of compute hardware on various different scientific application kernels, and also a popular public distributed computing application, Einstein@Home in the field of gravitational physics for the purposes of this evaluation.", "venue": "ArXiv", "authors": ["Glenn  Volkema", "Gaurav  Khanna"], "year": 2016, "n_citations": 0}
{"id": 2315517, "s2_id": "8fc8e07df2ef76af018851dca92f4f6f95edf2a4", "title": "Parallel implementation of a compatible high-order meshless method for the Stokes' equations", "abstract": "A parallel implementation of a compatible discretization scheme for steady-state Stokes problems is presented in this work. The scheme uses generalized moving least squares to generate differential operators and apply boundary conditions. This meshless scheme allows a high-order convergence for both the velocity and pressure, while also incorporates finite-difference-like sparse discretization. Additionally, the method is inherently scalable: the stencil generation process requires local inversion of matrices amenable to GPU acceleration, and the divergence-free treatment of velocity replaces the traditional saddle point structure of the global system with elliptic diagonal blocks amenable to algebraic multigrid. The implementation in this work uses a variety of Trilinos packages to exploit this local and global parallelism, and benchmarks demonstrating high-order convergence and weak scalability are provided.", "venue": "ArXiv", "authors": ["Quang-Thinh  Ha", "Paul A. Kuberry", "Nathaniel A. Trask", "Emily M. Ryan"], "year": 2021, "n_citations": 0}
{"id": 2317879, "s2_id": "86f9f5a46b19c96f9844eec9d383594151a81ff6", "title": "Speeding up Deep Learning with Transient Servers", "abstract": "Distributed training frameworks, like TensorFlow, have been proposed as a means to reduce the training time of deep learning models by using a cluster of GPU servers. While such speedups are often desirable-e.g., for rapidly evaluating new model designs-they often come with significantly higher monetary costs due to sublinear scalability. In this paper, we investigate the feasibility of using training clusters composed of cheaper transient GPU servers to get the benefits of distributed training without the high costs. We conduct the first large-scale empirical analysis, launching more than a thousand GPU servers of various capacities, aimed at understanding the characteristics of transient GPU servers and their impact on distributed training performance. Our study demonstrates the potential of transient servers with a speedup of 7.7X with more than 62.9% monetary savings for some cluster configurations. We also identify a number of important challenges and opportunities for redesigning distributed training frameworks to be transient-aware. For example, the dynamic cost and availability characteristics of transient servers suggest the need for frameworks to dynamically change cluster configurations to best take advantage of current conditions.", "venue": "2019 IEEE International Conference on Autonomic Computing (ICAC)", "authors": ["Shijian  Li", "Robert J. Walls", "Lijie  Xu", "Tian  Guo"], "year": 2019, "n_citations": 6}
{"id": 2318109, "s2_id": "61e81f3ffc2bc50d3b5d5b43b605827d8823029a", "title": "Designing and Implementing Data Warehouse for Agricultural Big Data", "abstract": "In recent years, precision agriculture that uses modern information and communication technologies is becoming very popular. Raw and semi-processed agricultural data are usually collected through various sources, such as: Internet of Thing (IoT), sensors, satellites, weather stations, robots, farm equipment, farmers and agribusinesses, etc. Besides, agricultural datasets are very large, complex, unstructured, heterogeneous, non-standardized, and inconsistent. Hence, the agricultural data mining is considered as Big Data application in terms of volume, variety, velocity and veracity. It is a key foundation to establishing a crop intelligence platform, which will enable resource efficient agronomy decision making and recommendations. In this paper, we designed and implemented a continental level agricultural data warehouse by combining Hive, MongoDB and Cassandra. Our data warehouse capabilities: (1) flexible schema; (2) data integration from real agricultural multi datasets; (3) data science and business intelligent support; (4) high performance; (5) high storage; (6) security; (7) governance and monitoring; (8) replication and recovery; (9) consistency, availability and partition tolerant; (10) distributed and cloud deployment. We also evaluate the performance of our data warehouse.", "venue": "BigData Congress", "authors": ["Vuong M. Ngo", "Nhien-An  Le-Khac", "M. Tahar Kechadi"], "year": 2019, "n_citations": 10}
{"id": 2318484, "s2_id": "813e0122bde7e5896ce312538b0b6c1d2c1cdd09", "title": "Optimal heavy-traffic queue length scaling in an incompletely saturated switch", "abstract": "We consider an input-queued switch operating under the MaxWeight scheduling algorithm. This system is interesting to study because it is a model for Internet routers and data center networks. Recently, it was shown that the MaxWeight algorithm has optimal heavy-traffic queue length scaling when all ports are uniformly saturated. Here we consider the case when an arbitrary number of ports are saturated (which we call the incompletely saturated case), and each port is allowed to saturate at a different rate. We use a recently developed drift technique to show that the heavy-traffic queue length under the MaxWeight scheduling algorithm has optimal scaling with respect to the switch size even in these cases.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Siva Theja Maguluri", "Sai Kiran Burle", "R.  Srikant"], "year": 2018, "n_citations": 9}
{"id": 2321760, "s2_id": "c32af135768f9a94206a9f55b1f8ad67c5f79a1a", "title": "When backpressure meets predictive scheduling", "abstract": "Motivated by the increasing popularity of learning and predicting human user behavior in communication and computing systems, in this paper, we investigate the fundamental benefit of predictive scheduling, i.e., predicting and pre-serving arrivals, in controlled queueing systems. Based on a lookahead-window prediction model, we first establish a novel queue-equivalence between the predictive queueing system with a fully-efficient scheduling scheme and an equivalent queueing system without prediction. This result allows us to analytically demonstrate that predictive scheduling necessarily improves system delay performance and drives it to zero with increasing prediction power. It also enables us to exactly determine the required prediction power for different systems and study its impact on tail delay. We then propose the Predictive, Backpressure, (PBP) algorithm for achieving optimal utility performance in such predictive systems. PBP efficiently incorporates prediction into stochastic system control and avoids the great complication due to the exponential state space growth in the prediction window size. We show that PBP achieves a utility performance that is within O(\u03b5) of the optimal, for any \u03b5>0, while guaranteeing that the system delay distribution is a shifted-to-the-left version of that under the original Backpressure algorithm. Hence, the average delay under PBP is strictly better than that under Backpressure, and vanishes with increasing prediction window size. This implies that the resulting utility-delay tradeoff with predictive scheduling can beat the known optimal [O(\u03b5), O(log(1/\u03b5))] tradeoff for systems without prediction.", "venue": "MobiHoc '14", "authors": ["Longbo  Huang", "Shaoquan  Zhang", "Minghua  Chen", "Xin  Liu"], "year": 2014, "n_citations": 2}
{"id": 2321909, "s2_id": "1bd5945679eaa6d94d10c24aa16ad643daac4ec0", "title": "Comparative study of performance of parallel alpha Beta Pruning for different architectures", "abstract": "Optimization of searching the best possible action depending on various states like state of environment, system goal etc. has been a major area of study in computer systems. In any search algorithm, searching best possible solution from the pool of every possibility known can lead to the construction of the whole state search space popularly called as minimax algorithm. This may lead to a impractical time complexities which may not be suitable for real time searching operations. One of the practical solution for the reduction in computational time is Alpha Beta pruning. Instead of searching for the whole state space, we prune the unnecessary branches, which helps reduce the time by significant amount. This paper focuses on the various possible implementations of the Alpha Beta pruning algorithms and gives an insight of what algorithm can be used for parallelism. Various studies have been conducted on how to make Alpha Beta pruning faster. Parallelizing Alpha Beta pruning for the GPUs specific architectures like mesh(CUDA) etc. or shared memory model(OpenMP) helps in the reduction of the computational time. This paper studies the comparison between sequential and different parallel forms of Alpha Beta pruning and their respective efficiency for the chess game as an application.", "venue": "2019 IEEE 9th International Conference on Advanced Computing (IACC)", "authors": ["Shubhendra Pal Singhal", "M.  Sridevi"], "year": 2019, "n_citations": 0}
{"id": 2329663, "s2_id": "bca4cf28caf723c273be027853d18d1a627e0758", "title": "Inferring Catchment in Internet Routing", "abstract": "BGP is the de-facto Internet routing protocol for exchanging prefix reachability information between Autonomous Systems (AS). It is a dynamic, distributed, path-vector protocol that enables rich expressions of network policies (typically treated as secrets). In this regime, where complexity is interwoven with information hiding, answering questions such as \"what is the expected catchment of the anycast sites of a content provider on the AS-level, if new sites are deployed?\", or \"how will load-balancing behave if an ISP changes its routing policy for a prefix?\", is a hard challenge. In this work, we present a formal model and methodology that takes into account policy-based routing and topological properties of the Internet graph, to predict the routing behavior of networks. We design algorithms that provide new capabilities for informative route inference (e.g., isolating the effect of randomness that is present in prior simulation-based approaches). We analyze the properties of these inference algorithms, and evaluate them using publicly available routing datasets and real-world experiments. The proposed framework can be useful in a number of applications: measurements, traffic engineering, network planning, Internet routing models, etc. As a use case, we study the problem of selecting a set of measurement vantage points to maximize route inference. Our methodology is general and can capture standard valley-free routing, as well as more complex topological and routing setups appearing in practice.", "venue": "ArXiv", "authors": ["Pavlos  Sermpezis", "Vasileios  Kotronis"], "year": 2019, "n_citations": 4}
{"id": 2330063, "s2_id": "190b630fcd80771fefd0a48f9646a3496d6d3caa", "title": "ProTuner: Tuning Programs with Monte Carlo Tree Search", "abstract": "We explore applying the Monte Carlo Tree Search (MCTS) algorithm in a notoriously difficult task: tuning programs for high-performance deep learning and image processing. We build our framework on top of Halide and show that MCTS can outperform the state-of-the-art beam-search algorithm. Unlike beam search, which is guided by greedy intermediate performance comparisons between partial and less meaningful schedules, MCTS compares complete schedules and looks ahead before making any intermediate scheduling decision. We further explore modifications to the standard MCTS algorithm as well as combining real execution time measurements with the cost model. Our results show that MCTS can outperform beam search on a suite of 16 real benchmarks.", "venue": "ArXiv", "authors": ["Ameer  Haj-Ali", "Hasan  Genc", "Qijing  Huang", "William  Moses", "John  Wawrzynek", "Krste  Asanovi'c", "Ion  Stoica"], "year": 2020, "n_citations": 6}
{"id": 2330087, "s2_id": "843b7c730b2ccfa584fa89b062d1b741bdf810f5", "title": "Waiting times in queueing networks with a single shared server", "abstract": "We study a queueing network with a single shared server that serves the queues in a cyclic order. External customers arrive at the queues according to independent Poisson processes. After completing service, a customer either leaves the system or is routed to another queue. This model is very generic and finds many applications in computer systems, communication networks, manufacturing systems, and robotics. Special cases of the introduced network include well-known polling models, tandem queues, systems with a waiting room, multi-stage models with parallel queues, and many others. A complicating factor of this model is that the internally rerouted customers do not arrive at the various queues according to a Poisson process, causing standard techniques to find waiting-time distributions to fail. In this paper, we develop a new method to obtain exact expressions for the Laplace\u2013Stieltjes transforms of the steady-state waiting-time distributions. This method can be applied to a wide variety of models which lacked an analysis of the waiting-time distribution until now.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Marko A. A. Boon", "Robert D. van der Mei", "Erik M. M. Winands"], "year": 2013, "n_citations": 9}
{"id": 2331399, "s2_id": "9904cbdd39ddb2f0307cefffe70b96f85cc16d37", "title": "A Survey on Agent-based Simulation Using Hardware Accelerators", "abstract": "Due to decelerating gains in single-core CPU performance, computationally expensive simulations are increasingly executed on highly parallel hardware platforms. Agent-based simulations, where simulated entities act with a certain degree of autonomy, frequently provide ample opportunities for parallelisation. Thus, a vast variety of approaches proposed in the literature demonstrated considerable performance gains using hardware platforms such as many-core CPUs and GPUs, merged CPU-GPU chips as well as Field Programmable Gate Arrays. Typically, a combination of techniques is required to achieve high performance for a given simulation model, putting substantial burden on modellers. To the best of our knowledge, no systematic overview of techniques for agent-based simulations on hardware accelerators has been given in the literature. To close this gap, we provide an overview and categorisation of the literature according to the applied techniques. Since, at the current state of research, challenges such as the partitioning of a model for execution on heterogeneous hardware are still addressed in a largely manual process, we sketch directions for future research towards automating the hardware mapping and execution. This survey targets modellers seeking an overview of suitable hardware platforms and execution techniques for a specific simulation model, as well as methodology researchers interested in potential research gaps requiring further exploration.", "venue": "ACM Comput. Surv.", "authors": ["Jiajian  Xiao", "Philipp  Andelfinger", "David  Eckhoff", "Wentong  Cai", "Alois  Knoll"], "year": 2019, "n_citations": 18}
{"id": 2333168, "s2_id": "b5f8a9184084c06113ba98154f1e93c5a897e838", "title": "A Performance Comparison of Dask and Apache Spark for Data-Intensive Neuroimaging Pipelines", "abstract": "In the past few years, neuroimaging has entered the Big Data era due to the joint increase in image resolution, data sharing, and study sizes. However, no particular Big Data engines have emerged in this field, and several alternatives remain available. We compare two popular Big Data engines with Python APIs, Apache Spark and Dask, for their runtime performance in processing neuroimaging pipelines. Our evaluation uses two synthetic pipelines processing the 81GB BigBrain image, and a real pipeline processing anatomical data from more than 1,000 subjects. We benchmark these pipelines using various combinations of task durations, data sizes, and numbers of workers, deployed on an 8-node (8 cores ea.) compute cluster in Compute Canada's Arbutus cloud. We evaluate PySpark's RDD API against Dask's Bag, Delayed and Futures. Results show that despite slight differences between Spark and Dask, both engines perform comparably. However, Dask pipelines risk being limited by Python's GIL depending on task type and cluster configuration. In all cases, the major limiting factor was data transfer. While either engine is suitable for neuroimaging pipelines, more effort needs to be placed in reducing data transfer time.", "venue": "2019 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS)", "authors": ["Mathieu  Dugr\u00e9", "Val\u00e9rie  Hayot-Sasson", "Tristan  Glatard"], "year": 2019, "n_citations": 4}
{"id": 2334873, "s2_id": "2c0b0868d642536d8b4d54a745d2373040984565", "title": "Federating OMNeT++ Simulations with Testbed Environments", "abstract": "We are in the process of developing a system architecture for opportunistic and information centric communications. This architecture (called Keetchi), meant for the Internet of Things (IoT) is focussed on enabling applications to perform distributed and decentralised communications among smart devices. To realise and evaluate this architecture, we follow a 3-step approach. Our first approach of evaluation is the development of a testbed with smart devices (mainly smart phones and tablets) deployed with this architecture including the applications. The second step is where the architecture is evaluated in large scale scenarios with the OMNeT++ simulation environment. The third step is where the OMNeT++ simulation environment is fed with traces of data collected from experiments done using the testbed. In realising these environments, we develop the functionality of this architecture as a common code base that is able to operate in the OMNeT++ environment as well as in the smart devices of the testbed (e.g., Android, iOS, Contiki, etc.). This paper presents the details of the \"Write once, compile anywhere\" (WOCA) code base architecture of Keetchi.", "venue": "ArXiv", "authors": ["Asanga  Udugama", "Koojana  Kuladinithi", "Anna  F\u00f6rster", "Carmelita  G\u00f6rg"], "year": 2015, "n_citations": 0}
{"id": 2336126, "s2_id": "f50466bc27adf88d5ed24cd8ac84fe9d82d758d8", "title": "Analysis and Optimization of Random Sensing Order in Cognitive Radio Networks", "abstract": "Developing an efficient spectrum access policy enables cognitive radios to dramatically increase spectrum utilization while ensuring the predetermined quality of service levels for primary users (PUs). In this paper, the modeling, performance analysis, and optimization of a distributed secondary network with a random sensing order policy are studied. Specifically, secondary users (SUs) create a random order of available channels upon PUs' return, and then, they find optimal transmission and handoff opportunities in a distributed manner. By a Markov chain analysis, the average throughputs of the SUs and the average interference level among the SUs and the PUs are investigated. A maximization of the secondary network performance in terms of the throughput while keeping under control the average interference is proposed. It is shown that, despite traditional views, a nonzero false alarm in the channel sensing can increase channel utilization, particularly in a dense secondary network where the contention is too high. Then, two simple and practical adaptive algorithms are established to optimize the network. The second algorithm follows the variations of the wireless channels in nonstationary conditions and outperforms even static brute force optimization while demanding few computations. The convergence of the distributed algorithms is theoretically investigated based on the analytical performance indicators established by the Markov chain analysis. Finally, numerical results validate the analytical derivations and demonstrate the efficiency of the proposed schemes. It is concluded that fully distributed sensing order algorithms can lead to substantial performance improvements in cognitive radio networks without the need for centralized management or message passing among the users.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Hossein Shokri Ghadikolaei", "Carlo  Fischione"], "year": 2015, "n_citations": 23}
{"id": 2337567, "s2_id": "21789a073d46064665694097219b0ef31030d532", "title": "Multi-GPU SNN Simulation with Static Load Balancing", "abstract": "We present a SNN simulator which scales to millions of neurons, billions of synapses, and 8 GPUs. This is made possible by 1) a novel, cache-aware spike transmission algorithm 2) a model parallel multi-GPU distribution scheme and 3) a static, yet very effective load balancing strategy. The simulator further features an easy to use API and the ability to create custom models. We compare the proposed simulator against two state of the art ones on a series of benchmarks using three well-established models. We find that our simulator is faster, consumes less memory, and scales linearly with the number of GPUs.", "venue": "2021 International Joint Conference on Neural Networks (IJCNN)", "authors": ["Antonis  Argyros"], "year": 2021, "n_citations": 1}
{"id": 2340588, "s2_id": "acf39918d7ef33a993287bb200a0bb568e742466", "title": "A Comparative Analysis of Forecasting Financial Time Series Using ARIMA, LSTM, and BiLSTM", "abstract": "Machine and deep learning-based algorithms are the emerging approaches in addressing prediction problems in time series. These techniques have been shown to produce more accurate results than conventional regression-based modeling. It has been reported that artificial Recurrent Neural Networks (RNN) with memory, such as Long Short-Term Memory (LSTM), are superior compared to Autoregressive Integrated Moving Average (ARIMA) with a large margin. The LSTM-based models incorporate additional \"gates\" for the purpose of memorizing longer sequences of input data. The major question is that whether the gates incorporated in the LSTM architecture already offers a good prediction and whether additional training of data would be necessary to further improve the prediction. \nBidirectional LSTMs (BiLSTMs) enable additional training by traversing the input data twice (i.e., 1) left-to-right, and 2) right-to-left). The research question of interest is then whether BiLSTM, with additional training capability, outperforms regular unidirectional LSTM. This paper reports a behavioral analysis and comparison of BiLSTM and LSTM models. The objective is to explore to what extend additional layers of training of data would be beneficial to tune the involved parameters. The results show that additional training of data and thus BiLSTM-based modeling offers better predictions than regular LSTM-based models. More specifically, it was observed that BiLSTM models provide better predictions compared to ARIMA and LSTM models. It was also observed that BiLSTM models reach the equilibrium much slower than LSTM-based models.", "venue": "ArXiv", "authors": ["Sima  Siami-Namini", "Neda  Tavakoli", "Akbar Siami Namin"], "year": 2019, "n_citations": 26}
{"id": 2345983, "s2_id": "82a6b7bb1509a212fb062975133f300f09962634", "title": "Delay-optimal Policies in Partial Fork-Join Systems with Redundancy and Random Slowdowns", "abstract": "We consider a large distributed service system consisting of n homogeneous servers with infinite capacity FIFO queues. Jobs arrive as a Poisson process of rate \u03bbn/k_n (for some positive constant \u03bb and integer k_n). Each incoming job consists of k_n identical tasks that can be executed in parallel, and that can be encoded into at least k_n \"replicas\" of the same size (by introducing redundancy) so that the job is considered to be completed when any k_n replicas associated with it finish their service. Moreover, we assume that servers can experience random slowdowns in their processing rate so that the service time of a replica is the product of its size and a random slowdown. First, we assume that the server slowdowns are shifted exponential and independent of the replica sizes. In this setting we show that the delay of a typical job is asymptotically minimized (as $n\\to\\infty$) when the number of replicas per task is a constant that only depends on the arrival rate \u03bb, and on the expected slowdown of servers. Second, we introduce a new model for the server slowdowns in which larger tasks experience less variable slowdowns than smaller tasks. In this setting we show that, under the class of policies where all replicas start their service at the same time, the delay of a typical job is asymptotically minimized (as n\\to\\infty) when the number of replicas per task is made to depend on the actual size of the tasks being replicated, with smaller tasks being replicated more than larger tasks.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Martin  Zubeldia"], "year": 2020, "n_citations": 2}
{"id": 2346786, "s2_id": "0088ffe176243514e21b3dfdcab473b2b32e7af6", "title": "Self-Organizing Mobility Robustness Optimization in LTE Networks with eICIC", "abstract": "We address the problem of Mobility Robustness Optimization (MRO) and describe centralized Self Organizing Network (SON) solutions that can optimize connected-mode mobility Key Performance Indicators (KPIs). Our solution extends the earlier work of eICIC parameter optimization [7], to heterogeneous networks with mobility, and outline methods of progressive complexity that optimize the Retaining/Offloading Bias which are macro/pico views of Cell Individual Offset parameters. Simulation results under real LTE network deployment assumptions of a US metropolitan area demonstrate the effects of such solutions on the mobility KPIs. To our knowledge, this solution is the first that demonstrates the joint optimization of eICIC and MRO.", "venue": "ArXiv", "authors": ["Carl  Weaver", "Pantelis  Monogioudis"], "year": 2013, "n_citations": 0}
{"id": 2349885, "s2_id": "e0cabb26310bf2d5fefd573e14dfbb3370279c0a", "title": "Reading from External Memory", "abstract": "Modern external memory is represented by several device classes. At present, HDD, SATA SSD andNVMe SSD are widely used. Recently ultra-low latency SSD such as Intel Optane became available on the market. Each of these types exhibits it\u2019s own pattern for throughput, latency and parallelism. To achieve the highest performance one has to pick an appropriate I/O interface provided by the operating system. In this work we present a detailed overview and evaluation of modern storage reading performance with regard to available Linux synchronous and asynchronous interfaces. While throughout this work we aim for the highest throughput we also measure latency and CPU usage. We provide this report inhope thedetailed results couldbe interesting toboth researchers andpractitioners. 1 ar X iv :2 10 2. 11 19 8v 1 [ cs .D C ] 2 2 Fe b 20 21", "venue": "ArXiv", "authors": ["Ruslan  Savchenko"], "year": 2021, "n_citations": 0}
{"id": 2350389, "s2_id": "bd00542977fdad9b22cefbf6d0fc7511fb6eb493", "title": "High-performance generation of the Hamiltonian and Overlap matrices in FLAPW methods", "abstract": "Abstract One of the greatest efforts of computational scientists is to translate the mathematical model describing a class of physical phenomena into large and complex codes. Many of these codes face the difficulty of implementing the mathematical operations in the model in terms of low level optimized kernels offering both performance and portability. Legacy codes suffer from the additional curse of rigid design choices based on outdated performance metrics (e.g.\u00a0minimization of memory footprint). Using a representative code from the Materials Science community, we propose a methodology to restructure the most expensive operations in terms of an optimized combination of dense linear algebra (BLAS3) kernels. The resulting algorithm guarantees an increased performance and an extended life span of this code, enabling larger scale simulations.", "venue": "Comput. Phys. Commun.", "authors": ["Edoardo Di Napoli", "Elmar  Peise", "Markus  Hrywniak", "Paolo  Bientinesi"], "year": 2017, "n_citations": 6}
{"id": 2354386, "s2_id": "20cedf71878cadc6d07f4bc77f050fec52e2083e", "title": "TapirXLA: Embedding Fork-Join Parallelism into the XLA Compiler in TensorFlow Using Tapir", "abstract": "This work introduces TapirXLA, a replacement for TensorFlow\u2019s XLA compiler that embeds recursive fork-join parallelism into XLA\u2019s low-level representation of code. Machine-learning applications employ a variety of technologies to improve performance, including compiler technology. But compilers in machine-learning frameworks lack a deep understanding of parallelism, causing them to lose performance by missing optimizations on parallel computation. This work studies how Tapir, a compiler intermediate representation (IR) that embeds parallelism into a mainstream compiler IR, can be incorporated into a compiler for machine learning to remedy this problem. TapirXLA modifies the XLA compiler in TensorFlow to employ the Tapir/LLVM compiler to optimize low-level parallel computation. TapirXLA encodes the parallelism within high-level TensorFlow operations using Tapir\u2019s representation of fork-join parallelism. Furthermore, TapirXLA exposes to the compiler implementations of linear-algebra library routines whose parallel operations are encoded using Tapir\u2019s representation. We compared the performance of TensorFlow using TapirXLA against TensorFlow using an unmodified XLA compiler. On four neural-network benchmarks, TapirXLA speeds up the parallel running time of the network by a geometric-mean multiplicative factor of 30% to 100%, across four CPU architectures.", "venue": "2019 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Tao B. Schardl", "Siddharth  Samsi"], "year": 2019, "n_citations": 1}
{"id": 2357870, "s2_id": "df014e831b1155d8119ee025537d7f09acc5945d", "title": "Occupancy Distributions of Homogeneous Queueing Systems Under Opportunistic Scheduling", "abstract": "This paper analyzes opportunistic schemes for transmission scheduling from one of n homogeneous queues whose channel states fluctuate independently. Considered schemes consist of an LCQ policy that transmits from a longest connected queue in the entire system, and its low-complexity variant LCQ(d) that transmits from a longest queue within a randomly chosen subset of d \u2265 1 connected queues. A Markovian model is studied where mean packet transmission time is n-1 and packet arrival rate is \u03bb <; 1 per queue. Transient and equilibrium distributions of queue lengths are obtained in the limit as the system size n tends to infinity. It is shown that under LCQ almost all queues are empty in equilibrium, maximum queue length is 1, and the overall system occupancy is \u0398(1) as n \u2192 \u221e. Limiting distribution of the system occupancy is characterized. Limiting queue length distributions under LCQ(d) are also given. It is shown that if d is fixed then the system occupancy is \u0398(n) and the queue length distribution has infinite support. If d = \u03c9(1) but d = o(n) then the maximum queue length is 1 and the system occupancy reduces to O(n/d). Numerical comparison of the obtained asymptotic mean packet delays suggests that LCQ and LCQ(d) may have comparable delay performance for moderate values of n and d.", "venue": "IEEE Transactions on Information Theory", "authors": ["Murat  Alanyali", "Maxim  Dashouk"], "year": 2011, "n_citations": 8}
{"id": 2359480, "s2_id": "f983904d21012d3d5aaa3078f92652ef71283e4b", "title": "Machine Learning\u2013enabled Scalable Performance Prediction of Scientific Codes", "abstract": "Hardware architectures become increasingly complex as the compute capabilities grow to exascale. We present the Analytical Memory Model with Pipelines (AMMP) of the Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and hardware architecture parameters as input and predicts runtime of that code on the target hardware platform, which is defined in the input parameters. PPT-AMMP transforms the code to an (architecture-independent) intermediate representation, then (i) analyzes the basic block structure of the code, (ii) processes architecture-independent virtual memory access patterns that it uses to build memory reuse distance distribution models for each basic block, and (iii) runs detailed basic-block level simulations to determine hardware pipeline usage. PPT-AMMP uses machine learning and regression techniques to build the prediction models based on small instances of the input code, then integrates into a higher-order discrete-event simulation model of PPT running on Simian PDES engine. We validate PPT-AMMP on four standard computational physics benchmarks and present a use case of hardware parameter sensitivity analysis to identify bottleneck hardware resources on different code inputs. We further extend PPT-AMMP to predict the performance of a scientific application code, namely, the radiation transport mini-app SNAP. To this end, we analyze multi-variate regression models that accurately predict the reuse profiles and the basic block counts. We validate predicted SNAP runtimes against actual measured times.", "venue": "ACM Trans. Model. Comput. Simul.", "authors": ["Gopinath  Chennupati", "Nandakishore  Santhi", "Phill  Romero", "Stephan  Eidenbenz"], "year": 2021, "n_citations": 1}
{"id": 2364264, "s2_id": "68afa59403225449ea8b8fc5cc5d24c1fca02f3e", "title": "Power Modelling for Heterogeneous Cloud-Edge Data Centers", "abstract": "Existing power modelling research focuses not on the method used for developing models but rather on the model itself. This paper aims to develop a method for deploying power models on emerging processors that will be used, for example, in cloud-edge data centers. Our research first develops a hardware counter selection method that appropriately selects counters most correlated to power on ARM and Intel processors. Then, we propose a two stage power model that works across multiple architectures. The key results are: (i) the automated hardware performance counter selection method achieves comparable selection to the manual selection methods reported in literature, and (ii) the two stage power model can predict dynamic power more accurately on both ARM and Intel processors when compared to classic power models.", "venue": "PARCO", "authors": ["Kai  Chen", "Blesson  Varghese", "Peter  Kilpatrick", "Dimitrios S. Nikolopoulos"], "year": 2017, "n_citations": 1}
{"id": 2366467, "s2_id": "00ea81ede8eb65dacfa810d31fe46b9fe69dfe33", "title": "Towards Optimality in Parallel Scheduling", "abstract": "To keep pace with Moore's law, chip designers have focused on increasing the number of cores per chip rather than single core performance. In turn, modern jobs are often designed to run on any number of cores. However, to effectively leverage these multi-core chips, one must address the question of how many cores to assign to each job. Given that jobs receive sublinear speedups from additional cores, there is an obvious tradeoff: allocating more cores to an individual job reduces the job's runtime, but in turn decreases the efficiency of the overall system. We ask how the system should schedule jobs across cores so as to minimize the mean response time over a stream of incoming jobs. To answer this question, we develop an analytical model of jobs running on a multi-core machine. We prove that EQUI, a policy which continuously divides cores evenly across jobs, is optimal when all jobs follow a single speedup curve and have exponentially distributed sizes. EQUI requires jobs to change their level of parallelization while they run. Since this is not possible for all workloads, we consider a class of \"fixed-width\" policies, which choose a single level of parallelization, k, to use for all jobs. We prove that, surprisingly, it is possible to achieve EQUI's performance without requiring jobs to change their levels of parallelization by using the optimal fixed level of parallelization, k*. We also show how to analytically derive the optimal k* as a function of the system load, the speedup curve, and the job size distribution. In the case where jobs may follow different speedup curves, finding a good scheduling policy is even more challenging. In particular, we find that policies like EQUI which performed well in the case of a single speedup function now perform poorly. We propose a very simple policy, GREEDY*, which performs near-optimally when compared to the numerically-derived optimal policy.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Benjamin  Berg", "Jan-Pieter L. Dorsman", "Mor  Harchol-Balter"], "year": 2017, "n_citations": 13}
{"id": 2366757, "s2_id": "0f7ef66f081167c8a401254fcf2a0268ad5df51b", "title": "HPC AI500: Representative, Repeatable and Simple HPC AI Benchmarking", "abstract": "Recent years witness a trend of applying large-scale distributed deep learning algorithms (HPC AI) in both business and scientific computing areas, whose goal is to speed up the training time to achieve a state-of-the-art quality. The HPC AI benchmarks accelerate the process. Unfortunately, benchmarking HPC AI systems at scale raises serious challenges. This paper presents a representative, repeatable and simple HPC AI benchmarking methodology. Among the seventeen AI workloads of AIBench Training\u2014by far the most comprehensive AI Training benchmarks suite\u2014we choose two representative and repeatable AI workloads. The selected HPC AI benchmarks include both business and scientific computing: Image Classification and Extreme Weather Analytics. To rank HPC AI systems, we present a new metric named Valid FLOPS, emphasizing both throughput performance and a target quality. The specification, source code, datasets, and HPC AI500 ranking numbers are publicly available from https: //www.benchcouncil.org/HPCAI500/.", "venue": "ArXiv", "authors": ["Zihan  Jiang", "Wanling  Gao", "Fei  Tang", "Xingwang  Xiong", "Lei  Wang", "Chuanxin  Lan", "Chunjie  Luo", "Hongxiao  Li", "Jianfeng  Zhan"], "year": 2021, "n_citations": 1}
{"id": 2367986, "s2_id": "8e8a59c73b8e8e19ff3cccb0a83e54647d2d8dfb", "title": "Crossing the architectural barrier: Evaluating representative regions of parallel HPC applications", "abstract": "Exascale computing will get mankind closer to solving important social, scientific and engineering problems. Due to high prototyping costs, High Performance Computing (HPC) system architects make use of simulation models for design space exploration and hardware-software co-design. However, as HPC systems reach exascale proportions, the cost of simulation increases, since simulators themselves are largely single-threaded. Tools for selecting representative parts of parallel applications to reduce running costs are widespread, e.g., BarrierPoint achieves this by analysing, in simulation, abstract characteristics such as basic blocks and reuse distances. However, architectures new to HPC have a limited set of tools available. In this work, we provide an independent cross-architectural evaluation on real hardware \u2014 across Intel and ARM \u2014 of the BarrierPoint methodology, when applied to parallel HPC proxy applications. We present both cases: when the methodology can be applied and when it cannot. In the former case, results show that we can predict the performance of full application execution by running shorter representative sections. In the latter case, we dive into the underlying issues and suggest improvements. We demonstrate a total simulation time reduction of up to 178x, whilst keeping the error below 2.3% for both cycles and instructions.", "venue": "2017 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Alexandra  Ferreron", "Radhika  Jagtap", "Sascha  Bischoff", "Roxana  Rusitoru"], "year": 2017, "n_citations": 3}
{"id": 2372140, "s2_id": "3d930c3a723216a5c42c1e7c3a73190915c74cd8", "title": "Using meta-heuristics and machine learning for software optimization of parallel computing systems: a systematic literature review", "abstract": "While modern parallel computing systems offer high performance, utilizing these powerful computing resources to the highest possible extent demands advanced knowledge of various hardware architectures and parallel programming models. Furthermore, optimized software execution on parallel computing systems demands consideration of many parameters at compile-time and run-time. Determining the optimal set of parameters in a given execution context is a complex task, and therefore to address this issue researchers have proposed different approaches that use heuristic search or machine learning. In this paper, we undertake a systematic literature review to aggregate, analyze and classify the existing software optimization methods for parallel computing systems. We review approaches that use machine learning or meta-heuristics for software optimization at compile-time and run-time. Additionally, we discuss challenges and future research directions. The results of this study may help to better understand the state-of-the-art techniques that use machine learning and meta-heuristics to deal with the complexity of software optimization for parallel computing systems. Furthermore, it may aid in understanding the limitations of existing approaches and identification of areas for improvement.", "venue": "Computing", "authors": ["Suejb  Memeti", "Sabri  Pllana", "Al\u00e9cio P. D. Binotto", "Joanna  Kolodziej", "Ivona  Brandic"], "year": 2018, "n_citations": 22}
{"id": 2372674, "s2_id": "eddf1a55005f6526df5c60ddc55d2240641fdc03", "title": "GPU-Based Heuristic Solver for Linear Sum Assignment Problems Under Real-time Constraints", "abstract": "In this thesis, we explore the use of a centrally-coordinated peer-to-peer overlay as a possible solution to the live streaming problem. Our contribution lies in showing that such approach is indeed feasible given that a number of key challenges are met. The motivation behind exploring an alternative design is that, although a number of approaches have been investigated in the past, e.g. mesh-pull and tree-push, hybrids and best-of-both-worlds mesh-push, no consensus has been reached on the best solution for the problem of peer-to-peer live streaming, despite current deployments and reported successes. In the proposed system, we model sender/receiver peer assignments as an optimization problem. Optimized peer selection based on multiple utility factors, such as bandwidth availability, delays and connectivity compatibility, make it possible to achieve large source bandwidth savings and provide high quality of user experience. Clear benefits of our approach are observed when Network Address Translation constraints are present on the network. We have addressed key scalability issues of our platform by parallelizing the heuristic which is the core of our optimization engine and by implementing the resulting algorithm on commodity Graphic Processing Units (GPUs). The outcome is a Linear Sum Assignment Problem (LSAP) solver for time-constrained systems which produces near-optimal results and can be used for any instance of LSAP, i.e. not only in our system. As part of this work, we also present our experience in working with Network Address Translators (NATs) traversal in peer-to-peer systems. Our contribution in this context is threefold. First, we provide a semi-formal model of state of the art NAT behaviors. Second, we use our model to show which NAT combinations can be theoretically traversed and which not. Last, for each of the combinations, we state which traversal technique should be used. Our findings are confirmed by experimental results on a real network. Finally, we address the problem of reproducibility in testing, debugging and evaluation of our peer-to-peer application. We achieve this by providing a software framework which can be transparently integrated with any already-existing software and which is able to handle concurrency, system time and network events in a reproducible manner.", "venue": "ArXiv", "authors": ["Roberto  Roverso", "Amgad  Naiem", "Mohammed  El-Beltagy", "Sameh  El-Ansary"], "year": 2011, "n_citations": 0}
{"id": 2373471, "s2_id": "66d5cde3339fba19ab916c62280fd37faf84d1ce", "title": "Joint Data Compression and Caching: Approaching Optimality with Guarantees", "abstract": "We consider the problem of optimally compressing and caching data across a communication network. Given the data generated at edge nodes and a routing path, our goal is to determine the optimal data compression ratios and caching decisions across the network in order to minimize average latency, which can be shown to be equivalent to maximizing the compression and caching gain under an energy consumption constraint. We show that this problem is NP-hard in general and the hardness is caused by the caching decision subproblem, while the compression sub-problem is polynomial-time solvable. We then propose an approximation algorithm that achieves a $(1-1/e)$-approximation solution to the optimum in strongly polynomial time. We show that our proposed algorithm achieve the near-optimal performance in synthetic-based evaluations. In this paper, we consider a tree-structured network as an illustrative example, but our results easily extend to general network topology at the expense of more complicated notations.", "venue": "ICPE", "authors": ["Jian  Li", "Faheem  Zafari", "Donald F. Towsley", "Kin K. Leung", "Ananthram  Swami"], "year": 2018, "n_citations": 5}
{"id": 2377751, "s2_id": "85b0b05d08f779b28138d9f6e17a4cfcaf06f680", "title": "Study and evaluation of an Irregular Graph Algorithm on Multicore and GPU Processor Architectures", "abstract": "One area of Computing applications which poses significant challenge of performance scalability on Chip Multiprocessors(CMP's) are Irregular applications. Such applications have very little computation and unpredictable memory access patterns making them memory-bound in contrast to compute-bound applications. Since the gap between processor and memory performance continues to exist, difficulty to hide and decrease this gap is one of the important factors which results in poor performance of these applications on CMP's. \nThe goal of this thesis is to overcome many such challenges posed during performance acceleration of an irregular graph algorithm called Triad Census. We accelerated the Triad Census algorithm on two significantly different Chip Multiprocessors: Dual-socket Intel Xeon Multicore (8 hardware threads per socket) and 240-processor core NVIDIA Tesla C1060 GPGPU(128 hardware threads per core). \nThe experimental results obtained on Intel Multicore Xeon system shows performance speedups (w.r.t baseline sequential) of maximum 56x , average 33x and minimum 8.3x for real world graph data sets. On NVIDIA Tesla C1060 GPGPU, we were able to match almost equally the Multicore results - 58.4x maximum, 32.8x average and 4.2x minimum speedups w.r.t baseline sequential. In terms of raw performance, for the graph data set called Patents network, our results on Intel Xeon Multicore(16 hw threads) were 1.27x times faster than previous results on Cray XMT(16 hw threads) while results achieved on GPGPU were comparatively slower(0.72x). To the best of our knowledge, this algorithm has only been accelerated on supercomputer class computer named Cray XMT and no work exists that demonstrates performance evaluation and comparison of this algorithm on relatively lower-cost Multicore and GPGPU based platforms.", "venue": "ArXiv", "authors": ["Varun  Nagpal"], "year": 2016, "n_citations": 0}
{"id": 2381259, "s2_id": "9abc08a2162038a2a79da4fe65f555bc82069309", "title": "Sonic: A Sampling-based Online Controller for Streaming Applications", "abstract": "Many applications in important problem domains such as machine learning and computer vision are streaming applications that take a sequence of inputs over time. It is challenging to find knob settings that optimize the run-time performance of such applications because the optimal knob settings are usually functions of inputs, computing platforms, time as well as user\u2019s requirements, which can be very diverse. Most prior works address this problem by offline profiling followed by training models for control. However, profiling-based approaches incur large overhead before execution; it is also difficult to redeploy them in other run-time configurations. In this paper, we propose Sonic, a sampling-based online controller for streaming applications that does not require profiling ahead of time. Within each phase of a streaming application\u2019s execution, Sonic utilizes the beginning portion to sample the knob space strategically and aims to pick the optimal knob setting for the rest of the phase, given a user-specified constrained optimization problem. A hybrid approach of machine learning regressors and Bayesian optimization are used for better sampling choices. Sonic is implemented independent of application, device, input, performance objective and constraints. We evaluate Sonic on traditional parallel benchmarks as well as on deep learning inference benchmarks across multiple platforms. Our experiments show that when using Sonic to control knob settings, application run-time performance is only 5.3% less than if optimal knob settings were used, demonstrating that Sonic is able to find near-optimal knob settings under diverse run-time configurations without prior knowledge.", "venue": "ArXiv", "authors": ["Yan  Pei", "Keshav  Pingali"], "year": 2021, "n_citations": 0}
{"id": 2382785, "s2_id": "46c0689ece315b311086014b86b80c7cf21f911b", "title": "Combinatorial BLAS 2.0: Scaling Combinatorial Algorithms on Distributed-Memory Systems", "abstract": "Combinatorial algorithms such as those that arise in graph analysis, modeling of discrete systems, bioinformatics, and chemistry, are often hard to parallelize. The Combinatorial BLAS library implements key computational primitives for rapid development of combinatorial algorithms in distributed-memory systems. During the decade since its first introduction, the Combinatorial BLAS library has evolved and expanded significantly. This article details many of the key technical features of Combinatorial BLAS version 2.0, such as communication avoidance, hierarchical parallelism via in-node multithreading, accelerator support via GPU kernels, generalized semiring support, implementations of key data structures and functions, and scalable distributed I/O operations for human-readable files. Our article also presents several rules of thumb for choosing the right data structures and functions in Combinatorial BLAS 2.0, under various common application scenarios.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Ariful  Azad", "Oguz  Selvitopi", "Md Taufique Hussain", "John R. Gilbert", "Ayd\u0131n  Bulu\u00e7"], "year": 2022, "n_citations": 1}
{"id": 2389890, "s2_id": "3319d446c0f11e0dbe529f5bc0531e7652448bf5", "title": "Distributed systems and trusted execution environments: Trade-offs and challenges", "abstract": "Security and privacy concerns in computer systems have grown in importance with the ubiquity of connected devices. TEEs provide security guarantees based on cryptographic constructs built in hardware. Intel software guard extensions (SGX), in particular, implements powerful mechanisms that can shield sensitive data even from privileged users with full control of system software. In this work, we essentially explore some of the challenges of designing secure distributed systems by using Intel SGX as cornerstone. We do so by designing and experimentally evaluating several elementary systems ranging from communication and processing middleware to a peer-to-peer privacy-preserving solution. We start with support systems that naturally fit cloud deployment scenarios, namely content-based routing, batching and stream processing frameworks. We implement prototypes and use them to analyse the manifested memory usage issues intrinsic to SGX. Next, we aim at protecting very sensitive data: cryptographic keys. By leveraging TEEs, we design protocols for group data sharing that have lower computational complexity than legacy methods. As a bonus, our proposals allow large savings on metadata volume and processing time of cryptographic operations, all with equivalent security guarantees. Finally, we propose privacy-preserving systems against established services like web-search engines. Our evaluation shows that we propose the most robust system in comparison to existing solutions with regard to user re-identification rates and results accuracy in a scalable way. Overall, this thesis proposes new mechanisms that take advantage of TEEs for distributed system architectures. We show through an empirical approach on top of Intel SGX what are the trade-offs of distinct designs applied to distributed communication and processing, cryptographic protocols and private web search.", "venue": "ArXiv", "authors": ["Rafael Pereira Pires"], "year": 2020, "n_citations": 1}
{"id": 2390368, "s2_id": "89756a6aeea133a338e77b3e34d4d117058b10c4", "title": "Spatial Fluid Limits for Stochastic Mobile Networks", "abstract": "We consider Markov models of large-scale networks where nodes are characterized by their local behavior and by a mobility model over a two-dimensional lattice. By assuming random walk, we prove convergence to a system of partial differential equations (PDEs) whose size depends neither on the lattice size nor on the population of nodes. This provides a macroscopic view of the model which approximates discrete stochastic movements with continuous deterministic diffusions. We illustrate the practical applicability of this result by modeling a network of mobile nodes with on/off behavior performing file transfers with connectivity to 802.11 access points. By means of an empirical validation against discrete-event simulation we show high quality of the PDE approximation even for low populations and coarse lattices. In addition, we confirm the computational advantage in using the PDE limit over a traditional ordinary differential equation limit where the lattice is modeled discretely, yielding speed-ups of up to two orders of magnitude.", "venue": "Perform. Evaluation", "authors": ["Max  Tschaikowski", "Mirco  Tribastone"], "year": 2017, "n_citations": 11}
{"id": 2391898, "s2_id": "968d8922cb739bd4394d1084f044274d118551ff", "title": "MARS: Memory Aware Reordered Source", "abstract": "Memory bandwidth is critical in today's high performance computing systems. The bandwidth is particularly paramount for GPU workloads such as 3D Gaming, Imaging and Perceptual Computing, GPGPU due to their data-intensive nature. As the number of threads and data streams in the GPUs increases with each generation, along with a high available memory bandwidth, memory efficiency is also crucial in order to achieve desired performance. In presence of multiple concurrent data streams, the inherent locality in a single data stream is often lost as these streams are interleaved while moving through multiple levels of memory system. In DRAM based main memory, the poor request locality reduces row-buffer reuse resulting in underutilized and inefficient memory bandwidth. \nIn this paper we propose Memory-Aware Reordered Source (\\textit{MARS}) architecture to address memory inefficiency arising from highly interleaved data streams. The key idea of \\textit{MARS} is that with a sufficiently large lookahead before the main memory, data streams can be reordered based on their row-buffer address to regain the lost locality and improve memory efficiency. We show that \\textit{MARS} improves achieved memory bandwidth by 11\\% for a set of synthetic microbenchmarks. Moreover, MARS does so without any specific knowledge of the memory configuration.", "venue": "ArXiv", "authors": ["Ishwar  Bhati", "Udit  Dhawan", "Jayesh  Gaur", "Sreenivas  Subramoney", "Hong  Wang"], "year": 2018, "n_citations": 0}
{"id": 2392383, "s2_id": "0fdcb57e7d332f6d08abaabe11a44840b156d58c", "title": "The MAP/M/s+G Call Center Model with General Patience Times: Stationary Solutions and First Passage Times", "abstract": "We study the MAP/M/s+G queuing model with MAP (Markovian Arrival Process) arrivals, exponentially distributed service times, infinite waiting room, and generally distributed patience times. Using sample-path arguments, we propose to obtain the steady-state distribution of the virtual waiting time and subsequently the other relevant performance metrics of interest for the MAP/M/s+G queue by means of finding the steady-state solution of a properly constructed Continuous Feedback Fluid Queue (CFFQ). The proposed method is exact when the patience time is a discrete random variable and is asymptotically exact when it is continuous/hybrid for which case discretization of the patience time distribution and subsequently the steady-state solution of a Multi-Regime Markov Fluid Queue (MRMFQ) is required. Besides the steady-state distribution, we also propose a new method to approximately obtain the first passage time distribution for the virtual and actual waiting times in the $MAP/M/s+G$ queue. Again, using sample-path arguments, finding the desired distribution is also shown to reduce to obtaining the steady-state solution of a larger dimensionality CFFQ where the deterministic time horizon is to be approximated by Erlang or Concentrated Matrix Exponential (CME) distributions. Numerical results are presented to validate the effectiveness of the proposed method.", "venue": "ArXiv", "authors": ["\u00d6mer  G\u00fcrsoy", "Kamal Adli Mehr", "Nail  Akar"], "year": 2019, "n_citations": 0}
{"id": 2399740, "s2_id": "e152c295879c409008dc7c2da0fd568135a07e57", "title": "An aggregation technique for large-scale PEPA models with non-uniform populations", "abstract": "Performance analysis based on modelling consists of two major steps: model construction and model analysis. Formal modelling techniques significantly aid model construction but can exacerbate model analysis. In particular, here we consider the analysis of large-scale systems which consist of one or more entities replicated many times to form large populations. The replication of entities in such models can cause their state spaces to grow exponentially to the extent that their exact stochastic analysis becomes computationally expensive or even infeasible. \n \nIn this paper, we propose a new approximate aggregation algorithm for a class of large-scale PEPA models. For a given model, the method quickly checks if it satisfies a syntactic condition, indicating that the model may be solved approximately with high accuracy. If so, an aggregated CTMC is generated directly from the model description. This CTMC can be used for efficient derivation of an approximate marginal probability distribution over some of the model's populations. In the context of a large-scale client-server system, we demonstrate the usefulness of our method.", "venue": "VALUETOOLS", "authors": ["Alireza  Pourranjbar", "Jane  Hillston"], "year": 2013, "n_citations": 4}
{"id": 2400198, "s2_id": "ec0ff85496c4d8ac4026dde01fc0975e52a58b73", "title": "Benchmarking for Metaheuristic Black-Box Optimization: Perspectives and Open Challenges", "abstract": "Research on new optimization algorithms is often funded based on the motivation that such algorithms might improve the capabilities to deal with real-world and industrially relevant optimization challenges. Besides a huge variety of different evolutionary and metaheuristic optimization algorithms, also a large number of test problems and benchmark suites have been developed and used for comparative assessments of algorithms, in the context of global, continuous, and black-box optimization. For many of the commonly used synthetic benchmark problems or artificial fitness landscapes, there are however, no methods available, to relate the resulting algorithm performance assessments to technologically relevant real-world optimization problems, or vice versa. Also, from a theoretical perspective, many of the commonly used benchmark problems and approaches have little to no generalization value. Based on a mini-review of publications with critical comments, advice, and new approaches, this communication aims to give a constructive perspective on several open challenges and prospective research directions related to systematic and generalizable benchmarking for black-box optimization.", "venue": "2020 IEEE Congress on Evolutionary Computation (CEC)", "authors": ["Ramses  Sala", "Ralf  M\u00fcller"], "year": 2020, "n_citations": 4}
{"id": 2402053, "s2_id": "c023b3d48b35be3dd3d01a0d5d20bf97f56f6f7d", "title": "Fast Data: Moving beyond from Big Data's map-reduce", "abstract": "Big Data may not be the solution many are looking for. The latest rise of Big Data methods and systems is partly due to the new abilities these techniques provide, partly to the simplicity of the software design and partly because the buzzword itself has value to investors and clients. That said, popularity is not a measure for suitability and the Big Data approach might not be the best solution, or even an applicable one, to many common problems. Namely, time dependent problems whose solution may be bound or cached in any manner can benefit greatly from moving to partly stateless, flow oriented functions and data models. This paper presents such a model to substitute the traditional map-shuffle-reduce models.", "venue": "ArXiv", "authors": ["Adam  Lev-Libfeld", "Alexander  Margolin"], "year": 2019, "n_citations": 2}
{"id": 2408995, "s2_id": "760ed55b8d8aa9b92cc82acff4354d9607c528c5", "title": "Exploration of Fine-Grained Parallelism for Load Balancing Eager K-truss on GPU and CPU", "abstract": "In this work we present a performance exploration on Eager K-truss, a linear-algebraic formulation of the K-truss graph algorithm. We address performance issues related to load imbalance of parallel tasks in symmetric, triangular graphs by presenting a fine-grained parallel approach to executing the support computation. This approach also increases available parallelism, making it amenable to GPU execution. We demonstrate our fine-grained parallel approach using implementations in Kokkos and evaluate them on an Intel Skylake CPU and an Nvidia Tesla V100 GPU. Overall, we observe between a 1.261. 48x improvement on the CPU and a 9.97-16.92x improvement on the GPU due to our fine-grained parallel formulation.", "venue": "2019 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Mark P. Blanco", "Tze Meng Low", "Kyungjoo  Kim"], "year": 2019, "n_citations": 10}
{"id": 2410692, "s2_id": "654dded68682745deee0c1e26fa969537747604c", "title": "HUNTER: AI based Holistic Resource Management for Sustainable Cloud Computing", "abstract": "The worldwide adoption of cloud data centers (CDCs) has given rise to the ubiquitous demand for hosting application services on the cloud. Further, contemporary data-intensive industries have seen a sharp upsurge in the resource requirements of modern applications. This has led to the provisioning of an increased number of cloud servers, giving rise to higher energy consumption and, consequently, sustainability concerns. Traditional heuristics and reinforcement learning based algorithms for energy-efficient cloud resource management address the scalability and adaptability related challenges to a limited extent. Existing work often fails to capture dependencies across thermal characteristics of hosts, resource consumption of tasks and the corresponding scheduling decisions. This leads to poor scalability and an increase in the compute resource requirements, particularly in environments with non-stationary resource demands. To address these limitations, we propose an artificial intelligence (AI) based holistic resource management technique for sustainable cloud computing called HUNTER. The proposed model formulates the goal of optimizing energy efficiency in data centers as a multi-objective scheduling problem, considering three important models: energy, thermal and cooling. HUNTER utilizes a Gated Graph Convolution Network as a surrogate model for approximating the Quality of Service (QoS) for a system state and generating optimal scheduling decisions. Experiments on simulated and physical cloud environments using the CloudSim toolkit and the COSCO framework show that HUNTER outperforms state-of-the-art baselines in terms of energy consumption, SLA violation, scheduling time, cost and temperature by up to 12, 35, 43, 54 and 3 percent respectively.", "venue": "Journal of Systems and Software", "authors": ["Shreshth  Tuli", "Sukhpal Singh Gill", "Minxian  Xu", "Peter  Garraghan", "Rami  Bahsoon", "Scharam  Dustdar", "Rizos  Sakellariou", "Omer  Rana", "Rajkumar  Buyya", "Giuliano  Casale", "Nicholas R. Jennings"], "year": 2021, "n_citations": 3}
{"id": 2411060, "s2_id": "4437190bd60ecc7a687767dc135dc6c8e1ad9b57", "title": "A Non-stationary Service Curve Model for Performance Analysis of Transient Phases", "abstract": "Steady-state solutions for a variety of relevant queueing systems are known today, e.g., from queueing theory, effective bandwidths, and network calculus. The behavior during transient phases, on the other hand, is understood to a much lesser extent as its analysis poses significant challenges. Considering the majority of short-lived flows, transient effects that have diverse causes, such as TCP slow start, sleep scheduling in wireless networks, or signalling in cellular networks, are, however, predominant. This paper contributes a general model of regenerative service processes to characterize the transient behavior of systems. The model leads to a notion of non-stationary service curves that can be conveniently integrated into the framework of the stochastic network calculus. We derive respective models of sleep scheduling and show the significant impact of transient phases on backlogs and delays. We also consider measurement methods that estimate the service of an unknown system from observations of selected probe traffic. We find that the prevailing rate scanning method does not recover the service during transient phases well. This limitation is fundamental as it is explained by the non-convexity of nonstationary service curves. A second key difficulty is proven to be due to the super-additivity of network service processes. We devise a novel two-phase probing technique that first determines a minimal pattern of probe traffic. This probe is used to obtain an accurate estimate of the unknown transient service.", "venue": "2015 27th International Teletraffic Congress", "authors": ["Nico  Becker", "Markus  Fidler"], "year": 2015, "n_citations": 10}
{"id": 2413697, "s2_id": "85e0de590d760f9d1529b97be721d4a9c9f2a7d0", "title": "The Art of CPU-Pinning: Evaluating and Improving the Performance of Virtualization and Containerization Platforms", "abstract": "Cloud providers offer a variety of execution platforms in form of bare-metal, VM, and containers. However, due to the pros and cons of each execution platform, choosing the appropriate platform for a specific cloud-based application has become a challenge for solution architects. The possibility to combine these platforms (e.g., deploying containers within VMs) offers new capacities that makes the challenge even further complicated. However, there is a little study in the literature on the pros and cons of deploying different application types on various execution platforms. In particular, evaluation of diverse hardware configurations and different CPU provisioning methods, such as CPU pinning, have not been sufficiently studied in the literature. In this work, the performance overhead of container, VM, and bare-metal execution platforms are measured and analyzed for four categories of real-world applications, namely video processing, parallel processing (MPI), web processing, and No-SQL, respectively representing CPU intensive, parallel processing, and two IO intensive processes. Our analyses reveal a set of interesting and sometimes counterintuitive findings that can be used as best practices by the solution architects to efficiently deploy cloud-based applications. Here are some notable mentions: (A) Under specific circumstances, containers can impose a higher overhead than VMs; (B) Containers on top of VMs can mitigate the overhead of VMs for certain applications; (C) Containers with a large number of cores impose a lower overhead than those with a few cores.", "venue": "ICPP", "authors": ["Davood Ghatreh Samani", "Chavit  Denninnart", "Josef  Bacik", "Mohsen Amini Salehi"], "year": 2020, "n_citations": 5}
{"id": 2414824, "s2_id": "88d43ed54fa0ca48811316d2e22fd37dd5cc28dd", "title": "Image classification on IoT edge devices: profiling and modeling", "abstract": "With the advent of powerful, low-cost IoT systems, processing data closer to where the data originates, known as edge computing, has become an increasingly viable option. In addition to lowering the cost of networking infrastructures, edge computing reduces edge-cloud delay, which is essential for mission-critical applications. In this paper, we show the feasibility and study the performance of image classification using IoT devices. Specifically, we explore the relationships between various factors of image classification algorithms that may affect energy consumption, such as dataset size, image resolution, algorithm type, algorithm phase, and device hardware. In order to provide a means of predicting the energy consumption of an edge device performing image classification, we investigate the usage of three machine learning algorithms using the data generated from our experiments. The performance as well as the trade-offs for using linear regression, Gaussian process, and random forests are discussed and validated. Our results indicate that the random forest model outperforms the two former algorithms, with an R-squared value of 0.95 and 0.79 for two different validation datasets. The random forest also served as a feature extraction mechanism which enabled us to identify which predictor variables influenced our model the most.", "venue": "Cluster Computing", "authors": ["Salma  Abdel Magid", "Francesco  Petrini", "Behnam  Dezfouli"], "year": 2019, "n_citations": 16}
{"id": 2418566, "s2_id": "c8c27face9d671fc7043975a638c85d77882bfa1", "title": "A Technique for Finding Optimal Program Launch Parameters Targeting Manycore Accelerators", "abstract": "In this paper, we present a new technique to dynamically determine the values of program parameters in order to optimize the performance of a multithreaded program P. To be precise, we describe a novel technique to statically build another program, say, R, that can dynamically determine the optimal values of program parameters to yield the best program performance for P given values for its data and hardware parameters. While this technique can be applied to parallel programs in general, we are particularly interested in programs targeting manycore accelerators. Our technique has successfully been employed for GPU kernels using the MWP-CWP performance model for CUDA.", "venue": "ArXiv", "authors": ["Alexander  Brandt", "Davood  Mohajerani", "Marc Moreno Maza", "Jeeva  Paudel", "Lin-Xiao  Wang"], "year": 2019, "n_citations": 0}
{"id": 2419838, "s2_id": "64f853a54611cf1e58f335d1a793999be4faefca", "title": "A Streaming Approach for Efficient Batched Beam Search", "abstract": "We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically \"refills\" the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines' BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.", "venue": "EMNLP", "authors": ["Kevin  Yang", "Violet  Yao", "John  DeNero", "Dan  Klein"], "year": 2020, "n_citations": 0}
{"id": 2420324, "s2_id": "2c286b53b73a09ae6d46ff847c0d5475d9b0b62e", "title": "Collaborative Management of Benchmark Instances and their Attributes", "abstract": "Experimental evaluation is an integral part in the design process of algorithms. Publicly available benchmark instances are widely used to evaluate methods in SAT solving. For the interpretation of results and the design of algorithm portfolios their attributes are crucial. Capturing the interrelation of benchmark instances and their attributes is considerably simplified through our specification of a benchmark instance identifier. Thus, our tool increases the availability of both by providing means to manage and retrieve benchmark instances by their attributes and vice versa. Like this, it facilitates the design and analysis of SAT experiments and the exchange of results.", "venue": "ArXiv", "authors": ["Markus  Iser", "Luca  Springer", "Carsten  Sinz"], "year": 2020, "n_citations": 0}
{"id": 2421240, "s2_id": "f7c0b985f2c332e00c301205f341da8a10bd449c", "title": "Performance of SSE and AVX Instruction Sets", "abstract": "SSE (streaming SIMD extensions) and AVX (advanced vector extensions) are SIMD (single instruction multiple data streams) instruction sets supported by recent CPUs manufactured in Intel and AMD. This SIMD programming allows parallel processing by multiple cores in a single CPU. Basic arithmetic and data transfer operations such as sum, multiplication and square root can be processed simultaneously. Although popular compilers such as GNU compilers and Intel compilers provide automatic SIMD optimization options, one can obtain better performance by a manual SIMD programming with proper optimization: data packing, data reuse and asynchronous data transfer. In particular, linear algebraic operations of vectors and matrices can be easily optimized by the SIMD programming. Typical calculations in lattice gauge theory are composed of linear algebraic operations of gauge link matrices and fermion vectors, and so can adopt the manual SIMD programming to improve the performance.", "venue": "ArXiv", "authors": ["Hwancheol  Jeong", "Sunghoon  Kim", "Weonjong  Lee", "Seok-Ho  Myung"], "year": 2012, "n_citations": 20}
{"id": 2422501, "s2_id": "f57b0694903b457b2d2114b4eed57ac11d2fa414", "title": "On the Feasibility of FPGA Acceleration of Molecular Dynamics Simulations", "abstract": "Classical molecular dynamics (MD) simulations are important tools in life and material sciences since they allow studying chemical and biological processes in detail. However, the inherent scalability problem of particle-particle interactions and the sequential dependency of subsequent time steps render MD computationally intensive and difficult to scale. To this end, specialized FPGA-based accelerators have been repeatedly proposed to ameliorate this problem. However, to date none of the leading MD simulation packages fully support FPGA acceleration and a direct comparison of GPU versus FPGA accelerated codes has remained elusive so far. With this report, we aim at clarifying this issue by comparing measured application performance on GPU-dense compute nodes with performance and cost estimates of a FPGA-based single- node system. Our results show that an FPGA-based system can indeed outperform a similarly configured GPU-based system, but the overall application-level speedup remains in the order of 2x due to software overheads on the host. Considering the price for GPU and FPGA solutions, we observe that GPU-based solutions provide the better cost/performance tradeoff, and hence pure FPGA-based solutions are likely not going to be commercially viable. However, we also note that scaled multi-node systems could potentially benefit from a hybrid composition, where GPUs are used for compute intensive parts and FPGAs for latency and communication sensitive tasks.", "venue": "ArXiv", "authors": ["Michael  Schaffner", "Luca  Benini"], "year": 2018, "n_citations": 2}
{"id": 2423063, "s2_id": "38fea999ffd5a979cb039f67043dae359f90213c", "title": "MLPerf Training Benchmark", "abstract": "Machine learning (ML) needs industry-standard performance benchmarks to support design and competitive evaluation of the many emerging software and hardware solutions for ML. But ML training presents three unique benchmarking challenges absent from other domains: optimizations that improve training throughput can increase the time to solution, training is stochastic and time to solution exhibits high variance, and software and hardware systems are so diverse that fair benchmarking with the same binary, code, and even hyperparameters is difficult. We therefore present MLPerf, an ML benchmark that overcomes these challenges. Our analysis quantitatively evaluates MLPerf's efficacy at driving performance and scalability improvements across two rounds of results from multiple vendors.", "venue": "MLSys", "authors": ["Peter  Mattson", "Christine  Cheng", "Cody  Coleman", "Greg  Diamos", "Paulius  Micikevicius", "David  Patterson", "Hanlin  Tang", "Gu-Yeon  Wei", "Peter  Bailis", "Victor  Bittorf", "David  Brooks", "Dehao  Chen", "Debojyoti  Dutta", "Udit  Gupta", "Kim  Hazelwood", "Andrew  Hock", "Xinyuan  Huang", "Bill  Jia", "Daniel  Kang", "David  Kanter", "Naveen  Kumar", "Jeffery  Liao", "Guokai  Ma", "Deepak  Narayanan", "Tayo  Oguntebi", "Gennady  Pekhimenko", "Lillian  Pentecost", "Vijay Janapa Reddi", "Taylor  Robie", "Tom St. John", "Carole-Jean  Wu", "Lingjie  Xu", "Cliff  Young", "Matei  Zaharia"], "year": 2020, "n_citations": 119}
{"id": 2423456, "s2_id": "41769123eb829cacfcf24e4a58fc6c3cf2a7106f", "title": "Decreasing log data of multi-tier services for effective request tracing", "abstract": "Previous work shows request tracing systems help understand and debug the performance problems of multi-tier services. However, for large-scale data centers, more than hundreds of thousands of service instances provide online service at the same time. Previous work such as white-box or black box tracing systems will produce large amount of log data, which would be correlated into large quantities of causal paths for performance debugging. In this paper, we propose an innovative algorithm to eliminate valueless logs of multitiers services. Our experiment shows our method filters 84% valueless causal paths and is promising to be used in large-scale data centers.", "venue": "ArXiv", "authors": ["Bo  Sang", "Jianfeng  Zhan", "Guanhua  Tian"], "year": 2010, "n_citations": 3}
{"id": 2423799, "s2_id": "0ea0ed0a3451e8ab2f4fd2e2a40af70da70b6141", "title": "FedEval: A Benchmark System with a Comprehensive Evaluation Model for Federated Learning", "abstract": "As an innovative solution for privacy-preserving machine learning (ML), federated learning (FL) is attracting much attention from research and industry areas. While new technologies proposed in the past few years do evolve the FL area, unfortunately, the evaluation results presented in these works fall short in integrity and are hardly comparable because of the inconsistent evaluation metrics and the lack of a common platform. In this paper, we propose a comprehensive evaluation framework for FL systems. Specifically, we first introduce the ACTPR model, which defines five metrics that cannot be excluded in FL evaluation, including Accuracy, Communication, Time efficiency, Privacy, and Robustness. Then we design and implement a benchmarking system called FedEval, which enables the systematic evaluation and comparison of existing works under consistent experimental conditions. We then provide an in-depth benchmarking study between the two most widely-used FL mechanisms, FedSGD and FedAvg. The benchmarking results show that FedSGD and FedAvg both have advantages and disadvantages under the ACTPR model. For example, FedSGD is barely influenced by the none independent and identically distributed (non-IID) data problem, but FedAvg suffers from a decline in accuracy of up to 9% in our experiments. On the other hand, FedAvg is more efficient than FedSGD regarding time consumption and communication. Lastly, we excavate a set of take-away conclusions, which are very helpful for researchers in the FL area.", "venue": "ArXiv", "authors": ["Di  Chai", "Leye  Wang", "Kai  Chen", "Qiang  Yang"], "year": 2020, "n_citations": 5}
{"id": 2424595, "s2_id": "711e6b20bbef7cd369703c30505b87294692632b", "title": "Formal analysis of SEU mitigation for early dependability and performability analysis of FPGA-based space applications", "abstract": "Abstract SRAM-based FPGAs are increasingly popular in the aerospace industry due to their field programmability and low cost. However, they suffer from cosmic radiation induced Single Event Upsets (SEUs). In safety-critical applications, the dependability of the design is a prime concern since failures may have catastrophic consequences. An early analysis of the relationship between dependability metrics, performability-area trade-off, and different mitigation techniques for such applications can reduce the design effort while increasing the design confidence. This paper introduces a novel methodology based on probabilistic model checking, for the analysis of the reliability, availability, safety and performance-area tradeoffs of safety-critical systems for early design decisions. Starting from the high-level description of a system, a Markov reward model is constructed from the Control Data Flow Graph (CDFG) and a component characterization library targeting FPGAs. The proposed model and exhaustive analysis capture all the failure states (based on the fault detection coverage) and repairs possible in the system. We present quantitative results based on an FIR filter circuit to illustrate the applicability of the proposed approach and to demonstrate that a wide range of useful dependability and performability properties can be analyzed using the proposed methodology. The modeling results show the relationship between different mitigation techniques and fault detection coverage, exposing their direct impact on the design for early decisions.", "venue": "J. Appl. Log.", "authors": ["Khaza Anuarul Hoque", "Otmane A\u00eft Mohamed", "Yvon  Savaria"], "year": 2017, "n_citations": 13}
{"id": 2424814, "s2_id": "da84204f6d0fd70d1f8d287c35f2629cc0c4f85a", "title": "Queries mining for efficient routing in P2P communities", "abstract": "Peer-to-peer (P2P) computing is currently attracting enormous attention. In P2P systems a very large number of autonomous computing nodes (the peers) pool together their resources and rely on each other for data and services. Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of Internet traffic. Examples include P2P systems for network storage, web caching, searching and indexing of relevant documents and distributed network-threat analysis. Requirements for widely distributed information systems supporting virtual organizations have given rise to a new category of P2P systems called schema-based. In such systems each peer exposes its own schema and the main objective is the efficient search across the P2P network by processing each incoming query without overly consuming bandwidth. The usability of these systems depends on effective techniques to find and retrieve data; however, efficient and effective routing of content-based queries is a challenging problem in P2P networks. This work was attended as an attempt to motivate the use of mining algorithms and hypergraphs context to develop two different methods that improve significantly the efficiency of P2P communications. The proposed query routing methods direct the query to a set of relevant peers in such way as to avoid network traffic and bandwidth consumption. We compare the performance of the two proposed methods with the baseline one and our experimental results prove that our proposed methods generate impressive levels of performance and scalability.", "venue": "ArXiv", "authors": ["Anis  Ismail", "Mohamed  Quafafou", "Nicolas  Durand", "Gilles  Nachouki", "Mohammad  Hajjar"], "year": 2011, "n_citations": 4}
{"id": 2425130, "s2_id": "5f856eab99d7f69f2aa4c0e084afd3c016477535", "title": "Fully Unleashing the Power of Paying Multiplexing Only Once in Stochastic Network Calculus", "abstract": "The stochastic network calculus (SNC) holds promise as a framework to calculate probabilistic performance bounds in networks of queues. A great challenge to accurate bounds and efficient calculations are stochastic dependencies between flows due to resource sharing inside the network. However, by carefully utilizing the basic SNC concepts in the network analysis the necessity of taking these dependencies into account can be minimized. To that end, we fully unleash the power of the pay multiplexing only once principle (PMOO, known from the deterministic network calculus) in the SNC analysis. We choose an analytic combinatorics presentation of the results in order to ease complex calculations. In tree-reducible networks, a subclass of a general feedforward networks, we obtain a perfect analysis in terms of avoiding the need to take internal flow dependencies into account. In a comprehensive numerical evaluation, we demonstrate how this unleashed PMOO analysis can reduce the known gap between simulations and SNC calculations significantly, and how it favourably compares to state-of-the art SNC calculations in terms of accuracy and computational effort. Driven by these promising results, we also consider general feedforward networks, when some flow dependencies have to be taken into account. To that end, the unleashed PMOO analysis is extended to the partially dependent case and a case study of a canonical example topology, known as the diamond network, is provided, again displaying favourable results over the state of the art.", "venue": "ArXiv", "authors": ["Anne  Bouillard", "Paul  Nikolaus", "Jens  Schmitt"], "year": 2021, "n_citations": 0}
{"id": 2426244, "s2_id": "0b6ce997df68a2656032424db82005d37266cca8", "title": "Faster 64-bit universal hashing using carry-less multiplications", "abstract": "Intel and AMD support the carry-less multiplication (CLMUL) instruction set in their x64 processors. We use CLMUL to implement an almost universal 64-bit hash family (CLHASH). We compare this new family with what might be the fastest almost universal family on x64 processors (VHASH). We find that CLHASH is at least 60\u00a0% faster. We also compare CLHASH with a popular hash function designed for speed (Google\u2019s CityHash). We find that CLHASH is 40\u00a0% faster than CityHash on inputs larger than 64\u00a0bytes and just as fast otherwise.", "venue": "Journal of Cryptographic Engineering", "authors": ["Daniel  Lemire", "Owen  Kaser"], "year": 2015, "n_citations": 19}
{"id": 2428397, "s2_id": "e6726aab0ebfa1a627e71ccb0003bd0280858e99", "title": "Hardware and Interference Limited Cooperative CR-NOMA Networks Under Imperfect SIC and CSI", "abstract": "The conflation of cognitive radio (CR) and non-orthogonal multiple access (NOMA) concepts is a promising approach to fulfill the massive connectivity goals of future networks given the spectrum scarcity. Accordingly, this paper investigates the performance of a cooperative CR-NOMA network in the presence of system impairments and interference. Our analysis is involved with the derivation of the end-to-end outage probability for primary and secondary networks by accounting for channel state information (CSI), hardware imperfections, and residual interference caused by successive interference cancellation errors as well as coexisting primary/secondary users. Moreover, a mathematically tractable upper bound on spectral efficiency (SE) with its high-SNR approximations are derived. Besides, we propose an optimal power allocation scheme for CR-NOMA users to guarantee their outage and SE fairness. The numerical results validated by Monte Carlo simulations show that the CR-NOMA network provides a superior outage performance over orthogonal multiple access. Furthermore, the higher level of system imperfections leads to the performance degradation of the CR-NOMA networks. As imperfections become more severe, the CR-NOMA is observed to deliver relatively inferior outage and SE performance as compared to the perfect system scenario.", "venue": "IEEE Open Journal of the Communications Society", "authors": ["Sultangali  Arzykulov", "Galymzhan  Nauryzbayev", "Abdulkadir  Celik", "Ahmed M. Eltawil"], "year": 2021, "n_citations": 2}
{"id": 2429256, "s2_id": "109a6a208709d5fe214b1b40a1cc2a93e68e7b16", "title": "Scalable load balancing in networked systems: A survey of recent advances", "abstract": "The basic load balancing scenario involves a single dispatcher where tasks arrive that must immediately be forwarded to one of $N$ single-server queues. We discuss recent advances on scalable load balancing schemes which provide favorable delay performance when $N$ grows large, and yet only require minimal implementation overhead. \nJoin-the-Shortest-Queue (JSQ) yields vanishing delays as $N$ grows large, as in a centralized queueing arrangement, but involves a prohibitive communication burden. In contrast, power-of-$d$ or JSQ($d$) schemes that assign an incoming task to a server with the shortest queue among $d$ servers selected uniformly at random require little communication, but lead to constant delays. In order to examine this fundamental trade-off between delay performance and implementation overhead, we consider JSQ($d(N)$) schemes where the diversity parameter $d(N)$ depends on $N$ and investigate what growth rate of $d(N)$ is required to asymptotically match the optimal JSQ performance on fluid and diffusion scale. \nStochastic coupling techniques and stochastic-process limits play an instrumental role in establishing the asymptotic optimality. We demonstrate how this methodology carries over to infinite-server settings, finite buffers, multiple dispatchers, servers arranged on graph topologies, and token-based load balancing including the popular Join-the-Idle-Queue (JIQ) scheme. In this way we provide a broad overview of the many recent advances in the field. This survey extends the short review presented at ICM 2018 (arXiv:1712.08555).", "venue": "ArXiv", "authors": ["Mark van der Boor", "Sem C. Borst", "Johan van Leeuwaarden", "Debankur  Mukherjee"], "year": 2018, "n_citations": 34}
{"id": 2430162, "s2_id": "968a2821781033dd6e230d60576e6388aaf09c13", "title": "Evaluation of Docker Containers for Scientific Workloads in the Cloud", "abstract": "The HPC community is actively researching and evaluating tools to support execution of scientific applications in cloud-based environments. Among the various technologies, containers have recently gained importance as they have significantly better performance compared to full-scale virtualization, support for microservices and DevOps, and work seamlessly with workflow and orchestration tools. Docker is currently the leader in containerization technology because it offers low overhead, flexibility, portability of applications, and reproducibility. Singularity is another container solution that is of interest as it is designed specifically for scientific applications. It is important to conduct performance and feature analysis of the container technologies to understand their applicability for each application and target execution environment. This paper presents a (1) performance evaluation of Docker and Singularity on bare metal nodes in the Chameleon cloud (2) mechanism by which Docker containers can be mapped with InfiniBand hardware with RDMA communication and (3) analysis of mapping elements of parallel workloads to the containers for optimal resource management with container-ready orchestration tools. Our experiments are targeted toward application developers so that they can make informed decisions on choosing the container technologies and approaches that are suitable for their HPC workloads on cloud infrastructure. Our performance analysis shows that scientific workloads for both Docker and Singularity based containers can achieve near-native performance. Singularity is designed specifically for HPC workloads. However, Docker still has advantages over Singularity for use in clouds as it provides overlay networking and an intuitive way to run MPI applications with one container per rank for fine-grained resources allocation. Both Docker and Singularity make it possible to directly use the underlying network fabric from the containers for coarsegrained resource allocation.", "venue": "PEARC", "authors": ["Pankaj  Saha", "Angel  Beltre", "Piotr  Uminski", "Madhusudhan  Govindaraju"], "year": 2018, "n_citations": 25}
{"id": 2432146, "s2_id": "bbf36761ea9a82cd0a730b7379144e01a2f916a8", "title": "Survivable MPLS Over Optical Transport Networks: Cost and Resource Usage Analysis", "abstract": "In this paper we study different options for the survivability implementation in MPLS over optical transport networks (OTN) in terms of network resource usage and configuration cost. We investigate two approaches to the survivability deployment: single layer and multilayer survivability and present various methods for spare capacity allocation (SCA) to reroute disrupted traffic. The comparative analysis shows the influence of the offered traffic granularity and the physical network structure on the survivability cost: for high bandwidth LSPs, close to the optical channel capacity, the multilayer survivability outperforms the single layer one, whereas for low bandwidth LSPs the single layer survivability is more cost-efficient. On the other hand, sparse networks of low connectivity parameter use more wavelengths for optical path routing and increase the configuration cost, as compared with dense networks. We demonstrate that by mapping efficiently the spare capacity of the MPLS layer onto the resources of the optical layer one can achieve up to 22% savings in the total configuration cost and up to 37% in the optical layer cost. Further savings (up to 9 %) in the wavelength use can be obtained with the integrated approach to network configuration over the sequential one, however, at the increase in the optimization problem complexity. These results are based on a cost model with different cost variations, and were obtained for networks targeted to a nationwide coverage", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Wojtek  Bigos", "Bernard  Cousin", "St\u00e9phane  Gosselin", "Morgane Le Foll", "Hisao  Nakajima"], "year": 2007, "n_citations": 39}
{"id": 2435092, "s2_id": "73d4f2920effebcf3dd0b0f9ae2564a6cd3986cc", "title": "Overlay secondary spectrum sharing with independent re-attempts in cognitive radios", "abstract": "Opportunistic spectrum access (OSA) is a promising reform paradigm envisioned to address the issue of spectrum scarcity in cognitive radio networks (CRNs). While current models consider various aspects of the OSA scheme, the impact of retrial phenomenon in multi-channel CRNs has not yet been analyzed. In this work, we present a continuous-time Markov chain (CTMC) model in which the blocked/preempted secondary users (SUs) enter a finite retrial group (or orbit) and re-attempt independently for service in an exponentially distributed random manner. Taking into account the inherent retrial tendency of SUs, we numerically assess the performance of the proposed scheme in terms of dropping probability and throughput of SUs.", "venue": "2016 IEEE 37th Sarnoff Symposium", "authors": ["Muthukrishnan Senthil Kumar", "Aresh  Dadlani", "Kiseon  Kim", "Richard O. Afolabi"], "year": 2016, "n_citations": 0}
{"id": 2437390, "s2_id": "c7d7f4394be9a796299880a1f8f5357bff2a708d", "title": "Throughput and Delay Analysis of Slotted Aloha with Batch Service", "abstract": "In this paper, we study the throughput and delay performances of the slotted Aloha with batch service, which has wide applications in random access networks. Different from the classical slotted Aloha, each node in the slotted Aloha with batch service can transmit up to M packets once it succeeds in channel competition. The throughput is substantially improved because up to M packets jointly undertake the overhead due to contention. In an innovative vacation model developed in this paper, we consider each batch of data transmission as a busy period of each node, and the process between two successive busy periods as a vacation period. We then formulate the number of arrivals during a vacation period in a renewal-type equation, which characterizes the dependency between busy periods and vacation periods. Based on this formulation, we derive the mean waiting time of a packet and the bounded delay region for the slotted Aloha with batch service. Our results indicate the throughput and delay performances are substantially improved with the increase of batch sizeM, and the bounded delay region is enlarged accordingly. As M goes to infinity, we find the saturated throughput can approach 100% of channel capacity, and the system remains stable irrespective of the population size and transmission probability.", "venue": "ArXiv", "authors": ["Huanhuan  Huang", "Tong  Ye", "Tony T. Lee"], "year": 2019, "n_citations": 1}
{"id": 2440186, "s2_id": "c67fd991fdcea8c89be2ad632ea8476e07a24edc", "title": "Cloud-Based or On-Device: An Empirical Study of Mobile Deep Inference", "abstract": "Modern mobile applications are benefiting significantly from the advancement in deep learning, e.g., implementing real-time image recognition and conversational system. Given a trained deep learning model, applications usually need to perform a series of matrix operations based on the input data, in order to infer possible output values. Because of computational complexity and size constraints, these trained models are often hosted in the cloud. To utilize these cloud-based models, mobile apps will have to send input data over the network. While cloud-based deep learning can provide reasonable response time for mobile apps, it restricts the use case scenarios, e.g. mobile apps need to have network access. With mobile specific deep learning optimizations, it is now possible to employ on-device inference. However, because mobile hardware, such as GPU and memory size, can be very limited when compared to its desktop counterpart, it is important to understand the feasibility of this new on-device deep learning inference architecture. In this paper, we empirically evaluate the inference performance of three Convolutional Neural Networks (CNNs) using a benchmark Android application we developed. Our measurement and analysis suggest that on-device inference can cost up to two orders of magnitude greater response time and energy when compared to cloud-based inference, and that loading model and computing probability are two performance bottlenecks for on-device deep inferences.", "venue": "2018 IEEE International Conference on Cloud Engineering (IC2E)", "authors": ["Tian  Guo"], "year": 2018, "n_citations": 36}
{"id": 2440405, "s2_id": "b8193029b696db35f7d658108e2cd30b4057a17f", "title": "FlexClock: Generic Clock Reconfiguration for Low-end IoT Devices", "abstract": "Clock configuration within constrained general-purpose microcontrollers takes a key role in tuning performance, power consumption, and timing accuracy of applications in the Internet of Things (IoT). Subsystems governing the underlying clock tree must nonetheless cope with a huge parameter space, complex dependencies, and dynamic constraints. Manufacturers expose the underlying functions in very diverse ways, which leads to specialized implementations of low portability. In this paper, we propose FlexClock, an approach for generic online clock reconfiguration on constrained IoT devices. We argue that (costly) generic clock configuration of general purpose computers and powerful mobile devices need to slim down to the lower end of the device spectrum. In search of a generalized solution, we identify recurring patterns and building blocks, which we use to decompose clock trees into independent, reusable components. With this segmentation we derive an abstract representation of vendor-specific clock trees, which then can be dynamically reconfigured at runtime. We evaluate our implementation on common hardware. Our measurements demonstrate how FlexClock significantly improves peak power consumption and energy efficiency by enabling dynamic voltage and frequency scaling (DVFS) in a platform-agnostic way.", "venue": "ArXiv", "authors": ["Michel  Rottleuthner", "Thomas C. Schmidt", "Matthias  W\u00e4hlisch"], "year": 2021, "n_citations": 0}
{"id": 2440597, "s2_id": "7d97310fb9ee5c8c776549ea9e18c7fe95634a4e", "title": "CHOP: Bypassing Runtime Bounds Checking Through Convex Hull OPtimization", "abstract": "Unsafe memory accesses in programs written using popular programming languages like C/C++ have been among the leading causes for software vulnerability. Prior memory safety checkers such as SoftBound enforce memory spatial safety by checking if every access to array elements are within the corresponding array bounds. However, it often results in high execution time overhead due to the cost of executing the instructions associated with bounds checking. To mitigate this problem, redundant bounds check elimination techniques are needed. In this paper, we propose CHOP, a Convex Hull OPtimization based framework, for bypassing redundant memory bounds checking via profile-guided inferences. In contrast to existing check elimination techniques that are limited by static code analysis, our solution leverages a model-based inference to identify redundant bounds checking based on runtime data from past program executions. For a given function, it rapidly derives and updates a knowledge base containing sufficient conditions for identifying redundant array bounds checking. We evaluate CHOP on real-world applications and benchmark (such as SPEC) and the experimental results show that on average 80.12% of dynamic bounds check instructions can be avoided, resulting in improved performance up to 95.80% over SoftBound.", "venue": "Comput. Secur.", "authors": ["Yurong  Chen", "Hongfa  Xue", "Tian  Lan", "Guru  Venkataramani"], "year": 2020, "n_citations": 0}
{"id": 2442143, "s2_id": "76259bcebb44e61a4c6fdac66905afdb528ea0c6", "title": "Program Synthesis and Linear Operator Semantics", "abstract": "For deterministic and probabilistic programs we investigate the problem of program synthesis and program optimisation (with respect to non-functional properties) in the general setting of global optimisation. This approach is based on the representation of the semantics of programs and program fragments in terms of linear operators, i.e. as matrices. We exploit in particular the fact that we can automatically generate the representation of the semantics of elementary blocks. These can then can be used in order to compositionally assemble the semantics of a whole program, i.e. the generator of the corresponding Discrete Time Markov Chain (DTMC). We also utilise a generalised version of Abstract Interpretation suitable for this linear algebraic or functional analytical framework in order to formulate semantical constraints (invariants) and optimisation objectives (for example performance requirements).", "venue": "SYNT", "authors": ["Herbert  Wiklicky"], "year": 2014, "n_citations": 0}
{"id": 2444410, "s2_id": "5358f1584a6795333737525c7f9f634381e0539f", "title": "On the Asymptotic Optimality of Work-Conserving Disciplines in Completion Time Minimization", "abstract": "In this paper, we prove that under mild stochastic assumptions, work-conserving disciplines are asymptotic optimal for minimizing total completion time. As a byproduct of our analysis, we obtain tight upper bound on the competitive ratios of work-conserving disciplines on minimizing the metric of flow time.", "venue": "2020 29th International Conference on Computer Communications and Networks (ICCCN)", "authors": ["Wenxin  Li", "Ness  Shroff"], "year": 2020, "n_citations": 1}
{"id": 2459169, "s2_id": "413edf9e66657de2461ff1eef06c06bae3daf038", "title": "Joint channel assignment and opportunistic routing for maximizing throughput in cognitive radio networks", "abstract": "In this paper, we consider the joint opportunistic routing and channel assignment problem in multi-channel multi-radio (MCMR) cognitive radio networks (CRNs) for improving aggregate throughput of the secondary users. We first present the linear programming optimization model for this joint problem, taking into account the feature of CRNs-channel uncertainty. Then considering the queue state of a node, we propose a new scheme to select proper forwarding candidates for opportunistic routing. Furthermore, a new algorithm for calculating the forwarding probability of any packet at a node is proposed, which is used to calculate how many packets a forwarder should send, so that the duplicate transmission can be reduced compared with MAC-independent opportunistic routing & encoding (MORE) [11]. Our numerical results show that the proposed scheme performs significantly better that traditional routing and opportunistic routing in which channel assignment strategy is employed.", "venue": "2014 IEEE Global Communications Conference", "authors": ["Yang  Qin", "Xiaoxiong  Zhong", "Yuanyuan  Yang", "Yanlin  Li", "Li  Li"], "year": 2014, "n_citations": 7}
{"id": 2459708, "s2_id": "b1907f593d835d3775248d329edfaac12d46b27a", "title": "Fast Query Processing by Distributing an Index over CPU Caches", "abstract": "Data intensive applications on clusters often require requests quickly be sent to the node managing the desired data. In many applications, one must look through a sorted tree structure to determine the responsible node for accessing or storing the data. Examples include object tracking in sensor networks, packet routing over the Internet, request processing in publish-subscribe middleware, and query processing in database systems. When the tree structure is larger than the CPU cache, the standard implementation potentially incurs many cache misses for each lookup; one cache miss at each successive level of the tree. As the CPU-RAM gap grows, this performance degradation will only become worse in the future. We propose a solution that takes advantage of the growing speed of local area networks for clusters. We split the sorted tree structure among the nodes of the cluster. We assume that the structure will fit inside the aggregation of the CPU caches of the entire cluster. We then send a word over the network (as part of a larger packet containing other words) in order to examine the tree structure in another node's CPU cache. We show that this is often faster than the standard solution, which locally incurs multiple cache misses while accessing each successive level of the tree. The principle is demonstrated with a cluster configured with Pentium III nodes connected with a Myrinet network. The new approach is shown to be 50% faster on this current cluster. In the future, the new approach is expected to have a still greater advantage as networks grow in speed, and as cache lines grow in length (greater cache miss penalty). This can be used to successfully overcome the inherent memory latency associated with cache misses", "venue": "2005 IEEE International Conference on Cluster Computing", "authors": ["Xiaoqin  Ma", "Gene  Cooperman"], "year": 2005, "n_citations": 0}
{"id": 2459931, "s2_id": "fdfdfa1a4f40bf777e525ef7e3a9a68e2f6b1568", "title": "A New Approach to Manage QoS in Distributed Multimedia Systems", "abstract": "Dealing with network congestion is a criterion used to enhance quality of service (QoS) in distributed multimedia systems. The existing solutions for the problem of network congestion ignore scalability considerations because they maintain a separate classification for each video stream. In this paper, we propose a new method allowing to control QoS provided to clients according to the network congestion, by discarding some frames when needed. The technique proposed, called (m,k)-frame, is scalable with little degradation in application performances. (m,k)-frame method is issued from the notion of (m,k)-firm real-time constraints which means that among k invocations of a task, m invocations must meet their deadline. Our simulation studies show the usefulness of (m,k)- frame method to adapt the QoS to the real conditions in a multimedia application, according to the current system load. Notably, the system must adjust the QoS provided to active clients 1 when their number varies, i.e. dynamic arrival of clients. The main problems are related to the adaptation of available resources (bandwidth, buffer size, video servers, etc.) and to the proposition of new techniques which deal with system instability periods (overload or under-utilization). The proposition must allow to ensure an acceptable QoS while respecting the multiple requirements of the video streams. An example of multimedia applications is a video-on- demand or a streaming audio. It is important for the application to receive and process the information at an almost constant rate, eg., 30 frames per second for video information. However, due to the network problems, some packets of a video frame can be lost, resulting in little or no noticeable degradation in the QoS at the receiver. More concretely, we consider the MPEG transmission where some frames containing control information (for example synchronization) are inserted into the packet stream in regular way. The video packet can tolerate a certain deadline miss rate only if the deadline misses are uniformly distributed. A large number of consecutive deadline misses cannot be acceptable. Therefore, we should provide adaptive mechanism for controlling deadline miss distribution, to achieve graceful performance degradation (3)(4)(5).", "venue": "ArXiv", "authors": ["Bechir  Alaya", "Claude  Duvallet", "Bruno  Sadeg"], "year": 2009, "n_citations": 6}
{"id": 2460370, "s2_id": "e3791c1083eea118fe68575648127f76aab9a20c", "title": "TiFL: A Tier-based Federated Learning System", "abstract": "Federated Learning (FL) enables learning a shared model acrossmany clients without violating the privacy requirements. One of the key attributes in FL is the heterogeneity that exists in both resource and data due to the differences in computation and communication capacity, as well as the quantity and content of data among different clients. We conduct a case study to show that heterogeneity in resource and data has a significant impact on training time and model accuracy in conventional FL systems. To this end, we propose TiFL, a Tier-based Federated Learning System, which divides clients into tiers based on their training performance and selects clients from the same tier in each training round to mitigate the straggler problem caused by heterogeneity in resource anddata quantity. To further tame the heterogeneity caused by non-IID (Independent and Identical Distribution) data and resources, TiFL employs an adaptive tier selection approach to update the tiering on-the-fly based on the observed training performance and accuracy. We prototype TiFL in a FL testbed following Google's FL architecture and evaluate it using the state-of-the-art FL benchmarks. Experimental evaluation shows that TiFL outperforms the conventional FL in various heterogeneous conditions. With the proposed adaptive tier selection policy, we demonstrate that TiFL achieves much faster training performance while achieving the same or better test accuracy across the board.", "venue": "HPDC", "authors": ["Zheng  Chai", "Ahsan  Ali", "Syed  Zawad", "Stacey  Truex", "Ali  Anwar", "Nathalie  Baracaldo", "Yi  Zhou", "Heiko  Ludwig", "Feng  Yan", "Yue  Cheng"], "year": 2020, "n_citations": 34}
{"id": 2460802, "s2_id": "1c285410cb172acae5b5fecd387c0724b30974b9", "title": "Hash-Based Ray Path Prediction: Skipping BVH Traversal Computation by Exploiting Ray Locality", "abstract": "State-of-the-art ray tracing techniques operate on hierarchical acceleration structures such as BVH trees which wrap objects in a scene into bounding volumes of decreasing sizes. Acceleration structures reduce the amount of ray-scene intersections that a ray has to perform to find the intersecting object. However, we observe a large amount of redundancy when rays are traversing these acceleration structures. While modern acceleration structures explore the spatial organization of the scene, they neglect similarities between rays that traverse the structures and thereby cause redundant traversals. This paper provides a limit study of a new promising technique, Hash-Based Ray Path Prediction (HRPP), which exploits the similarity between rays to predict leaf nodes to avoid redundant acceleration structure traversals. Our data shows that acceleration structure traversal consumes a significant proportion of the ray tracing rendering time regardless of the platform or the target image quality. Our study quantifies unused ray locality and evaluates the theoretical potential for improved ray traversal performance for both coherent and seemingly incoherent rays. We show that HRPP is able to skip, on average, 40% of all hit-all traversal computations.", "venue": "ArXiv", "authors": ["Francois  Demoullin", "Ayub  Gubran", "Tor  Aamodt"], "year": 2019, "n_citations": 2}
{"id": 2461907, "s2_id": "354dd4cff8bc3a53820fa42cef3113f8f26d7545", "title": "Refined Mean Field Analysis of the Gossip Shuffle Protocol - extended version -", "abstract": "Gossip protocols form the basis of many smart collective adaptive systems. They are a class of fully decentralised, simple but robust protocols for the distribution of information throughout large scale networks with hundreds or thousands of nodes. Mean field analysis methods have made it possible to approximate and analyse performance aspects of such large scale protocols in an efficient way. Taking the gossip shuffle protocol as a benchmark, we evaluate a recently developed refined mean field approach. We illustrate the gain in accuracy this can provide for the analysis of medium size models analysing two key performance measures. We also show that refined mean field analysis requires special attention to correctly capture the coordination aspects of the gossip shuffle protocol.", "venue": "ArXiv", "authors": ["Nicolas  Gast", "Diego  Latella", "Mieke  Massink"], "year": 2020, "n_citations": 2}
{"id": 2463985, "s2_id": "6c2fe9c1ec7d007ade31481dc969f9d217088dba", "title": "NeuralPower: Predict and Deploy Energy-Efficient Convolutional Neural Networks", "abstract": "\"How much energy is consumed for an inference made by a convolutional neural network (CNN)?\" With the increased popularity of CNNs deployed on the wide-spectrum of platforms (from mobile devices to workstations), the answer to this question has drawn significant attention. From lengthening battery life of mobile devices to reducing the energy bill of a datacenter, it is important to understand the energy efficiency of CNNs during serving for making an inference, before actually training the model. In this work, we propose NeuralPower: a layer-wise predictive framework based on sparse polynomial regression, for predicting the serving energy consumption of a CNN deployed on any GPU platform. Given the architecture of a CNN, NeuralPower provides an accurate prediction and breakdown for power and runtime across all layers in the whole network, helping machine learners quickly identify the power, runtime, or energy bottlenecks. We also propose the \"energy-precision ratio\" (EPR) metric to guide machine learners in selecting an energy-efficient CNN architecture that better trades off the energy consumption and prediction accuracy. The experimental results show that the prediction accuracy of the proposed NeuralPower outperforms the best published model to date, yielding an improvement in accuracy of up to 68.5%. We also assess the accuracy of predictions at the network level, by predicting the runtime, power, and energy of state-of-the-art CNN architectures, achieving an average accuracy of 88.24% in runtime, 88.34% in power, and 97.21% in energy. We comprehensively corroborate the effectiveness of NeuralPower as a powerful framework for machine learners by testing it on different GPU platforms and Deep Learning software tools.", "venue": "ArXiv", "authors": ["Ermao  Cai", "Da-Cheng  Juan", "Dimitrios  Stamoulis", "Diana  Marculescu"], "year": 2017, "n_citations": 62}
{"id": 2464344, "s2_id": "b44773a506cdad751e5a94446ca83307157249ba", "title": "Measurement and Evaluation of ENUM Server Performance", "abstract": "ENUM is a DNS-based protocol standard for mapping E.164 telephone numbers to Internet Uniform Resource Identifiers (URIs). It places unique requirements on the existing DNS infrastructure, such as data scalability, query throughput, response time, and database update rates. This paper measures and evaluates the performance of existing name server implementation as ENUM servers. We compared PowerDNS (PDNS), BIND and Navitas. Results show that BIND is not suitable for ENUM due to its poor scaling property. Both PDNS and Navitas can serve ENUM. However, Navitas turns out to be highly optimized and clearly outperforms PDNS in all aspects we have tested. We also instrumented the PDNS server to identify its performance bottleneck and investigated ways to improve it.", "venue": "2007 IEEE International Conference on Communications", "authors": ["Charles  Shen", "Henning  Schulzrinne"], "year": 2007, "n_citations": 3}
{"id": 2468280, "s2_id": "3ea5866c3f2a4d8d367a6694b0a377dbaa2141ad", "title": "Broadcast Strategies and Performance Evaluation of IEEE 802.15.4 in Wireless Body Area Networks WBAN", "abstract": "The rapid advances in sensors and ultra-low power wireless communication has enabled a new generation of wireless sensor networks: Wireless Body Area Networks (WBAN). To the best of our knowledge the current paper is the first to address broadcast in WBAN. We first analyze several broadcast strategies inspired from the area of Delay Tolerant Networks (DTN). The proposed strategies are evaluated via the OMNET++ simulator that we enriched with realistic human body mobility models and channel models issued from the recent research on biomedical and health informatics. Contrary to the common expectation, our results show that existing research in DTN cannot be transposed without significant modifications in WBANs area. That is, existing broadcast strategies for DTNs do not perform well with human body mobility. However, our extensive simulations give valuable insights and directions for designing efficient broadcast in WBAN. Furthermore, we propose a novel broadcast strategy that outperforms the existing ones in terms of end-to-end delay, network coverage and energy consumption. Additionally, we performed investigations of independent interest related to the ability of all the studied strategies to ensure the total order delivery property when stressed with various packet rates. These investigations open new and challenging research directions.", "venue": "Ad Hoc Networks", "authors": ["Wafa  Badreddine", "Claude  Chaudet", "Federico  Petruzzi", "Maria Gradinariu Potop-Butucaru"], "year": 2020, "n_citations": 6}
{"id": 2468706, "s2_id": "39173c47d6603169682d8527e31106a89dbdf039", "title": "TeaMPI\u2014Replication-Based Resilience Without the (Performance) Pain", "abstract": "In an era where we can not afford to checkpoint frequently, replication is a generic way forward to construct numerical simulations that can continue to run even if hardware parts fail. Yet, replication often is not employed on larger scales, as na\u00efvely mirroring a computation once effectively halves the machine size, and as keeping replicated simulations consistent with each other is not trivial. We demonstrate for the ExaHyPE engine\u2014a task-based solver for hyperbolic equation systems\u2014that it is possible to realise resiliency without major code changes on the user side, while we introduce a novel algorithmic idea where replication reduces the time-to-solution. The redundant CPU cycles are not burned \u201cfor nothing\u201d. Our work employs a weakly consistent data model where replicas run independently yet inform each other through heartbeat messages whether they are still up and running. Our key performance idea is to let the tasks of the replicated simulations share some of their outcomes, while we shuffle the actual task execution order per replica. This way, replicated ranks can skip some local computations and automatically start to synchronise with each other. Our experiments with a production-level seismic wave-equation solver provide evidence that this novel concept has the potential to make replication affordable for large-scale simulations in high-performance computing.", "venue": "ISC", "authors": ["Philipp  Samfass", "Tobias  Weinzierl", "Benjamin  Hazelwood", "Michael  Bader"], "year": 2020, "n_citations": 3}
{"id": 2469334, "s2_id": "948ed8bcb30eab7046d00a258b93758f74d1b0cc", "title": "An Empirical Study on variants of TCP over AODV routing protocol in MANET", "abstract": "The cardinal concept of TCP development was to carry data within the network where network congestion plays a vital role to cause packet loss. On the other hand, there are several other reasons to lose packets in Mobile Ad Hoc Networks due to fading, interfaces, multi-path routing, malicious node, and black hole. Along with throughput, fairness of TCP protocols is important to establish a good communication. In this paper, an empirical study has been done by simulation and analysis of TCP variations under AODV routing protocol. In our simulation, we studied multiple variations of TCP, such as Reno, New-Reno, Vegas, and Tahoe. The simulation work has been done in NS2 environment. Based on the analysis simulation result of we carried out our observations with respect to the behavior of AODV routing protocol for different TCP packets under several QoS metrics such as drop, throughput, delay, and jitter.", "venue": "ArXiv", "authors": ["Md. Monzur Morshed", "Meftah Ur Rahman", "Md. Rafiqul Islam"], "year": 2011, "n_citations": 3}
{"id": 2469679, "s2_id": "4321b1fbe929a0e52e51aff7ee21fea6f583e380", "title": "A Generalized Performance Evaluation Framework for Parallel Systems with Output Synchronization", "abstract": "Frameworks, such as MapReduce and Hadoop are abundant nowadays. They seek to reap benefits of parallelization, albeit subject to a synchronization constraint at the output. Fork-Join (FJ) queuing models are used to analyze such systems. Arriving jobs are split into tasks each of which is mapped to exactly one server. A job leaves the system when all of its tasks are executed. \nAs a metric of performance, we consider waiting times for both work-conserving and non-work conserving server systems under a mathematical set-up general enough to take into account possible phase-type behavior of the servers, and as suggested by recent evidences, bursty arrivals. \nTo this end, we present a Markov-additive process framework for an FJ system and provide computable bounds on tail probabilities of steady-state waiting times, for both types of servers separately. We apply our results to three scenarios, namely, non-renewal (Markov-modulated) arrivals, servers showing phase-type behavior, and Markov-modulated arrivals and services. We compare our bounds against estimates obtained through simulations and also provide a theoretical conceptualization of provisions in FJ systems. Finally, we calibrate our model with real data traces, and illustrate how our bounds can be used to devise provisions.", "venue": "ArXiv", "authors": ["Wasiur R. KhudaBukhsh", "Sounak  Kar", "Amr  Rizk", "Heinz  Koeppl"], "year": 2016, "n_citations": 2}
{"id": 2474115, "s2_id": "8e5a30011f16715d3ca58a81cf0b2f96a2f26534", "title": "Architectural Implications of Graph Neural Networks", "abstract": "Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This letter tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.", "venue": "IEEE Computer Architecture Letters", "authors": ["Zhihui  Zhang", "Jingwen  Leng", "Lingxiao  Ma", "Youshan  Miao", "Chao  Li", "Minyi  Guo"], "year": 2020, "n_citations": 9}
{"id": 2478733, "s2_id": "c6dfa687c30aba92330badd5ba15756aab1b558f", "title": "Delay-Optimal Scheduling for Queueing Systems with Switching Overhead", "abstract": "We study the scheduling polices for asymptotically optimal delay in queueing systems with switching overhead. Such systems consist of a single server that serves multiple queues, and some capacity is lost whenever the server switches to serve a different set of queues. The capacity loss due to this switching overhead can be significant in many emerging applications, and needs to be explicitly addressed in the design of scheduling policies. For example, in 60GHz wireless networks with directional antennas, base stations need to train and reconfigure their beam patterns whenever they switch from one client to another. Considerable switching overhead can also be observed in many other queueing systems such as transportation networks and manufacturing systems. While the celebrated Max-Weight policy achieves asymptotically optimal average delay for systems without switching overhead, it fails to preserve throughput-optimality, let alone delay-optimality, when switching overhead is taken into account. We propose a class of Biased Max-Weight scheduling policies that explicitly takes switching overhead into account. The Biased Max-Weight policy can use either queue length or head-of-line waiting time as an indicator of the system status. We prove that our policies not only are throughput-optimal, but also can be made arbitrarily close to the asymptotic lower bound on average delay. To validate the performance of the proposed policies, we provide extensive simulation with various system topologies and different traffic patterns. We show that the proposed policies indeed achieve much better delay performance than that of the state-of-the-art policy.", "venue": "ArXiv", "authors": ["Ping-Chun  Hsieh", "I-Hong  Hou", "Xi  Liu"], "year": 2017, "n_citations": 7}
{"id": 2480387, "s2_id": "a3b572b758cf5e21bfdd3de0638c6860543b79e1", "title": "An upper bound on the convergence time for quantized consensus", "abstract": "We analyze a class of distributed quantized consensus algorithms for arbitrary networks. In the initial setting, each node in the network has an integer value. Nodes exchange their current estimate of the mean value in the network, and then update their estimate by communicating with their neighbors in a limited capacity channel in an asynchronous clock setting. Eventually, all nodes reach consensus with quantized precision. We start the analysis with a special case of a distributed binary voting algorithm, then proceed to the expected convergence time for the general quantized consensus algorithm proposed by Kashyap et al. We use the theory of electric networks, random walks, and couplings of Markov chains to derive an O(N3 log N) upper bound for the expected convergence time on an arbitrary graph of size N, improving on the state of art bound of O(N4 log N) for binary consensus and O(N5) for quantized consensus algorithms. Our result is not dependent on the graph topology. Simulations are performed to validate the analysis.", "venue": "INFOCOM", "authors": ["Shang  Shang", "Paul  Cuff", "Pan  Hui", "Sanjeev R. Kulkarni"], "year": 2013, "n_citations": 14}
{"id": 2485726, "s2_id": "a9c750a67f00b398924674fb372c5c6d356e9a5a", "title": "Bi-objective Optimisation of Data-parallel Applications on Heterogeneous Platforms for Performance and Energy via Workload Distribution", "abstract": "Performance and energy are the two most important objectives for optimisation on modern parallel platforms. Latest research demonstrated the importance of workload distribution as a decision variable in the bi-objective optimisation for performance and energy on homogeneous multicore clusters. We show in this work that bi-objective optimisation for performance and energy on heterogeneous processors results in a large number of Pareto-optimal optimal solutions (workload distributions) even in the simple case of linear performance and energy profiles. We then study performance and energy profiles of real-life data-parallel applications and find that their shapes are non-linear, complex and non-smooth. We, therefore, propose an efficient and exact global optimisation algorithm, which takes as an input most general discrete performance and dynamic energy profiles of the heterogeneous processors and solves the bi-objective optimisation problem. The algorithm is also used as a building block to solve the bi-objective optimisation problem for performance and total energy. We also propose a novel methodology to build discrete dynamic energy profiles of individual computing devices, which are input to the algorithm. The methodology is based purely on system-level measurements and addresses the fundamental challenge of accurate component-level energy modelling of a hybrid data-parallel application running on a heterogeneous platform integrating CPUs and accelerators. We experimentally validate the proposed method using two data-parallel applications, matrix multiplication and 2D fast Fourier transform (2D-FFT).", "venue": "ArXiv", "authors": ["Hamidreza  Khaleghzadeh", "Muhammad  Fahad", "Arsalan  Shahid", "Ravi  Reddy", "Alexey L. Lastovetsky"], "year": 2019, "n_citations": 2}
{"id": 2486524, "s2_id": "7bfb3a0a12d7b42cd70333689236e3de3a5d2409", "title": "Throughput metrics and packet delay in TCP/IP networks", "abstract": "In the paper the method for estimation of throughput metrics like available bandwidth and end-t-end capacity is supposed. This method is based on measurement of network delay $D_i$ for packets of different sizes $W_i$. The simple expression for available bandwidth $B_{av} =(W_2-W_1)/(D_2-D_1)$ is substantiated. The number of experiments on matching of the results received new and traditional methods is spent. The received results testify to possibility of application of new model.", "venue": "ArXiv", "authors": ["Andrei M. Sukhov", "T. G. Sultanov", "M. V. Strizhov", "A. P. Platonov"], "year": 2009, "n_citations": 3}
{"id": 2488954, "s2_id": "1ded0cf1dd1882e4f12e324ced6bdc00d9c33a7d", "title": "SOAP Serialization Performance Enhancement, Design And Implementation Of A Middleware", "abstract": "The most straightforward way to improve performance of any system is to define the bottlenecks and think of ways to remove them. Web services are the inseparable part of any web application, as a result enhancing performance of web services will have a great effect on the overall performance of the system. The most widely used communication protocol in the web services model, SOAP, is a simple protocol for the exchange of messages. The serialization of large SOAP responses is a major performance bottleneck in a SOAP message exchange. Clearly, some web servers can expect to receive many similar messages for a particular web service as they share the same signature. The idea behind this paper is to avoid the redundant serialization stage of SOAP responses for request which have the same call parameters. The technique exploits the similarities between call parameters to improve web service Response Time by avoiding redundant serialization of the same response with the help of a middleware running on top of web server. The middleware will maintain a trie of incoming parameters for every set of current requests. This way request processing and serialization of the response of same requests will be done only once. In a nutshell, to serialize only the different responses is the simplest way to avoid extra work done by a serializer. It might worth noting that although our approach is to utilize the exact repeating portion parameters, the middleware can be configured to apply changes made to the result set of response to the serialized response being maintained in a trie to generate valid results.", "venue": "ArXiv", "authors": ["Behrouz  Minaei-Bidgoli", "Parinaz  Saadat"], "year": 2009, "n_citations": 6}
{"id": 2491464, "s2_id": "9cdb8228d328229884536823bdec9be0ac219517", "title": "A token-based central queue with order-independent service rates", "abstract": "We study a token-based central queue with multiple customer types. Customers of each type arrive according to a Poisson process and have an associated set of compatible tokens. Customers may only receive service when they have claimed a compatible token. If upon arrival, more than one compatible token is available, an assignment rule determines which token will be claimed. The service rate obtained by a customer is state-dependent, i.e., it depends on the set of claimed tokens and on the number of customers in the system. Our first main result shows that, provided the assignment rule and the service rates satisfy certain conditions, the steady-state distribution has a product form. We show that our model subsumes known families of models that have product-form steady-state distributions including the order-independent queue of Krzesinski (2011) and the model of Visschers et al. (2012). Our second main contribution involves the derivation of expressions for relevant performance measures such as the sojourn time and the number of customers present in the system. We apply our framework to relevant models, including an M/M/K queue with heterogeneous service rates, the MSCCC queue, multi-server models with redundancy and matching models. For some of these models, we present expressions for performance measures that have not been derived before.", "venue": "ArXiv", "authors": ["Urtzi  Ayesta", "Tejas  Bodas", "J. L. Dorsman", "Maaike  Verloop"], "year": 2019, "n_citations": 10}
{"id": 2491616, "s2_id": "9a5c5174722dbae5a81c9b242c78828c594456e1", "title": "A new GPU implementation for lattice-Boltzmann simulations on sparse geometries", "abstract": "We describe a high-performance implementation of the lattice Boltzmann method (LBM) for sparse 3D geometries on graphic processors (GPU). The main contribution of this work is a data layout that allows to minimise the number of redundant memory transactions during the propagation step of LBM. We show that by using a uniform mesh of small three-dimensional tiles and a careful data placement it is possible to utilise more than 70% of maximum theoretical GPU memory bandwidth for D3Q19 lattice and double precision numbers. The performance of our implementation is thoroughly examined and compared with other GPU implementations of LBM. The proposed method performs the best for sparse geometries with good spatial locality.", "venue": "Comput. Phys. Commun.", "authors": ["Tadeusz  Tomczak", "Roman G. Szafran"], "year": 2019, "n_citations": 12}
{"id": 2492240, "s2_id": "45631c0c7eda91d26b5c9a1c7049164951562f10", "title": "A Characterization of the SPARC T3-4 System", "abstract": "This technical report covers a set of experiments on the 64-core SPARC T3-4 system, comparing it to two similar AMD and Intel systems. Key characteristics as maximum integer and floating point arithmetic throughput are measured as well as memory throughput, showing the scalability of the SPARC T3-4 system. The performance of POSIX threads primitives is characterized and compared in detail, such as thread creation and mutex synchronization. Scalability tests with a fine grained multithreaded runtime are performed, showing problems with atomic CAS operations on such physically highly parallel systems.", "venue": "ArXiv", "authors": ["Michiel W. van Tol"], "year": 2011, "n_citations": 5}
{"id": 2492262, "s2_id": "93ef9dbcefa99ebe5b8593dcb6bb2136f9f905e5", "title": "A Two-Queue Polling Model with Two Priority Levels in the First Queue", "abstract": "In this paper we consider a single-server cyclic polling system consisting of two queues. Between visits to successive queues, the server is delayed by a random switch-over time. Two types of customers arrive at the first queue: high and low priority customers. For this situation the following service disciplines are considered: gated, globally gated, and exhaustive. We study the cycle time distribution, the waiting times for each customer type, the joint queue length distribution at polling epochs, and the steady-state marginal queue length distributions for each customer type.", "venue": "Discret. Event Dyn. Syst.", "authors": ["Marko A. A. Boon", "Ivo J. B. F. Adan", "Onno J. Boxma"], "year": 2010, "n_citations": 30}
{"id": 2496661, "s2_id": "2ed9a82e85114d9e220217375acceefedba64844", "title": "Quantifying the Performance of Federated Transfer Learning", "abstract": "The scarcity of data and isolated data islands encourage different organizations to share data with each other to train machine learning models. However, there are increasing concerns on the problems of data privacy and security, which urges people to seek a solution like Federated Transfer Learning (FTL) to share training data without violating data privacy. FTL leverages transfer learning techniques to utilize data from different sources for training, while achieving data privacy protection without significant accuracy loss. However, the benefits come with a cost of extra computation and communication consumption, resulting in efficiency problems. In order to efficiently deploy and scale up FTL solutions in practice, we need a deep understanding on how the infrastructure affects the efficiency of FTL. Our paper tries to answer this question by quantitatively measuring a real-world FTL implementation FATE on Google Cloud. According to the results of carefully designed experiments, we verified that the following bottlenecks can be further optimized: 1) Inter-process communication is the major bottleneck; 2) Data encryption adds considerable computation overhead; 3) The Internet networking condition affects the performance a lot when the model is large.", "venue": "ArXiv", "authors": ["Qinghe  Jing", "Weiyan  Wang", "Junxue  Zhang", "Han  Tian", "Kai  Chen"], "year": 2019, "n_citations": 8}
{"id": 2500415, "s2_id": "87a28009f9ff5f5bba552b7e78370ac8d0962239", "title": "ALERT: Accurate Learning for Energy and Timeliness", "abstract": "An increasing number of software applications incorporate runtime Deep Neural Networks (DNNs) to process sensor data and return inference results to humans. Effective deployment of DNNs in these interactive scenarios requires meeting latency and accuracy constraints while minimizing energy, a problem exacerbated by common system dynamics. Prior approaches handle dynamics through either (1) system-oblivious DNN adaptation, which adjusts DNN latency/accuracy tradeoffs, or (2) application-oblivious system adaptation, which adjusts resources to change latency/energy tradeoffs. In contrast, this paper improves on the state-of-the-art by coordinating application- and system-level adaptation. ALERT, our runtime scheduler, uses a probabilistic model to detect environmental volatility and then simultaneously select both a DNN and a system resource configuration to meet latency, accuracy, and energy constraints. We evaluate ALERT on CPU and GPU platforms for image and speech tasks in dynamic environments. ALERT's holistic approach achieves more than 13% energy reduction, and 27% error reduction over prior approaches that adapt solely at the application or system level. Furthermore, ALERT incurs only 3% more energy consumption and 2% higher DNN-inference error than an oracle scheme with perfect application and system knowledge.", "venue": "USENIX Annual Technical Conference", "authors": ["Chengcheng  Wan", "Muhammad Husni Santriaji", "Eri  Rogers", "Henry  Hoffmann", "Michael  Maire", "Shan  Lu"], "year": 2020, "n_citations": 7}
{"id": 2501952, "s2_id": "c4063ed21d743d4611d3471cea2f2ec830d4f6b7", "title": "M2M massive access in LTE: RACH performance evaluation in a Smart City scenario", "abstract": "Several studies assert that the random access procedure of the Long Term Evolution (LTE) cellular standard may not be effective whenever a massive number of simultaneous connection attempts are performed by terminals, as may happen in a typical Internet of Things or Smart City scenario. Nevertheless, simulation studies in real deployment scenarios are missing because many system-level simulators do not implement the LTE random access procedure in detail. In this paper, we propose a patch for the LTE module of ns-3, one of the most prominent open-source network simulators, to improve the accuracy of the routine that simulates the LTE Random Access Channel (RACH). The patched version of the random access procedure is compared with the default one and the issues arising from massive simultaneous access from mobile terminals in LTE are assessed via a simulation campaign.", "venue": "2016 IEEE International Conference on Communications (ICC)", "authors": ["Michele  Polese", "Marco  Centenaro", "Andrea  Zanella", "Michele  Zorzi"], "year": 2016, "n_citations": 35}
{"id": 2501961, "s2_id": "a12e12d15b1a1ba2354c79ae59f7de3a2bbfcd44", "title": "ALTO: adaptive linearized storage of sparse tensors", "abstract": "The analysis of high-dimensional sparse data is becoming increasingly popular in many important domains. However, real-world sparse tensors are challenging to process due to their irregular shapes and data distributions. We propose the Adaptive Linearized Tensor Order (ALTO) format, a novel mode-agnostic (general) representation that keeps neighboring nonzero elements in the multi-dimensional space close to each other in memory. To generate the indexing metadata, ALTO uses an adaptive bit encoding scheme that trades off index computations for lower memory usage and more effective use of memory bandwidth. Moreover, by decoupling its sparse representation from the irregular spatial distribution of nonzero elements, ALTO eliminates the workload imbalance and greatly reduces the synchronization overhead of tensor computations. As a result, the parallel performance of ALTO-based tensor operations becomes a function of their inherent data reuse. On a gamut of tensor datasets, ALTO outperforms an oracle that selects the best state-of-the-art format for each dataset, when used in key tensor decomposition operations. Specifically, ALTO achieves a geometric mean speedup of 8x over the best mode-agnostic (coordinate and hierarchical coordinate) formats, while delivering a geometric mean compression ratio of 4.x relative to the best mode-specific (compressed sparse fiber) formats.", "venue": "ICS", "authors": ["Ahmed E. Helal", "Jan  Laukemann", "Fabio  Checconi", "Jesmin Jahan Tithi", "Teresa  Ranadive", "Fabrizio  Petrini", "Jeewhan  Choi"], "year": 2021, "n_citations": 0}
{"id": 2504930, "s2_id": "32d37f7aae3e5f04d7fbd0f5e6bafe5f1b2ca214", "title": "Timing Aware Dummy Metal Fill Methodology", "abstract": "In this paper, we analyzed parasitic coupling capacitance coming from dummy metal fill and its impact on timing. Based on the modeling, we proposed two approaches to minimize the timing impact from dummy metal fill. The first approach applies more spacing between critical nets and metal fill, while the second approach leverages the shielding effects of reference nets. Experimental results show consistent improvement compared to traditional metal fill method.", "venue": "ArXiv", "authors": ["Luis  Charre", "Bruno  Gravano", "R\u00e9mi  P\u00f4ssas", "Chen  Zheng"], "year": 2017, "n_citations": 2}
{"id": 2507357, "s2_id": "fc8f9ff657cd7d257ce5285a40166a3b2ae26f57", "title": "Survey and Benchmarking of Machine Learning Accelerators", "abstract": "Advances in multicore processors and accelerators have opened the flood gates to greater exploration and application of machine learning techniques to a variety of applications. These advances, along with breakdowns of several trends including Moore\u2019s Law, have prompted an explosion of processors and accelerators that promise even greater computational and machine learning capabilities. These processors and accelerators are coming in many forms, from CPUs and GPUs to ASICs, FPGAs, and dataflow accelerators. This paper surveys the current state of these processors and accelerators that have been publicly announced with performance and power consumption numbers. The performance and power values are plotted on a scatter graph and a number of dimensions and observations from the trends on this plot are discussed and analyzed. For instance, there are interesting trends in the plot regarding power consumption, numerical precision, and inference versus training. We then select and benchmark two commercially available low size, weight, and power (SWaP) accelerators as these processors are the most interesting for embedded and mobile machine learning inference applications that are most applicable to the DoD and other SWaP constrained users. We determine how they actually perform with real-world images and neural network models, compare those results to the reported performance and power consumption values and evaluate them against an Intel CPU that is used in some embedded applications.", "venue": "2019 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Albert  Reuther", "Peter  Michaleas", "Michael  Jones", "Vijay  Gadepally", "Siddharth  Samsi", "Jeremy  Kepner"], "year": 2019, "n_citations": 70}
{"id": 2509380, "s2_id": "d9de0c5794bd3d11aaa040687dca3a9fd62a36a0", "title": "Enabling Cross-Event Optimization in Discrete-Event Simulation Through Compile-Time Event Batching", "abstract": "A discrete-event simulation (DES) involves the execution of a sequence of event handlers dynamically scheduled at runtime. As a consequence, a priori knowledge of the control flow of the overall simulation program is limited. In particular, powerful optimizations supported by modern compilers can only be applied on the scope of individual event handlers, which frequently involve only a few lines of code. We propose a method that extends the scope for compiler optimizations in discrete-event simulations by generating batches of multiple events that are subjected to compiler optimizations as contiguous procedures. A runtime mechanism executes suitable batches at negligible overhead. Our method does not require any compiler extensions and introduces only minor additional effort during model development. The feasibility and potential performance gains of the approach are illustrated on the example of an idealized proof-ofconcept model. We believe that the applicability of the approach extends to general event-driven programs.", "venue": "ArXiv", "authors": ["Marc  Leinweber", "Hannes  Hartenstein", "Philipp  Andelfinger"], "year": 2018, "n_citations": 0}
{"id": 2512019, "s2_id": "28a9738e84f7f5e17183dd82d18c38ae8544f8c6", "title": "A Scalable Framework for Quality Assessment of RDF Datasets", "abstract": "Over the last years, Linked Data has grown continuously. Today, we count more than 10,000 datasets being available online following Linked Data standards. These standards allow data to be machine readable and inter-operable. Nevertheless, many applications, such as data integration, search, and interlinking, cannot take full advantage of Linked Data if it is of low quality. There exist a few approaches for the quality assessment of Linked Data, but their performance degrades with the increase in data size and quickly grows beyond the capabilities of a single machine. In this paper, we present DistQualityAssessment \u2013 an open source implementation of quality assessment of large RDF datasets that can scale out to a cluster of machines. This is the first distributed, in-memory approach for computing different quality metrics for large RDF datasets using Apache Spark. We also provide a quality assessment pattern that can be used to generate new scalable metrics that can be applied to big data. The work presented here is integrated with the SANSA framework and has been applied to at least three use cases beyond the SANSA community. The results show that our approach is more generic, efficient, and scalable as compared to previously proposed approaches.", "venue": "SEMWEB", "authors": ["Gezim  Sejdiu", "Anisa  Rula", "Jens  Lehmann", "Hajira  Jabeen"], "year": 2019, "n_citations": 8}
{"id": 2512878, "s2_id": "2eca0851b8ab7991739ffe7f84b930e771b14d1b", "title": "Design, Generation, and Validation of Extreme Scale Power-Law Graphs", "abstract": "Massive power-law graphs drive many fields: metagenomics, brain mapping, Internet-of-things, cybersecurity, and sparse machine learning. The development of novel algorithms and systems to process these data requires the design, generation, and validation of enormous graphs with exactly known properties. Such graphs accelerate the proper testing of new algorithms and systems and are a prerequisite for success on real applications. Many random graph generators currently exist that require realizing a graph in order to know its exact properties: number of vertices, number of edges, degree distribution, and number of triangles. Designing graphs using these random graph generators is a time-consuming trial-and-error process. This paper presents a novel approach that uses Kronecker products to allow the exact computation of graph properties prior to graph generation. In addition, when a real graph is desired, it can be generated quickly in memory on a parallel computer with no-interprocessor communication. To test this approach, graphs with 1012 edges are generated on a 40,000+ core supercomputer in 1 second and exactly agree with those predicted by the theory. In addition, to demonstrate the extensibility of this approach, decetta-scale graphs with up to 10^30 edges are simulated in a few minutes on a laptop.", "venue": "2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Jeremy  Kepner", "Siddharth  Samsi", "William  Arcand", "David  Bestor", "Bill  Bergeron", "Tim  Davis", "Vijay  Gadepally", "Michael  Houle", "Matthew  Hubbell", "Hayden  Jananthan", "Michael  Jones", "Anna  Klein", "Peter  Michaleas", "Roger  Pearce", "Lauren  Milechin", "Julie  Mullen", "Andrew  Prout", "Antonio  Rosa", "Geoffrey  Sanders", "Charles  Yee", "Albert  Reuther"], "year": 2018, "n_citations": 14}
{"id": 2515349, "s2_id": "54948ddced0bb45548b9960b25a24dec4f6b3f07", "title": "On the Feasibility of Wireless Interconnects for High-throughput Data Centers", "abstract": "Data Centers (DCs) are required to be scalable to large data sets so as to accommodate ever increasing demands of resource-limited embedded and mobile devices. Thanks to the availability of recent high data rate millimeter-wave frequency spectrum such as 60GHz and due to the favorable attributes of this technology, wireless DC (WDC) exhibits the potentials of being a promising solution especially for small to medium scale DCs. This paper investigates the problem of throughput scalability of WDCs using the established theory of the asymptotic throughput of wireless multi-hop networks that are primarily proposed for homogeneous traffic conditions. The rate-heterogeneous traffic distribution of a data center however, requires the asymptotic heterogeneous throughput knowledge of a wireless network in order to study the performance and feasibility of WDCs for practical purposes. To answer these questions this paper presents a lower bound for the throughput scalability of a multi-hop rate-heterogeneous network when traffic generation rates of all nodes are similar, except one node. We demonstrate that the throughput scalability of conventional multi-hopping and the spatial reuse of the above bi-rate network is inefficient and henceforth develop a speculative 2-partitioning scheme that improves the network throughput scaling potentials. A better lower bound of the throughput is then obtained. Finally, we obtain the throughput scaling of an i.i.d. rate-heterogeneous network and obtain its lower bound. Again we propose a speculative 2-partitioning scheme to achieve a network with higher throughput in terms of improved lower bound. All of the obtained results have been verified using simulation experiments.", "venue": "ArXiv", "authors": ["Ahmad  Khonsari", "Seyed Pooya Shariatpanahi", "Abolfazl  Diyanat", "Hosein  Shafiei"], "year": 2015, "n_citations": 1}
{"id": 2516599, "s2_id": "575386a9833b6db337792e2bc65103839bcfaeeb", "title": "On the performance and energy efficiency of the PGAS programming model on multicore architectures", "abstract": "Using large-scale multicore systems to get the maximum performance and energy efficiency with manageable programmability is a major challenge. The partitioned global address space (PGAS) programming model enhances programmability by providing a global address space over large-scale computing systems. However, so far the performance and energy efficiency of the PGAS model on multicore-based parallel architectures have not been investigated thoroughly. In this paper we use a set of selected kernels from the well-known NAS Parallel Benchmarks to evaluate the performance and energy efficiency of the UPC programming language, which is a widely used implementation of the PGAS model. In addition, the MPI and OpenMP versions of the same parallel kernels are used for comparison with their UPC counterparts. The investigated hardware platforms are based on multicore CPUs, both within a single 16-core node and across multiple nodes involving up to 1024 physical cores. On the multi-node platform we used the hardware measurement solution called High definition Energy Efficiency Monitoring tool in order to measure energy. On the single-node system we used the hybrid measurement solution to make an effort into understanding the observed performance differences, we use the Intel Performance Counter Monitor to quantify in detail the communication time, cache hit/miss ratio and memory usage. Our experiments show that UPC is competitive with OpenMP and MPI on single and multiple nodes, with respect to both the performance and energy efficiency.", "venue": "2016 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["J\u00e9r\u00e9mie  Lagravi\u00e8re", "Johannes  Langguth", "Mohammed  Sourouri", "Phuong Hoai Ha", "Xing  Cai"], "year": 2016, "n_citations": 4}
{"id": 2518372, "s2_id": "14054e51a12d5eb721a2e4af86f3a8dc9bd25287", "title": "Verifiable Failure Localization in Smart Grid under Cyber-Physical Attacks", "abstract": "Cyber-physical attacks impose a significant threat to the smart grid, as the cyber attack makes it difficult to identify the actual damage caused by the physical attack. To defend against such attacks, various inference-based solutions have been proposed to estimate the states of grid elements (e.g., transmission lines) from measurements outside the attacked area, out of which a few have provided theoretical conditions for guaranteed accuracy. However, these conditions are usually based on the ground truth states and thus not verifiable in practice. To solve this problem, we develop (i) verifiable conditions that can be tested based on only observable information, and (ii) efficient algorithms for verifying the states of links (i.e., transmission lines) within the attacked area based on these conditions. Our numerical evaluations based on the Polish power grid and IEEE 300-bus system demonstrate that the proposed algorithms are highly successful in verifying the states of truly failed links, and can thus greatly help in prioritizing repairs during the recovery process.", "venue": "ArXiv", "authors": ["Yudi  Huang", "Ting  He", "Nilanjan Ray Chaudhuri", "Thomas La Porta"], "year": 2021, "n_citations": 0}
{"id": 2523286, "s2_id": "287eda7e01ed256d3e69da17c02aeefc9990c664", "title": "A hierarchical approach for dependability analysis of a commercial cache-based RAID storage architecture", "abstract": "We present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based RAID storage system. The architecture is complex and includes several layers of overlapping error detection and recovery mechanisms. Three abstraction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. The impact of faults and errors occurring in the cache and in the disks is analyzed at each level of the hierarchy. A simulation submodel is associated with each abstraction level. The models have been developed using DEPEND, a simulation-based environment for system-level dependability analysis, which provides facilities to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. Several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. Some of the parameters characterizing fault injection in a given submodel correspond to probabilities evaluated from the simulation of the lower-level submodel. Based on the proposed methodology, we evaluate and analyze: the system behavior under a real workload and high error rate (focusing on error bursts); the coverage of the error detection mechanisms implemented in the system and the error latency distributions; and the accumulation of errors in the cache and in the disks.", "venue": "Digest of Papers. Twenty-Eighth Annual International Symposium on Fault-Tolerant Computing (Cat. No.98CB36224)", "authors": ["Mohamed  Ka\u00e2niche", "Luigi  Romano", "Zbigniew T. Kalbarczyk", "Ravishankar K. Iyer", "Richard M. Karcich"], "year": 1998, "n_citations": 29}
{"id": 2523340, "s2_id": "bc1c5c2dac1a9209192d47e2fc792cdf6db1e95e", "title": "Performance Comparison for Scientific Computations on the Edge via Relative Performance", "abstract": "In a typical Internet-of-Things setting that involves scientific applications, a target computation can be evaluated in many different ways depending on the split of computations among various devices. On the one hand, different implementations (or algorithms)\u2014equivalent from a mathematical perspective\u2014might exhibit significant difference in terms of performance. On the other hand, some of the implementations are likely to show similar performance characteristics. In this paper, we focus on analysing the performance of a given set of algorithms by clustering them into performance classes. To this end, we use a measurement-based approach to evaluate and score algorithms based on pair-wise comparisons; we refer to this approach as \"Relative performance analysis\". Each comparison yields one of three outcomes: one algorithm can be \"better\", \"worse\", or \"equivalent\" to another; those algorithms evaluating to have \"equivalent\" performance are merged into the same performance class. We show that our clustering methodology facilitates algorithm selection with respect to more than one metric; for instance, from the subset of equivalently fast algorithms, one could then select an algorithm that consumes the least energy on a certain device.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Aravind  Sankaran", "Paolo  Bientinesi"], "year": 2021, "n_citations": 1}
{"id": 2524445, "s2_id": "101582899145d9fd2e17ff51f5b59a9f79c09ea0", "title": "Analysis of Implementation Hierocrypt-3 algorithm (and its comparison to Camellia algorithm) using ALTERA devices", "abstract": "Alghoritms: HIEROCRYPT-3, CAMELLIA and ANUBIS, GRAND CRU, NOEKEON, NUSH, Q, RC6, SAFER++128, SC2000, SHACAL were requested for the submission of block ciphers (high level block cipher) to NESSIE (New European Schemes for Signatures, Integrity, and Encryption) project. The main purpose of this project was to put forward a portfolio of strong cryptographic primitives of various types. The NESSIE project was a three year long project and has been divided into two phases. The first was finished in June 2001r. CAMELLIA, RC6, SAFER++128 and SHACAL were accepted for the second phase of the evaluation process. HIEROCRYPT-3 had key schedule problems, and there were attacks for up to 3,5 rounds out of 6, at least hardware implementations of this cipher were extremely slow [12]. HIEROCRYPT-3 was not selected to Phase II. CAMELLIA was selected as an algorithm suggested for future standard. In the paper we present the hardware implementations these two algorithms with 128-bit blocks and 128-bit keys, using ALTERA devices and their comparisons.", "venue": "IACR Cryptol. ePrint Arch.", "authors": ["Marcin  Rogawski"], "year": 2003, "n_citations": 4}
{"id": 2525304, "s2_id": "641813cb94e9087387fdd520f26e3391a1dc52c7", "title": "Magnetohydrodynamics on Heterogeneous architectures: a performance comparison", "abstract": "We present magneto-hydrodynamic simulation results for heterogeneous systems. Heterogeneous architectures combine high floating point performance many-core units hosted in conventional server nodes. Examples include Graphics Processing Units (GPU's) and Cell. They have potentially large gains in performance, at modest power and monetary cost. We implemented a magneto-hydrodynamic (MHD) simulation code on a variety of heterogeneous and multi-core architectures --- multi-core x86, Cell, Nvidia and ATI GPU --- in different languages, FORTRAN, C, Cell, CUDA and OpenCL. We present initial performance results for these systems. To our knowledge, this is the widest comparison of heterogeneous systems for MHD simulations. We review the different challenges faced in each architecture, and potential bottlenecks. We conclude that substantial gains in performance over traditional systems are possible, and in particular that is possible to extract a greater percentage of peak theoretical performance from some systems when compared to x86 architectures.", "venue": "ArXiv", "authors": ["Bijia  Pang", "Ue-li  Pen", "Michael  Perrone"], "year": 2010, "n_citations": 9}
{"id": 2526887, "s2_id": "f033fb9906a9d1b246041d2c233a4d3a84909a36", "title": "Red-blue pebbling revisited: near optimal parallel matrix-matrix multiplication", "abstract": "We propose COSMA: a parallel matrix-matrix multiplication algorithm that is near communication-optimal for all combinations of matrix dimensions, processor counts, and memory sizes. The key idea behind COSMA is to derive an optimal (up to a factor of 0.03% for 10MB of fast memory) sequential schedule and then parallelize it, preserving I/O optimality. To achieve this, we use the red-blue pebble game to precisely model MMM dependencies and derive a constructive and tight sequential and parallel I/O lower bound proofs. Compared to 2D or 3D algorithms, which fix processor decomposition upfront and then map it to the matrix dimensions, it reduces communication volume by up to \u221a times. COSMA outperforms the established ScaLAPACK, CARMA, and CTF algorithms in all scenarios up to 12.8x (2.2x on average), achieving up to 88% of Piz Daint's peak performance. Our work does not require any hand tuning and is maintained as an open source implementation.", "venue": "SC", "authors": ["Grzegorz  Kwasniewski", "Marko  Kabic", "Maciej  Besta", "Joost  VandeVondele", "Raffaele  Solc\u00e0", "Torsten  Hoefler"], "year": 2019, "n_citations": 39}
{"id": 2529311, "s2_id": "7c7d6e2d4b949211c1d1072bad785359cc03f524", "title": "Learning Algorithms for Minimizing Queue Length Regret", "abstract": "We consider a system consisting of a single transmitter and $N$ channels. Packets randomly arrive to the transmitter's queue, and at each time slot a controller can schedule one of the $N$ channels for transmission. The channel's rates are time-varying with unknown statistics and must be learned through observation. Our objective is to minimize the number of packets in the system's queue over $T$ time slots. We define the regret of the system to be the expected difference between the total queue length of a controller that must learn the channels' average rates and a controller that knows the rates, a priori. One approach to solving this problem would be to apply algorithms from the literature that were developed to solve the closely-related stochastic multi-armed bandit problem. However, we show that these methods have $\\Omega(\\log(T))$ queue length regret. On the other hand, we show that there exists a set of queue-length based policies that are able to obtain order optimal, $O(1)$, regret.", "venue": "2018 IEEE International Symposium on Information Theory (ISIT)", "authors": ["Thomas  Stahlbuhk", "Brooke  Shrader", "Eytan  Modiano"], "year": 2018, "n_citations": 12}
{"id": 2530128, "s2_id": "e7641540f1d0801a2dceed8b6b59f7e74808d942", "title": "DeepPicar: A Low-Cost Deep Neural Network-Based Autonomous Car", "abstract": "We present DeepPicar, a low-cost deep neural network based autonomous car platform. DeepPicar is a small scale replication of a real self-driving car called DAVE-2 by NVIDIA. DAVE-2 uses a deep convolutional neural network (CNN), which takes images from a front-facing camera as input and produces car steering angles as output. DeepPicar uses the same network architecture\u20149 layers, 27 million connections and 250K parameters\u2014and can drive itself in real-time using a web camera and a Raspberry Pi 3 quad-core platform. Using DeepPicar, we analyze the Pi 3's computing capabilities to support end-to-end deep learning based real-time control of autonomous vehicles. We also systematically compare other contemporary embedded computing platforms using the DeepPicar's CNN-based real-time control workload. We find that all tested platforms, including the Pi 3, are capable of supporting the CNN-based real-time control, from 20 Hz up to 100 Hz, depending on hardware platform. However, we find that shared resource contention remains an important issue that must be considered in applying CNN models on shared memory based embedded computing platforms; we observe up to 11.6X execution time increase in the CNN based control loop due to shared resource contention. To protect the CNN workload, we also evaluate state-of-the-art cache partitioning and memory bandwidth throttling techniques on the Pi 3. We find that cache partitioning is ineffective, while memory bandwidth throttling is an effective solution.", "venue": "2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)", "authors": ["Michael Garrett Bechtel", "Elise  McEllhiney", "Heechul  Yun"], "year": 2018, "n_citations": 62}
{"id": 2538038, "s2_id": "29867cca7fe07d769f397d1bd08d8b1bb75adf7d", "title": "Optimum Transmission Window for EPONs with Gated-Limited Service", "abstract": "This paper studies the Ethernet Passive Optical Network (EPON) with gated-limited service. The transmission window (TW) is limited in this system to guaranteeing a bounded delay experienced by disciplined users, and to constrain malicious users from monopolizing the transmission channel. Thus, selecting an appropriate TW size is critical to the performance of EPON with gated-limited service discipline. To investigate the impact of TW size on packet delay, we derive a generalized mean waiting time formula for M/G/1 queue with vacation times and gated-limited service discipline. A distinguished feature of this model is that there are two queues in the buffer of each optical network unit (ONU): one queue is inside the gate and the other one is outside the gate. Furthermore, based on the Chernoff bound of queue length, we provide a simple rule to determine an optimum TW size for gated-limited service EPONs. Analytic results reported in this paper are all verified by simulations.", "venue": "ArXiv", "authors": ["Huanhuan  Huang", "Tong  Ye", "Tony T. Lee", "Weisheng  Hu"], "year": 2017, "n_citations": 0}
{"id": 2543617, "s2_id": "5acd1a9b7a84e3f38a15bd5ea512806af5364a7e", "title": "Towards Ubiquitous Indoor Positioning: Comparing Systems across Heterogeneous Datasets", "abstract": "The evaluation of Indoor Positioning Systems (IPSs) mostly relies on local deployments in the researchers\u2019 or partners\u2019 facilities. The complexity of preparing comprehensive experiments, collecting data, and considering multiple scenarios usually limits the evaluation area and, therefore, the assessment of the proposed systems. The requirements and features of controlled experiments cannot be generalized since the use of the same sensors or anchors density cannot be guaranteed. The dawn of datasets is pushing IPS evaluation to a similar level as machinelearning models, where new proposals are evaluated over many heterogeneous datasets. This paper proposes a way to evaluate IPSs in multiple scenarios, that is validated with three use cases. The results prove that the proposed aggregation of the evaluation metric values is a useful tool for high-level comparison of IPSs.", "venue": "ArXiv", "authors": ["Joaqu'in  Torres-Sospedra", "Ivo  Silva", "Lucie  Klus", "Darwin  Quezada-Gaibor", "Antonino  Crivello", "Paolo  Barsocchi", "Cristiano  Pendao", "Elena Simona Lohan", "Jari  Nurmi", "Adriano  Moreira"], "year": 2021, "n_citations": 0}
{"id": 2544108, "s2_id": "8893cc95b32eb3716a7c51807088bbdea5bb3e7f", "title": "Performance Evaluation of Java File Security System (JFSS)", "abstract": "Security is a critical issue of the modern file and storage systems, it is imperative to protect the stored data from unauthorized access. We have developed a file security system named as Java File Security System (JFSS) [1] that guarantee the security to files on the demand of all users. It has been developed on Java platform. Java has been used as programming language in order to provide portability, but it enforces some performance limitations. It is developed in FUSE (File System in User space) [3]. Many efforts have been done over the years for developing file systems in user space (FUSE). All have their own merits and demerits. In this paper we have evaluated the performance of Java File Security System (JFSS). Over and over again, the increased security comes at the expense of user convenience, performance or compatibility with other systems. JFSS system performance evaluations show that encryption overheads are modest as compared to security.", "venue": "ArXiv", "authors": ["Brijender  Kahanwal", "Tejinder Pal Singh", "R. K. Tuteja"], "year": 2013, "n_citations": 6}
{"id": 2544374, "s2_id": "11f2a5d947f5899d6060009462feb6888a07fb1c", "title": "Efficient multicore-aware parallelization strategies for iterative stencil computations", "abstract": "Abstract Stencil computations consume a major part of runtime in many scientific simulation codes. As prototypes for this class of algorithms we consider the iterative Jacobi and Gauss-Seidel smoothers and aim at highly efficient parallel implementations for cache-based multicore architectures. Temporal cache blocking is a known advanced optimization technique, which can reduce the pressure on the memory bus significantly. We apply and refine this optimization for a recently presented temporal blocking strategy designed to explicitly utilize multicore characteristics. Especially for the case of Gauss-Seidel smoothers we show that simultaneous multi-threading (SMT) can yield substantial performance improvements for our optimized algorithm on some architectures.", "venue": "J. Comput. Sci.", "authors": ["Jan  Treibig", "Gerhard  Wellein", "Georg  Hager"], "year": 2011, "n_citations": 59}
{"id": 2545821, "s2_id": "20c791dd95a48d1e529e4750270472c869a47466", "title": "Performance Analysis of Multicast Mobility in a Hierarchical Mobile IP Proxy Environment", "abstract": "Mobility support in IPv6 networks is ready for release as an RFC, stimulating major discussions on improvements to meet real-time communication requirements. Sprawling hot spots of IP-only wireless networks at the same time await voice and videoconferencing as standard mobile Internet services, thereby adding the request for multicast support to real-time mobility. This paper briefly introduces current approaches for seamless multicast extensions to Mobile IPv6. Key issues of multicast mobility are discussed. Both analytically and in simulations comparisons are drawn between handover performance characteristics, dedicating special focus on the M-HMIPv6 approach.", "venue": "TERENA Networking Conference", "authors": ["Thomas C. Schmidt", "Matthias  W\u00e4hlisch"], "year": 2004, "n_citations": 16}
{"id": 2547465, "s2_id": "77bd5f629c4e965c3017e19eabd55a0f8d9bab77", "title": "Dependability Analysis of Control Systems using SystemC and Statistical Model Checking", "abstract": "Stochastic Petri nets are commonly used for modeling distributed systems in order to study their performance and dependability. This paper proposes a realization of stochastic Petri nets in SystemC for modeling large embedded control systems. Then statistical model checking is used to analyze the dependability of the constructed model. Our verification framework allows users to express a wide range of useful properties to be verified which is illustrated through a case study.", "venue": "ArXiv", "authors": ["Van Chan Ngo", "Axel  Legay", "Jean  Quilbeuf"], "year": 2015, "n_citations": 1}
{"id": 2548747, "s2_id": "994b9ecc1cfacba7a84996e9f16bfeb9ccec3ee0", "title": "Performance analysis of low latency multiple full-duplex selective decode and forward relays", "abstract": "In order to follow up with mission-critical applications, new features need to be carried to satisfy a reliable communication with reduced latency. With this regard, this paper proposes a low latency cooperative transmission scheme, where multiple full-duplex relays, simultaneously, assist the communication between a source node and a destination node. First, we present the communication model of the proposed transmission scheme. Then, we derive the outage probability closed-form for two cases: asynchronous transmission (where all relays have different processing delay) and synchronous transmissions (where all relays have the same processing delay). Finally, using simulations, we confirm the theoretical results and compare the proposed multi-relays transmission scheme with relay selection schemes.", "venue": "2018 IEEE Wireless Communications and Networking Conference (WCNC)", "authors": ["Fatima Ezzahra Airod", "Houda  Chafnaji", "Halim  Yanikomeroglu"], "year": 2018, "n_citations": 3}
{"id": 2550819, "s2_id": "e286e659adb69c3e96b780d93efd93bfe64587f8", "title": "Optimal Threshold Policies for Robust Data Center Control", "abstract": "With the simultaneous rise of energy costs and demand for cloud computing, efficient control of data centers becomes crucial. In the data center control problem, one needs to plan at every time step how many servers to switch on or off in order to meet stochastic job arrivals while trying to minimize electricity consumption. This problem becomes particularly challenging when servers can be of various types and jobs from different classes can only be served by certain types of server, as it is often the case in real data centers. We model this problem as a robust Markov Decision Process (i.e., the transition function is not assumed to be known precisely). We give sufficient conditions (which seem to be reasonable and satisfied in practice) guaranteeing that an optimal threshold policy exists. This property can then be exploited in the design of an efficient solving method, which we provide. Finally, we present some experimental results demonstrating the practicability of our approach and compare with a previous related approach based on model predictive control.", "venue": "ArXiv", "authors": ["Paul  Weng", "Zeqi  Qiu", "John A. W. B. Costanzo", "Xiaoqi  Yin", "Bruno  Sinopoli"], "year": 2017, "n_citations": 3}
{"id": 2554442, "s2_id": "7e601556ee5a539dccff0b4ea71347e1ef8ebf29", "title": "Hierarchical Roofline Analysis: How to Collect Data using Performance Tools on Intel CPUs and NVIDIA GPUs", "abstract": "This paper surveys a range of methods to collect necessary performance data on Intel CPUs and NVIDIA GPUs for hierarchical Roofline analysis. As of mid-2020, two vendor performance tools, Intel Advisor and NVIDIA Nsight Compute, have integrated Roofline analysis into their supported feature set. This paper fills the gap for when these tools are not available, or when users would like a more customized workflow for certain analysis. Specifically, we will discuss how to use Intel Advisor, RRZE LIKWID, Intel SDE and Intel Amplifier on Intel architectures, and nvprof, Nsight Compute metrics, and Nsight Compute section files on NVIDIA architectures. These tools will be used to collect information for as many memory/cache levels in the memory hierarchy as possible in order to provide insights into application's data reuse and cache locality characteristics.", "venue": "ArXiv", "authors": ["Charlene  Yang"], "year": 2020, "n_citations": 7}
{"id": 2556543, "s2_id": "217d1b66d45adfe73c9324a6751890d651e99dd5", "title": "Multiplying Matrices Without Multiplying", "abstract": "Multiplying matrices is among the most fundamental and compute-intensive operations in machine learning. Consequently, there has been significant work on efficiently approximating matrix multiplies. We introduce a learning-based algorithm for this task that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs 100\u00d7 faster than exact matrix products and 10\u00d7 faster than current approximate methods. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires zero multiply-adds. These results suggest that a mixture of hashing, averaging, and byte shuffling\u2014\u2013the core operations of our method\u2014\u2013could be a more promising building block for machine learning than the sparsified, factorized, and/or scalar quantized matrix products that have recently been the focus of substantial research and hardware investment.", "venue": "ICML", "authors": ["Davis  Blalock", "John  Guttag"], "year": 2021, "n_citations": 1}
{"id": 2557913, "s2_id": "66e55a8f78cc90323327d8d3cfac0f4566697ede", "title": "Next Stop \"NoOps\": Enabling Cross-System Diagnostics Through Graph-Based Composition of Logs and Metrics", "abstract": "Performing diagnostics in IT systems is an increasingly complicated task, and it is not doable in satisfactory time by even the most skillful operators. Systems and their architecture change very rapidly in response to business and user demand. Many organizations see value in the maintenance and management model of NoOps that stands for No Operations. One of the implementations of this model is a system that is maintained automatically without any human intervention. The path to NoOps involves not only precise and fast diagnostics but also reusing as much knowledge as possible after the system is reconfigured or changed. The biggest challenge is to leverage knowledge on one IT system and reuse this knowledge for diagnostics of another, different system. We propose a framework of weighted graphs which can transfer knowledge, and perform high-quality diagnostics of IT systems. We encode all possible data in a graph representation of a system state and automatically calculate weights of these graphs. Then, thanks to the evaluation of similarity between graphs, we transfer knowledge about failures from one system to another and use it for diagnostics. We successfully evaluate the proposed approach on Spark, Hadoop, Kafka and Cassandra systems.", "venue": "2018 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Michal  Zasadzinski", "Marc  Sol\u00e9", "\u00c1lvaro  Brand\u00f3n", "Victor  Munt\u00e9s-Mulero", "David  Carrera"], "year": 2018, "n_citations": 2}
{"id": 2560443, "s2_id": "5b5aecdb7e097a5226c46c7f87fd50d77c220cdf", "title": "Analysis of M2/M2/1/R, N Queuing Model for Multimedia over 3.5G Wireless Network Downlink", "abstract": "Analysis of an M2/M2/1/R, N queuing model for the multimedia transmission over HSDPA/3.5G downlink is presented. The queue models the downlink buffer with source multimedia traffic streams comprising two classes of flows: realtime and non real-time. Time priority is accorded to the real-time flows while the non real-time flows are given buffer space priority. An analytic evaluation of the impact of varying the buffer partition threshold on the QoS performance of both classes of customers is undertaken. The results are validated with a discrete event simulation model developed in C language. Finally, a cost function for the joint optimization of the traffic QoS parameters is derived.", "venue": "ArXiv", "authors": ["Suleiman Y. Yerima", "Khalid  Al-Begain"], "year": 2016, "n_citations": 0}
{"id": 2562572, "s2_id": "0ae8839e481ece71a8dc15fd07372c672524c19c", "title": "Faster remainder by direct computation: Applications to compilers and software libraries", "abstract": "On common processors, integer multiplication is many times faster than integer division. Dividing a numerator n by a divisor d is mathematically equivalent to multiplication by the inverse of the divisor (n/d=n\u22171/d). If the divisor is known in advance, or if repeated integer divisions will be performed with the same divisor, it can be beneficial to substitute a less costly multiplication for an expensive division. Currently, the remainder of the division by a constant is computed from the quotient by a multiplication and a subtraction. However, if just the remainder is desired and the quotient is unneeded, this may be suboptimal. We present a generally applicable algorithm to compute the remainder more directly. Specifically, we use the fractional portion of the product of the numerator and the inverse of the divisor. On this basis, we also present a new and simpler divisibility algorithm to detect nonzero remainders. We also derive new tight bounds on the precision required when representing the inverse of the divisor. Furthermore, we present simple C implementations that beat the optimized code produced by state\u2010of\u2010the\u2010art C compilers on recent x64 processors (eg, Intel Skylake and AMD Ryzen), sometimes by more than 25%. On all tested platforms, including 64\u2010bit ARM and POWER8, our divisibility test functions are faster than state\u2010of\u2010the\u2010art Granlund\u2010Montgomery divisibility test functions, sometimes by more than 50%.", "venue": "Softw. Pract. Exp.", "authors": ["Daniel  Lemire", "Owen  Kaser", "Nathan  Kurz"], "year": 2019, "n_citations": 8}
{"id": 2564845, "s2_id": "4960279b1f1cd225915c064ee6650abc89d0ce09", "title": "Data Center Server Provision: Distributed Asynchronous Control for Coupled Renewal Systems", "abstract": "This paper considers a cost minimization problem for data centers with  $N$  servers and randomly arriving service requests. A central router decides which server to use for each new request. Each server has three types of states (active, idle, and setup) with different costs and time durations. The servers operate asynchronously over their own states and can choose one of multiple sleep modes when idle. We develop an online distributed control algorithm so that each server makes its own decisions. The request queues are bounded and the overall time average cost is near optimal with probability 1. First the algorithm does not need probability information for the arrival rate or job sizes. Finally, an improved algorithm that uses a single queue is developed via a \u201cvirtualization\u201d technique, which is shown to provide the same (near optimal) costs. Simulation experiments on a real data center traffic trace demonstrate the efficiency of our algorithm compared with other existing algorithms.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Xiaohan  Wei", "Michael J. Neely"], "year": 2017, "n_citations": 12}
{"id": 2565683, "s2_id": "343b48fd02c7ccd6b346cfacdc606eedb35af91d", "title": "Throughput-Optimal Scheduling in Multihop Wireless Networks Without Per-Flow Information", "abstract": "In this paper, we consider the problem of link scheduling in multihop wireless networks under general interference constraints. Our goal is to design scheduling schemes that do not use per-flow or per-destination information, maintain a single data queue for each link, and exploit only local information, while guaranteeing throughput optimality. Although the celebrated back-pressure algorithm maximizes throughput, it requires per-flow or per-destination information. It is usually difficult to obtain and maintain this type of information, especially in large networks, where there are numerous flows. Also, the back-pressure algorithm maintains a complex data structure at each node, keeps exchanging queue-length information among neighboring nodes, and commonly results in poor delay performance. In this paper, we propose scheduling schemes that can circumvent these drawbacks and guarantee throughput optimality. These schemes use either the readily available hop-count information or only the local information for each link. We rigorously analyze the performance of the proposed schemes using fluid limit techniques via an inductive argument and show that they are throughput-optimal. We also conduct simulations to validate our theoretical results in various settings and show that the proposed schemes can substantially improve the delay performance in most scenarios.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Bo  Ji", "Changhee  Joo", "Ness B. Shroff"], "year": 2013, "n_citations": 42}
{"id": 2567053, "s2_id": "b1d1339900c50360be14919cfb34eed1b52f1fdf", "title": "Semi-dynamic load balancing: efficient distributed learning in non-dedicated environments", "abstract": "Machine learning (ML) models are increasingly trained in clusters with non-dedicated workers possessing heterogeneous resources. In such scenarios, model training efficiency can be negatively affected by stragglers---workers that run much slower than others. Efficient model training requires eliminating such stragglers, yet for modern ML workloads, existing load balancing strategies are inefficient and even infeasible. In this paper, we propose a novel strategy called semi-dynamic load balancing to eliminate stragglers of distributed ML workloads. The key insight is that ML workers shall be load-balanced at iteration boundaries, being non-intrusive to intra-iteration execution. We develop LB-BSP based on such an insight, which is an integrated worker coordination mechanism that adapts workers' load to their instantaneous processing capabilities by right-sizing the sample batches at the synchronization barriers. We have custom-designed the batch sizing algorithm respectively for CPU and GPU clusters based on their own characteristics. LB-BSP has been implemented as a Python module for ML frameworks like TensorFlow and PyTorch. Our EC2 deployment confirms that LB-BSP is practical, effective and light-weight, and is able to accelerating distributed training by up to 54%.", "venue": "SoCC", "authors": ["Chen  Chen", "Qizhen  Weng", "Wei  Wang", "Baochun  Li", "Bo  Li"], "year": 2020, "n_citations": 2}
{"id": 2568355, "s2_id": "9ae00264e1f17f5d2068f5a3293e7633c4f06c8e", "title": "Ultra-Fast Accurate AoA Estimation via Automotive Massive-MIMO Radar", "abstract": "Massive multiple-input multiple-output (MIMO) radar, enabled by millimeter-wave virtual MIMO techniques, provides great promises to the high-resolution automotive sensing and target detection in unmanned ground/aerial vehicles (UGA/UAV). As a long-established problem, however, existing subspace methods suffer from either high complexity or low accuracy. In this work, we propose two efficient methods, to accomplish fast subspace computation and accurate angle of arrival (AoA) acquisition. By leveraging randomized low-rank approximation, our fast multiple signal classification (MUSIC) methods, relying on random sampling and projection techniques, substantially accelerate the subspace estimation by orders of magnitude. Moreover, we establish the theoretical bounds of our proposed methods, which ensure the accuracy of the approximated pseudo-spectrum. As demonstrated, the pseudo-spectrum acquired by our fast-MUSIC would be highly precise; and the estimated AoA is almost as accurate as standard MUSIC. In contrast, our new methods are tremendously faster than standard MUSIC. Thus, our fast-MUSIC enables the high-resolution realtime environmental sensing with massive MIMO radars, which has great potential in the emerging unmanned systems.", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Bin  Li", "Shusen  Wang", "Jun  Zhang", "Xianbin  Cao", "Chenglin  Zhao"], "year": 2021, "n_citations": 0}
{"id": 2571026, "s2_id": "67459f72b29e54d1e0a88119cd1e1af124a7c6f7", "title": "Similarity Analysis in Automatic Performance Debugging of SPMD Parallel Programs", "abstract": "Different from sequential programs, parallel programs possess their own characteristics which are difficult to analyze in the multi-process or multi-thread environment. This paper presents an innovative method to automatically analyze the SPMD programs. Firstly, with the help of clustering method focusing on similarity analysis, an algorithm is designed to locate performance problems in parallel programs automatically. Secondly a Rough Set method is used to uncover the performance problem and provide the insight into the micro-level causes. Lastly, we have analyzed a production parallel application to verify the effectiveness of our method and system.", "venue": "ArXiv", "authors": ["Xu  Liu", "Jianfeng  Zhan", "Bibo  Tu", "Ming  Zou", "Dan  Meng"], "year": 2009, "n_citations": 2}
{"id": 2571558, "s2_id": "bf38345ec6928662953684fdec075a9412bd0c88", "title": "Persistent Stochastic Non-Interference", "abstract": "In this paper we present an information flow security property for stochastic, cooperating, processes expressed as terms of the Performance Evaluation Process Algebra (PEPA). We introduce the notion of Persistent Stochastic Non-Interference (PSNI) based on the idea that every state reachable by a process satisfies a basic Stochastic Non-Interference (SNI) property. The structural operational semantics of PEPA allows us to give two characterizations of PSNI: the first involves a single bisimulation-like equivalence check, while the second is formulated in terms of unwinding conditions. The observation equivalence at the base of our definition relies on the notion of lumpability and ensures that, for a secure process P, the steady state probability of observing the system being in a specific state P' is independent from its possible high level interactions.", "venue": "EXPRESS/SOS", "authors": ["Jane  Hillston", "Carla  Piazza", "Sabina  Rossi"], "year": 2018, "n_citations": 1}
{"id": 2572625, "s2_id": "70cfb98fbc1886f9c01b1b76eca800423198a1c1", "title": "Tuning Streamed Applications on Intel Xeon Phi: A Machine Learning Based Approach", "abstract": "Many-core accelerators, as represented by the XeonPhi coprocessors and GPGPUs, allow software to exploit spatial and temporal sharing of computing resources to improve the overall system performance. To unlock this performance potential requires software to effectively partition the hardware resource to maximize the overlap between hostdevice communication and accelerator computation, and to match the granularity of task parallelism to the resource partition. However, determining the right resource partition and task parallelism on a per program, per dataset basis is challenging. This is because the number of possible solutions is huge, and the benefit of choosing the right solution may be large, but mistakes can seriously hurt the performance. In this paper, we present an automatic approach to determine the hardware resource partition and the task granularity for any given application, targeting the Intel XeonPhi architecture. Instead of hand-crafting the heuristic for which the process will have to repeat for each hardware generation, we employ machine learning techniques to automatically learn it. We achieve this by first learning a predictive model offline using training programs; we then use the learned model to predict the resource partition and task granularity for any unseen programs at runtime. We apply our approach to 23 representative parallel applications and evaluate it on a CPU-XeonPhi mixed heterogenous many-core platform. Our approach achieves, on average, a 1.6x (upto 5.6x) speedup, which translates to 94.5% of the performance delivered by a theoretically perfect predictor.", "venue": "ArXiv", "authors": ["Peng  Zhang", "Jianbin  Fang", "Tao  Tang", "Canqun  Yang", "Zheng  Wang"], "year": 2018, "n_citations": 2}
{"id": 2573016, "s2_id": "f0d0dd52c77d93821da41ad2ab2af1d63a65f0e1", "title": "Link Enhancer for Vehicular Wireless ATM Communications", "abstract": "Majority of the applications used in defense are voice, video and data oriented and has strict QoS requirements. One of the technologies that enabled this is Asynchronous Transfer Mode (ATM) networking. Traditional ATM networks are wired networks. But Tactical networks are meant to be mobile and this necessitates the use of radio relays for Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. ATM networks assume a physical link layer BER of 10^-9 or better because of the availability of reliable media like optical fiber links. But this assumption is no longer valid when ATM switches are connected through radio relay where error rates are in the rage of 10^-3. This paper presents the architecture of a Link Enhancer meant to improve the Bit Error Rate of the Wireless links used for V2I and V2V communications from 1 in 10^4 to 1 in 10^8", "venue": "ArXiv", "authors": ["P.  ArunKumarS.", "Diganta  Baishya", "Amrendra  Kumar"], "year": 2008, "n_citations": 0}
{"id": 2575804, "s2_id": "681f8e2231dc2a03c8f9a1d99d96ba9ecb5d5a35", "title": "Measuring Bandwidth for Super Computer Workloads", "abstract": "Parallel computing plays a major role in almost all the fields from research to major concern problem solving purposes. Many researches are till now focusing towards the area of parallel processing. Nowadays it extends its usage towards the end user application such as GPU as well as multi-core processor development. The bandwidth measurement is essential for resource management and for studying the various performance factors of the existing super computer systems which will be helpful for better system utilization since super computers are very few and their resources should be properly utilized. In this paper the real workload trace of one of the super computers LANL is taken and shown how the bandwidth is estimated with the given parameters.", "venue": "ArXiv", "authors": ["A. Neela Madheswari", "R. S. D. Wahida Banu"], "year": 2010, "n_citations": 0}
{"id": 2576497, "s2_id": "0a1fd7d7d409654be073818e46964394870b072f", "title": "Dark Memory and Accelerator-Rich System Optimization in the Dark Silicon Era", "abstract": "Unlike traditional dark silicon works that attack the computing logic, this article puts a focus on the memory part, which dissipates most of the energy for memory-bound CPU applications. This article discusses the dark memory state and present Pareto curves for compute units, accelerators, and on-chip memory, and motivates the need for HW/SW codesign for parallelism and locality. \u2013Muhammad Shafique, Vienna University of Technology", "venue": "IEEE Design & Test", "authors": ["Ardavan  Pedram", "Stephen  Richardson", "Mark  Horowitz", "Sameh  Galal", "Shahar  Kvatinsky"], "year": 2017, "n_citations": 83}
{"id": 2578118, "s2_id": "6bf307a462d6e07e9089566b9d4d282b4a884e94", "title": "Conditional Waiting Time Analysis in Tandem Polling Queues", "abstract": "We analyze a tandem network of polling queues with two product types and two stations. We assume that external arrivals to the network follow a Poisson process, and service times at each station are exponentially distributed. For this system, we determine the mean conditional waiting time for an arriving customer using a sample path analysis approach. The approach classifies system state upon arrival into scenarios and exploits an inherent structure in the sequence of events that occur till the customer departs to obtain conditional waiting time estimates. We conduct numerical studies to show both the accuracy of our conditional waiting time estimates and their practical importance.", "venue": "ArXiv", "authors": ["Ravi  Suman", "Ananth  Krishnamurthy"], "year": 2021, "n_citations": 0}
{"id": 2580499, "s2_id": "70494c3ba1f4c5aaddbd36c9cf7d537573a0f75f", "title": "Forward Correction and Fountain codes in Delay Tolerant Networks", "abstract": "Delay tolerant ad-hoc networks leverage the mobility of relay nodes to compensate for lack of permanent connectivity and thus enable communication between nodes that are out of range of each other. To decrease delivery delay, the information to be delivered is replicated in the network. Our objective in this paper is to study a class of replication mechanisms that include coding in order to improve the probability of successful delivery within a given time limit. We propose an analytical approach that allows to quantify tradeoffs between resources and performance measures (energy and delay). We study the effect of coding on the performance of the network while optimizing parameters that govern routing. Our results, based on fluid approximations, are compared to simulations which validate the model.", "venue": "INFOCOM", "authors": ["Eitan  Altman", "Francesco De Pellegrini"], "year": 2009, "n_citations": 5}
{"id": 2586122, "s2_id": "b58103bb3627b510299d017f487948ac3ecd7c36", "title": "Scaling through abstractions - high-performance vectorial wave simulations for seismic inversion with Devito", "abstract": "[Devito] is an open-source Python project based on domain-specific language and compiler technology. Driven by the requirements of rapid HPC applications development in exploration seismology, the language and compiler have evolved significantly since inception. Sophisticated boundary conditions, tensor contractions, sparse operations and features such as staggered grids and sub-domains are all supported; operators of essentially arbitrary complexity can be generated. To accommodate this flexibility whilst ensuring performance, data dependency analysis is utilized to schedule loops and detect computational-properties such as parallelism. In this article, the generation and simulation of MPI-parallel propagators (along with their adjoints) for the pseudo-acoustic wave-equation in tilted transverse isotropic media and the elastic wave-equation are presented. Simulations are carried out on industry scale synthetic models in a HPC Cloud system and reach a performance of 28TFLOP/s, hence demonstrating Devito's suitability for production-grade seismic inversion problems.", "venue": "ArXiv", "authors": ["Mathias  Louboutin", "Fabio  Luporini", "Philipp  Witte", "Rhodri  Nelson", "George  Bisbas", "Jan  Thorbecke", "Felix J. Herrmann", "Gerard  Gorman"], "year": 2020, "n_citations": 0}
{"id": 2586565, "s2_id": "bacae8783338bac62578340df07d3855b0201d53", "title": "Stability analysis of two-class retrial systems with constant retrial rates and general service times", "abstract": "We establish stability criterion for a two-class retrial system with Poisson inputs, general classdependent service times and class-dependent constant retrial rates. We also characterise an interesting phenomenon of partial stability when one orbit is tight but the other orbit goes to infinity in probability. All theoretical results are illustrated by numerical experiments.", "venue": "ArXiv", "authors": ["Konstantin  Avrachenkov", "Evsey  Morozov", "Ruslana  Nekrasova"], "year": 2021, "n_citations": 0}
{"id": 2589449, "s2_id": "5724099944551ff5a90a55b4b1c9d7cb6ff3aff4", "title": "Enhancing neural-network performance via assortativity", "abstract": "The performance of attractor neural networks has been shown to depend crucially on the heterogeneity of the underlying topology. We take this analysis a step further by examining the effect of degree-degree correlations--assortativity--on neural-network behavior. We make use of a method recently put forward for studying correlated networks and dynamics thereon, both analytically and computationally, which is independent of how the topology may have evolved. We show how the robustness to noise is greatly enhanced in assortative (positively correlated) neural networks, especially if it is the hub neurons that store the information.", "venue": "Physical review. E, Statistical, nonlinear, and soft matter physics", "authors": ["Sebastiano de Franciscis", "Samuel  Johnson", "Joaqu\u00edn J. Torres"], "year": 2011, "n_citations": 34}
{"id": 2589501, "s2_id": "585ecad418debcdc2a8942214fc2be22c26f3a30", "title": "A Survey of Embedded Software Profiling Methodologies", "abstract": "Embedded Systems combine one or more processor cores with dedicated logic running on an ASIC or FPGA to meet design goals at reasonable cost. It is achieved by profiling the application with variety of aspects like performance, memory usage, cache hit versus cache miss, energy consumption, etc. Out of these, performance estimation is more important than others. With ever increasing system complexities, it becomes quite necessary to carry out performance estimation of embedded software implemented in a particular processor for fast design space exploration. Such profiled data also guides the designer how to partition the system for Hardware (HW) and Software (SW) environments. In this paper, we propose a classification for currently available Embedded Software Profiling Tools, and we present different academic and industrial approaches in this context. Based on these observations, it will be easy to identify such common principles and needs which are required for a true Software Profiling Tool for a particular application.", "venue": "ArXiv", "authors": ["Rajendra  Patel", "Arvind  Rajawat"], "year": 2013, "n_citations": 18}
{"id": 2590165, "s2_id": "44634bc8c82153e04126f38915b91991a6a43a6a", "title": "An upper bound on the convergence time for distributed binary consensus", "abstract": "The problem addressed in this paper is the analysis of a distributed consensus algorithm for arbitrary networks, proposed by Be\u0301ne\u0301zit et al. In the initial setting, each node in the network has one of two possible states (\u201cyes\u201d or \u201cno\u201d). Nodes can update their states by communicating with their neighbors via a 2-bit message in an asynchronous clock setting. Eventually, all nodes reach consensus on the majority states. We use the theory of electric networks, random walks, and couplings of Markov chains to derive an O(N4 log N) upper bound for the expected convergence time on an arbitrary graph of size N.", "venue": "2012 15th International Conference on Information Fusion", "authors": ["Shang  Shang", "Paul W. Cuff", "Sanjeev R. Kulkarni", "Pan  Hui"], "year": 2012, "n_citations": 12}
{"id": 2593175, "s2_id": "190470955a17be6bce8edad49e02e5c55c3e4165", "title": "On-device neural speech synthesis", "abstract": "Recent advances in text-to-speech (TTS) synthesis, such as Tacotron and WaveRNN, have made it possible to construct a fully neural network based TTS system, by coupling the two components together. Such a system is conceptually simple as it only takes grapheme or phoneme input, uses Melspectrogram as an intermediate feature, and directly generates speech samples. The system achieves quality equal or close to natural speech. However, the high computational cost of the system and issues with robustness have limited their usage in real-world speech synthesis applications and products. In this paper, we present key modeling improvements and optimization strategies that enable deploying these models, not only on GPU servers, but also on mobile devices. The proposed system can generate high-quality 24 kHz speech at 5x faster than real time on server and 3x faster than real time on mobile devices.", "venue": "ArXiv", "authors": ["Sivanand  Achanta", "Albert  Antony", "Ladan  Golipour", "Jiangchuan  Li", "Tuomo  Raitio", "Ramya  Rasipuram", "Francesco  Rossi", "Jennifer  Shi", "Jaimin  Upadhyay", "David  Winarsky", "Hepeng  Zhang"], "year": 2021, "n_citations": 2}
{"id": 2595930, "s2_id": "b4141b178540d32a60c48976338c02a26b8fe424", "title": "Computationally Efficient Modulation Level Classification Based on Probability Distribution Distance Functions", "abstract": "We present a novel modulation level classification (MLC) method based on probability distribution distance functions. The proposed method uses modified Kuiper and Kolmogorov-Smirnov distances to achieve low computational complexity and outperforms the state of the art methods based on cumulants and goodness-of-fit tests. We derive the theoretical performance of the proposed MLC method and verify it via simulations. The best classification accuracy, under AWGN with SNR mismatch and phase jitter, is achieved with the proposed MLC method using Kuiper distances.", "venue": "IEEE Communications Letters", "authors": ["Paulo  Urriza", "Eric  Rebeiz", "Przemyslaw  Pawelczak", "Danijela  Cabric"], "year": 2011, "n_citations": 43}
{"id": 2596704, "s2_id": "e4a52d605709ae052505ae3c9cfcbbb8e8e0c0c8", "title": "There is no fast lunch: an examination of the running speed of evolutionary algorithms in several languages", "abstract": "It is quite usual when an evolutionary algorithm tool or library uses a language other than C, C++, Java or Matlab that a reviewer or the audience questions its usefulness based on the speed of those other languages, purportedly slower than the aforementioned ones. Despite speed being not everything needed to design a useful evolutionary algorithm application, in this paper we will measure the speed for several very basic evolutionary algorithm operations in several languages which use different virtual machines and approaches, and prove that, in fact, there is no big difference in speed between interpreted and compiled languages, and that in some cases, interpreted languages such as JavaScript or Python can be faster than compiled languages such as Scala, making them worthy of use for evolutionary algorithm experimentation.", "venue": "ArXiv", "authors": ["Juan Juli\u00e1n Merelo Guerv\u00f3s", "Pablo  Garc\u00eda-S\u00e1nchez", "Mario Garc\u00eda Valdez", "Israel  Blancas"], "year": 2015, "n_citations": 5}
{"id": 2598772, "s2_id": "5a141a2d5857099b29f421ef9ddd4176c430f2e6", "title": "Global finite element matrix construction based on a CPU-GPU implementation", "abstract": "The finite element method (FEM) has several computational steps to numerically solve a particular problem, to which many efforts have been directed to accelerate the solution stage of the linear system of equations. However, the finite element matrix construction, which is also time-consuming for unstructured meshes, has been less investigated. The generation of the global finite element matrix is performed in two steps, computing the local matrices by numerical integration and assembling them into a global system, which has traditionally been done in serial computing. This work presents a fast technique to construct the global finite element matrix that arises by solving the Poisson's equation in a three-dimensional domain. The proposed methodology consists in computing the numerical integration, due to its intrinsic parallel opportunities, in the graphics processing unit (GPU) and computing the matrix assembly, due to its intrinsic serial operations, in the central processing unit (CPU). In the numerical integration, only the lower triangular part of each local stiffness matrix is computed thanks to its symmetry, which saves GPU memory and computing time. As a result of symmetry, the global sparse matrix also contains non-zero elements only in its lower triangular part, which reduces the assembly operations and memory usage. This methodology allows generating the global sparse matrix from any unstructured finite element mesh size on GPUs with little memory capacity, only limited by the CPU memory.", "venue": "ArXiv", "authors": ["Francisco Javier Ram\u00edrez-Gil", "Marcos de Sales Guerra Tsuzuki", "Wilfredo  Montealegre-Rubio"], "year": 2015, "n_citations": 3}
{"id": 2599691, "s2_id": "357d52620e0a76eed7b441bd37db4747b12a3d40", "title": "Load balancing with heterogeneous schedulers", "abstract": "Load balancing is a common approach in web server farms or inventory routing problems. An important issue in such systems is to determine the server to which an incoming request should be routed to optimize a given performance criteria. In this paper, we assume the server's scheduling disciplines to be heterogeneous. More precisely, a server implements a scheduling discipline which belongs to the class of limited processor sharing (LPS-$d$) scheduling disciplines. Under LPS-$d$, up to $d$ jobs can be served simultaneously, and hence, includes as special cases First Come First Served ($d=1$) and Processor Sharing ($d=\\infty$). \nIn order to obtain efficient heuristics, we model the above load-balancing framework as a multi-armed restless bandit problem. Using the relaxation technique, as first developed in the seminal work of Whittle, we derive Whittle's index policy for general cost functions and obtain a closed-form expression for Whittle's index in terms of the steady-state distribution. Through numerical computations, we investigate the performance of Whittle's index with two different performance criteria: linear cost criterion and a cost criterion that depends on the first and second moment of the throughput. Our results show that \\emph{(i)} the structure of Whittle's index policy can strongly depend on the scheduling discipline implemented in the server, i.e., on $d$, and that \\emph{(ii)} Whittle's index policy significantly outperforms standard dispatching rules such as Join the Shortest Queue (JSQ), Join the Shortest Expected Workload (JSEW), and Random Server allocation (RSA).", "venue": "ArXiv", "authors": ["Urtzi  Ayesta", "Manu K. Gupta", "Maaike  Verloop"], "year": 2018, "n_citations": 0}
{"id": 2599720, "s2_id": "3a33cbd0bde84f9745a245a2f761a31c1a310200", "title": "Stability, memory, and messaging tradeoffs in heterogeneous service systems", "abstract": "We consider a heterogeneous distributed service system consisting of n servers with unknown and possibly different processing rates. Jobs with unit mean arrive as a renewal process of rate proportional to n and are immediately dispatched to one of several queues associated with the servers. We assume that the dispatching decisions are made by a central dispatcher with the ability to exchange messages with the servers and endowed with a finite memory used to store information from one decision epoch to the next, about the current state of the queues and about the service rates of the servers. We study the fundamental resource requirements (memory bits and message exchange rate) in order for a dispatching policy to be always stable. First, we present a policy that is always stable while using a positive (but arbitrarily small) message rate and [Formula: see text] bits of memory. Second, we show that within a certain broad class of policies, a dispatching policy that exchanges [Formula: see text] messages per unit of time, and with [Formula: see text] bits of memory, cannot be always stable.", "venue": "Mathematics of Operations Research", "authors": ["David  Gamarnik", "John N. Tsitsiklis", "Martin  Zubeldia"], "year": 2021, "n_citations": 0}
{"id": 2600526, "s2_id": "2c88fb685864fe18644937aec0c67da56c9bdda8", "title": "Performance Analysis of Priority-Aware NoCs with Deflection Routing under Traffic Congestion", "abstract": "Priority-aware networks-on-chip (NoCs) are used in industry to achieve predictable latency under different workload conditions. These NoCs incorporate deflection routing to minimize queuing resources within routers and achieve low latency during low traffic load. However, deflected packets can exacerbate congestion during high traffic load since they consume the NoC bandwidth. State-of-the-art analytical models for priority-aware NoCs ignore deflected traffic despite its significant latency impact during congestion. This paper proposes a novel analytical approach to estimate end-to-end latency of priority-aware NoCs with deflection routing under bursty and heavy traffic scenarios. Experimental evaluations show that the proposed technique outperforms alternative approaches and estimates the average latency for real applications with less than 8% error compared to cycle-accurate simulations.", "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Sumit K. Mandal", "Anish  Krishnakumar", "Raid  Ayoub", "Michael  Kishinevsky", "\u00dcmit Y. Ogras"], "year": 2020, "n_citations": 1}
{"id": 2602312, "s2_id": "c51923fe740ab15f54a4670f766088ca2cf0961c", "title": "An Approach for Realistically Simulating the Performance of Scientific Applications on High Performance Computing Systems", "abstract": "Scientific applications often contain large, computationally-intensive, and irregular parallel loops or tasks that exhibit stochastic characteristics. Applications may suffer from load imbalance during their execution on high-performance computing (HPC) systems due to such characteristics. Dynamic loop self-scheduling (DLS) techniques are instrumental in improving the performance of scientific applications on HPC systems via load balancing. Selecting a DLS technique that results in the best performance for different problems and system sizes requires a large number of exploratory experiments. A theoretical model that can be used to predict the scheduling technique that yields the best performance for a given problem and system has not yet been identified. Therefore, simulation is the most appropriate approach for conducting such exploratory experiments with reasonable costs. This work devises an approach to realistically simulate computationally-intensive scientific applications that employ DLS and execute on HPC systems. Several approaches to represent the application tasks (or loop iterations) are compared to establish their influence on the simulative application performance. A novel simulation strategy is introduced, which transforms a native application code into a simulative code. The native and simulative performance of two computationally-intensive scientific applications are compared to evaluate the realism of the proposed simulation approach. The comparison of the performance characteristics extracted from the native and simulative performance shows that the proposed simulation approach fully captured most of the performance characteristics of interest. This work shows and establishes the importance of simulations that realistically predict the performance of DLS techniques for different applications and system configurations.", "venue": "Future Gener. Comput. Syst.", "authors": ["Ali  Mohammed", "Ahmed  Eleliemy", "Florina M. Ciorba", "Franziska  Kasielke", "Ioana  Banicescu"], "year": 2020, "n_citations": 5}
{"id": 2603022, "s2_id": "f26ce27037d95a566cb9c8cac8b1dde327cdba03", "title": "Two-level Dynamic Load Balancing for High Performance Scientific Applications", "abstract": "Scientific applications are often complex, irregular, and computationally-intensive. To accommodate the ever-increasing computational demands of scientific applications, high-performance computing (HPC) systems have become larger and more complex, offering parallelism at multiple levels (e.g., nodes, cores per node, threads per core). Scientific applications need to exploit all the available multilevel hardware parallelism to harness the available computational power. The performance of applications executing on such HPC systems may adversely be affected by load imbalance at multiple levels, caused by problem, algorithmic, and systemic characteristics. Nevertheless, most existing load balancing methods do not simultaneously address load imbalance at multiple levels. This work investigates the impact of load imbalance on the performance of three scientific applications at the thread and process levels. We jointly apply and evaluate selected dynamic loop self-scheduling (DLS) techniques to both levels. Specifically, we employ the extended LaPeSD OpenMP runtime library at the thread level and extend the DLS4LB MPI-based dynamic load balancing library at the process level. This approach is generic and applicable to any multiprocess-multithreaded computationally-intensive application (programmed using MPI and OpenMP). We conduct an exhaustive set of experiments to assess and compare six DLS techniques at the thread level and eleven at the process level. The results show that improved application performance, by up to 21%, can only be achieved by jointly addressing load imbalance at the two levels. We offer insights into the performance of the selected DLS techniques and discuss the interplay of load balancing at the thread level and process level.", "venue": "PPSC", "authors": ["Ali  Mohammed", "Aur\u00e9lien  Cavelan", "Florina M. Ciorba", "Rub\u00e9n M. Cabezon", "Ioana  Banicescu"], "year": 2020, "n_citations": 2}
{"id": 2604276, "s2_id": "0ed8c01c0053ca439f09ca28ac0654d736596595", "title": "Robust and Scalable Entity Alignment in Big Data", "abstract": "Entity alignment has always had significant uses within a multitude of diverse scientific fields. In particular, the concept of matching entities across networks has grown in significance in the world of social science as communicative networks such as social media have expanded in scale and popularity. With the advent of big data, there is a growing need to provide analysis on graphs of massive scale. However, with millions of nodes and billions of edges, the idea of alignment between a myriad of graphs of similar scale using features extracted from potentially sparse or incomplete datasets becomes daunting. In this paper we will propose a solution to the issue of large-scale alignments in the form of a multi-step pipeline. Within this pipeline we introduce scalable feature extraction for robust temporal attributes, accompanied by novel and efficient clustering algorithms in order to find groupings of similar nodes across graphs. The features and their clusters are fed into a versatile alignment stage that accurately identifies partner nodes among millions of possible matches. Our results show that the pipeline can process large data sets, achieving efficient runtimes within the memory constraints.", "venue": "2020 IEEE International Conference on Big Data (Big Data)", "authors": ["James  Flamino", "Christopher  Abriola", "Ben  Zimmerman", "Zhongheng  Li", "Joel  Douglas"], "year": 2020, "n_citations": 0}
{"id": 2611952, "s2_id": "32a6c63f3e1805c83d08525978e25c3949cf2eca", "title": "BIRL: Benchmark on Image Registration methods with Landmark validation", "abstract": "This report presents a generic image registration benchmark with automatic evaluation using landmark annotations. The key features of the BIRL framework are: easily extendable, performance evaluation, parallel experimentation, simple visualisations, experiment's time-out limit, resuming unfinished experiments. From the research practice, we identified and focused on these two main use-cases: (a) comparison of user's (newly developed) method with some State-of-the-Art (SOTA) methods on a common dataset and (b) experimenting SOTA methods on user's custom dataset (which should contain landmark annotation). Moreover, we present an integration of several standard image registration methods aiming at biomedical imaging into the BIRL framework. This report also contains experimental results of these SOTA methods on the CIMA dataset, which is a dataset of Whole Slice Imaging (WSI) from histology/pathology containing several multi-stain tissue samples from three tissue kinds. Source and results: this https URL", "venue": "ArXiv", "authors": ["Jiri  Borovec"], "year": 2019, "n_citations": 5}
{"id": 2615417, "s2_id": "071e2d29fdb816f5bfc95bcadc13c81838af0bd5", "title": "CodeNet: Training Large Scale Neural Networks in Presence of Soft-Errors", "abstract": "This work proposes the first strategy to make distributed training of neural networks resilient to computing errors, a problem that has remained unsolved despite being first posed in 1956 by von Neumann. He also speculated that the efficiency and reliability of the human brain is obtained by allowing for low power but error-prone components with redundancy for error-resilience. It is surprising that this problem remains open, even as massive artificial neural networks are being trained on increasingly low-cost and unreliable processing units. Our coding-theory-inspired strategy, \"CodeNet,\" solves this problem by addressing three challenges in the science of reliable computing: (i) Providing the first strategy for error-resilient neural network training by encoding each layer separately; (ii) Keeping the overheads of coding (encoding/error-detection/decoding) low by obviating the need to re-encode the updated parameter matrices after each iteration from scratch. (iii) Providing a completely decentralized implementation with no central node (which is a single point of failure), allowing all primary computational steps to be error-prone. We theoretically demonstrate that CodeNet has higher error tolerance than replication, which we leverage to speed up computation time. Simultaneously, CodeNet requires lower redundancy than replication, and equal computational and communication costs in scaling sense. We first demonstrate the benefits of CodeNet in reducing expected computation time over replication when accounting for checkpointing. Our experiments show that CodeNet achieves the best accuracy-runtime tradeoff compared to both replication and uncoded strategies. CodeNet is a significant step towards biologically plausible neural network training, that could hold the key to orders of magnitude efficiency improvements.", "venue": "ArXiv", "authors": ["Sanghamitra  Dutta", "Ziqian  Bai", "Tze Meng Low", "Pulkit  Grover"], "year": 2019, "n_citations": 11}
{"id": 2617337, "s2_id": "f5c6d6c791b62af6f5007fce8ced92b0091cf0cf", "title": "A Model of WiFi Performance With Bounded Latency", "abstract": "In September 2020, the Broadband Forum published a new industry standard for measuring network quality. The standard centers on the notion of quality attenuation. Quality attenuation is a measure of the distribution of latency and packet loss between two points connected by a network path. A vital feature of the quality attenuation idea is that we can express detailed application requirements and network performance measurements in the same mathematical framework. Performance requirements and measurements are both modeled as latency distributions. To the best of our knowledge, existing models of the 802.11 WiFi protocol do not permit the calculation of complete latency distributions without assuming steady-state operation. We present a novel model of theWiFi protocol. Instead of computing throughput numbers from a steady-state analysis of a Markov chain, we explicitly model latency and packet loss. Explicitly modeling latency and loss allows for both transient and steady-state analysis of latency distributions, and we can derive throughput numbers from the latency results. Our model is, therefore, more general than the standard Markov chain methods. We reproduce several known results with this method. Using transient analysis, we derive bounds on WiFi throughput under the requirement that latency and packet loss must be bounded.", "venue": "ArXiv", "authors": ["Bjorn Ivar Teigen", "Neil  Davies", "Kai Olav Ellefsen", "Tor  Skeie", "Jim  Torresen"], "year": 2021, "n_citations": 0}
{"id": 2617879, "s2_id": "72e112f14f4e4d02b9b929ba89587eae5fbecfeb", "title": "SOAP: One Clean Analysis of All Age-Based Scheduling Policies", "abstract": "We consider an extremely broad class of M/G/1 scheduling policies called SOAP: Schedule Ordered by Age-based Priority. The SOAP policies include almost all scheduling policies in the literature as well as an infinite number of variants which have never been analyzed, or maybe not even conceived. SOAP policies range from classic policies, like first-come, first-serve (FCFS), foreground-background (FB), class-based priority, and shortest remaining processing time (SRPT); to much more complicated scheduling rules, such as the famously complex Gittins index policy and other policies in which a job's priority changes arbitrarily with its age. While the response time of policies in the former category is well understood, policies in the latter category have resisted response time analysis. We present a universal analysis of all SOAP policies, deriving the mean and Laplace-Stieltjes transform of response time.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Ziv  Scully", "Mor  Harchol-Balter", "Alan  Scheller-Wolf"], "year": 2018, "n_citations": 23}
{"id": 2619121, "s2_id": "59f4316b8ba63fcce9e1c6d3f0d8bcc89cfc30ad", "title": "Not Half Bad: Exploring Half-Precision in Graph Convolutional Neural Networks", "abstract": "With the growing significance of graphs as an effective representation of data in numerous applications, efficient graph analysis using modern machine learning is receiving a growing level of attention. Deep learning approaches often operate over the entire adjacency matrix \u2013 as the input and intermediate network layers are all designed in proportion to the size of the adjacency matrix \u2013 leading to intensive computation and large memory requirements as the graph size increases. It is therefore desirable to identify efficient measures to reduce both run-time and memory requirements allowing for the analysis of the largest graphs possible. The use of reduced precision operations within the forward and backward passes of a deep neural network along with novel specialised hardware in modern GPUs could offer promising avenues towards efficiency. In this paper, we provide an in-depth exploration of the use of reduced-precision operations, easily integrable into the highly popular PyTorch framework, and an analysis of the effects of Tensor Cores on graph convolutional neural networks. We perform an extensive experimental evaluation of three GPU architectures and two widely-used graph analysis tasks (vertex classification and link prediction) using well-known benchmark and synthetically generated datasets. Thus allowing us to make important observations on the effects of reduced-precision operations and Tensor Cores on computational and memory usage of graph convolutional neural networks \u2013 often neglected in the literature.", "venue": "2020 IEEE International Conference on Big Data (Big Data)", "authors": ["John  Brennan", "Stephen  Bonner", "Amir Atapour Abarghouei", "Philip T. G. Jackson", "Boguslaw  Obara", "A. Stephen McGough"], "year": 2020, "n_citations": 0}
{"id": 2619163, "s2_id": "e0ae69241ee62a760502b917c91bd57c1d300ca1", "title": "Hierarchical Performance Modeling for Ranking Dense Linear Algebra Algorithms", "abstract": "A large class of dense linear algebra operations, such as LU decomposition or inversion of a triangular matrix, are usually performed by blocked algorithms. For one such operation, typically, not only one but many algorithmic variants exist; depending on computing architecture, libraries and problem size, each variant attains a different performances. We propose methods and tools to rank the algorithmic variants according to their performance for a given scenario without executing them. \nFor this purpose, we identify the routines upon which the algorithms are built. A first tool - the Sampler - measures the performance of these routines. Using the Sampler, a second tool models their performance. The generated models are then used to predict the performance of the considered algorithms. For a given scenario, these predictions allow us to correctly rank the algorithms according to their performance without executing them. With the help of the same tools, algorithmic parameters such as block-size can be optimally tuned.", "venue": "ArXiv", "authors": ["Elmar  Peise"], "year": 2012, "n_citations": 2}
{"id": 2621361, "s2_id": "0a5ef892e280a637e9c53214a35cf3b1efa1dd51", "title": "Quality of service improvement for high-speed railway communications", "abstract": "With the fast development of highspeed railways, a call for fulfilling the notion of communication at \u201canytime, anywhere\u201d for high-speed train passengers in the Train Operating Control System is on the way. In order to make a realization of that, new railway wireless communication networks are needed. The most promising one is the Long Term Evolution for Railway which will provide broadband access, fast handover, and reliable communication for high mobility users. However, with the increase of speed, the system is subjected to high bit error rate, Doppler frequency shift and handover failure just like other system does. This paper is trying to solve these problems by employing MIMO technique. Specifically, the goal is to provide higher data rate, higher reliability, less delay, and other relative quality of services for passengers. MIMO performance analysis, resource allocation, and access control for handover and various services in a two-hop model are proposed in this paper. Analytical results and simulation results show that the proposed model and schemes perform well in improving the system performances.", "venue": "China Communications", "authors": ["Zhou  Yuzhe", "Ai  Bo"], "year": 2014, "n_citations": 12}
{"id": 2621599, "s2_id": "66529e2d6ffcfa813b58b2b9006e075f20bb31d6", "title": "A Note on Parallel Algorithmic Speedup Bounds", "abstract": "A parallel program can be represented as a directed acyclic graph. An important performance bound is the time to execute the critical path through the graph. We show how this performance metric is related to Amdahl speedup and the degree of average parallelism. These bounds formally exclude superlinear performance.", "venue": "ArXiv", "authors": ["Neil J. Gunther"], "year": 2011, "n_citations": 2}
{"id": 2621666, "s2_id": "a27614bb4e9eddc6f5c6c342f174b4ea613c6281", "title": "Efficient Realization of Householder Transform Through Algorithm-Architecture Co-Design for Acceleration of QR Factorization", "abstract": "QR factorization is a ubiquitous operation in many engineering and scientific applications. In this paper, we present efficient realization of Householder Transform (HT) based QR factorization through algorithm-architecture co-design where we achieve performance improvement of 3-90x in-terms of Gflops/watt over state-of-the-art multicore, General Purpose Graphics Processing Units (GPGPUs), Field Programmable Gate Arrays (FPGAs), and ClearSpeed CSX700. Theoretical and experimental analysis of classical HT is performed for opportunities to exhibit higher degree of parallelism where parallelism is quantified as a number of parallel operations per level in the Directed Acyclic Graph (DAG) of the transform. Based on theoretical analysis of classical HT, an opportunity to re-arrange computations in the classical HT is identified that results in Modified HT (MHT) where it is shown that MHT exhibits 1.33x times higher parallelism than classical HT. Experiments in off-the-shelf multicore and General Purpose Graphics Processing Units (GPGPUs) for HT and MHT suggest that MHT is capable of achieving slightly better or equal performance compared to classical HT based QR factorization realizations in the optimized software packages for Dense Linear Algebra (DLA). We implement MHT on a customized platform for Dense Linear Algebra (DLA) and show that MHT achieves 1.3x better performance than native implementation of classical HT on the same accelerator. For custom realization of HT and MHT based QR factorization, we also identify macro operations in the DAGs of HT and MHT that are realized on a Reconfigurable Data-path (RDP). We also observe that due to re-arrangement in the computations in MHT, custom realization of MHT is capable of achieving 12 percent better performance improvement over multicore and GPGPUs than the performance improvement reported by General Matrix Multiplication (GEMM) over highly tuned DLA software packages for multicore and GPGPUs which is counter-intuitive.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Farhad  Merchant", "Tarun  Vatwani", "Anupam  Chattopadhyay", "Soumyendu  Raha", "S. K. Nandy", "Ranjani  Narayan"], "year": 2018, "n_citations": 6}
{"id": 2621980, "s2_id": "3179ffe783db31d299b56d85e9d3193e86eca12e", "title": "Optimal Scheduling for Maximizing Information Freshness & System Performance in Industrial Cyber-Physical Systems", "abstract": "Age of Information is a newly introduced metric, getting vivid attention for measuring the freshness of information in real-time networks. This parameter has evolved to guarantee the reception of timely information from the latest status update, received by a user from any real-time application. In this paper, we study a centralized, closed-loop, networked controlled industrial wireless sensor-actuator network for cyber-physical production systems. Here, we jointly address the problem of transmission scheduling of sensor updates and the restoration of an information flow-line after any real-time update having hard-deadline drops from it, resulting a break in the loop. Unlike existing real-time scheduling policies that only ensure timely updates, this work aims to accomplish both the time-sensitivity and data freshness in new and regenerative real-time updates in terms of the age of information. Here, the coexistence of both cyber and physical units and their individual requirements for providing the quality of service to the system, as a whole, seems to be one of the major challenges to handle. In this work, minimization of staleness of the time-critical updates to extract maximum utilization out of its information content and its effects on other network performances are thoroughly investigated. A greedy scheduling policy called Deadline-aware highest latency first has been used to solve this problem; its performance optimality is proved analytically. Finally, our claim is validated by comparing the results obtained by our algorithm with those of other popular scheduling policies through extensive simulations.", "venue": "Comput. Commun.", "authors": ["Devarpita  Sinha", "Rajarshi  Roy"], "year": 2021, "n_citations": 2}
{"id": 2632781, "s2_id": "fce31057d3778f8784d1ff241fa90ce767bf0429", "title": "Toward efficient interactions between Python and native libraries", "abstract": "Python has become a popular programming language because of its excellent programmability. Many modern software packages utilize Python for high-level algorithm design and depend on native libraries written in C/C++/Fortran for efficient computation kernels. Interaction between Python code and native libraries introduces performance losses because of the abstraction lying on the boundary of Python and native libraries. On the one side, Python code, typically run with interpretation, is disjoint from its execution behavior. On the other side, native libraries do not include program semantics to understand algorithm defects. To understand the interaction inefficiencies, we extensively study a large collection of Python software packages and categorize them according to the root causes of inefficiencies. We extract two inefficiency patterns that are common in interaction inefficiencies. Based on these patterns, we develop PieProf, a lightweight profiler, to pinpoint interaction inefficiencies in Python applications. The principle of PieProf is to measure the inefficiencies in the native execution and associate inefficiencies with high-level Python code to provide a holistic view. Guided by PieProf, we optimize 17 real-world applications, yielding speedups up to 6.3\u00d7 on application level.", "venue": "ESEC/SIGSOFT FSE", "authors": ["Jialiang  Tan", "Yu  Chen", "Zhenming  Liu", "Bin  Ren", "Shuaiwen Leon Song", "Xipeng  Shen", "Xu  Liu"], "year": 2021, "n_citations": 0}
{"id": 2635027, "s2_id": "d634afae9c2025d25d8cfb0be46f59af936a3ad6", "title": "Dissecting demand response mechanisms: the role of consumption forecasts and personalized offers", "abstract": "Demand-Response (DR) programs, whereby users of an electricity network are encouraged by economic incentives to re-arrange their consumption in order to reduce production costs, are envisioned to be a key feature of the smart grid paradigm. Several recent works proposed DR mechanisms and used analytical models to derive optimal incentives. Most of these works, however, rely on a macroscopic description of the population that does not model individual choices of users. In this paper, we conduct a detailed analysis of those models and we argue that the macroscopic descriptions hide important assumptions that can jeopardize the mechanisms' implementation (such as the ability to make personalized offers and to perfectly estimate the demand that is moved from a timeslot to another). Then, we start from a microscopic description that explicitly models each user's decision. We introduce four DR mechanisms with various assumptions on the provider's capabilities. Contrarily to previous studies, we find that the optimization problems that result from our mechanisms are complex and can be solved numerically only through a heuristic. We present numerical simulations that compare the different mechanisms and their sensitivity to forecast errors. At a high level, our results show that the performance of DR mechanisms under reasonable assumptions on the provider's capabilities are significantly lower than those suggested by previous studies, but that the gap reduces when the population's flexibility increases.", "venue": "2016 American Control Conference (ACC)", "authors": ["Alberto  Benegiamo", "Patrick  Loiseau", "Giovanni  Neglia"], "year": 2016, "n_citations": 2}
{"id": 2639192, "s2_id": "bc96a07663c3b7b62274ffb6949b2499edbf61c0", "title": "Load Balancing Guardrails: Keeping Your Heavy Traffic on the Road to Low Response Times", "abstract": "Load balancing systems, comprising a central dispatcher and a scheduling policy at each server, are widely used in practice, and their response time has been extensively studied in the theoretical literature. While much is known about the scenario where the scheduling at the servers is First-Come-First-Served (FCFS), to minimize mean response time we must use Shortest-Remaining- Processing-Time (SRPT) scheduling at the servers. Much less is known about dispatching polices when SRPT scheduling is used. Unfortunately, traditional dispatching policies that are used in practice in systems with FCFS servers often have poor performance in systems with SRPT servers. In this paper, we devise a simple fix that can be applied to any dispatching policy. This fix, called guardrails, ensures that the dispatching policy yields optimal mean response time under heavy traffic when used in a system with SRPT servers. Any dispatching policy, when augmented with guardrails, becomes heavy-traffic optimal. Our results yield the first analytical bounds on mean response time for load balancing systems with SRPT scheduling at the servers.", "venue": "PERV", "authors": ["Isaac  Grosof", "Ziv  Scully", "Mor  Harchol-Balter"], "year": 2019, "n_citations": 1}
{"id": 2640695, "s2_id": "c43c6b52ab7b363f4619ca8862770c3acc6469e3", "title": "Performance Modeling of BitTorrent Peer-to-Peer File Sharing Networks", "abstract": "BitTorrent is undoubtedly the most popular P2P file sharing application on today's Internet. The widespread popularity of BitTorrent has attracted a great deal of attention from networking researchers who conducted various performance studies on it. This paper presents a comprehensive survey of analytical performance modeling techniques for BitTorrent networks. The performance models examined in this study include deterministic models, Markov chain models, fluid flow models, and queuing network models. These models evaluate the performance metrics of BitTorrent networks at different regimes with various realistic factors considered. Furthermore, a comparative analysis is conducted on those modeling techniques in the aspects of complexity, accuracy, extensibility, and scalability.", "venue": "ArXiv", "authors": ["Kunjie  Xu"], "year": 2013, "n_citations": 7}
{"id": 2641964, "s2_id": "a6502b695f5d2f2de8653369f5cfc40625594413", "title": "Wanted: Floating-Point Add Round-off Error instruction", "abstract": "We propose a new instruction (FPADDRE) that computes the round-o error in oating-point addition. We explain how this instruction benets high-precision arithmetic operations in applications where double precision is not sucient. Performance estimates on Intel Haswell, Intel Skylake, and AMD Steamroller processors, as well as Intel Knights Corner co-processor, demonstrate that such an instruction would improve the latency of double-double addition by up to 55% and increase double-double addition throughput by up to 103%, with smaller, but non-negligible benets for doubledouble multiplication. The new instruction delivers up to 2 speedups on three benchmarks that use high-precision oating-point arithmetic: double-double matrix-matrix multiplication, compensated dot product, and polynomial evaluation via the compensated Horner scheme.", "venue": "ArXiv", "authors": ["Marat  Dukhan", "Richard W. Vuduc", "E. Jason Riedy"], "year": 2016, "n_citations": 2}
{"id": 2643608, "s2_id": "c1ca5b91de68ccc1d9414098532ce07ebaa49dab", "title": "A Closer Look at Lightweight Graph Reordering", "abstract": "Graph analytics power a range of applications in areas as diverse as finance, networking and business logistics. A common property of graphs used in the domain of graph analytics is a power-law distribution of vertex connectivity, wherein a small number of vertices are responsible for a high fraction of all connections in the graph. These richly-connected (hot) vertices inherently exhibit high reuse. However, their sparse distribution in memory leads to a severe underutilization of on-chip cache capacity. Prior works have proposed lightweight skew-aware vertex reordering that places hot vertices adjacent to each other in memory, reducing the cache footprint of hot vertices and thus improving cache efficiency. However, in doing so, they may inadvertently destroy the inherent community structure within the graph, which may negate the performance gains achieved from the reduced footprint of hot vertices. In this work, we study existing reordering techniques and demonstrate the inherent tension between reducing the cache footprint of hot vertices and preserving original graph structure. We quantify the potential performance loss due to disruption in graph structure for different graph datasets. We further show that reordering techniques that employ fine-grain reordering significantly increase misses in the higher level caches, even when they reduce misses in the last level cache. To overcome the limitations of existing reordering techniques, we propose Degree-Based Grouping (DBG), a novel lightweight reordering technique that employs a coarse-grain reordering to largely preserve graph structure while reducing the cache footprint of hot vertices. Our evaluation on 40 combinations of various graph applications and datasets shows that, compared to a baseline with no reordering, DBG yields an average application speed-up of 16.8% vs 11.6% for the best-performing existing lightweight technique.", "venue": "2019 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Priyank  Faldu", "Jeff  Diamond", "Boris  Grot"], "year": 2019, "n_citations": 15}
{"id": 2646676, "s2_id": "c8e0a557fd3ef3d9082029568b0e499a7f7d2bbf", "title": "On transaction parallelizability in Ethereum", "abstract": "Ethereum clients execute transactions in a sequential order prescribed by the consensus protocol. This is a safe and conservative approach to blockchain transaction processing which forgoes running transactions in parallel even when doing so would be beneficial and safe, e.g., when there is no intersection in the sets of accounts that the transactions read or modify. In this work we study the degree of transaction parallelizability and present results from three different simulations using real Ethereum transaction data. Our simulations demonstrate that notable gains are achievable with parallelization, and suggest that the potential for parallelizability improves as transaction rates increase.", "venue": "ArXiv", "authors": ["Nadi  Sarrar"], "year": 2019, "n_citations": 0}
{"id": 2648008, "s2_id": "2a3dc7d3aa2518848ad21a75e0bbf091312b8005", "title": "Benchmarking data analysis and machine learning applications on the Intel KNL many-core processor", "abstract": "Knights Landing (KNL) is the code name for the second-generation Intel Xeon Phi product family. KNL has generated significant interest in the data analysis and machine learning communities because its new many-core architecture targets both of these workloads. The KNL many-core vector processor design enables it to exploit much higher levels of parallelism. At the Lincoln Laboratory Supercomputing Center (LLSC), the majority of users are running data analysis applications such as MATLAB and Octave. More recently, machine learning applications, such as the UC Berkeley Caffe deep learning framework, have become increasingly important to LLSC users. Thus, the performance of these applications on KNL systems is of high interest to LLSC users and the broader data analysis and machine learning communities. Our data analysis benchmarks of these application on the Intel KNL processor indicate that single-core double-precision generalized matrix multiply (DGEMM) performance on KNL systems has improved by \u223c3.5\u00d7 compared to prior Intel Xeon technologies. Our data analysis applications also achieved \u223c60% of the theoretical peak performance. Also a performance comparison of a machine learning application, Caffe, between the two different Intel CPUs, Xeon E5 v3 and Xeon Phi 7210, demonstrated a 2.7\u00d7 improvement on a KNL node.", "venue": "2017 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Chansup  Byun", "Jeremy  Kepner", "William  Arcand", "David  Bestor", "Bill  Bergeron", "Vijay  Gadepally", "Michael  Houle", "Matthew  Hubbell", "Michael  Jones", "Anna  Klein", "Peter  Michaleas", "Lauren  Milechin", "Julie  Mullen", "Andrew  Prout", "Antonio  Rosa", "Siddharth  Samsi", "Charles  Yee", "Albert  Reuther"], "year": 2017, "n_citations": 14}
{"id": 2648225, "s2_id": "f0aa935736212ab5d16b4746e497b91c05dcd001", "title": "Analysis and Verification of Relation between Digitizer's Sampling Properties and Energy Resolution of HPGe Detectors", "abstract": "The CDEX (China Dark matter Experiment) aims at detection of WIMPs (Weakly Interacting Massive Particles) and 0vbb (Neutrinoless double beta decay) of 76Ge. It now uses ~10 kg HPGe (High Purity Germanium) detectors in CJPL (China Jinping Underground Laboratory). The energy resolution of detectors is calculated via height spectrum of waveforms with 6-us shaping time. It is necessary to know how sampling properties of a digitizer effect the energy resolution. This paper will present preliminary energy resolution results of waveforms at different sampling properties. The preliminary results show that the ENOB (effective number of bits) with 8.25-bit or better can meet the energy resolution @122keV of CDEX HPGe detectors. Based on the ADC (Analog-to-Digital Converter) quantized error theory, this paper will also make a quantitative analysis on energy resolution in CDEX HPGe detectors. It will provide guidance for ADC design in full-chain cryogenic readout electronics for HPGe detectors.", "venue": "ArXiv", "authors": ["Jinfu  Zhu", "Tianhao  Wang", "Tao  Xue", "Liangjun  Wei", "Jingjun  Wen", "Lin  Jiang", "Jianmin  Li"], "year": 2020, "n_citations": 0}
{"id": 2648583, "s2_id": "a921625bbd8bd7573cc6f5574843e4ec9bd6910b", "title": "Network Resilience Assessment via QoS Degradation Metrics: An Algorithmic Approach", "abstract": "This paper focuses on network resilience to perturbation of edge weight. Other than connectivity, many network applications nowadays rely upon some measure of network distance between a pair of connected nodes. In these systems, a metric related to network functionality is associated to each edge. A pair of nodes only being functional if the weighted, shortest-path distance between the pair is below a given threshold T. Consequently, a natural question is on which degree the change of edge weights can damage the network functionality? With this motivation, we study a new problem, Quality of Service Degradation: given a set of pairs, find a minimum budget to increase the edge weights which ensures the distance between each pair exceeds T. We introduce four algorithms with theoretical performance guarantees for this problem. Each of them has its own strength in trade-off between effectiveness and running time, which are illustrated both in theory and comprehensive experimental evaluation.", "venue": "PERV", "authors": ["Lan N. Nguyen", "My T. Thai"], "year": 2019, "n_citations": 1}
{"id": 2653451, "s2_id": "c8a626fcff4eee6dc37fead8a2d1fedbbf2b3531", "title": "A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning", "abstract": "We introduce Deep500: the first customizable benchmarking infrastructure that enables fair comparison of the plethora of deep learning frameworks, algorithms, libraries, and techniques. The key idea behind Deep500 is its modular design, where deep learning is factorized into four distinct levels: operators, network processing, training, and distributed training. Our evaluation illustrates that Deep500 is customizable (enables combining and benchmarking different deep learning codes) and fair (uses carefully selected metrics). Moreover, Deep500 is fast (incurs negligible overheads), verifiable (offers infrastructure to analyze correctness), and reproducible. Finally, as the first distributed and reproducible benchmarking system for deep learning, Deep500 provides software infrastructure to utilize the most powerful supercomputers for extreme-scale workloads.", "venue": "2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Tal  Ben-Nun", "Maciej  Besta", "Simon  Huber", "Alexandros Nikolaos Ziogas", "Daniel  Peter", "Torsten  Hoefler"], "year": 2019, "n_citations": 63}
{"id": 2655569, "s2_id": "9d376682895ac1c49227b360fb5c010ad3652f7f", "title": "Tsunami propagation for singular topographies", "abstract": "We consider a tsunami wave equation with singular coefficients and prove that it has a very weak solution. Moreover, we show the uniqueness results and consistency theorem of the very weak solution with the classical one in some appropriate sense. Numerical experiments are done for the families of regularised problems in one- and two-dimensional cases. In particular, the appearance of a substantial second wave is observed, travelling in the opposite direction from the point/line of singularity. Its structure and strength are analysed numerically. In addition, for the two-dimensional tsunami wave equation, we develop GPU computing algorithms to reduce the computational cost.", "venue": "ArXiv", "authors": ["Arshyn  Altybay", "Michael  Ruzhansky", "Mohammed Elamine Sebih", "Niyaz  Tokmagambetov"], "year": 2020, "n_citations": 0}
{"id": 2659121, "s2_id": "9497a3f9b7a66b90b07627cc77fd73039c80de5d", "title": "Network Load Analysis and Provisioning of MapReduce Applications", "abstract": "In this paper, we study the dependency between MapReduce configuration parameters and network load of fixed-size MapReduce jobs during the shuffle phase, then we propose an analytical method to model this dependency. Our approach consists of three key phases: profiling, modeling, and prediction. In the first stage, an application is run several times with different sets of MapReduce configuration parameters (here number of map tasks and number of reduce tasks) to profile the network load of an application in the shuffle phase on a given cluster. Then, the relation between these parameters and the network load is modeled by multivariate linear regression. For evaluation, three applications (Word Count, Exim Main log parsing, and TeraSort) are utilized to evaluate our technique on a 5-node MapReduce private cluster.", "venue": "2012 13th International Conference on Parallel and Distributed Computing, Applications and Technologies", "authors": ["Nikzad Babaii Rizvandi", "Javid  Taheri", "Reza  Moraveji", "Albert Y. Zomaya"], "year": 2012, "n_citations": 10}
{"id": 2661093, "s2_id": "eb2896246bdcd098838bc25e0def41d66ce550c9", "title": "AIBench Scenario: Scenario-Distilling AI Benchmarking", "abstract": "Modern real-world application scenarios like Internet services consist of a diversity of AI and non-AI modules with huge code sizes and long and complicated execution paths, which raises serious benchmarking or evaluating challenges. Using AI components or micro benchmarks alone can lead to error-prone conclusions. This paper presents a methodology to attack the above challenge. We formalize a real-world application scenario as a Directed Acyclic Graph-based model and propose the rules to distill it into a permutation of essential AI and non-AI tasks, which we call a scenario benchmark. Together with seventeen industry partners, we extract nine typical scenario benchmarks. We design and implement an extensible, configurable, and flexible benchmark framework. We implement two Internet service AI scenario benchmarks based on the framework as proxies to two real-world application scenarios. We consider scenario, component, and micro benchmarks as three indispensable parts for evaluating. Our evaluation shows the advantage of our methodology against using component or micro AI benchmarks alone. The specifications, source code 11Zenodo: https://doi.org/10.5281/zenodo.5158715 GitHub: https://github.com/BenchCouncil/aibench_scenario, testbed, and results are publicly available from https://www.benchcouncil.org/aibench/scenario/.", "venue": "2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)", "authors": ["Wanling  Gao", "Fei  Tang", "Jianfeng  Zhan", "Xu  Wen", "Lei  Wang", "Zheng  Cao", "Chuanxin  Lan", "Chunjie  Luo", "Xiaoli  Liu", "Zihan  Jiang"], "year": 2021, "n_citations": 3}
{"id": 2661547, "s2_id": "20e4ceabf686e56687f4c645abe320c236f7eab8", "title": "A Visual Analytics Framework for Reviewing Streaming Performance Data", "abstract": "Understanding and tuning the performance of extreme-scale parallel computing systems demands a streaming approach due to the computational cost of applying offline algorithms to vast amounts of performance log data. Analyzing large streaming data is challenging because the rate of receiving data and limited time to comprehend data make it difficult for the analysts to sufficiently examine the data without missing important changes or patterns. To support streaming data analysis, we introduce a visual analytic framework comprising of three modules: data management, analysis, and interactive visualization. The data management module collects various computing and communication performance metrics from the monitored system using streaming data processing techniques and feeds the data to the other two modules. The analysis module automatically identifies important changes and patterns at the required latency. In particular, we introduce a set of online and progressive analysis methods for not only controlling the computational costs but also helping analysts better follow the critical aspects of the analysis results. Finally, the interactive visualization module provides the analysts with a coherent view of the changes and patterns in the continuously captured performance data. Through a multi-faceted case study on performance analysis of parallel discrete-event simulation, we demonstrate the effectiveness of our framework for identifying bottlenecks and locating outliers.", "venue": "2020 IEEE Pacific Visualization Symposium (PacificVis)", "authors": ["Suraj P. Kesavan", "Takanori  Fujiwara", "Jianping Kelvin Li", "Caitlin  Ross", "Misbah  Mubarak", "Christopher D. Carothers", "Robert B. Ross", "Kwan-Liu  Ma"], "year": 2020, "n_citations": 6}
{"id": 2661645, "s2_id": "927ae9d7fe2c660df7ee88efb11f6a65dfa8f91e", "title": "Detecting State Transitions of a Markov Source: Sampling Frequency and Age Trade-off", "abstract": "We consider a finite-state Discrete-Time Markov Chain (DTMC) source that can be sampled for detecting the events when the DTMC transits to a new state. Our goal is to study the trade-off between sampling frequency and staleness in detecting the events. We argue that, for the problem at hand, using Age of Information (AoI) for quantifying the staleness of a sample is conservative and therefore, introduce age penalty for this purpose. We study two optimization problems: minimize average age penalty subject to an average sampling frequency constraint, and minimize average sampling frequency subject to an average age penalty constraint; both are Constrained Markov Decision Problems. We solve them using linear programming approach and compute Markov policies that are optimal among all causal policies. Our numerical results demonstrate that the computed Markov policies not only outperform optimal periodic sampling policies, but also achieve sampling frequencies close to or lower than that of an optimal clairvoyant (non-causal) sampling policy, if a small age penalty is allowed.", "venue": "IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)", "authors": ["Jaya Prakash Champati", "Mikael  Skoglund", "James  Gross"], "year": 2020, "n_citations": 2}
{"id": 2661991, "s2_id": "0fad598fd1d0438cea94a4816e5ce4479518fa56", "title": "Multi-step Uniformization with Steady-State Detection in Nonstationary M/M/s Queuing Systems", "abstract": "AbstractA new approach to the steady state detection in the uniformization method ofsolving continuous time Markov chains is introduced. The method is particularlyuseful in solving inhomogenous CTMC\u2019s in multiple steps, where the desirederror bound of the whole solution can be distributed not proportionally to thelengths of the respective intervals, but rather in a way, that maximizes thechances of detecting a steady state. Additionally, the convergence properties ofthe underlying DTMC are used to further enhance the computational savingsdue to the steady state detection. The method is applied to the problem ofmodeling a Call Center using inhomogenous CTMC model of a M(t)/M(t)/s(t)queuing systems.Keywords: Markov processes, OR in service industries, transientsolutions, uniformization, numerical methods, nonstationary,algorithms, Contact Centre1. IntroductionOne of the most important characteristics of the telephone Call Centers istheir varying number of service requests (calls) in time. As the cost of laboris the most signi cant one in such service systems, the problem of adequatescheduling of its employees has a long history in the area of operational re-search. The methods proposed to analyze such time-varying behavior in orderto nd optimal working schedules for given demand forecasts are based mostlyon approximations, by adopting stationary queuing models. Examples of suchwell established methods can be found e.g in Green et al. (2007) or Aksin et al.(2007).However, stationary solutions for generating such schedules are, in manysituations, not adequate. Ingolfsson et al. (2010) demonstrated that such ap-proximations can be either entirely unreliable or deliver results signi cantlydi erent than methods based on inherently transient models. Despite this,", "venue": "ArXiv", "authors": ["Maciej  Burak"], "year": 2014, "n_citations": 8}
{"id": 2666055, "s2_id": "e55015799029bc28ffd0b793c43971c427a96cfb", "title": "Feature-Specific Profiling", "abstract": "While high-level languages come with significant readability and maintainability benefits, their performance remains difficult to predict. For example, programmers may unknowingly use language features inappropriately, which cause their programs to run slower than expected. To address this issue, we introduce feature-specific profiling, a technique that reports performance costs in terms of linguistic constructs. Feature-specific profilers help programmers find expensive uses of specific features of their language. We describe the architecture of a profiler that implements our approach, explain prototypes of the profiler for two languages with different characteristics and implementation strategies, and provide empirical evidence for the approach\u2019s general usefulness as a performance debugging tool.", "venue": "ACM Trans. Program. Lang. Syst.", "authors": ["Leif  Andersen", "Vincent  St-Amour", "Jan  Vitek", "Matthias  Felleisen"], "year": 2019, "n_citations": 2}
{"id": 2674380, "s2_id": "c701b3a40f7942bc098cbd32a15591e86551f1ff", "title": "Coarse-Grain Performance Estimator for Heterogeneous Parallel Computing Architectures like Zynq All-Programmable SoC", "abstract": "Heterogeneous computing is emerging as a mandatory requirement for power-efficient system design. With this aim, modern heterogeneous platforms like Zynq All-Programmable SoC, that integrates ARM-based SMP and programmable logic, have been designed. However, those platforms introduce large design cycles consisting on hardware/software partitioning, decisions on granularity and number of hardware accelerators, hardware/software integration, bitstream generation, etc. \nThis paper presents a performance parallel heterogeneous estimation for systems where hardware/software co-design and run-time heterogeneous task scheduling are key. The results show that the programmer can quickly decide, based only on her/his OmpSs (OpenMP + extensions) application, which is the co-design that achieves nearly optimal heterogeneous parallel performance, based on the methodology presented and considering only synthesis estimation results. The methodology presented reduces the programmer co-design decision from hours to minutes and shows high potential on hardware/software heterogeneous parallel performance estimation on the Zynq All-Programmable SoC.", "venue": "ArXiv", "authors": ["Daniel  Jim\u00e9nez-Gonz\u00e1lez", "Carlos  \u00c1lvarez", "Antonio  Filgueras", "Xavier  Martorell", "Jan  Langer", "Juanjo  Noguera", "Kees A. Vissers"], "year": 2015, "n_citations": 10}
{"id": 2679414, "s2_id": "b6502b61bf8f0332c6caa30198cff3619a9790aa", "title": "When do redundant requests reduce latency ?", "abstract": "Several systems possess the flexibility to serve requests in more than one way. For instance, a distributed storage system storing multiple replicas of the data can serve a request from any of the multiple servers that store the requested data, or a computational task may be performed in a compute-cluster by any one of multiple processors. In such systems, the latency of serving the requests may potentially be reduced by sending redundant requests: a request may be sent to an excess number of servers, and it is deemed served when the requisite number of servers complete service. Such a mechanism trades off the possibility of faster execution of at least one copy of the request with the increase in the delay due to an increased load on the system. Due to this tradeoff, it is unclear when redundant requests may actually help. Several recent works empirically evaluate the latency performance of redundant requests in diverse settings. This work aims at a rigorous analytical study of the latency performance of redundant requests, with the primary goals of characterizing the situations when sending redundant requests will help (and when not), and designing optimal redundant-requesting policies. We first present a model that captures the key features of such systems. We show that when service times are i.i.d. memoryless or \u201cheavy\u201d, and when the additional copies of already-completed jobs can be removed with negligible costs, redundant requests reduce the average latency. On the other hand, when service times are \u201clight\u201d or when service times are memoryless and removal of jobs results in a non-negligible penalty, not having any redundancy in the request is optimal under high loads. Our results hold for arbitrary arrival processes.", "venue": "2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "authors": ["Nihar B. Shah", "Kangwook  Lee", "Kannan  Ramchandran"], "year": 2013, "n_citations": 158}
{"id": 2679689, "s2_id": "21e4c2eb049fd38a825a068e59c0673282ff12be", "title": "Multidimensional Visualization of ORACLE Performance Using Barry007", "abstract": "Most generic performance tools display only system-level performance data using 2-dimensional plots or diagrams and this limits the informational detail that can be displayed. Moreover, a modern relational database system, like Oracle, can concurrently serve thousands of client processes with different workload characteristics, so that generic performance-data displays inevitably hide important information. Drawing on our previous work, this paper demonstrates the application of Barry007 multidimensional visualization to the analysis of Oracle end-user, session-level, performance data, showing both collective trends and individual performance anomalies.", "venue": "Int. CMG Conference", "authors": ["Tanel  Poder", "Neil J. Gunther"], "year": 2008, "n_citations": 0}
{"id": 2681017, "s2_id": "44d72d1247460bd345a762d6c4ed71954b6d2066", "title": "Multigrid Solvers in Reconfigurable Hardware", "abstract": "The problem of finding the solution of partial differential equations (PDEs) plays a central role in modeling real world problems. Over the past years, Multigrid solvers have showed their robustness over other techniques, due to its high convergence rate which is independent of the problem size. For this reason, many attempts for exploiting the inherent parallelism of Multigrid have been made to achieve the desired efficiency and scalability of the method. Yet, most efforts fail in this respect due to many factors (time, resources) governed by software implementations. In this paper, we present a hardware implementation of the V-cycle Multigrid method for finding the solution of a 2D-Poisson equation. We use Handel-C to implement our hardware design, which we map onto available field programmable gate arrays (FPGAs). We analyze the implementation performance using the FPGA vendor's tools. We demonstrate the robustness of Multigrid over other similar iterative solvers, such as Jacobi and successive over relaxation (SOR), in both hardware and software. We compare our findings with a C++ version of each algorithm. The obtained results show better performance when compared to existing software versions.", "venue": "ArXiv", "authors": ["Safaa J. Kasbah", "Issam W. Damaj", "Ramzi A. Haraty"], "year": 2019, "n_citations": 8}
{"id": 2681496, "s2_id": "5ab53c97d026512bd2cb61f8a21d6ae6d42ac523", "title": "Profiling Resource Utilization of Bioinformatics Workflows", "abstract": "We present a software tool, the Container Profiler, that measures and records the resource usage of any containerized task. Our tool profiles the CPU, memory, disk, and network utilization of a containerized job by collecting Linux operating system metrics at the virtual machine, container, and process levels. The Container Profiler can produce utilization snapshots at multiple time points, allowing for continuous monitoring of the resources consumed by a container workflow. \nTo investigate the utility of the Container Profiler we profiled the resource utilization requirements of a multi-stage bioinformatics analytical workflow (RNA sequencing using unique molecular identifiers). We examined the collected profile metrics and confirmed that they were consistent with the expected CPU, disk, network resource utilization patterns for the different stages of the workflow. We also quantified the profiling overhead and found that this was negligible. \nThe Container Profiler is a useful tool that can be used to continuously monitor the resource consumption of long and complex containerized workflows that run locally or on the cloud. This can identify bottlenecks where more resources are needed to improve performance.", "venue": "ArXiv", "authors": ["Huazeng  Deng", "Ling-Hong  Hung", "Raymond  Schooley", "David  Perez", "Niharika  Arumilli", "Ka Yee Yeung", "Wes  Lloyd"], "year": 2020, "n_citations": 2}
{"id": 2682066, "s2_id": "5a40874631908f9e52e241b7971d7475989b3b48", "title": "Benchmarking the graphulo processing framework", "abstract": "Graph algorithms have wide applicablity to a variety of domains and are often used on massive datasets. Recent standardization efforts such as the GraphBLAS specify a set of key computational kernels that hardware and software developers can adhere to. Graphulo is a processing framework that enables GraphBLAS kernels in the Apache Accumulo database. In our previous work, we have demonstrated a core Graphulo operation called TableMult that performs large-scale multiplication operations of database tables. In this article, we present the results of scaling the Graphulo engine to larger problems and scalablity when a greater number of resources is used. Specifically, we present two experiments that demonstrate Graphulo scaling performance is linear with the number of available resources. The first experiment demonstrates cluster processing rates through Graphulo's TableMult operator on two large graphs, scaled between 217 and 219 vertices. The second experiment uses TableMult to extract a random set of rows from a large graph (219 nodes) to simulate a cued graph analytic. These benchmarking results are of relevance to Graphulo users who wish to apply Graphulo to their graph problems.", "venue": "2016 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Timothy  Weale", "Vijay  Gadepally", "Dylan  Hutchison", "Jeremy  Kepner"], "year": 2016, "n_citations": 4}
{"id": 2684505, "s2_id": "f79f5e632260455c9685a3454fb7ae3ac58cb270", "title": "Multiple Server SRPT With Speed Scaling Is Competitive", "abstract": "Can the popular shortest remaining processing time (SRPT) algorithm achieve a constant competitive ratio on multiple servers when server speeds are adjustable (speed scaling) with respect to the flow time plus energy consumption metric? This question has remained open for a while, where a negative result in the absence of speed scaling is well known. The main result of this paper is to show that multi-server SRPT with speed scaling can be constant competitive, with a competitive ratio that only depends on the power-usage function of the servers, but not on the number of jobs/servers or the job sizes (unlike when speed scaling is not allowed). When all job sizes are unity, we show that round-robin routing is optimal and can achieve the same competitive ratio as the best known algorithm for the single server problem. Finally, we show that a class of greedy dispatch policies, including policies that route to the least loaded or the shortest queue, do not admit a constant competitive ratio. When job arrivals are stochastic, with Poisson arrivals and i.i.d. job sizes, we show that random routing and a simple gated-static speed scaling algorithm achieves a constant competitive ratio.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Rahul  Vaze", "Jayakrishnan  Nair"], "year": 2020, "n_citations": 4}
{"id": 2688060, "s2_id": "11190d1a39503d7c1e9dca453be0280220bb5f3c", "title": "Achievable Stability in Redundancy Systems", "abstract": "We consider a system with N~parallel servers where incoming jobs are immediately replicated to, say, d~servers. Each of the N servers has its own queue and follows a FCFS discipline. As soon as the first job replica is completed, the remaining replicas are abandoned. We investigate the achievable stability region for a quite general workload model with different job types and heterogeneous servers, reflecting job-server affinity relations which may arise from data locality issues and soft compatibility constraints. Under the assumption that job types are known beforehand we show for New-Better-than-Used (NBU) distributed speed variations that no replication $(d=1)$ gives a strictly larger stability region than replication $(d>1)$. Strikingly, this does not depend on the underlying distribution of the intrinsic job sizes, but observing the job types is essential for this statement to hold. In case of non-observable job types we show that for New-Worse-than-Used (NWU) distributed speed variations full replication ($d=N$) gives a larger stability region than no replication $(d=1)$.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Youri  Raaijmakers", "Sem  Borst"], "year": 2020, "n_citations": 6}
{"id": 2690730, "s2_id": "4805fd529fe58798d01689b397fe6dfcc65c03e2", "title": "Noise Limited Computational Speed", "abstract": "In modern transistor based logic gates, the impact of noise on computation has become increasingly relevant since the voltage scaling strategy aimed at decreasing the dissipated power, has increased the probability of error due to the reduced switching threshold voltages. In this paper, we discuss the role of noise in a two state model that mimic the dynamics of standard logic gates and show that the presence of the noise sets a fundamental limit to the computing speed. An optimal idle time interval that minimizes the error probability is derived.", "venue": "ArXiv", "authors": ["Luca  Gammaitoni"], "year": 2007, "n_citations": 18}
{"id": 2690939, "s2_id": "825fd1ce31941e6b90819849f8ebcea55b5a114d", "title": "Real Time Lidar and Radar High-Level Fusion for Obstacle Detection and Tracking with evaluation on a ground truth", "abstract": "- Both Lidars and Radars are sensors for obstacle detection. While Lidars are very accurate on obstacles positions and less accurate on their velocities, Radars are more precise on obstacles velocities and less precise on their positions. Sensor fusion between Lidar and Radar aims at improving obstacle detection using advantages of the two sensors. The present paper proposes a real-time Lidar/Radar data fusion algorithm for obstacle detection and tracking based on the global nearest neighbour standard filter (GNN). This algorithm is implemented and embedded in an automative vehicle as a component generated by a real-time multisensor software. The benefits of data fusion comparing with the use of a single sensor are illustrated through several tracking scenarios (on a highway and on a bend) and using real-time kinematic sensors mounted on the ego and tracked vehicles as a ground truth.", "venue": "ArXiv", "authors": ["Hatem  Hajri", "Mohamed-Cherif  Rahal"], "year": 2018, "n_citations": 14}
{"id": 2692990, "s2_id": "ce8308c1faec755303730e720ddbdcc645a4be9b", "title": "Straggler Mitigation at Scale", "abstract": "Runtime performance variability has been a major issue, hindering predictable and scalable performance in modern distributed systems. Executing requests or jobs redundantly over multiple servers have been shown to be effective for mitigating variability, both in theory and practice. Systems that employ redundancy has drawn significant attention, and numerous papers have analyzed the pain and gain of redundancy under various service models and assumptions on the runtime variability. This paper presents a cost (pain) vs. latency (gain) analysis of executing jobs of many tasks by employing replicated or erasure coded redundancy. The tail heaviness of service time variability is decisive on the pain and gain of redundancy and we quantify its effect by deriving expressions for cost and latency. Specifically, we try to answer four questions: 1) How do replicated and coded redundancy compare in the cost vs. latency tradeoff? 2) Can we introduce redundancy after waiting some time and expect it to reduce the cost? 3) Can relaunching the tasks that appear to be straggling after some time help to reduce cost and/or latency? 4) Is it effective to use redundancy and relaunching together? We validate the answers we found for each of these questions via simulations that use empirical distributions extracted from a Google cluster data.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Mehmet Fatih Akta\u015f", "Emina  Soljanin"], "year": 2019, "n_citations": 24}
{"id": 2695146, "s2_id": "1431b206c08e504c02b9d2d470b83d81270e767a", "title": "Cache Miss Estimation for Non-Stationary Request Processes", "abstract": "The aim of the paper is to evaluate the miss probability of a Least Recently Used (LRU) cache, when it is offered a non-stationary request process given by a Poisson cluster point process. First, we construct a probability space using Palm theory, describing how to consider a tagged document with respect to the rest of the request process. This framework allows us to derive a general integral formula for the expected number of misses of the tagged document. Then, we consider the limit when the cache size and the arrival rate go to infinity proportionally, and use the integral formula to derive an asymptotic expansion of the miss probability in powers of the inverse of the cache size. This enables us to quantify and improve the accuracy of the so-called Che approximation.", "venue": "ArXiv", "authors": ["Felipe  Olmos", "Carl  Graham", "Alain  Simonian"], "year": 2015, "n_citations": 1}
{"id": 2695377, "s2_id": "8c8f83ae02a364d6b4bc755bee4b4a115682b3ee", "title": "DeepScaleTool: A Tool for the Accurate Estimation of Technology Scaling in the Deep-Submicron Era", "abstract": "The estimation of classical CMOS \"constant-field\" or \"Dennard\" scaling methods that define scaling factors for various dimensional and electrical parameters have become less accurate in the deep-submicron regime, which drives the need for better estimation approaches especially in the educational and research domains. We present DeepScaleTool, a tool for the accurate estimation of deep-submicron technology scaling by modeling and curve fitting published data by a leading commercial fabrication company for silicon fabrication technology generations from 130 nm to 7 nm for the key parameters of area, delay, and energy. Compared to 10 nm-7 nm scaling data published by a leading foundry, the DeepScaleTool achieves an error of 1.7% in area, 2.5% in delay, and 5% in power. This compares favorably with another leading academic estimation method that achieves an error of 24% in area, 9.1% in delay, and 24.9% in power.", "venue": "2021 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Satyabrata  Sarangi", "Bevan  Baas"], "year": 2021, "n_citations": 1}
{"id": 2701622, "s2_id": "09a311e116eef260a134812b86fae3949cc6d820", "title": "Relocation in car sharing systems with shared stackable vehicles: Modelling challenges and outlook", "abstract": "Car sharing is expected to reduce traffic congestion and pollution in cities while at the same time improving accessibility to public transport. However, the most popular form of car sharing, one-way car sharing, still suffers from the vehicle unbalance problem. Innovative solutions to this issue rely on custom vehicles with stackable capabilities: customers or operators can drive a train of vehicles if necessary, thus efficiently bringing several cars from an area with few requests to an area with many requests. However, how to model a car sharing system with stackable vehicles is an open problem in the related literature. In this paper, we propose a queueing theoretical model to fill this gap, and we use this model to derive an upper-bound on user-based relocation capabilities. We also validate, for the first time in the related literature, legacy queueing theoretical models against a trace of real car sharing data. Finally, we present preliminary results about the impact, on car availability, of simple user-based relocation heuristics with stackable vehicles. Our results indicate that user-based relocation schemes that exploit vehicle stackability can significantly improve car availability at stations.", "venue": "2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)", "authors": ["Chiara  Boldrini", "Riccardo  Incaini", "Raffaele  Bruno"], "year": 2017, "n_citations": 4}
{"id": 2701893, "s2_id": "8876f4e779933ed8edee8e6bf68cf23595e06f13", "title": "Performance of a Quantum Annealer for Ising Ground State Computations on Chimera Graphs", "abstract": "Quantum annealing is getting increasing attention in combinatorial optimization. The quantum processing unit by D-Wave is constructed to approximately solve Ising models on so-called Chimera graphs. Ising models are equivalent to quadratic unconstrained binary optimization (QUBO) problems and maximum cut problems on the associated graphs. We have tailored branch-and-cut as well as semidefinite programming algorithms for solving Ising models for Chimera graphs to provable optimality and use the strength of these approaches for comparing our solution values to those obtained on the current quantum annealing machine D-Wave 2000Q. This allows for the assessment of the quality of solutions produced by the D-Wave hardware. It has been a matter of discussion in the literature how well the D-Wave hardware performs at its native task, and our experiments shed some more light on this issue.", "venue": "ArXiv", "authors": ["Michael  J\u00fcnger", "Elisabeth  Lobe", "Petra  Mutzel", "Gerhard  Reinelt", "Franz  Rendl", "Giovanni  Rinaldi", "Tobias  Stollenwerk"], "year": 2019, "n_citations": 13}
{"id": 2702295, "s2_id": "8f5191e9975cecc1f8b67a0f9ccb0462724befac", "title": "Back-of-the-Envelope Computation of Throughput Distributions in CSMA Wireless Networks", "abstract": "This work started out with our discovery of a pattern of throughput distributions among links in IEEE 802.11 networks from experimental results. This pattern gives rise to an easy computation method, which we term back-of-the-envelop (BoE) computation. For many network configurations, very accurate results can be obtained by BoE within minutes, if not seconds, by simple hand computation. This allows us to make shortcuts in performance evaluation, bypassing complicated stochastic analysis. To explain BoE, we construct a theory based on the model of an \u201cideal CSMA network\u201d (ICN). The BoE computation method emerges from ICN when we take the limit c \u2192 0, where c is the ratio of the mean backoff countdown time to the mean transmission time in the CSMA protocol. Importantly, we derive a new mathematical result: the link throughputs of ICN are insensitive to the distributions of the backoff countdown time and transmission time (packet duration) given the ratio of their means c. This insensitivity result explains why BoE works so well for practical 802.11 networks, in which the backoff countdown process is one that has memory, and in which the packet size can be arbitrarily distributed. Our results indicate that BoE is a good approximation technique for modest-size networks such as those typically seen in 802.11 deployments. Beyond explaining BoE, the theoretical framework of ICN is also a foundation for fundamental understanding of very-large-scale CSMA networks. In particular, ICN is similar to the Ising model in statistical physics used to explain phenomena arising out of the interactions of a large number of entities. Many new research directions arise out of the ICN model.", "venue": "2009 IEEE International Conference on Communications", "authors": ["Soung Chang Liew", "Caihong  Kai", "Jason  Leung", "Bill  Wong"], "year": 2009, "n_citations": 135}
{"id": 2703607, "s2_id": "78a8cddd57f770a4054ca07942e6ff192d8f0f16", "title": "GPGPU Performance Estimation with Core and Memory Frequency Scaling", "abstract": "Graphics processing units (GPUs) support dynamic voltage and frequency scaling to balance computational performance and energy consumption. However, simple and accurate performance estimation for a given GPU kernel under different frequency settings is still lacking for real hardware, which is important to decide the best frequency configuration for energy saving. We reveal a fine-grained analytical model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Over a 2 x range of both core and memory frequencies among 20 GPU kernels, our model achieves accurate results (4.83 % error on average) with real hardware. Compared to the cycle-level simulators, our model only needs simple micro-benchmarks to extract a set of hardware parameters and kernel performance counters to produce such high accuracy without kernel source analysis.", "venue": "2018 IEEE 24th International Conference on Parallel and Distributed Systems (ICPADS)", "authors": ["Qiang  Wang", "Xiaowen  Chu"], "year": 2018, "n_citations": 33}
{"id": 2707292, "s2_id": "9c00b5531396d1eb249533562f90ccc63cd28534", "title": "A Brief Review on Models for Performance Evaluation in DSS Architecture", "abstract": "Distributed Software Systems (DSS) are used these days by many people in the real time operations and modern enterprise applications. One of the most important and essential attributes of measurements for the quality of service of distributed software is performance. Performance models can be employed at early stages of the software development cycle to characterize the quantitative behavior of software systems. In this research, performance models based on fuzzy logic approach, queuing network approach and Petri net approach have been reviewed briefly. One of the most common ways in performance analysis of distributed software systems is translating the UML diagrams to mathematical modeling languages for the description of distributed systems such as queuing networks or Petri nets. In this paper, some of these approaches are reviewed briefly. Attributes which are used for performance modeling in the literature are mostly machine based. On the other hand, end users and client parameters for performance evaluation are not covered extensively. In this way, future research could be based on developing hybrid models to capture user's decision variables which make system performance evaluation more user driven.", "venue": "ArXiv", "authors": ["Ghassem  Tofighi", "Kaamran  Raahemifar", "Anastasios N. Venetsanopoulos"], "year": 2014, "n_citations": 0}
{"id": 2714473, "s2_id": "e42fccd1df4dc5e7638f12fe9c4ab3bfb265c5fe", "title": "Reward processes and performance simulation in supermarket models with different servers", "abstract": "Supermarket models with different servers become a key in modeling resource management of stochastic networks, such as, computer networks, manufacturing systems and transportation networks. While these different servers always make analysis of such a supermarket model more interesting, difficult and challenging. This paper provides a new novel method for analyzing the supermarket model with different servers through a multi-dimensional continuous-time Markov reward processes. Firstly, the utility functions are constructed for expressing a routine selection mechanism that depends on queue lengths, on service rates, and on some probabilities of individual preference. Then applying the continuous-time Markov reward processes, some segmented stochastic integrals of the random reward function are established by means of an event-driven technique. Based on this, the mean of the random reward function in a finite time period is effectively computed by means of the state jump points of the Markov reward process, and also the mean of the discounted random reward function in an infinite time period can be calculated through the same event-driven technique. Finally, some simulation experiments are given to indicate how the expected queue length of each server depends on the main parameters of this supermarket model.", "venue": "Int. J. Simul. Process. Model.", "authors": ["Quan-Lin  Li", "Feifei  Yang", "Na  Li"], "year": 2016, "n_citations": 0}
{"id": 2718449, "s2_id": "f0870352558e79fa23fe4100f61162b4e40f1749", "title": "OMP2HMPP: Compiler Framework for Energy Performance Trade-off Analysis of Automatically Generated Codes", "abstract": "We present OMP2HMPP, a tool that, in a first step, automatically translates OpenMP code into various possible transformations of HMPP. In a second step OMP2HMPP executes all variants to obtain the performance and power consumption of each transformation. The resulting trade-off can be used to choose the more convenient version. After running the tool on a set of codes from the Polybench benchmark we show that the best automatic transformation is equivalent to a manual one done by an expert. Compared with original OpenMP code running in 2 quad-core processors we obtain an average speed-up of 31\u00d7 and 5.86\u00d7 factor in operations per watt.", "venue": "ArXiv", "authors": ["Albert  Sa\u00e0-Garriga", "David  Castells-Rufas", "Jordi  Carrabina"], "year": 2015, "n_citations": 0}
{"id": 2726309, "s2_id": "2e123669f377498b63632a0a6ebf6d29d0148c25", "title": "Heavy-traffic Delay Optimality in Pull-based Load Balancing Systems: Necessary and Sufficient Conditions", "abstract": "In this paper, we consider a load balancing system under a general pull-based policy. In particular, each arrival is randomly dispatched to any server whose queue length is below a threshold; if no such server exists, then the arrival is randomly assigned to any server. We are interested in the fundamental relationship between the threshold and the delay performance of the system in heavy traffic. To this end, we first establish the following necessary condition to guarantee heavy-traffic delay optimality: the threshold needs to grow to infinity as the exogenous arrival rate approaches the boundary of the capacity region (i.e., the load intensity approaches one) but the growth rate should be slower than a polynomial function of the mean number of tasks in the system. As a special case of this result, we directly show that the delay performance of the popular pull-based policy Join-Idle-Queue (JIQ) is not heavy traffic optimal, but performs strictly better than random routing. We further show that a sufficient condition for heavy-traffic delay optimality is that the threshold grows logarithmically with the mean number of tasks in the system. This result directly resolves a generalized version of the conjecture by Kelly and Laws.", "venue": "PERV", "authors": ["Xingyu  Zhou", "Jian  Tan", "Ness  Shroff"], "year": 2019, "n_citations": 1}
{"id": 2730844, "s2_id": "e9bb419606a0030e450476da0905560a24a69014", "title": "MLOS: An Infrastructure for Automated Software Performance Engineering", "abstract": "MLOS is a Data Science powered infrastructure and methodology to democratize and automate Software Performance Engineering. MLOS enables continuous, instance-level, robust, and trackable systems optimization.", "venue": "DEEM@SIGMOD", "authors": ["Carlo  Curino", "Neha  Godwal", "Brian  Kroth", "Sergiy  Kuryata", "Greg  Lapinski", "Siqi  Liu", "Slava  Oks", "Olga  Poppe", "Adam  Smiechowski", "Ed  Thayer", "Markus  Weimer", "Yiwen  Zhu"], "year": 2020, "n_citations": 2}
{"id": 2732299, "s2_id": "e8aa31c5cdd8a5d40739d1b412dbbf5f41be7de5", "title": "Blind estimation of primary user traffic parameters under sensing errors", "abstract": "In this work we investigate the bounds on the estimation accuracy of Primary User (PU) traffic parameters with exponentially distributed busy and idle times. We derive closed-form expressions for the Crame\u0301r-Rao bounds on the mean squared estimation error for the blind joint estimation of the PU traffic parameters, specifically, the duty cycle, and the mean arrival and departure rates. Moreover, we present the corresponding maximum-likelihood estimators for the traffic parameters and discuss the effect of sensing errors in the joint estimation of PU traffic.", "venue": "2013 IEEE International Conference on Communications (ICC)", "authors": ["Wesam  Gabran", "Przemyslaw  Pawelczak", "Chun-Hao  Liu", "Danijela  Cabric"], "year": 2013, "n_citations": 4}
{"id": 2733390, "s2_id": "5e9603fae3a7ebbeb03634aa752b5db28b594663", "title": "Agile calibration process of full-stack simulation frameworks for V2X communications", "abstract": "Computer simulations and real-world car trials are essential to investigate the performance of Vehicle-to-Everything (V2X) networks. However, simulations are imperfect models of the physical reality and can be trusted only when they indicate agreement with the real-world. On the other hand, trials lack reproducibility and are subject to uncertainties and errors. In this paper, we will illustrate a case study where the interrelationship between trials, simulation, and the reality-of-interest is presented. Results are then compared in a holistic fashion. Our study will describe the procedure followed to macroscopically calibrate a full-stack network simulator to conduct high-fidelity full-stack computer simulations.", "venue": "2017 IEEE Vehicular Networking Conference (VNC)", "authors": ["Ioannis  Mavromatis", "Andrea  Tassi", "Robert J. Piechocki", "Andrew R. Nix"], "year": 2017, "n_citations": 7}
{"id": 2733540, "s2_id": "406be439b13a843990136274af3a9b4c841048cb", "title": "Comparison and Improvement for Delay Analysis Approaches: Theoretical Models and Experimental Tests", "abstract": "Computer network tends to be subjected to the proliferation of mobile demands and increasingly multifarious, therefore it poses a great challenge to guarantee the quality of network service. By designing the model according to different requirements, we may get some related indicators such as delay and packet loss rate in order to evaluate the quality of network service and verify the user data surface and capacity of the network environment. In this paper, we describe an analytical model based on the measurement for the delay of each packet passing through the single existing routers in the network environment. In previous studies, the emulation of real network service behaviors was generally under ideal condition. In our work, the test environment is built to get the relevant test results of the actual network, and the corresponding theoretical results are obtained by our model. The test results are compared with the theoretical results, analyzed and corrected, in order to verify the feasibility of our analysis model for the performance analysis of the actual network. With this concern, calculation results are modified with different schemes to realize more precise calculation of delay boundary with the comparison with the experimental test results. The results show the analysis methods after the amendment can realistically estimate the performance of network element. \u00a9 2020 Published by Elsevier Ltd.", "venue": "ArXiv", "authors": ["Yue Hong Gao", "Xiao  Hong", "Hao Tian Yang", "Lu  Chen", "Xiao Nan Zhang"], "year": 2021, "n_citations": 0}
{"id": 2735705, "s2_id": "d89e3d18dcceb6434192f5278d8357fdd47b5eed", "title": "Load Balancing Under Strict Compatibility Constraints", "abstract": "Consider a system with N identical single-server queues and M(N) task types, where each server is able to process only a small subset of possible task types. Arriving tasks select d\u22652 random compatible servers, and join the shortest queue among them. The compatibility constraints are captured by a fixed bipartite graph GN between the servers and the task types. When GN is complete bipartite, the meanfield approximation is accurate. However, such dense compatibility graphs are infeasible for large-scale implementation. We characterize a class of sparse compatibility graphs for which the meanfield approximation remains valid. For this, we introduce a novel notion, called proportional sparsity, and establish that systems with proportionally sparse compatibility graphs asymptotically match the performance of a fully flexible system. Furthermore, we show that proportionally sparse random compatibility graphs can be constructed, which reduce the server-degree almost by a factor N/ln(N) compared to the complete bipartite compatibility graph.", "venue": "SIGMETRICS", "authors": ["Daan  Rutten", "Debankur  Mukherjee"], "year": 2021, "n_citations": 6}
{"id": 2737622, "s2_id": "902e776621e40ccc7b2f8e65970cad2a555d610b", "title": "LIKWID: Lightweight Performance Tools", "abstract": "Exploiting the performance of today\u2019s microprocessors requires intimate knowledge of the microarchitecture as well as an awareness of the ever-growing complexity in thread and cache topology. LIKWID is a set of command line utilities that addresses four key problems: Probing the thread and cache topology of a shared-memory node, enforcing thread-core affinity on a program, measuring performance counter metrics, and microbenchmarking for reliable upper performance bounds. Moreover, it includes an mpirun wrapper allowing for portable thread-core affinity in MPI and hybrid MPI/threaded applications. To demonstrate the capabilities of the tool set we show the influence of thread affinity on performance using the well-known OpenMP STREAM triad benchmark, use hardware counter tools to study the performance of a stencil code, and finally show how to detect bandwidth problems on ccNUMA-based compute nodes.", "venue": "CHPC", "authors": ["Jan  Treibig", "Georg  Hager", "Gerhard  Wellein"], "year": 2010, "n_citations": 164}
{"id": 2738443, "s2_id": "e02861c4fed6dc98069c220808c49eb408ca75b6", "title": "Enabling highly-scalable remote memory access programming with MPI-3 One Sided", "abstract": "Modern interconnects offer remote direct memory access (RDMA) features. Yet, most applications rely on explicit message passing for communications albeit their unwanted overheads. The MPI-3.0 standard defines a programming interface for exploiting RDMA networks directly, however, it's scalability and practicability has to be demonstrated in practice. In this work, we develop scalable bufferless protocols that implement the MPI-3.0 specification. Our protocols support scaling to millions of cores with negligible memory consumption while providing highest performance and minimal overheads. To arm programmers, we provide a spectrum of performance models for all critical functions and demonstrate the usability of our library and models with several application studies with up to half a million processes. We show that our design is comparable to, or better than UPC and Fortran Coarrays in terms of latency, bandwidth and message rate. We also demonstrate application performance improvements with comparable programming complexity.", "venue": "Sci. Program.", "authors": ["Robert  Gerstenberger", "Maciej  Besta", "Torsten  Hoefler"], "year": 2014, "n_citations": 41}
{"id": 2738947, "s2_id": "3821b2732375c576ecc29562b6ea6d6862a81a93", "title": "High Performance Implementation of Boris Particle Pusher on DPC++. A First Look at oneAPI", "abstract": "New hardware architectures open up immense opportunities for supercomputer simulations. However, programming techniques for different architectures vary significantly, which leads to the necessity of developing and supporting multiple code versions, each being optimized for specific hardware features. The oneAPI framework, recently introduced by Intel, contains a set of programming tools for the development of portable codes that can be compiled and fine-tuned for CPUs, GPUs, FPGAs, and accelerators. In this paper, we report on the experience of porting the implementation of Boris particle pusher to oneAPI. Boris particle pusher is one of the most demanding computational stages of the Particle-in-Cell method, which, in particular, is used for supercomputer simulations of laser-plasma interactions. We show how to adapt the C++ implementation of the particle push algorithm from the Hi-Chi project to the DPC++ programming language and report the performance of the code on high-end Intel CPUs (Xeon Platinum 8260L) and Intel GPUs (P630 and Iris Xe Max). It turned out that our C++ code can be easily ported to DPC++. We found that on CPUs the resulting DPC++ code is only ~10% on average inferior to the optimized C++ code. Moreover, the code is compiled and run on new Intel GPUs without any specific optimizations and shows the expected performance, taking into account the parameters of the hardware.", "venue": "PaCT", "authors": ["Valentin  Volokitin", "Alexey  Bashinov", "Evgeny  Efimenko", "Arkady  Gonoskov", "Iosif  Meyerov"], "year": 2021, "n_citations": 0}
{"id": 2741979, "s2_id": "80ccbbb5be3ffd2d6e25dd42305cfc21fd62dd68", "title": "On the tradeoff of average delay and average power for fading point-to-point links with monotone policies", "abstract": "We consider a fading point-to-point link with packets arriving randomly at rate $\\lambda$ per slot to the transmitter queue. We assume that the transmitter can control the number of packets served in a slot by varying the transmit power for the slot. We restrict to transmitter scheduling policies that are monotone and stationary, i.e., the number of packets served is a non-decreasing function of the queue length at the beginning of the slot for every slot fade state. For such policies, we obtain asymptotic lower bounds for the minimum average delay of the packets, when average transmitter power is a small positive quantity $V$ more than the minimum average power required for transmitter queue stability. We show that the minimum average delay grows either to a finite value or as $\\Omega\\brap{\\log(1/V)}$ or $\\Omega\\brap{1/V}$ when $V \\downarrow 0$, for certain sets of values of $\\lambda$. These sets are determined by the distribution of fading gain, the maximum number of packets which can be transmitted in a slot, and the transmit power function of the fading gain and the number of packets transmitted that is assumed. We identify a case where the above behaviour of the tradeoff differs from that obtained from a previously considered approximate model, in which the random queue length process is assumed to evolve on the non-negative real line, and the transmit power function is strictly convex. We also consider a fading point-to-point link, where the transmitter, in addition to controlling the number of packets served, can also control the number of packets admitted in every slot. Our approach, which uses bounds on the stationary probability distribution of the queue length, also leads to an intuitive explanation of the asymptotic behaviour of average delay in the regime where $V \\downarrow 0$.", "venue": "ArXiv", "authors": ["Vineeth Bala Sukumaran", "Utpal  Mukherji"], "year": 2013, "n_citations": 0}
{"id": 2744942, "s2_id": "d051f0bead1c8e12d7e9a97f92f01c14d46d5dc4", "title": "A Stochastic Resource-Sharing Network for Electric Vehicle Charging", "abstract": "We consider a distribution grid used to charge electric vehicles (EVs) such that voltage drops stay bounded. We model this as a class of resource-sharing networks, known as bandwidth-sharing networks in the communication network literature. We focus on resource-sharing networks that are driven by a class of greedy control rules that can be implemented in a decentralized fashion. For a large number of such control rules, we can characterize the performance of the system by a fluid approximation. This leads to a set of dynamic equations that take into account the stochastic behavior of EVs. We show that the invariant point of these equations is unique and can be computed by solving a specific AC optimal-power-flow problem (ACOPF), which admits an exact convex relaxation. We illustrate our findings with a case study using the SCE 47-bus network and several special cases that allow for explicit computations.", "venue": "IEEE Transactions on Control of Network Systems", "authors": ["Angelos  Aveklouris", "Maria  Vlasiou", "Bert  Zwart"], "year": 2019, "n_citations": 11}
{"id": 2749308, "s2_id": "8d25340fdf6996898e4b3e6a035145700cb5ed9f", "title": "Learning-Aided Optimization for Energy-Harvesting Devices With Outdated State Information", "abstract": "This paper considers utility optimal power control for energy-harvesting wireless devices with a finite capacity battery. The distribution information of the underlying wireless environment and harvestable energy is unknown, and only outdated system state information is known at the device controller. This scenario shares similarity with Lyapunov opportunistic optimization and online learning but is different from both. By a novel combination of Zinkevich\u2019s online gradient learning technique and the drift-plus-penalty technique from Lyapunov opportunistic optimization, this paper proposes a learning-aided algorithm that achieves utility within <inline-formula> <tex-math notation=\"LaTeX\">$O(\\epsilon)$ </tex-math></inline-formula> of the optimal, for any desired <inline-formula> <tex-math notation=\"LaTeX\">$\\epsilon >0$ </tex-math></inline-formula>, by using a battery with an <inline-formula> <tex-math notation=\"LaTeX\">$O(1/\\epsilon)$ </tex-math></inline-formula> capacity. The proposed algorithm has low complexity and makes power investment decisions based on system history, without requiring knowledge of the system state or its probability distribution.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Hao  Yu", "Michael J. Neely"], "year": 2019, "n_citations": 5}
{"id": 2751349, "s2_id": "5f39d8bf94e79099fa26d540f5363c8435e82d47", "title": "Information ranking and power laws on trees", "abstract": "In this paper we consider the stochastic analysis of information ranking algorithms of large interconnected data sets, e.g. Google's PageRank algorithm for ranking pages on the World Wide Web. The stochastic formulation of the problem results in an equation of the form where N, Q, {R i } i\u22651, and {C, C i } i\u22651 are independent nonnegative random variables, the {C, C i } i\u22651 are identically distributed, and the {R i } i\u22651 are independent copies of stands for equality in distribution. We study the asymptotic properties of the distribution of R that, in the context of PageRank, represents the frequencies of highly ranked pages. The preceding equation is interesting in its own right since it belongs to a more general class of weighted branching processes that have been found to be useful in the analysis of many other algorithms. Our first main result shows that if ENE[C \u03b1] = 1, \u03b1 > 0, and Q, N satisfy additional moment conditions, then R has a power law distribution of index \u03b1. This result is obtained using a new approach based on an extension of Goldie's (1991) implicit renewal theorem. Furthermore, when N is regularly varying of index \u03b1 > 1, ENE[C \u03b1] < 1, and Q, C have higher moments than \u03b1, then the distributions of R and N are tail equivalent. The latter result is derived via a novel sample path large deviation method for recursive random sums. Similarly, we characterize the situation when the distribution of R is determined by the tail of Q. The preceding approaches may be of independent interest, as they can be used for analyzing other functionals on trees. We also briefly discuss the engineering implications of our results.", "venue": "Advances in Applied Probability", "authors": ["Predrag R. Jelenkovic", "Mariana  Olvera-Cravioto"], "year": 2010, "n_citations": 48}
{"id": 2752009, "s2_id": "4407c96b94cd3b2dfe5358bfb9a732ec159ccd0b", "title": "Reserved or On-Demand Instances? A Revenue Maximization Model for Cloud Providers", "abstract": "We examine the problem of managing a server farm in a way that attempts to maximize the net revenue earned by a cloud provider by renting servers to customers according to a typical Platform-as-a-Service model. The Cloud provider offers its resources to two classes of customers: 'premium' and 'basic'. Premium customers pay upfront fees to reserve servers for a specified period of time (e.g. a year). Premium customers can submit jobs for their reserved servers at any time and pay a fee for the server-hours they use. The provider is liable to pay a penalty every time a 'premium' job can not be executed due to lack of resources. On the other hand, 'basic' customers are served on a best-effort basis, and pay a server-hour fee that may be higher than the one paid by premium customers. The provider incurs energy costs when running servers. Hence, it has an incentive to turn off idle servers. The question of how to choose the number of servers to allocate to each pool (basic and premium) is answered by analyzing a suitable queuing model and maximizing a revenue function. Experimental results show that the proposed scheme adapts to different traffic conditions, penalty levels, energy costs and usage fees.", "venue": "2011 IEEE 4th International Conference on Cloud Computing", "authors": ["Michele  Mazzucco", "Marlon  Dumas"], "year": 2011, "n_citations": 32}
{"id": 2758473, "s2_id": "28210e3adaa5db27a00c0f7326cf1475be57d23a", "title": "Information model for model driven safety requirements management of complex systems", "abstract": "The aim of this paper is to propose a rigorous and complete design framework for complex system based on system engineering (SE) principles. The SE standard EIA-632 is used to guide the approach. Within this framework, two aspects are presented. The first one concerns the integration of safety requirements and management in system engineering process. The objective is to help designers and engineers in managing safety of complex systems. The second aspect concerns model driven design through the definition of an information model. This model is based on SysML (System Modeling Language) to address requirements definition and their traceability towards the solution and the Verification and Validation (V&V) elements.", "venue": "ArXiv", "authors": ["Romaric  Guillerm", "Hamid  Demmou", "Nabil  Sadou"], "year": 2012, "n_citations": 11}
{"id": 2760102, "s2_id": "8ca9b31b957a8bf45b27d9caeb93b91437d50571", "title": "Performance Analysis of the Kahan-Enhanced Scalar Product on Current Multicore Processors", "abstract": "We investigate the performance characteristics of a numerically enhanced scalar product (dot) kernel loop that uses the Kahan algorithm to compensate for numerical errors, and describe efficient SIMD-vectorized implementations on recent Intel processors. Using low-level instruction analysis and the execution-cache-memory (ECM) performance model we pinpoint the relevant performance bottlenecks for single-core and thread-parallel execution, and predict performance and saturation behavior. We show that the Kahan-enhanced scalar product comes at almost no additional cost compared to the naive (non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD vectorization and unrolling, are applied. We also investigate the impact of architectural changes across four generations of Intel Xeon processors.", "venue": "PPAM", "authors": ["Johannes  Hofmann", "Dietmar  Fey", "Michael  Riedmann", "Jan  Eitzinger", "Georg  Hager", "Gerhard  Wellein"], "year": 2015, "n_citations": 11}
{"id": 2762592, "s2_id": "f47bfa1a70e7b46a9826070d21c12e8bf2d8e762", "title": "The Universe at extreme scale: Multi-petaflop sky simulation on the BG/Q", "abstract": "Remarkable observational advances have established a compelling cross-validated model of the Universe. Yet, two key pillars of this model -- dark matter and dark energy -- remain mysterious. Next-generation sky surveys will map billions of galaxies to explore the physics of the 'Dark Universe'. Science requirements for these surveys demand simulations at extreme scales; these will be delivered by the HACC (Hybrid/Hardware Accelerated Cosmology Code) framework. HACC's novel algorithmic structure allows tuning across diverse architectures, including accelerated and multi-core systems. On the IBM BG/Q, HACC attains unprecedented scalable performance - currently 6.23 PFlops at 62% of peak and 92% parallel efficiency on 786,432 cores (48 racks) - at extreme problem sizes with up to almost two trillion particles, larger than any cosmological simulation yet performed. HACC simulations at these scales will for the first time enable tracking individual galaxies over the entire volume of a cosmological survey.", "venue": "2012 International Conference for High Performance Computing, Networking, Storage and Analysis", "authors": ["Salman  Habib", "Vitali A. Morozov", "Hal  Finkel", "Adrian  Pope", "Katrin  Heitmann", "Kalyan  Kumaran", "Tom  Peterka", "Joseph A. Insley", "David  Daniel", "Patricia K. Fasel", "Nicholas  Frontiere", "Zarija  Lukic"], "year": 2012, "n_citations": 65}
{"id": 2762940, "s2_id": "ca82af027ce43153e4ff88d13a7bebd9de378995", "title": "Pareto-Optimization Framework for Automated Network-on-Chip Design", "abstract": "With the advent of multi-core processors, network-on-chip design has been key in addressing network performances, such as bandwidth, power consumption, and communication delays when dealing with on-chip communication between the increasing number of processor cores. As the numbers of cores increase, network design becomes more complex. Therefore, there is a critical need in soliciting computer aid in determining network configurations that afford optimal performance given resources and design constraints. We propose a Pareto-optimization framework that explores the space of possible network configurations to determine optimal network latencies, power consumption, and the corresponding link allocations. For a given number of routers, average network latency and power consumption as example performance objectives can be displayed in form of Pareto-optimal fronts, thus not only offering a design tool, but also enabling trade-off studies.", "venue": "ArXiv", "authors": ["Tzyy-Juin  Kao", "Wolfgang  Fink"], "year": 2018, "n_citations": 0}
{"id": 2763865, "s2_id": "76b70ed063425b0a0afae0e6bbe889cd6d8ee498", "title": "Dealing with Zero Density Using Piecewise Phase-Type Approximation", "abstract": "Every probability distribution can be approximated up to a given precision by a phase-type distribution, i.e. a distribution encoded by a continuous time Markov chain (CTMC). However, an excessive number of states in the corresponding CTMC is needed for some standard distributions, in particular most distributions with regions of zero density such as uniform or shifted distributions. Addressing this class of distributions, we suggest an alternative representation by CTMC extended with discrete-time transitions. Using discrete-time transitions we split the density function into multiple intervals. Within each interval, we then approximate the density with standard phase-type fitting. We provide an experimental evidence that our method requires only a moderate number of states to approximate such distributions with regions of zero density. Furthermore, the usage of CTMC with discrete-time transitions is supported by a number of techniques for their analysis. Thus, our results promise an efficient approach to the transient analysis of a class of non-Markovian models.", "venue": "EPEW", "authors": ["Lubos  Korenciak", "Jan  Krc\u00e1l", "Vojtech  Reh\u00e1k"], "year": 2014, "n_citations": 9}
{"id": 2764607, "s2_id": "c69cbdc394467bc48e3dd6b9d521a31e680bf18f", "title": "Characterising Across-Stack Optimisations for Deep Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are extremely computationally demanding, presenting a large barrier to their deployment on resource-constrained devices. Since such systems are where some of their most useful applications lie (e.g. obstacle detection for mobile robots, vision-based medical assistive technology), significant bodies of work from both machine learning and systems communities have attempted to provide optimisations that will make CNNs available to edge devices. In this paper we unify the two viewpoints in a Deep Learning Inference Stack and take an across-stack approach by implementing and evaluating the most common neural network compression techniques (weight pruning, channel pruning, and quantisation) and optimising their parallel execution with a range of programming approaches (OpenMP, OpenCL) and hardware architectures (CPU, GPU). We provide comprehensive Pareto curves to instruct trade-offs under constraints of accuracy, execution time, and memory space.", "venue": "2018 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Jack  Turner", "Jos\u00e9  Cano", "Valentin  Radu", "Elliot  Crowley", "Michael  O'Boyle", "Amos J. Storkey"], "year": 2018, "n_citations": 23}
{"id": 2771468, "s2_id": "210ae7a5ad558f3c3128feba5603a342333df395", "title": "A Monte-Carlo approach to lifespan failure performance analysis of the network fabric in modular data centers", "abstract": "Data centers have been evolved from a passive element of compute infrastructure to become an active, core part of any ICT solution. In particular, modular data centers (MDCs), which are a promising design approach to improve resiliency of data centers, can play a key role in deploying ICT infrastructure in remote and inhospitable environments in order to take advantage of low temperatures and hydro- and wind-electric capabilities. This is because of capability of the modular data centers to survive even in lack of continuous on-site maintenance and support. The most critical part of a data center is its network fabric that could impede the whole system even if all other components are fully functional, assuming that other analyses have been already performed to ensure the reliability of the underlying infrastructure and support systems. In this work, a complete failure analysis of modular data centers using failure models of various components including servers, switches, and links is performed using a proposed Monte-Carlo approach. The proposed Monte-Carlo approach, which is based on the concept of snapshots, allows us to effectively calculate the performance of a design along its lifespan even up to the terminal stages. To show the capabilities of the proposed approach, various network topologies, such as FatTree, BCube, MDCube, and their modifications, are considered. The performance and also the lifespan of each topology design in the presence of failures of their components are studied against the topology parameters.", "venue": "J. Netw. Comput. Appl.", "authors": ["Reza Farrahi Moghaddam", "Vahid  Asghari", "Fereydoun Farrahi Moghaddam", "Yves  Lemieux", "Mohamed  Cheriet"], "year": 2017, "n_citations": 3}
{"id": 2772028, "s2_id": "28300a64994af6936543905af7bc996083e21bec", "title": "SoS-RPL: Securing Internet of Things Against Sinkhole Attack Using RPL Protocol-Based Node Rating and Ranking Mechanism", "abstract": "Through the Internet of Things (IoT) the internet scope is established by the integration of physical things to classify themselves into mutual things. A physical thing can be created by this inventive perception to signify itself in the digital world. Regarding the physical things that are related to the internet, it is worth noting that considering numerous theories and upcoming predictions, they mostly require protected structures, moreover, they are at risk of several attacks. IoTs are endowed with particular routing disobedience called sinkhole attack owing to their distributed features. In these attacks, a malicious node broadcasts illusive information regarding the routings to impose itself as a route towards specific nodes for the neighboring nodes and thus, attract data traffic. RPL (IP-V6 routing protocol for efficient and low-energy networks) is a standard routing protocol which is mainly employed in sensor networks and IoT. This protocol is called SoS-RPL consisting of two key sections of the sinkhole detection. In the first section rating and ranking the nodes in the RPL is carried out based on distance measurements. The second section is in charge of discovering the misbehavior sources within the IoT network through, the Average Packet Transmission RREQ (APT-RREQ). Here, the technique is assessed through wide simulations performed within the NS-3 environment. Based on the results of the simulation, it is indicated that the IoT network behavior metrics are enhanced based on the detection rate, false-negative rate, false-positive rate, packet delivery rate, maximum throughput, and packet loss rate.", "venue": "Wirel. Pers. Commun.", "authors": ["Mina  Zaminkar", "Reza  Fotohi"], "year": 2020, "n_citations": 27}
{"id": 2772247, "s2_id": "03fcaca9ecd29513aaa9e32842328918fa9c30cf", "title": "Solid State Disk Object-Based Storage with Trim Commands", "abstract": "This paper presents a model of NAND flash SSD utilization and write amplification when the ATA/ATAPI SSD Trim command is incorporated into object-based storage under a variety of user workloads, including a uniform random workload with objects of fixed size and a uniform random workload with objects of varying sizes. We first summarize the existing models for write amplification in SSDs for workloads with and without the Trim command, then propose an alteration of the models that utilizes a framework of object-based storage. The utilization of objects and pages in the SSD is derived, with the analytic results compared to simulation. Finally, the effect of objects on write amplification and its computation is discussed along with a potential application to optimization of SSD usage through object storage metadata servers that allocate object classes of distinct object size.", "venue": "ArXiv", "authors": ["Tasha  Frankie", "Gordon F. Hughes", "Kenneth  Kreutz-Delgado"], "year": 2012, "n_citations": 2}
{"id": 2775031, "s2_id": "ab3c3ccd277777a7614a364c02b4967eee00ffbf", "title": "Do Energy-Oriented Changes Hinder Maintainability?", "abstract": "Energy efficiency is a crucial quality requirement for mobile applications. However, improving energy efficiency is far from trivial as developers lack the knowledge and tools to aid in this activity. In this paper we study the impact of changes to improve energy efficiency on the maintainability of Android applications. Using a dataset containing 539 energy efficiency-oriented commits, we measure maintainability \u2013 as computed by the Software Improvement Group's web-based source code analysis service Better Code Hub (BCH) \u2013 before and after energy efficiency-related code changes. Results show that in general improving energy efficiency comes with a significant decrease in maintainability. This is particularly evident in code changes to accommodate the Power Save Mode and Wakelock Addition energy patterns. In addition, we perform manual analysis to assess how real examples of energy-oriented changes affect maintainability. Our results help mobile app developers to 1) avoid common maintainability issues when improving the energy efficiency of their apps; and 2) adopt development processes to build maintainable and energy-efficient code. We also support researchers by identifying challenges in mobile app development that still need to be addressed.", "venue": "2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)", "authors": ["Luis  Cruz", "Rui  Abreu", "John  Grundy", "Li  Li", "Xin  Xia"], "year": 2019, "n_citations": 14}
{"id": 2779443, "s2_id": "34cde103ef326e64497b5007523401222d08cc0c", "title": "AutoTune: Improving End-to-end Performance and Resource Efficiency for Microservice Applications", "abstract": "Most large web-scale applications are now built by composing collections (from a few up to 100s or 1000s) of microservices. Operators need to decide how many resources are allocated to each microservice, and these allocations can have a large impact on application performance. Manually determining allocations that are both cost-efficient and meet performance requirements is challenging, even for experienced operators. In this paper we present AutoTune, an end-to-end tool that automatically minimizes resource utilization while maintaining good application performance.", "venue": "ArXiv", "authors": ["Michael Alan Chang", "Aurojit  Panda", "Hantao  Wang", "Yuancheng  Tsai", "Rahul  Balakrishnan", "Scott  Shenker"], "year": 2021, "n_citations": 0}
{"id": 2785021, "s2_id": "fd5fffdeb280cc0511e913164174e428b52d9a01", "title": "Lower Bound on the BER of a Decode-and-Forward Relay Network Under Chaos Shift Keying Communication System", "abstract": "This paper carries out the first-ever investigation of the analysis of a cooperative Decode-and-Forward (DF) relay network with Chaos Shift Keying (CSK) modulation. The performance analysis of DF-CSK in this paper takes into account the dynamical nature of chaotic signal, which is not similar to a conventional binary modulation performance computation methodology. The expression of a lower bound bit error rate (BER) is derived in order to investigate the performance of the cooperative system under independently and identically distributed (i.i.d.) Gaussian fading wireless environments. The effect of the non-periodic nature of chaotic sequence leading to a non constant bit energy of the considered modulation is also investigated. A computation approach of the BER expression based on the probability density function of the bit energy of the chaotic sequence, channel distribution, and number of relays is presented. Simulation results prove the accuracy of our BER computation methodology.", "venue": "ArXiv", "authors": ["Georges  Kaddoum", "Fran\u00e7ois  Gagnon"], "year": 2013, "n_citations": 1}
{"id": 2790965, "s2_id": "726b0cdf9dd9f17b0006b0170a98607581b5c41d", "title": "Catalog dynamics: Impact of content publishing and perishing on the performance of a LRU cache", "abstract": "The Internet heavily relies on Content Distribution Networks and transparent caches to cope with the ever-increasing traffic demand of users. Content, however, is essentially versatile: once published at a given time, its popularity vanishes over time. All requests for a given document are then concentrated between the publishing time and an effective perishing time. In this paper, we propose a new model for the arrival of content requests, which takes into account the dynamical nature of the content catalog. Based on two large traffic traces collected on the Orange network, we use the semi-experimental method and determine invariants of the content request process. This allows us to define a simple mathematical model for content requests; by extending the so-called \u201cChe approximation\u201d, we then compute the performance of a LRU cache fed with such a request process, expressed by its hit ratio. We numerically validate the good accuracy of our model by comparison to trace-based simulation.", "venue": "2014 26th International Teletraffic Congress (ITC)", "authors": ["Felipe  Olmos", "Bruno  Kauffmann", "Alain  Simonian", "Yannick  Carlinet"], "year": 2014, "n_citations": 36}
{"id": 2791747, "s2_id": "c10ac1da715dedcdff94a53f3d54b94d8b69abdd", "title": "Improving the Performance of a NoC-based CNN Accelerator with Gather Support", "abstract": "The increasing application of deep learning technology drives the need for an efficient parallel computing architecture for Convolutional Neural Networks (CNNs). A significant challenge faced when designing a many-core CNN accelerator is to handle the data movement between the processing elements. The CNN workload introduces many-to-one traffic in addition to one-to-one and one-to-many traffic. As the de-facto standard for on-chip communication, Network-on-Chip (NoC) can support various unicast and multicast traffic. For many-to-one traffic, repetitive unicast is employed which is not an efficient way. In this paper, we propose to use the gather packet on mesh-based NoCs employing output stationary systolic array in support of many-to-one traffic. The gather packet will collect the data from the intermediate nodes eventually leading to the destination efficiently. This method is evaluated using the traffic traces generated from the convolution layer of AlexNet and VGG-16 with improvement in the latency and power than the repetitive unicast method.", "venue": "2020 IEEE 33rd International System-on-Chip Conference (SOCC)", "authors": ["Binayak  Tiwari", "Mei  Yang", "Xiaohang  Wang", "Yingtao  Jiang", "Venkatesan  Muthukumar"], "year": 2020, "n_citations": 1}
{"id": 2794454, "s2_id": "839f001725e45bbde1d7086f32c810f9526795e1", "title": "Domain-Specialized Cache Management for Graph Analytics", "abstract": "Graph analytics power a range of applications in areas as diverse as finance, networking and business logistics. A common property of graphs used in the domain of graph analytics is a power-law distribution of vertex connectivity, wherein a small number of vertices are responsible for a high fraction of all connections in the graph. These richly-connected, hot, vertices inherently exhibit high reuse. However, this work finds that state-of-the-art hardware cache management schemes struggle in capitalizing on their reuse due to highly irregular access patterns of graph analytics. In response, we propose GRASP, domain-specialized cache management at the last-level cache for graph analytics. GRASP augments existing cache policies to maximize reuse of hot vertices by protecting them against cache thrashing, while maintaining sufficient flexibility to capture the reuse of other vertices as needed. GRASP keeps hardware cost negligible by leveraging lightweight software support to pinpoint hot vertices, thus eliding the need for storage-intensive prediction mechanisms employed by state-of-the-art cache management schemes. On a set of diverse graph-analytic applications with large high-skew graph datasets, GRASP outperforms prior domain-agnostic schemes on all datapoints, yielding an average speed-up of 4.2% (max 9.4%) over the best-performing prior scheme. GRASP remains robust on low-/no-skew datasets, whereas prior schemes consistently cause a slowdown.", "venue": "2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Priyank  Faldu", "Jeff  Diamond", "Boris  Grot"], "year": 2020, "n_citations": 11}
{"id": 2794630, "s2_id": "67178234ec25099dc789b1b359aaab40bc23c063", "title": "Cultivating Software Performance in Cloud Computing", "abstract": "There exist multitudes of cloud performance metrics, including workload performance, application placement, software/hardware optimization, scalability, capacity, reliability, agility and so on. In this paper, we consider jointly optimizing the performance of the software applications in the cloud. The challenges lie in bringing a diversity of raw data into tidy data format, unifying performance data from multiple systems based on timestamps, and assessing the quality of the processed performance data. Even after verifying the quality of cloud performance data, additional challenges block optimizing cloud computing. In this paper, we identify the challenges of cloud computing from the perspectives of computing environment, data collection, performance analytics and production environment.", "venue": "ArXiv", "authors": ["Li  Chen", "Colin  Cunningham", "Pooja  Jain", "Chenggang  Qin", "Kingsum  Chow"], "year": 2016, "n_citations": 2}
{"id": 2798137, "s2_id": "b513711621e81d0abd042e0877ca751581a993f5", "title": "GraphMat: High performance graph analytics made productive", "abstract": "Given the growing importance of large-scale graph analytics, there is a need to improve the performance of graph analysis frameworks without compromising on productivity. GraphMat is our solution to bridge this gap between a user-friendly graph analytics framework and native, hand-optimized code. GraphMat functions by taking vertex programs and mapping them to high performance sparse matrix operations in the backend. We thus get the productivity benefits of a vertex programming framework without sacrificing performance. GraphMat is a single-node multicore graph framework written in C++ which has enabled us to write a diverse set of graph algorithms with the same effort compared to other vertex programming frameworks. GraphMat performs 1.1-7X faster than high performance frameworks such as GraphLab, CombBLAS and Galois. GraphMat also matches the performance of MapGraph, a GPU-based graph framework, despite running on a CPU platform with significantly lower compute and bandwidth resources. It achieves better multicore scalability (13-15X on 24 cores) than other frameworks and is 1.2X off native, hand-optimized code on a variety of graph algorithms. Since GraphMat performance depends mainly on a few scalable and well-understood sparse matrix operations, GraphMat can naturally benefit from the trend of increasing parallelism in future hardware.", "venue": "Proc. VLDB Endow.", "authors": ["Narayanan  Sundaram", "Nadathur  Satish", "Md. Mostofa Ali Patwary", "Subramanya  Dulloor", "Michael J. Anderson", "Satya Gautam Vadlamudi", "Dipankar  Das", "Pradeep  Dubey"], "year": 2015, "n_citations": 252}
{"id": 2802201, "s2_id": "6b8befb91e4b526ba0c6402954cc5f8697d84716", "title": "Extreme Scale FMM-Accelerated Boundary Integral Equation Solver for Wave Scattering", "abstract": "Algorithmic and architecture-oriented optimizations are essential for achieving performance worthy of anticipated energy-austere exascale systems. In this paper, we present an extreme scale FMM-accelerated boundary integral equation solver for wave scattering, which uses FMM as a matrix-vector multiplication inside the GMRES iterative method. Our FMM Helmholtz kernels treat nontrivial singular and near-field integration points. We implement highly optimized kernels for both shared and distributed memory, targeting emerging Intel extreme performance HPC architectures. We extract the potential thread- and data-level parallelism of the key Helmholtz kernels of FMM. Our application code is well optimized to exploit the AVX-512 SIMD units of Intel Skylake and Knights Landing architectures. We provide different performance models for tuning the task-based tree traversal implementation of FMM, and develop optimal architecture-specific and algorithm aware partitioning, load balancing, and communication reducing mechanisms to scale up to 6,144 compute nodes of a Cray XC40 with 196,608 hardware cores. With shared memory optimizations, we achieve roughly 77% of peak single precision floating point performance of a 56-core Skylake processor, and on average 60% of peak single precision floating point performance of a 72-core KNL. These numbers represent nearly 5.4x and 10x speedup on Skylake and KNL, respectively, compared to the baseline scalar code. With distributed memory optimizations, on the other hand, we report near-optimal efficiency in the weak scalability study with respect to both the logarithmic communication complexity as well as the theoretical scaling complexity of FMM. In addition, we exhibit up to 85% efficiency in strong scaling. We compute in excess of 2 billion DoF on the full-scale of the Cray XC40 supercomputer.", "venue": "SIAM J. Sci. Comput.", "authors": ["Mustafa Abdul Jabbar", "Mohammed A. Al Farhan", "Noha  Al-Harthi", "Rui  Chen", "Rio  Yokota", "Hakan  Bagci", "David E. Keyes"], "year": 2019, "n_citations": 12}
{"id": 2803240, "s2_id": "59e1cde7c888b9b6419716cf8ef79adc7fc06bd1", "title": "Delay Evaluation of OpenFlow Network Based on Queueing Model", "abstract": "As one of the most popular south-bound protocol of software-defined networking(SDN), OpenFlow decouples the network control from forwarding devices. It offers flexible and scalable functionality for networks. These advantages may cause performance issues since there are performance penalties in terms of packet processing speed. It is important to understand the performance of OpenFlow switches and controllers for its deployments. In this paper we model the packet processing time of OpenFlow switches and controllers. We mainly analyze how the probability of packet-in messages impacts the performance of switches and controllers. Our results show that there is a performance penalty in OpenFlow networks. However, the penalty is not much when probability of packet-in messages is low. This model can be used for a network designer to approximate the performance of her deployments.", "venue": "ArXiv", "authors": ["Zhihao  Shang", "Katinka  Wolter"], "year": 2016, "n_citations": 16}
{"id": 2804080, "s2_id": "8f65d389422dcefd1701478a8a817e09af4f2273", "title": "Queueing systems with renovation vs. queues with RED. Supplementary Material", "abstract": "In this note we consider M/D/1/N queue with renovation and derive analytic expressions for the following performance characteristics: stationary loss rate, moments of the number in the system. Moments of consecutive losses, waiting/sojourn time are out of scope. The motivation for studying these characteristics is in the comparison of renovation with known active queue mechanisms like RED.", "venue": "ArXiv", "authors": ["Mikhail  Konovalov", "Rostislav  Razumchik"], "year": 2017, "n_citations": 3}
{"id": 2805750, "s2_id": "bf0113d13e4300d2bb1f12a3b66c232bc072675b", "title": "Non-Asymptotic Delay Bounds for Multi-Server Systems with Synchronization Constraints", "abstract": "Parallel computing has become a standard tool with architectures such as Google MapReduce, Hadoop, and Spark being broadly used in applications such as data processing and machine learning. Common to these systems are a fork operation, where jobs are first divided into tasks that are processed in parallel, and a join operation where completed tasks wait for the other tasks of the job before leaving the system. The synchronization constraint of the join operation makes the analysis of fork-join systems challenging, and few explicit results are known. In this work, we formulate a max-plus server model for parallel systems which allows us to derive performance bounds for a variety of systems in the GI<inline-formula><tex-math notation=\"LaTeX\">$\\mid$</tex-math><alternatives> <inline-graphic xlink:href=\"fidler-ieq1-2779872.gif\"/></alternatives></inline-formula>GI and G<inline-formula> <tex-math notation=\"LaTeX\">$\\mid$</tex-math><alternatives><inline-graphic xlink:href=\"fidler-ieq2-2779872.gif\"/> </alternatives></inline-formula>G cases. We contribute end-to-end delay bounds for multi-stage fork-join networks. We perform a detailed comparison of different multi-server configurations, including an analysis of single-queue fork-join systems that achieve a fundamental performance gain. We compare these results to both simulation and a live Spark system.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Markus  Fidler", "Brenton  Walker", "Yuming  Jiang"], "year": 2018, "n_citations": 12}
{"id": 2806595, "s2_id": "83230bc70f323814e549b4f52f3f43712bcd9f7c", "title": "LAMMPS' PPPM Long-Range Solver for the Second Generation Xeon Phi", "abstract": "Molecular Dynamics is an important tool for computational biologists, chemists, and materials scientists, consuming a sizable amount of supercomputing resources. Many of the investigated systems contain charged particles, which can only be simulated accurately using a long-range solver, such as PPPM. We extend the popular LAMMPS molecular dynamics code with an implementation of PPPM particularly suitable for the second generation Intel Xeon Phi. Our main target is the optimization of computational kernels by means of vectorization, and we observe speedups in these kernels of up to 12\\(\\times \\). These improvements carry over to LAMMPS users, with overall speedups ranging between 2\u20133\\(\\times \\), without requiring users to retune input parameters. Furthermore, our optimizations make it easier for users to determine optimal input parameters for attaining top performance.", "venue": "ISC", "authors": ["William  McDoniel", "Markus  H\u00f6hnerbach", "Rodrigo  Canales", "Ahmed E. Ismail", "Paolo  Bientinesi"], "year": 2017, "n_citations": 8}
{"id": 2808938, "s2_id": "8b88c45fe8a0925dac971dec3d0a9068fff83524", "title": "Inferring Unobserved Events in Systems With Shared Resources and Queues", "abstract": "To identify the causes of performance problems or to predict process behavior, it is essential to have correct and complete event data. This is particularly important for distributed systems with shared resources, e.g., one case can block another case competing for the same machine, leading to inter-case dependencies in performance. However, due to a variety of reasons, real-life systems often record only a subset of all events taking place. To understand and analyze the behavior and performance of processes with shared resources, we aim to reconstruct bounds for timestamps of events in a case that must have happened but were not recorded by inference over events in other cases in the system. We formulate and solve the problem by systematically introducing multi-entity concepts in event logs and process models. We introduce a partial-order based model of a multi-entity event log and a corresponding compositional model for multientity processes. We define PQR-systems as a special class of multi-entity processes with shared resources and queues. We then study the problem of inferring from an incomplete event log unobserved events and their timestamps that are globally consistent with a PQR-system. We solve the problem by reconstructing unobserved traces of resources and queues according to the PQR-model and derive bounds for their timestamps using a linear program. While the problem is illustrated for material handling systems like baggage handling systems in airports, the approach *Address for correspondence: TU Eindhoven, PO Box 513, 5600MB Eindhoven, NL ar X iv :2 10 3. 00 16 7v 3 [ cs .D C ] 9 D ec 2 02 1 204 D. Fahland et al. / Repairing Event Logs of Systems with Shared Resources can be applied to other settings where recording is incomplete. The ideas have been implemented in ProM and were evaluated using both synthetic and real-life event logs.", "venue": "ArXiv", "authors": ["Dirk  Fahland", "Vadim  Denisov", "Wil. M.P. van der Aalst"], "year": 2021, "n_citations": 0}
{"id": 2814518, "s2_id": "e9dc6cf8f09430c96a95cc2951ac06962bd29410", "title": "A Survey on Delay-Aware Resource Control for Wireless Systems\u2014Large Deviation Theory, Stochastic Lyapunov Drift, and Distributed Stochastic Learning", "abstract": "In this paper, a comprehensive survey is given on several major systematic approaches in dealing with delay-aware control problems, namely the equivalentrate constraint approach, the Lyapunov stability drift approach, and the approximate Markov decision process approach using stochastic learning. These approaches essentially embrace most of the existing literature regarding delay-aware resource control in wireless systems. They have their relative pros and cons in terms of performance, complexity, and implementation issues. For each of the approaches, the problem setup, the general solution, and the design methodology are discussed. Applications of these approaches to delay-aware resource allocation are illustrated with examples in single-hop wireless networks. Furthermore, recent results regarding delay-aware multihop routing designs in general multihop networks are elaborated. Finally, the delay performances of various approaches are compared through simulations using an example of the uplink OFDMA systems.", "venue": "IEEE Transactions on Information Theory", "authors": ["Ying  Cui", "Vincent K. N. Lau", "Rui  Wang", "Huang  Huang", "Shunqing  Zhang"], "year": 2012, "n_citations": 174}
{"id": 2814894, "s2_id": "6364f8b5b38556a4ca081f9b628cfd180dd81433", "title": "Population network structure impacts genetic algorithm optimisation performance", "abstract": "A genetic algorithm (GA) is a search method that optimises a population of solutions by simulating natural evolution. Good solutions reproduce together to create better candidates. The standard GA assumes that any two solutions can mate. However, in nature and social contexts, social networks can condition the likelihood that two individuals mate. This impact of population network structure over GAs performance is unknown. Here we introduce the Networked Genetic Algorithm (NGA) to evaluate how various random and scale-free population networks influence the optimisation performance of GAs on benchmark functions. We show evidence of significant variations in performance of the NGA as the network varies. In addition, we find that the best-performing population networks, characterised by intermediate density and low average shortest path length, significantly outperform the standard complete network GA. These results may constitute a starting point for network tuning and network control: seeing the network structure of the population as a parameter that can be tuned to improve the performance of evolutionary algorithms, and offer more realistic modelling of social learning. 1", "venue": "GECCO Companion", "authors": ["Aymeric  Vie"], "year": 2021, "n_citations": 0}
{"id": 2816682, "s2_id": "4a4a6825e191e485ab33c5d309ba1a444e3c7d54", "title": "Greening File Distribution: Centralized or Distributed?", "abstract": "Despite file-distribution applications are responsible for a major portion of the current Internet traffic, so far little effort has been dedicated to study file distribution from the point of view of energy efficiency. In this paper, we present a first approach at the problem of energy efficiency for file distribution. Specifically, we first demonstrate that the general problem of minimizing energy consumption in file distribution in heterogeneous settings is NP-hard. For homogeneous settings, we derive tight lower bounds on energy consumption, and we design a family of algorithms that achieve these bounds. Our results prove that collaborative p2p schemes achieve up to 50% energy savings with respect to the best available centralized file distribution scheme. Through simulation, we demonstrate that in more realistic cases (e.g., considering network congestion, and link variability across hosts) we validate this observation, since our collaborative algorithms always achieve significant energy savings with respect to the power consumption of centralized file distribution systems.", "venue": "ArXiv", "authors": ["Kshitiz  Verma", "Gianluca  Rizzo", "Antonio  Fern\u00e1ndez", "Rub\u00e9n Cuevas Rum\u00edn", "Arturo  Azcorra"], "year": 2011, "n_citations": 1}
{"id": 2817807, "s2_id": "fc91dc259d59f11e9a013359d97e7d3529f088ab", "title": "Time parallel gravitational collapse simulation", "abstract": "This article demonstrates the applicability of the parallel-in-time method Parareal to the numerical solution of the Einstein gravity equations for the spherical collapse of a massless scalar eld. To account for the shrinking of the spatial domain in time, a tailored load balancing scheme is proposed and compared to load balancing based on number of time steps alone. The performance of Parareal is studied for both the sub-critical and black hole case; our experiments show that Parareal generates substantial speedup and, in the super-critical regime, can reproduce Choptuik's black hole mass scaling law.", "venue": "ArXiv", "authors": ["Andreas  Kreienbuehl", "Pietro  Benedusi", "Daniel  Ruprecht", "Rolf  Krause"], "year": 2015, "n_citations": 6}
{"id": 2824342, "s2_id": "b9ab688b5efb12057b8965082cae1190e5699bbb", "title": "Asymptotically Optimal Load Balancing Topologies", "abstract": "We consider a system of N servers inter-connected by some underlying graph topology GN. Tasks with unit-mean exponential processing times arrive at the various servers as independent Poisson processes of rate \u03bb. Each incoming task is irrevocably assigned to whichever server has the smallest number of tasks among the one where it appears and its neighbors in GN. The above model arises in the context of load balancing in large-scale cloud networks and data centers, and has been extensively investigated in the case GN is a clique. Since the servers are exchangeable in that case, mean-field limits apply, and in particular it has been proved that for any \u03bb < 1, the fraction of servers with two or more tasks vanishes in the limit as N \u2192 \u221e. For an arbitrary graph GN, mean-field techniques break down, complicating the analysis, and the queue length process tends to be worse than for a clique. Accordingly, a graph GN is said to be N-optimal or \u221aN-optimal when the queue length process on GN is equivalent to that on a clique on an N-scale or \u221aN-scale, respectively. We prove that if GN is an Erd\u00f6o s-R\u00e9nyi random graph with average degree d(N), then with high probability it is N-optimal and \u221aN-optimal if d(N) \u2192 \u221e and d(N)/\u221aNlog(N)) \u2192 \u221e as N \u2192 \u221e, respectively. This demonstrates that optimality can be maintained at N-scale and \u221aN-scale while reducing the number of connections by nearly a factor N and \u221aN/log(N) compared to a clique, provided the topology is suitably random. It is further shown that if GN contains \u0398(N) bounded-degree nodes, then it cannot be N-optimal. In addition, we establish that an arbitrary graph GN is N-optimal when its minimum degree is N - o(N), and may not be N-optimal even when its minimum degree is cN + o(N) for any 0 < c < 1/2. Simulation experiments are conducted for various scenarios to corroborate the asymptotic results.", "venue": "ArXiv", "authors": ["Debankur  Mukherjee", "Sem C. Borst", "Johan van Leeuwaarden"], "year": 2017, "n_citations": 28}
{"id": 2824360, "s2_id": "8902eacb075244e68a3871cba0c521c90fbfe560", "title": "On the parallel I/O optimality of linear algebra kernels: near-optimal matrix factorizations", "abstract": "Matrix factorizations are among the most important building blocks of scientific computing. However, state-of-the-art libraries are not communication-optimal, underutilizing current parallel architectures. We present novel algorithms for Cholesky and LU factorizations that utilize an asymptotically communication-optimal 2.5D decomposition. We first establish a theoretical framework for deriving parallel I/O lower bounds for linear algebra kernels, and then utilize its insights to derive Cholesky and LU schedules, both communicating [EQUATION] elements per processor, where M is the local memory size. The empirical results match our theoretical analysis: our implementations communicate significantly less than Intel MKL, SLATE, and the asymptotically communication-optimal CANDMC and CAPITAL libraries. Our code outperforms these state-of-the-art libraries in almost all tested scenarios, with matrix sizes ranging from 2,048 to 524,288 on up to 512 CPU nodes of the Piz Daint supercomputer, decreasing the time-to-solution by up to three times. Our code is ScaLAPACK-compatible and available as an open-source library.", "venue": "SC", "authors": ["Grzegorz  Kwasniewski", "Marko  Kabi'c", "Tal  Ben-Nun", "Alexandros Nikolaos Ziogas", "Jens Eirik Saethre", "Andr'e  Gaillard", "Timo  Schneider", "Maciej  Besta", "Anton  Kozhevnikov", "Joost  VandeVondele", "Torsten  Hoefler"], "year": 2021, "n_citations": 1}
{"id": 2825192, "s2_id": "416a3348da1114e26171a50694b66f8c35024571", "title": "Flare: Native Compilation for Heterogeneous Workloads in Apache Spark", "abstract": "The need for modern data analytics to combine relational, procedural, and map-reduce-style functional processing is widely recognized. State-of-the-art systems like Spark have added SQL front-ends and relational query optimization, which promise an increase in expressiveness and performance. But how good are these extensions at extracting high performance from modern hardware platforms? \nWhile Spark has made impressive progress, we show that for relational workloads, there is still a significant gap compared with best-of-breed query engines. And when stepping outside of the relational world, query optimization techniques are ineffective if large parts of a computation have to be treated as user-defined functions (UDFs). \nWe present Flare: a new back-end for Spark that brings performance closer to the best SQL engines, without giving up the added expressiveness of Spark. We demonstrate order of magnitude speedups both for relational workloads such as TPC-H, as well as for a range of machine learning kernels that combine relational and iterative functional processing. \nFlare achieves these results through (1) compilation to native code, (2) replacing parts of the Spark runtime system, and (3) extending the scope of optimization and code generation to large classes of UDFs.", "venue": "ArXiv", "authors": ["Gr\u00e9gory M. Essertel", "Ruby Y. Tahboub", "James M. Decker", "Kevin J. Brown", "Kunle  Olukotun", "Tiark  Rompf"], "year": 2017, "n_citations": 13}
{"id": 2825827, "s2_id": "ef81a3c1198b95bddcaef19cbda094fb0a33d03c", "title": "Enabling EASEY Deployment of Containerized Applications for Future HPC Systems", "abstract": "The upcoming exascale era will push the changes in computing architecture from classical CPU-based systems towards hybrid GPU-heavy systems with much higher levels of complexity. While such clusters are expected to improve the performance of certain optimized HPC applications, it will also increase the difficulties for those users who have yet to adapt their codes or are starting from scratch with new programming paradigms. Since there are still no comprehensive automatic assistance mechanisms to enhance application performance on such systems, we propose a support framework for future HPC architectures, called EASEY (Enable exASclae for EverYone). Our solution builds on a layered software architecture, which offers different mechanisms on each layer for different tasks of tuning, including a workflow management system. This enables users to adjust the parameters on each of the layers, thereby enhancing specific characteristics of their codes. We introduce the framework with a Charliecloud-based solution, showcasing the LULESH benchmark on the upper layers of our framework. Our approach can automatically deploy optimized container computations with negligible overhead and at the same time reduce the time a scientist needs to spent on manual job submission configurations.", "venue": "ICCS", "authors": ["Maximilian  H\u00f6b", "Dieter  Kranzlm\u00fcller"], "year": 2020, "n_citations": 0}
{"id": 2826920, "s2_id": "3347fec38386c7e3b30085b6593699857e1597d3", "title": "Incorporating TSN/BLS in AFDX for Mixed-Criticality Avionics Applications: Specification and Analysis", "abstract": "In this paper, we propose an extension of the AFDX standard, incorporating a TSN/BLS shaper, to homogenize the avionics communication architecture, and enable the interconnection of different avionics domains with mixed-criticality levels, e.g., legacy AFDX traffic, Flight Control and In-Flight Entertainment. First, we present the main specifications of such a proposed solution. Then, we detail the corresponding worst-case timing analysis, using the Network Calculus framework, to infer real-time guarantees. Finally, we conduct the performance analysis of such a proposal on a realistic AFDX configuration. Results show the efficiency of the Extended AFDX standard to noticeably enhance the medium priority level delay bounds, while respecting the higher priority level constraints, in comparison with the legacy AFDX standard.", "venue": "ArXiv", "authors": ["Ana\u00efs  Finzi", "Ahlem  Mifdaoui", "Fabrice  Frances", "Emmanuel  Lochin"], "year": 2017, "n_citations": 2}
{"id": 2831438, "s2_id": "2c695460f4b723ca351ba326679f7cd823ca5070", "title": "Signalling storms in 3G mobile networks", "abstract": "We review the characteristics of signalling storms that have been caused by certain common apps and recently observed in cellular networks, leading to system outages. We then develop a mathematical model of a mobile user's signalling behaviour which focuses on the potential of causing such storms, and represent it by a large Markov chain. The analysis of this model allows us to determine the key parameters of mobile user device behaviour that can lead to signalling storms. We then identify the parameter values that will lead to worst case load for the network itself in the presence of such storms. This leads to explicit results regarding the manner in which individual mobile behaviour can cause overload conditions on the network and its signalling servers, and provides insight into how this may be avoided.", "venue": "2014 IEEE International Conference on Communications (ICC)", "authors": ["Omer H. Abdelrahman", "Erol  Gelenbe"], "year": 2014, "n_citations": 42}
{"id": 2833790, "s2_id": "f49c9139da3703f809aed581709f8c2b9f29a78a", "title": "Competitive Online Optimization under Inventory Constraints", "abstract": "This paper studies online optimization under inventory (budget) constraints. While online optimization is a well-studied topic, versions with inventory constraints have proven difficult. We consider a formulation of inventory-constrained optimization that is a generalization of the classic one-way trading problem and has a wide range of applications. We present a new algorithmic framework, CR-Pursuit, and prove that it achieves the optimal competitive ratio among all deterministic algorithms (up to a problem-dependent constant factor) for inventory-constrained online optimization. Our algorithm and its analysis not only simplify and unify the state-ofthe- art results for the standard one-way trading problem, but they also establish novel bounds for generalizations including concave revenue functions. For example, for one-way trading with price elasticity, CR-Pursuit achieves a competitive ratio within a small additive constant (i.e., 1/3) to the lower bound of ln \u03b8 + 1, where \u03b8 is the ratio between the maximum and minimum base prices.", "venue": "PERV", "authors": ["Qiulin  Lin", "Hanling  Yi", "John  Pang", "Minghua  Chen", "Adam  Wierman", "Michael  Honig", "Yuanzhang  Xiao"], "year": 2019, "n_citations": 0}
{"id": 2837744, "s2_id": "d27ce8a3d396cb0ff217dfa91783761fa0bfbcda", "title": "Analysis of Non-Persistent CSMA Protocols with Exponential Backoff Scheduling", "abstract": "This paper studies the performance of Non-persistent CSMA/CA protocols with Exponential Backoff scheduling algorithms. A multi-queue single-server system is proposed to model multiple access networks. The input buffer of each access node is modeled as a Geo/G/1 queue, and the service time distribution of head-of-line packets is derived from the Markov chain of underlying scheduling algorithm. The main results include the complete analysis of the throughput and delay distribution, from which we derived the characteristic equation of network throughput and obtained stable regions with respect to the throughput and bounded mean delay of the Exponential Backoff scheme. We show that the stable throughput region of Exponential Backoff can be obtained even for an infinite population. Since the variance of service time of Exponential Backoff can be unbounded due to the capture effect; thus, its bounded delay region is only a sub-set of its stable throughput region, and the maximum achievable throughput of the network within this region is slightly smaller than the absolute maximum throughput. Analytical results presented in this paper are all verified by simulation.", "venue": "IEEE Transactions on Communications", "authors": ["Pui King Wong", "Dongjie  Yin", "Tony T. Lee"], "year": 2011, "n_citations": 38}
{"id": 2845931, "s2_id": "e191670cd50e98c76363b12f47b3b851a92846a0", "title": "In-packet Bloom filters: Design and networking applications", "abstract": "The Bloom filter (BF) is a well-known randomized data structure that answers set membership queries with some probability of false positives. In an attempt to solve many of the limitations of current network architectures, some recent proposals rely on including small BFs in packet headers for routing, security, accountability or other purposes that move application states into the packets themselves. In this paper, we consider the design of such in-packet Bloom filters (iBF). Our main contributions are exploring the design space and the evaluation of a series of extensions (1) to increase the practicality and performance of iBFs, (2) to enable false-negative-free element deletion, and (3) to provide security enhancements. In addition to the theoretical estimates, extensive simulations of the multiple design parameters and implementation alternatives validate the usefulness of the extensions, providing for enhanced and novel iBF networking applications.", "venue": "Comput. Networks", "authors": ["Christian Esteve  Rothenberg", "Carlos Alberto Braz  Macapuna", "Maur\u00edcio Ferreira  Magalh\u00e3es", "F\u00e1bio Luciano  Verdi", "Alexander  Wiesmaier"], "year": 2011, "n_citations": 61}
{"id": 2847779, "s2_id": "d6ca36b25bcb8c094b584504f734b1bf869d5c1f", "title": "UNIT: Unifying Tensorized Instruction Compilation", "abstract": "Because of the increasing demand for intensive computation in deep neural networks, researchers have developed both hardware and software mechanisms to reduce the compute and memory burden. A widely adopted approach is to use mixed precision data types. However, it is hard to benefit from mixed precision without hardware specialization because of the overhead of data casting. Recently, hardware vendors offer tensorized instructions specialized for mixed-precision tensor operations, such as Intel VNNI, Nvidia Tensor Core, and ARM DOT. These instructions involve a new computing idiom, which reduces multiple low precision elements into one high precision element. The lack of compilation techniques for this emerging idiom makes it hard to utilize these instructions. In practice, one approach is to use vendor-provided libraries for computationally-intensive kernels, but this is inflexible and prevents further optimizations. Another approach is to manually write hardware intrinsics, which is error-prone and difficult for programmers. Some prior works tried to address this problem by creating compilers for each instruction. This requires excessive efforts when it comes to many tensorized instructions. In this work, we develop a compiler framework, UNIT, to unify the compilation for tensorized instructions. The key to this approach is a unified semantics abstraction which makes the integration of new instructions easy, and the reuse of the analysis and transformations possible. Tensorized instructions from different platforms can be compiled via UNIT with moderate effort for favorable performance. Given a tensorized instruction and a tensor operation, UNIT automatically detects the applicability of the instruction, transforms the loop organization of the operation, and rewrites the loop body to take advantage of the tensorized instruction. According to our evaluation, UNIT is able to target various mainstream hardware platforms. The generated end-to-end inference model achieves 1.3 x speedup over Intel oneDNN on an x86 CPU, 1.75x speedup over Nvidia cuDNN on an Nvidia GPU, and 1.13x speedup over a carefully tuned TVM solution for ARM DOT on an ARM CPU.", "venue": "2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)", "authors": ["Jian  Weng", "Animesh  Jain", "Jie  Wang", "Leyuan  Wang", "Yida  Wang", "Tony  Nowatzki"], "year": 2021, "n_citations": 3}
{"id": 2849670, "s2_id": "4d6a4ec00379fbe4206ea91b20ee8d40b1a38de1", "title": "Optimizing Memory Efficiency for Deep Convolutional Neural Networks on GPUs", "abstract": "Leveraging large data sets, deep Convolutional Neural Networks (CNNs) achieve state-of-the-art recognition accuracy. Due to the substantial compute and memory operations, however, they require significant execution time. The massive parallel computing capability of GPUs make them as one of the ideal platforms to accelerate CNNs and a number of GPU-based CNN libraries have been developed. While existing works mainly focus on the computational efficiency of CNNs, the memory efficiency of CNNs have been largely overlooked. Yet CNNs have intricate data structures and their memory behavior can have significant impact on the performance. In this work, we study the memory efficiency of various CNN layers and reveal the performance implication from both data layouts and memory access patterns. Experiments show the universal effect of our proposed optimizations on both single layers and various networks, with up to 27.9\u00d7 for a single layer and up to 5.6\u00d7 on the whole networks.", "venue": "SC16: International Conference for High Performance Computing, Networking, Storage and Analysis", "authors": ["Chao  Li", "Yi  Yang", "Min  Feng", "Srimat T. Chakradhar", "Huiyang  Zhou"], "year": 2016, "n_citations": 58}
{"id": 2851351, "s2_id": "d9af256258075f2096ac7064337457425bc50844", "title": "BayesPerf: minimizing performance monitoring errors using Bayesian statistics", "abstract": "Hardware performance counters (HPCs) that measure low-level architectural and microarchitectural events provide dynamic contextual information about the state of the system. However, HPC measurements are error-prone due to non determinism (e.g., undercounting due to event multiplexing, or OS interrupt-handling behaviors). In this paper, we present BayesPerf, a system for quantifying uncertainty in HPC measurements by using a domain-driven Bayesian model that captures microarchitectural relationships between HPCs to jointly infer their values as probability distributions. We provide the design and implementation of an accelerator that allows for low-latency and low-power inference of the BayesPerf model for x86 and ppc64 CPUs. BayesPerf reduces the average error in HPC measurements from 40.1% to 7.6% when events are being multiplexed. The value of BayesPerf in real-time decision-making is illustrated with a simple example of scheduling of PCIe transfers.", "venue": "ASPLOS", "authors": ["Subho S. Banerjee", "Saurabh  Jha", "Zbigniew T. Kalbarczyk", "Ravishankar K. Iyer"], "year": 2021, "n_citations": 1}
{"id": 2853832, "s2_id": "c54671ea3d65504d5bc7d976a63d4279bed5e1fe", "title": "IFogSim2: An Extended iFogSim Simulator for Mobility, Clustering, and Microservice Management in Edge and Fog Computing Environments", "abstract": "Internet of Things (IoT) has already proven to be the building block for next-generation Cyber-Physical Systems (CPSs). The considerable amount of data generated by the IoT devices needs latency-sensitive processing, which is not feasible by deploying the respective applications in remote Cloud datacentres. Edge/Fog computing, a promising extension of Cloud at the IoT-proximate network, can meet such requirements for smart CPSs. However, the structural and operational differences of Edge/Fog infrastructure resist employing Cloud-based service regulations directly to these environments. As a result, many research works have been recently conducted, focusing on efficient application and resource management in Edge/Fog computing environments. Scalable Edge/Fog infrastructure is a must to validate these policies, which is also challenging to accommodate in the real-world due to high cost and implementation time. Considering simulation as a key to this constraint, various software has been developed that can imitate the physical behaviour of Edge/Fog computing environments. Nevertheless, the existing simulators often fail to support advanced service management features because of their monolithic architecture, lack of actual dataset, and limited scope for a periodic update. To overcome these issues, we have developed multiple simulation models for service migration, dynamic distributed cluster formation, and microservice orchestration for Edge/Fog computing in this work and integrated with the existing iFogSim simulation toolkit for launching it as iFogSim2. The performance of iFogSim2 and its built-in policies are evaluated using three use case scenarios and compared with the contemporary simulators and benchmark policies under different settings. Results indicate that the proposed solution outperform others in service management time, network usage, ram consumption, and simulation time.", "venue": "ArXiv", "authors": ["Md. Redowan Mahmud", "Samodha  Pallewatta", "Mohammad  Goudarzi", "Rajkumar  Buyya"], "year": 2021, "n_citations": 0}
{"id": 2854812, "s2_id": "463648e11e0c36fe80001cd0eec2792abb43629b", "title": "Ergodic Capacity Analysis of Free-Space Optical Links With Nonzero Boresight Pointing Errors", "abstract": "A unified capacity analysis of a free-space optical (FSO) link that accounts for nonzero boresight pointing errors and both types of detection techniques (i.e. intensity modulation/direct detection as well as heterodyne detection) is addressed in this work. More specifically, an exact closed-form expression for the moments of the end-to-end signal-to-noise ratio (SNR) of a single link FSO transmission system is presented in terms of well-known elementary functions. Capitalizing on these new moments expressions, we present approximate and simple closed-form results for the ergodic capacity at high and low SNR regimes. All the presented results are verified via computer-based Monte-Carlo simulations.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Imran Shafique Ansari", "Mohamed-Slim  Alouini", "Julian  Cheng"], "year": 2015, "n_citations": 76}
{"id": 2859439, "s2_id": "efa426e829e887efdef3ba0df5566097685a6b0e", "title": "Accelerated Convolutions for Efficient Multi-Scale Time to Contact Computation in Julia", "abstract": "Convolutions have long been regarded as fundamental to applied mathematics, physics and engineering. Their mathematical elegance allows for common tasks such as numerical differentiation to be computed efficiently on large data sets. Efficient computation of convolutions is critical to artificial intelligence in real-time applications, like machine vision, where convolutions must be continuously and efficiently computed on tens to hundreds of kilobytes per second. In this paper, we explore how convolutions are used in fundamental machine vision applications. We present an accelerated n-dimensional convolution package in the high performance computing language, Julia, and demonstrate its efficacy in solving the time to contact problem for machine vision. Results are measured against synthetically generated videos and quantitatively assessed according to their mean squared error from the ground truth. We achieve over an order of magnitude decrease in compute time and allocated memory for comparable machine vision applications. All code is packaged and integrated into the official Julia Package Manager to be used in various other scenarios.", "venue": "ArXiv", "authors": ["Alexander  Amini", "Berthold K. P. Horn", "Alan  Edelman"], "year": 2016, "n_citations": 3}
{"id": 2860441, "s2_id": "d4b090c2987e26dc831cc8225ca351d2ec77cc4b", "title": "Performance Comparison for Neuroscience Application Benchmarks", "abstract": "Researchers within the Human Brain Project and related projects have in the last couple of years expanded their needs for high-performance computing infrastructures. The needs arise from a diverse set of science challenges that range from large-scale simulations of brain models to processing of extreme-scale experimental data sets. The ICEI project, which is in the process of creating a distributed infrastructure optimised for brain research, started to build-up a set of benchmarks that reflect the diversity of applications in this field. In this paper we analyse the performance of some selected benchmarks on an IBM POWER8 and Intel Skylake based systems with and without GPUs.", "venue": "ISC Workshops", "authors": ["Andreas  Herten", "Thorsten  Hater", "Wouter  Klijn", "Dirk  Pleiter"], "year": 2019, "n_citations": 1}
{"id": 2861146, "s2_id": "673331486d585cf77b6f1e408d8feefcd77d0561", "title": "Toward a Better Understanding and Evaluation of Tree Structures on Flash SSDs", "abstract": "Solid-state drives (SSDs) are extensively used to deploy persistent data stores, as they provide low latency random access, high write throughput, high data density, and low cost. Tree-based data structures are widely used to build persistent data stores, and indeed they lie at the backbone of many of the data management systems used in production and research today. In this paper, we show that benchmarking a persistent tree-based data structure on an SSD is a complex process, which may easily incur subtle pitfalls that can lead to an inaccurate performance assessment. At a high-level, these pitfalls stem from the interaction of complex software running on complex hardware. On one hand, tree structures implement internal operations that have nontrivial effects on performance. On the other hand, SSDs employ firmware logic to deal with the idiosyncrasies of the underlying flash memory, which are well known to lead to complex performance dynamics. We identify seven benchmarking pitfalls using RocksDB and WiredTiger, two widespread implementations of an LSM-Tree and a B+Tree, respectively. We show that such pitfalls can lead to incorrect measurements of key performance indicators, hinder the reproducibility and the representativeness of the results, and lead to suboptimal deployments in production environments. We also provide guidelines on how to avoid these pitfalls to obtain more reliable performance measurements, and to perform more thorough and fair comparison among different design points.", "venue": "Proc. VLDB Endow.", "authors": ["Diego  Didona", "Nikolas  Ioannou", "Radu  Stoica", "Kornilios  Kourtis"], "year": 2020, "n_citations": 3}
{"id": 2867268, "s2_id": "33f30e2570f0f29a37f5761861b0bb954f7144e9", "title": "Lattice Boltzmann Benchmark Kernels as a Testbed for Performance Analysis", "abstract": "Abstract Lattice Boltzmann methods (LBM) are an important part of current computational fluid dynamics (CFD). They allow easy implementations and boundary handling. However, competitive time to solution not only depends on the choice of a reasonable method, but also on an efficient implementation on modern hardware. Hence, performance optimization has a long history in the lattice Boltzmann community. A variety of options exists regarding the implementation with direct impact on the solver performance. Experimenting and evaluating each option often is hard as the kernel itself is typically embedded in a larger code base. With our suite of lattice Boltzmann kernels we provide the infrastructure for such endeavors. Already included are several kernels ranging from simple to fully optimized implementations. Although these kernels are not fully functional CFD solvers, they are equipped with a solid verification method. The kernels may act as an reference for performance comparisons and as a blue print for optimization strategies. In this paper we give an overview of already available kernels, establish a performance model for each kernel, and show a comparison of implementations and recent architectures.", "venue": "Computers & Fluids", "authors": ["Markus  Wittmann", "Viktor  Haag", "Thomas  Zeiser", "Harald  K\u00f6stler", "Gerhard  Wellein"], "year": 2018, "n_citations": 11}
{"id": 2868245, "s2_id": "0f15fc3b401a750a1221ebcae1f7ab12c166bd01", "title": "Stochastic Performance Modeling for Practical Byzantine Fault Tolerance Consensus in Blockchain", "abstract": "The practical Byzantine fault tolerant (PBFT) consensus mechanism is one of the most basic consensus algorithms (or protocols) in blockchain technologies, thus its performance evaluation is an interesting and challenging topic due to a higher complexity of its consensus work in the peer-to-peer network. This paper describes a simple stochastic performance model of the PBFT consensus mechanism, which is refined as not only a queueing system with complicated service times but also a level-independent quasi-birth-and-death (QBD) process. From the level-independent QBD process, we apply the matrix-geometric solution to obtain a necessary and sufficient condition under which the PBFT consensus system is stable, and to be able to numerically compute the stationary probability vector of the QBD process. Thus we provide four useful performance measures of the PBFT consensus mechanism, and can numerically calculate the four performance measures. Finally, we use some numerical examples to verify the validity of our theoretical results, and show how the four performance measures are influenced by some key parameters of the PBFT consensus. By means of the theory of multi-dimensional Markov processes, we are optimistic that the methodology and results given in this paper are applicable in a wide range research of PBFT consensus mechanism and even other types of consensus mechanisms.", "venue": "ArXiv", "authors": ["Fan-Qi  Ma", "Quan-Lin  Li", "Yi-Han  Liu", "Yan-Xia  Chang"], "year": 2021, "n_citations": 0}
{"id": 2868939, "s2_id": "d370cc0755d1e387592b5921d02eb64872610635", "title": "Simple and effective dynamic provisioning for power-proportional data centers", "abstract": "Energy consumption represents a significant cost in data center operation. A large fraction of the energy, however, is used to power idle servers when the workload is low. Dynamic provisioning techniques aim at saving this portion of the energy, by turning off unnecessary servers. In this paper, we explore how much gain knowing future workload information can bring to dynamic provisioning. In particular, we develop online dynamic provisioning solutions with and without future workload information available. We first reveal an elegant structure of the off-line dynamic provisioning problem, which allows us to characterize the optimal solution in a \u201cdivide-and-conquer\u201d manner. We then exploit this insight to design two online algorithms with competitive ratios 2 - \u03b1 and e/ (e - 1 + \u03b1), respectively, where 0 \u2264 \u03b1 \u2264 1 is the normalized size of a look-ahead window in which future workload information is available. A fundamental observation is that future workload information beyond the full-size look-ahead window (corresponding to \u03b1= 1) will not improve dynamic provisioning performance. Our algorithms are decentralized and easy to implement. We demonstrate their effectiveness in simulations using real-world traces.", "venue": "2012 46th Annual Conference on Information Sciences and Systems (CISS)", "authors": ["Tan  Lu", "Minghua  Chen", "Lachlan L.H.  Andrew"], "year": 2012, "n_citations": 72}
{"id": 2870767, "s2_id": "6828d0d4ce0c1cef0c65e1a29cab3d7244011613", "title": "Benchmarking network fabrics for data distributed training of deep neural networks", "abstract": "Artificial Intelligence/Machine Learning applications require the training of complex models on large amounts of labelled data. The large computational requirements for training deep models have necessitated the development of new methods for faster training. One such approach is the data parallel approach, where the training data is distributed across multiple compute nodes. This approach is simple to implement and supported by most of the commonly used machine learning frameworks. The data parallel approach leverages MPI for communicating gradients across all nodes. In this paper, we examine the effects of using different physical hardware interconnects and network-related software primitives for enabling data distributed deep learning. We compare the effect of using GPUDirect and NCCL on Ethernet and OmniPath fabrics. Our results show that using Ethernet-based networking in shared HPC systems does not have a significant effect on the training times for commonly used deep neural network architectures or traditional HPC applications such as Computational Fluid Dynamics.", "venue": "2020 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Siddharth  Samsi", "Andrew  Prout", "Michael  Jones", "Andrew  Kirby", "Bill  Arcand", "Bill  Bergeron", "David  Bestor", "Chansup  Byun", "Vijay  Gadepally", "Michael  Houle", "Matthew  Hubbell", "Anna  Klein", "Peter  Michaleas", "Lauren  Milechin", "Julie  Mullen", "Antonio  Rosa", "Charles  Yee", "Albert  Reuther", "Jeremy  Kepner"], "year": 2020, "n_citations": 2}
{"id": 2872999, "s2_id": "3599a429a3dc27b8af61db32ccc602d8ae8c17f0", "title": "MANETS: High Mobility Can Make Up for Low Transmission Power", "abstract": "We consider Mobile Ad-hoc NETworks (MANETs) formed by n nodes that move independently at random over a finite square region of the plane. Nodes exchange data if they are at distance at most r within each other, where r > 0 is the node transmission radius . The flooding time is the number of time steps required to broadcast a message from a source node to every node of the network. Flooding time is an important measure of the speed of information spreading in dynamic networks. \n \nWe derive a nearly-tight upper bound on the flooding time which is a decreasing function of the maximal velocity of the nodes. \n \nIt turns out that, when the node velocity is \"sufficiently\" high, even if the node transmission radius r is far below the connectivity threshold , the flooding time does not asymptotically depend on r . So, flooding can be very fast even though every snapshot (i.e. the static random geometric graph at any fixed time) of the MANET is fully disconnected. \n \nOur result is the first analytical evidence of the fact that high, random node mobility strongly speed-up information spreading and, at the same time, let nodes save energy .", "venue": "ICALP", "authors": ["Andrea E. F. Clementi", "Francesco  Pasquale", "Riccardo  Silvestri"], "year": 2009, "n_citations": 44}
{"id": 2873937, "s2_id": "bded4dc995bb16f2ec5437df0feb94d963c6f2e6", "title": "Scheduling in the Presence of Data Intensive Compute Jobs", "abstract": "We study the performance of non-adaptive scheduling policies in computing systems with multiple servers. Compute jobs are mostly regular, with modest service requirements. However, there are sporadic data intensive jobs, whose expected service time is much higher than that of the regular jobs. For this model, we are interested in the effect of scheduling policies on the average time a job spends in the system. To this end, we introduce two performance indicators in a simplified, only-arrival system. We believe that these performance indicators are good predictors of the relative performance of the policies in the queuing system, which is supported by simulations results.", "venue": "2019 IEEE International Conference on Big Data (Big Data)", "authors": ["Amir  Behrouzi-Far", "Emina  Soljanin"], "year": 2019, "n_citations": 2}
{"id": 2877346, "s2_id": "df9b1dbc3b62f5d082e515d92296a8ed11cd70b2", "title": "10-millisecond Computing", "abstract": "Despite computation becomes much complex on data with unprecedented large-scale, we argue computers or smart devices should and will consistently provide information and knowledge to human being in the order of a few tens milliseconds. We coin a new term 10-millisecond computing to call attention to this class of workloads. Public reports indicate that internet service users are sensitive to the service or job-level response time outliers, so we propose a very simple but powerful metric-outlier proportion to characterize the system behaviors.The outlier proportion is defined as follows: for N completed requests or jobs, if M jobs or requests' latencies exceed the outlier limit t, e.g. 10 milliseconds, the outlier proportion is M/N. 10-millisecond computing raises many challenges for both software and hardware stacks. In this paper, as a case study we investigate the challenges raised for conventional operating systems. For typical latency-critical services running with Linux on a 40-core server - a main-stream server hardware system in near future, we found, when the outlier limit decreases, the outlier proportion of a single server will significantly deteriorate. Meanwhile, the outlier proportion is further amplified by the system scale, including the system core number. For a 1K-scale system, we surprisingly find that to reduce the service or job-level outlier proportion to 10%, running Linux (version 2.6.32) or LXC (version 0.7.5 ) or XEN (version 4.0.0), respectively, the outlier proportion of a single server needs to be reduced by 871X, 2372X, 2372X accordingly. We also conducted a list of experiments to reveal the current Linux systems still suffer from poor outlier performance, including Linux kernel version 3.17.4, Linux kernel version 2.6.35M, a modified version of 2.6.35 integrated with sloppy counters and two representative real time schedulers.", "venue": "ArXiv", "authors": ["Gang  Lu", "Jianfeng  Zhan", "Tianshu  Hao", "Lei  Wang"], "year": 2016, "n_citations": 0}
{"id": 2878364, "s2_id": "ad9aca605304726d7271a91c4f0bb51cee606ec1", "title": "An autonomous distributed admission control scheme for IEEE 802.11 DCF", "abstract": "Admission control as a mechanism for providing QoS requires an accurate description of the requested flow as well as already admitted flows. Since 802.11 WLAN capacity is shared between flows belonging to all stations, admission control requires knowledge of all flows in the WLAN. Further, estimation of the load-dependent WLAN capacity through analytical model requires inputs about channel data rate, payload size and the number of stations. These factors combined point to a centralized admission control whereas for 802.11 DCF it is ideally performed in a distributed manner. The use of measurements from the channel avoids explicit inputs about the state of the channel described above. BUFFET, a model based measurement-assisted distributed admission control scheme for DCF proposed in this paper relies on measurements to derive model inputs and predict WLAN saturation, thereby maintaining average delay within acceptable limits. Being measurement based, it adapts to heterogeneous flows too, making it completely autonomous and distributed. Performance analysis and comparison with two other schemes using OPNET simulations suggests that BUFFET is able to ensure average delay under 7ms at a near-optimal throughput.", "venue": "QSHINE", "authors": ["Preetam  Patil", "Varsha  Apte"], "year": 2007, "n_citations": 3}
{"id": 2878381, "s2_id": "dfb039ee4b6befeabeb045389ca3a0e035dc9e4d", "title": "K-Athena: A Performance Portable Structured Grid Finite Volume Magnetohydrodynamics Code", "abstract": "Large scale simulations are a key pillar of modern research and require ever-increasing computational resources. Different novel manycore architectures have emerged in recent years on the way towards the exascale era. Performance portability is required to prevent repeated non-trivial refactoring of a code for different architectures. We combine <sc>Athena++</sc>, an existing magnetohydrodynamics (MHD) CPU code, with <sc>Kokkos</sc>, a performance portable on-node parallel programming paradigm, into <sc>K-Athena</sc> to allow efficient simulations on multiple architectures using a single codebase. We present profiling and scaling results for different platforms including Intel Skylake CPUs, Intel Xeon Phis, and NVIDIA GPUs. <sc>K-Athena</sc> achieves <inline-formula><tex-math notation=\"LaTeX\">$>10^8$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>></mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>8</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href=\"grete-ieq1-3010016.gif\"/></alternatives></inline-formula> cell-updates/s on a single V100 GPU for second-order double precision MHD calculations, and a speedup of 30 on up to 24 576 GPUs on Summit (compared to 172,032 CPU cores), reaching <inline-formula><tex-math notation=\"LaTeX\">$1.94\\times 10^{12}$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mn>94</mml:mn><mml:mo>\u00d7</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>12</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href=\"grete-ieq2-3010016.gif\"/></alternatives></inline-formula> total cell-updates/s at 76 percent parallel efficiency. Using a roofline analysis we demonstrate that the overall performance is currently limited by DRAM bandwidth and calculate a performance portability metric of 62.8 percent. Finally, we present the implementation strategies used and the challenges encountered in maximizing performance. This will provide other research groups with a straightforward approach to prepare their own codes for the exascale era. <sc>K-Athena</sc> is available at <uri>https://gitlab.com/pgrete/kathena</uri>.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Philipp  Grete", "Forrest W. Glines", "Brian W. O'Shea"], "year": 2021, "n_citations": 17}
{"id": 2878541, "s2_id": "932653bebdd5b22749e177c8314a39abddeb5768", "title": "Learning with Analytical Models", "abstract": "To understand and predict the performance of scientific applications, several analytical and machine learning approaches have been proposed, each having its advantages and disadvantages. In this paper, we propose and validate a hybrid approach for performance modeling and prediction, which combines analytical and machine learning models. The proposed hybrid model aims to minimize prediction cost while providing reasonable prediction accuracy. Our validation results show that the hybrid model is able to learn and correct the analytical models to better match the actual performance. Furthermore, the proposed hybrid model improves the prediction accuracy in comparison to pure machine learning techniques while using small training datasets, thus making it suitable for hardware and workload changes.", "venue": "2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Huda  Ibeid", "Siping  Meng", "Oliver  Dobon", "Luke N. Olson", "William  Gropp"], "year": 2019, "n_citations": 5}
{"id": 2878742, "s2_id": "06da7f592253257ff58bb407b16213e14fb083c2", "title": "Analysis of a reputation system for Mobile Ad-Hoc Networks with liars", "abstract": "The application of decentralized reputation systems is a promising approach to ensure cooperation and fairness, as well as to address random failures and malicious attacks in Mobile Ad-Hoc Networks. However, they are potentially vulnerable to liars. With our work, we provide a first step to analyzing robustness of a reputation system based on a deviation test. Using a mean-field approach to our stochastic process model, we show that liars have no impact unless their number exceeds a certain threshold (phase transition). We give precise formulae for the critical values and thus provide guidelines for an optimal choice of parameters.", "venue": "Perform. Evaluation", "authors": ["Jochen  Mundinger", "Jean-Yves Le Boudec"], "year": 2008, "n_citations": 41}
{"id": 2880424, "s2_id": "851612c6191eb5ef245fed72f6fe2d63113088ce", "title": "The implications from benchmarking three big data systems", "abstract": "Along with today's data explosion and application diversification, a variety of hardware platforms for data centers are emerging and are attracting interests from both industry and academia. The existing hardware platforms represent a wide range of implementation approaches, and different hardware have different strengths. In this paper, we conduct comprehensive evaluations on three representative data center systems based on BigDataBench, which is a benchmark suite for benchmarking and ranking systems running big data applications. Then we explore the relative performance of the three implementation approaches with different big data applications, and provide strong guidance for the data center system construction. Through our experiments, we has inferred that a data center system based on specific hardware has different performance in the context of different applications and data volumes. When we construct a system, we can take into account not only the performance or energy consumption of the pure hardwares, but also the application-level characteristics. Data scale, application type and complexity should be considered comprehensively when researchers or architects plan to choose fundamental components for their data center system.", "venue": "2013 IEEE International Conference on Big Data", "authors": ["Jing  Quan", "Yingjie  Shi", "Ming  Zhao", "Wei  Yang"], "year": 2013, "n_citations": 5}
{"id": 2882005, "s2_id": "fc3a8a0ed013249303b5ed7b2d90a3d5a8426fd6", "title": "The Computational and Storage Potential of Volunteer Computing", "abstract": "\"Volunteer computing\" uses Internet-connected computers, volunteered by their owners, as a source of computing power and storage. This paper studies the potential capacity of volunteer computing. We analyzed measurements of over 330,000 hosts participating in a volunteer computing project. These measurements include processing power, memory, disk space, network throughput, host availability, userspecified limits on resource usage, and host churn. We show that volunteer computing can support applications that are significantly more data-intensive, or have larger memory and storage requirements, than those in current projects.", "venue": "Sixth IEEE International Symposium on Cluster Computing and the Grid (CCGRID'06)", "authors": ["David P. Anderson", "Gilles  Fedak"], "year": 2006, "n_citations": 356}
{"id": 2883119, "s2_id": "01795d755145465d0393c94e830138731af99a67", "title": "In Search of Simplicity: A Self-Organizing Multi-Source Multicast Overlay", "abstract": "Multicast communication primitives have broad utility as building blocks for distributed applications. The challenge is to create and maintain the distributed structures that support these primitives while accounting for volatile end nodes and variable network characteristics. Most solutions proposed to date rely on complex algorithms or global information, thus limiting the scale of deployments and acceptance outside the academic realm. This article introduces a low-complexity, self organizing solution for maintaining multicast trees, that we refer to as UMM (Unstructured Multi-source Multicast). UMM uses traditional distributed systems techniques: layering, soft-state, and passive data collection to adapt to the dynamics of the physical network and maintain data dissemination trees. The result is a simple, adaptive system with lower overheads than more complex alternatives. We have implemented UMM and evaluated it on a 100-node PlanetLab testbed and on up to 1024-node emulated ModelNet networks Extensive experimental evaluations demonstrate UMM's low overhead, efficient network usage compared to alternative solutions, and ability to quickly adapt to network changes and to recover from failures.", "venue": "ArXiv", "authors": ["Matei  Ripeanu", "Adriana  Iamnitchi", "Ian T. Foster", "Anne  Rogers"], "year": 2007, "n_citations": 4}
{"id": 2886231, "s2_id": "99c9425074ccc382761db3c5b78aa95ada6f0489", "title": "Quantitative Performance Comparison of Various Traffic Shapers in Time-Sensitive Networking", "abstract": "Owning to the sub-standards being developed by IEEE Time-Sensitive Networking (TSN) Task Group, the traditional IEEE 802.1 Ethernet is enhanced to support real-time dependable communications for future timeand safety-critical applications. Several sub-standards have been recently proposed that introduce various traffic shapers (e.g., Time-Aware Shaper (TAS), Asynchronous Traffic Shaper (ATS), Credit-Based Shaper (CBS), Strict Priority (SP)) for flow control mechanisms of queuing and scheduling, targeting different application requirements. These shapers can be used in isolation or in combination and there is limited work that analyzes, evaluates and compares their performance, which makes it challenging for end-users to choose the right combination for their applications. This paper aims at (i) quantitatively comparing various traffic shapers and their combinations, (ii) summarizing, classifying and extending the architectures of individual and combined traffic shapers and their Network calculus (NC)-based performance analysis methods and (iii) filling the gap in the timing analysis research on handling two novel hybrid architectures of combined traffic shapers, i.e., TAS+ATS+SP and TAS+ATS+CBS. A large number of experiments, using both synthetic and realistic test cases, are carried out for quantitative performance comparisons of various individual and combined traffic shapers, from the perspective of upper bounds of delay, backlog and jitter. To the best of our knowledge, we are the first to quantitatively compare the performance of the main traffic shapers in TSN. The paper aims at supporting the researchers and practitioners in the selection of suitable TSN sub-protocols for their use cases.", "venue": "ArXiv", "authors": ["Luxi  Zhao", "Paul  Pop", "Sebastian  Steinhorst"], "year": 2021, "n_citations": 4}
{"id": 2889591, "s2_id": "d309bb608f92b544a44ed5cfd6670c2321e765b2", "title": "Evaluating Load Balancing Performance in Distributed Storage With Redundancy", "abstract": "To facilitate load balancing, distributed systems store data redundantly. We evaluate the load balancing performance of storage schemes in which each object is stored at <inline-formula> <tex-math notation=\"LaTeX\">$d$ </tex-math></inline-formula> different nodes, and each node stores the same number of objects. In our model, the load offered for the objects is sampled uniformly at random from all the load vectors with a fixed cumulative value. We find that the load balance in a system of <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> nodes improves multiplicatively with <inline-formula> <tex-math notation=\"LaTeX\">$d$ </tex-math></inline-formula> as long as <inline-formula> <tex-math notation=\"LaTeX\">${d} = {o}\\left ({\\log ({n})}\\right)$ </tex-math></inline-formula>, and improves exponentially once <inline-formula> <tex-math notation=\"LaTeX\">${d} = \\Theta \\left ({\\log ({n})}\\right)$ </tex-math></inline-formula>. We show that the load balance improves in the same way with <inline-formula> <tex-math notation=\"LaTeX\">$d$ </tex-math></inline-formula> when the service choices are created with XOR\u2019s of <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula> objects rather than object replicas. In such redundancy schemes, storage overhead is reduced multiplicatively by <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula>. However, recovery of an object requires downloading content from <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula> nodes. At the same time, the load balance increases additively by <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula>. We express the system\u2019s load balance in terms of the maximal spacing or maximum of <inline-formula> <tex-math notation=\"LaTeX\">$d$ </tex-math></inline-formula> consecutive spacings between the ordered statistics of uniform random variables. Using this connection and the limit results on the maximal <inline-formula> <tex-math notation=\"LaTeX\">$d$ </tex-math></inline-formula>-spacings, we derive our main results.", "venue": "IEEE Transactions on Information Theory", "authors": ["Mehmet Fatih Akta\u015f", "Amir Behruzi Far", "Emina  Soljanin", "Philip  Whiting"], "year": 2021, "n_citations": 0}
{"id": 2890390, "s2_id": "f99aa01e7567b6f238bf962e89a04a9d0f144fc3", "title": "Forest Packing: Fast, Parallel Decision Forests", "abstract": "Machine learning has an emerging critical role in high-performance computing to modulate simulations, extract knowledge from massive data, and replace numerical models with efficient approximations. Decision forests are a critical tool because they provide insight into model operation that is critical to interpreting learned results. While decision forests are trivially parallelizable, the traversals of tree data structures incur many random memory accesses and are very slow. We present memory packing techniques that reorganize learned forests to minimize cache misses during classification. The resulting layout is hierarchical. At low levels, we pack the nodes of multiple trees into contiguous memory blocks so that each memory access fetches data for multiple trees. At higher levels, we use leaf cardinality to identify the most popular paths through a tree and collocate those paths in cache lines. We extend this layout with out-of-order execution and cache-line prefetching to increase memory throughput. Together, these optimizations increase the performance of classification in ensembles by a factor of four over an optimized C++ implementation and a actor of 50 over a popular R language implementation.", "venue": "SDM", "authors": ["James  Browne", "Tyler M. Tomita", "Disa  Mhembere", "Randal C. Burns", "Joshua T. Vogelstein"], "year": 2019, "n_citations": 8}
{"id": 2893069, "s2_id": "f0239c4c69f600a04b28faabb6d7d84f7a635f46", "title": "DiPerF: an automated distributed performance testing framework", "abstract": "We present DiPerF, a distributed performance-testing framework, aimed at simplifying and automating service performance evaluation. DiPerF coordinates a pool of machines that test a target service, collects and aggregates performance metrics, and generates performance statistics. The aggregate data collected provide information on service throughput, on service fairness' when serving multiple clients concurrently, and on the impact of network latency on service performance. Furthermore, using this data, it is possible to build predictive models that estimate a service performance given the service load. We have tested DiPerF on 100+machines on two testbeds, Grid3 and PlanetLab, and explored the performance of job submission services (pre-WS GRAM and WS GRAM) included with Globus Toolkit/spl reg/ 3.2.", "venue": "Fifth IEEE/ACM International Workshop on Grid Computing", "authors": ["Catalin  Dumitrescu", "Ioan  Raicu", "Matei  Ripeanu", "Ian T. Foster"], "year": 2004, "n_citations": 72}
{"id": 2894398, "s2_id": "e5fa4bc4cd3ab7ed4a6edf06890f9a13128e605c", "title": "No-Regret Caching via Online Mirror Descent", "abstract": "We study an online caching problem in which requests can be served by a local cache to avoid retrieval costs from a remote server. The cache can update its state after a batch of requests and store an arbitrarily small fraction of each content. We study no-regret algorithms based on Online Mirror Descent (OMD) strategies. We show that the choice of OMD strategy depends on the request diversity present in a batch and that OMD caching policies may outperform traditional eviction-based policies.", "venue": "ICC 2021 - IEEE International Conference on Communications", "authors": ["Tareq Si Salem", "Giovanni  Neglia", "Stratis  Ioannidis"], "year": 2021, "n_citations": 4}
{"id": 2894724, "s2_id": "ffb9646f66895a9753a917d6b97de6c4fb4eb437", "title": "Parallel Multi Channel convolution using General Matrix Multiplication", "abstract": "Convolutional neural networks (CNNs) have emerged as one of the most successful machine learning technologies for image and video processing. The most computationally-intensive parts of CNNs are the convolutional layers, which convolve multi-channel images with multiple kernels. A common approach to implementing convolutional layers is to expand the image into a column matrix (im2col) and perform Multiple Channel Multiple Kernel (MCMK) convolution using an existing parallel General Matrix Multiplication (GEMM) library. This im2col conversion greatly increases the memory footprint of the input matrix and reduces data locality. In this paper we propose a new approach to MCMK convolution that is based on General Matrix Multiplication (GEMM), but not on im2col. Our algorithm eliminates the need for data replication on the input thereby enabling us to apply the convolution kernels on the input images directly. We have implemented several variants of our algorithm on a CPU processor and an embedded ARM processor. On the CPU, our algorithm is faster than im2col in most cases.", "venue": "2017 IEEE 28th International Conference on Application-specific Systems, Architectures and Processors (ASAP)", "authors": ["Aravind  Vasudevan", "Andrew  Anderson", "David  Gregg"], "year": 2017, "n_citations": 71}
{"id": 2895376, "s2_id": "e9aba365655a6831d9c5cdfdeeacab17ee216e8d", "title": "Time-efficient Garbage Collection in SSDs", "abstract": "SSDs are currently replacing magnetic disks in many application areas. A challenge of the underlying flash technology is that data cannot be updated in-place. A block consisting of many pages must be completely erased before a single page can be rewritten. This victim block can still contain valid pages which need to be copied to other blocks before erasure. The objective of garbage collection strategies is to minimize write amplification induced by copying valid pages from victim blocks while minimizing the performance overhead of the victim selection. Victim selection strategies minimizing write amplification, like the cost-benefit approach, have linear runtime, while the write amplifications of time-efficient strategies, like the greedy strategy, significantly reduce the lifetime of SSDs. In this paper, we propose two strategies which optimize the performance of cost-benefit, while (almost) preserving its write amplification. Trace-driven simulations for single- and multi-channel SSDs show that the optimizations help to keep the write amplification low while improving the runtime by up to 24-times compared to the original cost-benefit strategy, so that the new strategies can be used in multi-TByte SSDs.", "venue": "ArXiv", "authors": ["Lars  Nagel", "Tim  S\u00fc\u00df", "Kevin  Kremer", "M. Umar Hameed", "Lingfang  Zeng", "Andr\u00e9  Brinkmann"], "year": 2018, "n_citations": 2}
{"id": 2896065, "s2_id": "47e86128ff2e088df82d7edbcc4ab360692d2c33", "title": "Energy-Aware Lease Scheduling in Virtualized Data Centers", "abstract": "Energy efficiency has become an important measurement of scheduling algorithms in virtualized data centers. One of the challenges of energy-efficient scheduling algorithms, however, is the trade-off between minimizing energy consumption and satisfying quality of service (e.g. performance, resource availability on time for reservation requests). We consider resource needs in the context of virtualized data centers of a private cloud system, which provides resource leases in terms of virtual machines (VMs) for user applications. In this paper, we propose heuristics for scheduling VMs that address the above challenge. On performance evaluation, simulated results have shown a significant reduction on total energy consumption of our proposed algorithms compared with an existing First-Come-First-Serve (FCFS) scheduling algorithm with the same fulfillment of performance requirements. We also discuss the improvement of energy saving when additionally using migration policies to the above mentioned algorithms.", "venue": "HPSC", "authors": ["Nguyen  Quang-Hung", "Nam  Thoai", "Nguyen Thanh Son", "Duy-Khanh  Le"], "year": 2012, "n_citations": 0}
{"id": 2897453, "s2_id": "240135d042d2d1f2ca4e45c67baf5ea2f4b32923", "title": "A Model of Polarization on Social Media Caused by Empathy and Repulsion", "abstract": "In recent years, the ease with which social media can be accessed has led to the unexpected problem of a shrinkage in information sources. This phenomenon is caused by a system that facilitates the connection of people with similar ideas and recommendation systems. Bias in the selection of information sources promotes polarization that divides people into multiple groups with opposing views and creates conflicts between opposing groups. This paper elucidates the mechanism of polarization by proposing a model of opinion formation in social media that considers users' reactions of empathy and repulsion. Based on the idea that opinion neutrality is only relative, this model offers a novel technology for dealing with polarization.", "venue": "ArXiv", "authors": ["Naoki  Hirakura", "Masaki  Aida", "Konosuke  Kawashima"], "year": 2020, "n_citations": 0}
{"id": 2900086, "s2_id": "49083298fa76747cbd6c8157e5276c861e4de1a2", "title": "Effect of Thread Level Parallelism on the Performance of Optimum Architecture for Embedded Applications", "abstract": "According to the increasing complexity of network application and internet traffic, network processor as a subset of embedded processors have to process more computation intensive tasks. By scaling down the feature size and emersion of chip multiprocessors (CMP) that are usually multi-thread processors, the performance requirements are somehow guaranteed. As multithread processors are the heir of uni-thread processors and there isn't any general design flow to design a multithread embedded processor, in this paper we perform a comprehensive design space exploration for an optimum uni-thread embedded processor based on the limited area and power budgets. Finally we run multiple threads on this architecture to find out the maximum thread level parallelism (TLP) based on performance per power and area optimum uni-thread architecture.", "venue": "ArXiv", "authors": ["Mehdi  Alipour", "Hojjat  Taghdisi"], "year": 2012, "n_citations": 3}
{"id": 2900761, "s2_id": "3c2797a396f42c367dbdcdc9a9e332978849331f", "title": "Creating a Virtuous Cycle in Performance Testing at MongoDB", "abstract": "It is important to detect changes in software performance during development in order to avoid performance decreasing release to release or dealing with costly delays at release time. Performance testing is part of the development process at MongoDB, and integrated into our continuous integration system. We describe a set of changes to that performance testing environment designed to improve testing effectiveness. These changes help improve coverage, provide faster and more accurate signaling for performance changes, and help us better understand the state of performance. In addition to each component performing better, we believe that we have created and exploited a virtuous cycle: performance test improvements drive impact, which drives more use, which drives further impact and investment in improvements. Overall, MongoDB is getting faster and we avoid shipping major performance regressions to our customers because of this infrastructure.", "venue": "ICPE", "authors": ["David  Daly"], "year": 2021, "n_citations": 1}
{"id": 2901846, "s2_id": "5da12055612138d1bf195084e985bc3357f8cbee", "title": "Multi-level simulation of Internet of Things on smart territories", "abstract": "Abstract In this paper, a methodology is presented and employed for simulating the Internet of Things (IoT). The requirement for scalability, due to the possibly huge amount of involved sensors and devices, and the heterogeneous scenarios that might occur, impose resorting to sophisticated modeling and simulation techniques. In particular, multi-level simulation is regarded as a main framework that allows simulating large-scale IoT environments while keeping high levels of detail, when it is needed. We consider a use case based on the deployment of smart services in decentralized territories. A two level simulator is employed, which is based on a coarse agent-based, adaptive parallel and distributed simulation approach to model the general life of simulated entities. However, when needed a finer grained simulator (based on OMNeT++) is triggered on a restricted portion of the simulated area, which allows considering all issues concerned with wireless communications. Based on this use case, it is confirmed that the ad-hoc wireless networking technologies do represent a principle tool to deploy smart services over decentralized countrysides. Moreover, the performance evaluation confirms the viability of utilizing multi-level simulation for simulating large scale IoT environments.", "venue": "Simul. Model. Pract. Theory", "authors": ["Gabriele  D'Angelo", "Stefano  Ferretti", "Vittorio  Ghini"], "year": 2017, "n_citations": 55}
{"id": 2903721, "s2_id": "38b3ff6417dfe0866d3d99e8d89e24001387985b", "title": "Parallel Accelerated Custom Correlation Coefficient Calculations for Genomics Applications", "abstract": "The massive quantities of genomic data being made available through gene sequencing techniques are enabling breakthroughs in genomic science in many areas such as medical advances in the diagnosis and treatment of diseases. Analyzing this data, however, is a computational challenge insofar as the computational costs of the relevant algorithms can grow with quadratic, cubic or higher complexity-leading to the need for leadership scale computing. In this paper we describe a new approach to calculations of the Custom Correlation Coefficient (CCC) between Single Nucleotide Polymorphisms (SNPs) across a population, suitable for parallel systems equipped with graphics processing units (GPUs) or Intel Xeon Phi processors. We describe the mapping of the algorithms to accelerated processors, techniques used for eliminating redundant calculations due to symmetries, and strategies for efficient mapping of the calculations to many-node parallel systems. Results are presented demonstrating high per-node performance and near-ideal parallel scalability with rates of more than nine quadrillion elementwise comparisons achieved per second with the latest optimized code on the ORNL Titan system, this being orders of magnitude faster than rates achieved using other codes and platforms as reported in the literature. Also it is estimated that as many as 90 quadrillion comparisons per second may be achievable on the upcoming ORNL Summit system, an additional 10X performance increase. In a companion paper we describe corresponding techniques applied to calculations of the Proportional Similarity metric for comparative genomics applications.", "venue": "Parallel Comput.", "authors": ["Wayne  Joubert", "James  Nance", "Sharlee  Climer", "Deborah A. Weighill", "Daniel A. Jacobson"], "year": 2019, "n_citations": 10}
{"id": 2904628, "s2_id": "c1bbcb0a7566ed63e739ee148c2b1d6b563bcb04", "title": "PPT-Multicore: Performance Prediction of OpenMP applications using Reuse Profiles and Analytical Modeling", "abstract": "We present PPT-Multicore, an analytical model embedded in the Performance Prediction Toolkit (PPT) to predict parallel application performance running on a multicore processor. PPT-Multicore builds upon our previous work towards a multicore cache model. We extract LLVM basic block labeled memory trace using an architecture-independent LLVM-based instrumentation tool only once in an application\u2019s lifetime. The model uses the memory trace and other parameters from an instrumented sequentially executed binary. We use a probabilistic and computationally efficient reuse profile to predict the cache hit rates and runtimes of OpenMP programs\u2019 parallel sections. We model Intel\u2019s Broadwell, Haswell, and AMD\u2019s Zen2 architectures and validate our framework using different applications from PolyBench and PARSEC benchmark suites. The results show that PPT-Multicore can predict cache hit rates with an overall average error rate of 1.23% while predicting the runtime with an error rate of 9.08%.", "venue": "ArXiv", "authors": ["Atanu  Barai", "Yehia  Arafa", "Abdel-Hameed  Badawy", "Gopinath  Chennupati", "Nandakishore  Santhi", "Stephan  Eidenbenz"], "year": 2021, "n_citations": 1}
{"id": 2905334, "s2_id": "3ee87151808efb077768467402f169bf80b9f330", "title": "Scaling TensorFlow to 300 million predictions per second", "abstract": "We present the process of transitioning machine learning models to the TensorFlow framework at a large scale in an online advertising ecosystem. In this talk we address the key challenges we faced and describe how we successfully tackled them; notably, implementing the models in TF and serving them efficiently with low latency using various optimization techniques.", "venue": "RecSys", "authors": ["Jan  Hartman", "Davorin  Kopic"], "year": 2021, "n_citations": 0}
{"id": 2906214, "s2_id": "2c335261d779a5a31e488771cda26de6847954b1", "title": "Deseeding energy consumption of network stacks", "abstract": "Regular works on energy efficiency strategies for wireless communications are based on classical energy models that account for the wireless card only. Nevertheless, there is a non-negligible energy toll called cross-factor that encompasses the energy drained while a frame crosses the network stack of an OS. This paper addresses the challenge of deepen into the roots of the cross-factor, deseed its components and analyse its causes. Energy issues are critical for IoT devices. Thus, this paper conceives and validates a new comprehensive framework that enables us to measure a wide range of wireless devices, as well as multiple devices synchronously. We also present a rigorous methodology to perform whole-device energy measurements in laptops, a more generic and suitable device to perform energy debugging. Finally, and using this framework, we provide a collection of measurements and insights that deepens our understanding of the cross-factor.", "venue": "2015 IEEE 1st International Forum on Research and Technologies for Society and Industry Leveraging a better tomorrow (RTSI)", "authors": ["I\u00f1aki  Ucar", "Arturo  Azcorra"], "year": 2015, "n_citations": 2}
{"id": 2907759, "s2_id": "8d9c5042c2065020faabebbd3b1e6196fe5b8053", "title": "Acceleration of multiple precision matrix multiplication based on multi-component floating-point arithmetic using AVX2", "abstract": "In this paper, we report the results obtained from the acceleration of multi-binary64-type multiple precision matrix multiplication with AVX2. We target double-double (DD), triple-double (TD), and quad-double (QD) precision arithmetic designed by certain types of error-free transformation (EFT) arithmetic. Furthermore, we implement SIMDized EFT functions, which simultaneously compute with four binary64 numbers on x86 64 computing environment, and by using help of them, we also develop SIMDized DD, TD, and QD additions and multiplications. In addition, AVX2 load/store functions were adopted to efficiently speed up reading and storing matrix elements from/to memory. Owing to these combined techniques, our implemented multiple precision matrix multiplications have been accelerated more than three times compared with non-accelerated ones. Our accelerated matrix multiplication modifies the performance of parallelization with OpenMP.", "venue": "ICCSA", "authors": ["Tomonori  Kouya"], "year": 2021, "n_citations": 2}
{"id": 2909373, "s2_id": "c0a0cab6cef007d2fcbf39600235852c02ee98c1", "title": "QPEP: A QUIC-Based Approach to Encrypted Performance Enhancing Proxies for High-Latency Satellite Broadband", "abstract": "Satellite broadband services are critical infrastructures enabling advanced technologies to function in the most remote regions of the globe. However, status-quo services are often unencrypted by default and vulnerable to eavesdropping attacks. In this paper, we challenge the historical perception that over-the-air security must trade off with TCP performance in high-latency satellite networks due to the deep-packet inspection requirements of Performance Enhancing Proxies (PEPs). \nAfter considering why prior work in this area has failed to find wide adoption, we present an open-source encrypted-by-default PEP - QPEP - which seeks to address these issues. QPEP is built around the open QUIC standard and designed so individual customers may adopt it without ISP involvement. QPEP's performance is assessed through simulations in a replicable docker-based testbed. Across many benchmarks and network conditions, QPEP is found to avoid the perceived security-encryption trade-off in PEP design. Compared to unencrypted PEP implementations, QPEP reduces average page load times by more than 30% while also offering over-the-air privacy. Compared to the traditional VPN encryption available to customers today, QPEP more than halves average page load times. Together, these experiments lead to the conclusion that QPEP represents a promising new approach to protecting modern satellite broadband connections.", "venue": "ArXiv", "authors": ["James  Pavur", "Martin  Strohmeier", "Vincent  Lenders", "Ivan  Martinovic"], "year": 2020, "n_citations": 2}
{"id": 2909454, "s2_id": "2d3caadedfc0654ed0c7adee93512812856345c6", "title": "A New Benchmark For Evaluation Of Graph-Theoretic Algorithms", "abstract": "We propose a new graph-theoretic benchmark in this paper. The benchmark is developed to address shortcomings of an existing widely-used graph benchmark. We thoroughly studied a large number of traditional and contemporary graph algorithms reported in the literature to have clear understanding of their algorithmic and run-time characteristics. Based on this study, we designed a suite of kernels, each of which represents a specific class of graph algorithms. The kernels are designed to capture the typical run-time behavior of target algorithms accurately, while limiting computational and spatial overhead to ensure its computation finishes in reasonable time. We expect that the developed benchmark will serve as a much needed tool for evaluating different architectures and programming models to run graph algorithms.", "venue": "ArXiv", "authors": ["Andy B. Yoo", "Yang  Liu", "Sheila  Vaidya", "Stephen W. Poole"], "year": 2010, "n_citations": 0}
{"id": 2911619, "s2_id": "b433b67571f4ca6e7fcfce2e3d93106ff23c8c18", "title": "Analysis of an M/M/1 Queue Using Fixed Order of Search for Arrivals and Service", "abstract": "We analyze an M/M/1 queue with a service discipline in which customers, upon arriving when the server is busy, search a sequence of stations for a vacant station at which to wait, and in which the server, upon becoming free when one or more customers are waiting, searches the stations in the same order for a station occupied by a customer to serve. We show how to find complete asymptotic expansions for all the moments of the waiting time in the heavy traffic limit. We show in particular that the variance of the waiting time for this discipline is more similar to that of last-come-first-served (which has a pole of order three as the arrival rate approaches the service rate) than that of first-come-first-served (which has pole of order two).", "venue": "ArXiv", "authors": ["Patrick  Eschenfeldt", "Ben  Gross", "Nicholas  Pippenger"], "year": 2011, "n_citations": 0}
{"id": 2912466, "s2_id": "ce6760bf55ae0946ad175a58b0b56fd44e0d6ca7", "title": "Stability and Optimization of Speculative Queueing Networks", "abstract": "We provide a queueing-theoretic framework for job replication schemes based on the principle \u201creplicate a job as soon as the system detects it as a straggler\u201d. This is called job speculation. Recent works have analyzed replication on arrival, which we refer to as replication. Replication is motivated by its implementation in Google\u2019s BigTable. However, systems such as Apache Spark and Hadoop MapReduce implement speculative job execution. The performance and optimization of speculative job execution is not well understood. To this end, we propose a queueing network model for load balancing where each server can speculate on the execution time of a job. Specifically, each job is initially assigned to a single server by a frontend dispatcher. Then, when its execution begins, the server sets a timeout. If the job completes before the timeout, it leaves the network, otherwise the job is terminated and relaunched or resumed at another server where it will complete. We provide a necessary and sufficient condition for the stability of speculative queueing networks with heterogeneous servers, general job sizes and scheduling disciplines. We find that speculation can increase the stability region of the network when compared with standard load balancing models and replication schemes. We provide general conditions under which timeouts increase the size of the stability region and derive a formula for the optimal speculation time, i.e., the timeout that minimizes the load induced through speculation. We compare speculation with redundant-d and redundant-to-idle-queue-d rules under an S&X model. For light loaded systems, redundancy schemes provide better response times. However, for moderate to heavy loadings, redundancy schemes can lose capacity and have markedly worse response times when compared with the proposed speculative scheme.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Jonatha  Anselmi", "Neil  Walton"], "year": 2021, "n_citations": 0}
{"id": 2914762, "s2_id": "a8337e60a57e37d74d9584f2f0a3f22477b41976", "title": "Developing a Recommendation Benchmark for MLPerf Training and Inference", "abstract": "Deep learning-based recommendation models are used pervasively and broadly, for example, to recommend movies, products, or other information most relevant to users, in order to enhance the user experience. Among various application domains which have received significant industry and academia research attention, such as image classification, object detection, language and speech translation, the performance of deep learning-based recommendation models is less well explored, even though recommendation tasks unarguably represent significant AI inference cycles at large-scale datacenter fleets. To advance the state of understanding and enable machine learning system development and optimization for the commerce domain, we aim to define an industry-relevant recommendation benchmark for the MLPerf Training andInference Suites. The paper synthesizes the desirable modeling strategies for personalized recommendation systems. We lay out desirable characteristics of recommendation model architectures and data sets. We then summarize the discussions and advice from the MLPerf Recommendation Advisory Board.", "venue": "ArXiv", "authors": ["Carole-Jean  Wu", "Robin  Burke", "Ed  Chi", "Joseph  Konstan", "Julian  McAuley", "Yves  Raimond", "Hao  Zhang"], "year": 2020, "n_citations": 17}
{"id": 2915116, "s2_id": "bfc1fbd304707adf8e55f18b0ac71c70549a52e7", "title": "Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective", "abstract": "Graph Neural Networks (GNNs) have been widely used in various domains, and GNNs with sophisticated computational graph lead to higher latency and larger memory consumption. Optimizing the GNN computational graphs suffers from: (1) Redundant neural operator computation. The same data are propagated through the graph structure to perform the same neural operation multiple times in GNNs, leading to redundant computation which accounts for 92.4% of total operators. (2) Inconsistent thread mapping. Efficient thread mapping schemes for vertex-centric and edge-centric operators are different. This inconsistency prohibits operator fusion to reduce memory IO. (3) Excessive intermediate data. For GNN training which is usually performed concurrently with inference, intermediate data must be stored for the backward pass, consuming 91.9% of total memory requirement. To tackle these challenges, we propose following designs to optimize the GNN computational graph from a novel coordinated computation, IO, and memory perspective: (1) Propagation-postponed operator reorganization. We reorganize operators to perform neural operations before the propagation, thus the redundant computation is eliminated. (2) Unified thread mapping for fusion. We propose a unified thread mapping scheme for both vertexand edge-centric operators to enable fusion and reduce IO. (3) Intermediate data recomputation. Intermediate data are recomputed during the backward pass to reduce the total memory consumption. Extensive experimental results on three typical GNN models show that, we achieve up to 2.75\u00d7 end-to-end speedup, 6.89\u00d7 less memory IO, and 7.73\u00d7 less memory consumption over state-of-the-art frameworks.", "venue": "ArXiv", "authors": ["Hengrui  Zhang", "Zhongming  Yu", "Guohao  Dai", "Guyue  Huang", "Yufei  Ding", "Yuan  Xie", "Yu  Wang"], "year": 2021, "n_citations": 0}
{"id": 2915616, "s2_id": "b8cb5336976df8bf34ef7e15c6a8ba1d83a6e574", "title": "Scalable Analysis for Large Social Networks: The Data-Aware Mean-Field Approach", "abstract": "Studies on social networks have proved that endogenous and exogenous factors influence dynamics. Two streams of modeling exist on explaining the dynamics of social networks: 1) models predicting links through network properties, and 2) models considering the effects of social attributes. In this interdisciplinary study we work to overcome a number of computational limitations within these current models. We employ a mean-field model which allows for the construction of a population-specific model informed from empirical research for predicting links from both network and social properties in large social networks.. The model is tested on a population of conference coauthorship behavior, considering a number of parameters from available Web data. We address how large social networks can be modeled preserving both network and social parameters. We prove that the mean-field model, using a data-aware approach, allows us to overcome computational burdens and thus scalability issues in modeling large social networks in terms of both network and social parameters. Additionally, we confirm that large social networks evolve through both network and social-selection decisions; asserting that the dynamics of networks cannot singly be studied from a single perspective but must consider effects of social parameters.", "venue": "SocInfo", "authors": ["Julie M. Birkholz", "Rena  Bakhshi", "Ravindra  Harige", "Maarten van Steen", "Peter  Groenewegen"], "year": 2012, "n_citations": 4}
{"id": 2915758, "s2_id": "7afddf4c3aa354c6539252b332d2593533cb51c5", "title": "Understanding Cache Boundness of ML Operators on ARM Processors", "abstract": "Machine Learning (ML) compilers like TVM allow a fast and flexible deployment on embedded CPUs. This enables the use of non-standard operators, which are common in ML compression techniques. However, it is necessary to understand the limitations of typical compute-intense operators in ML workloads to design a proper solution. This is the first indetail analysis of dense and convolution operators, generated with TVM, that compares to the fundamental hardware limits of embedded ARM processors. Thereby it explains the gap between computational peak performance, theoretical and measured, and real-world state-of-the-art results, created with TVM and openBLAS. Instead, one can see that single-precision general matrix multiply (GEMM) and convolutions are bound by L1-cacheread bandwidth. Explorations of 8-bit and bit-serial quantized operators show that quantization can be used to achieve relevant speedups compared to cache-bound floating-point operators. However, the performance of quantized operators highly depends on the interaction between data layout and bit packing.", "venue": "ArXiv", "authors": ["Bernhard  Klein", "Christoph  Gratl", "Manfred  M\u00fccke", "Holger  Fr\u00f6ning"], "year": 2021, "n_citations": 0}
{"id": 2918397, "s2_id": "301b10f94630e548033edd75f59d865403cd2cf5", "title": "Spatial Multi-LRU Caching for Wireless Networks with Coverage Overlaps", "abstract": "This article introduces a novel family of decentralised caching policies for wireless networks, referred to as spatial multi-LRU. Based on these, cache inventories are updated in a way that provides content diversity to users that are covered by, and thus have access to, more than one station. Two variations are proposed, the multi-LRU-One and -All, which differ in the number of replicas inserted in the involved edge caches. Che-like approximations are proposed to accurately predict their hit probability under the Independent Reference Model (IRM). For IRM traffic multi-LRU-One outperforms multi-LRU-All, whereas when the traffic exhibits temporal locality the -All variation can perform better.", "venue": "SIGMETRICS 2016", "authors": ["Anastasios  Giovanidis", "Apostolos  Avranas"], "year": 2016, "n_citations": 26}
{"id": 2919405, "s2_id": "272ca328a75c7eba26205a2ed6db8661eec95aec", "title": "Online Caching with Optimal Switching Regret", "abstract": "We consider the classical uncoded caching problem from an online learning point-of-view. A cache of limited storage capacity can hold $C$ files at a time from a large catalog. A user requests an arbitrary file from the catalog at each time slot. The request sequence could be adversarial. Before the file request from the user arrives, a caching policy populates the cache with any $C$ files of its choice. In the case of a cache-hit, the policy receives a unit reward and zero rewards otherwise. In addition to that, there is a cost associated with fetching files to the cache, which we refer to as the switching cost. The objective is to design a caching policy that incurs minimal regret while considering both the rewards due to cache-hits and the switching cost due to the file fetches. The main contribution of this paper is the switching regret analysis of a Follow The Perturbed LEADER-based anytime caching policy, which is shown to have an order optimal switching regret. In this pursuit, we improve the best-known switching regret bound for this problem by a factor of \u0398(\u221aC). We conclude the paper by comparing the performance of different popular caching policies using a publicly available trace from a commercial CDN server.", "venue": "2021 IEEE International Symposium on Information Theory (ISIT)", "authors": ["Samrat  Mukhopadhyay", "Abhishek  Sinha"], "year": 2021, "n_citations": 2}
{"id": 2919701, "s2_id": "ddbf1f6091dde83d31ece0014a946c40061c17a1", "title": "Shawn: A new approach to simulating wireless sensor networks", "abstract": "We consider the simulation of wireless sensor networks (WSN) using a new approach. We present Shawn, an open-source discrete-event simulator that has considerable differences to all other existing simulators. Shawn is very powerful in simulating large scale networks with an abstract point of view. It is, to the best of our knowledge, the first simulator to support generic high-level algorithms as well as distributed protocols on exactly the same underlying networks.", "venue": "ArXiv", "authors": ["Alexander  Kr\u00f6ller", "Dennis  Pfisterer", "Carsten  Buschmann", "S\u00e1ndor P. Fekete", "Stefan  Fischer"], "year": 2005, "n_citations": 97}
{"id": 2921585, "s2_id": "46943a2194c28404faba2a07681cfb01840ba1e2", "title": "MemComputing Integer Linear Programming", "abstract": "Author(s): Traversa, Fabio L; Ventra, Massimiliano Di | Abstract: Integer linear programming (ILP) encompasses a very important class of optimization problems that are of great interest to both academia and industry. Several algorithms are available that attempt to explore the solution space of this class efficiently, while requiring a reasonable compute time. However, although these algorithms have reached various degrees of success over the years, they still face considerable challenges when confronted with particularly hard problem instances, such as those of the MIPLIB 2010 library. In this work we propose a radically different non-algorithmic approach to ILP based on a novel physics-inspired computing paradigm: Memcomputing. This paradigm is based on digital (hence scalable) machines represented by appropriate electrical circuits with memory. These machines can be either built in hardware or, as we do here, their equations of motion can be efficiently simulated on our traditional computers. We first describe a new circuit architecture of memcomputing machines specifically designed to solve for the linear inequalities representing a general ILP problem. We call these self-organizing algebraic circuits, since they self-organize dynamically to satisfy the correct (algebraic) linear inequalities. We then show simulations of these machines using MATLAB running on a single core of a Xeon processor for several ILP benchmark problems taken from the MIPLIB 2010 library, and compare our results against a renowned commercial solver. We show that our approach is very efficient when dealing with these hard problems. In particular, we find within minutes feasible solutions for one of these hard problems (f2000 from MIPLIB 2010) whose feasibility, to the best of our knowledge, has remained unknown for the past eight years.", "venue": "ArXiv", "authors": ["Fabio L. Traversa", "Massimiliano Di Ventra"], "year": 2018, "n_citations": 11}
{"id": 2926290, "s2_id": "f888397a021ed056f312ce791923a964ee04dc99", "title": "Optimizing Communication by Compression for Multi-GPU Scalable Breadth-First Searches", "abstract": "The Breadth First Search (BFS) algorithm is the foundation and building block of many higher graph-based operations such as spanning trees, shortest paths and betweenness centrality. The importance of this algorithm increases each day due to it is a key requirement for many data structures which are becoming popular nowadays. When the BFS algorithm is parallelized by distributing the graph between several processors the interconnection network limits the performance. Hence, improvements on this area may benefit the overall performance of the algorithm. \nThis work presents an alternative compression scheme for communications in distributed BFS processing. It focuses on BFS processors using General-Purpose Graphics Processing Units.", "venue": "ArXiv", "authors": ["Julian  Romera"], "year": 2017, "n_citations": 1}
{"id": 2929187, "s2_id": "93e0bd251b214fb1d427de38888e10ef00c6e317", "title": "On Memory Accelerated Signal Processing within Software Defined Radios", "abstract": "Since J. Mitola's work in 1992, Software Defined Radios (SDRs) have been quite a hot topic in wireless systems research. Though many notable achievements were reported in the field, the scarcity of computational power on general purpose CPUs has always constrained their wide adoption in production environments. If conveniently applied within an SDR context, classical concepts known in computer science as space/time tradeoffs can be extremely helpful when trying to mitigate this problem. Inspired by and building on those concepts, this paper presents a novel SDR implementation technique which we call Memory Acceleration (MA) that makes extensive use of the memory resources available on a general purpose computing system, in order to accelerate signal computation. MA can provide substantial acceleration factors when applied to conventional SDRs without reducing their peculiar flexibility. As a practical proof of this, an example of MA applied in the real world to the ETSI DVB-T Viterbi decoder is provided. Actually MA is shown able to provide, when applied to such Viterbi decoder, an acceleration factor of 10.4x, with no impact on error correction performances of the decoder and by making no use of any other typical performance enhancement techniques such as low level (Assembler) programming or parallel computation, which though remain compatible with MA. Opportunity for extending the MA approach to the entire radio system, thus implementing what we call a Memory-Based Software Defined Radio (MB-SDR) is finally considered and discussed.", "venue": "ArXiv", "authors": ["Vincenzo  Pellegrini", "Luca  Rose", "Mario Di Dio"], "year": 2010, "n_citations": 1}
{"id": 2933260, "s2_id": "d1f74ef34a83d053cefaa0343efa0abc19e3047a", "title": "Accelerating XOR-based erasure coding using program optimization techniques", "abstract": "Erasure coding (EC) affords data redundancy for large-scale systems. XOR-based EC is an easy-to-implement method for optimizing EC. This paper addresses a significant performance gap between the state-of-the-art XOR-based EC approach (~4.9 GB/s coding throughput) and Intel's high-performance EC library based on another approach (~6.7 GB/s). We propose a novel approach based on our observation that XOR-based EC virtually generates programs of a Domain Specific Language for XORing byte arrays. We formalize such programs as straight-line programs (SLPs) of compiler construction and optimize SLPs using various program optimization techniques. Our optimization flow is three-fold: 1) reducing the number of XORs using grammar compression algorithms; 2) reducing memory accesses using deforestation, a functional program optimization method; and 3) reducing cache misses using the (red-blue) pebble game of program analysis. We provide an experimental library, which outperforms Intel's library with an ~8.92 GB/s throughput.", "venue": "SC", "authors": ["Yuya  Uezato"], "year": 2021, "n_citations": 0}
{"id": 2935356, "s2_id": "9c46845ce2c782fcd157484b4f1e24af5c185284", "title": "GPU-Accelerated Discontinuous Galerkin Methods: 30x Speedup on 345 Billion Unknowns", "abstract": "A discontinuous Galerkin method for the discretization of the compressible Euler equations, the governing equations of inviscid fluid dynamics, on Cartesian meshes is developed for use of Graphical Processing Units via OCCA, a unified approach to performance portability on multi-threaded hardware architectures. A 30x time-to-solution speedup over CPU-only implementations using non-CUDA-Aware MPI communications is demonstrated up to 1,536 NVIDIA V100 GPUs and parallel strong scalability is shown up to 6,144 NVIDIA V100 GPUs for a problem containing 345 billion unknowns. A comparison of CUDA-Aware MPI communication to non-GPUDirect communication is performed demonstrating an additional 24 % speedup on eight nodes composed of 32 NVIDIA V100 GPUs.", "venue": "2020 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Andrew C. Kirby", "Dimitri J. Mavriplis"], "year": 2020, "n_citations": 2}
{"id": 2935596, "s2_id": "1fd79d89759f181a7a3905db417d8b5da0007c18", "title": "ELDA: Towards efficient and lightweight detection of cache pollution attacks in NDN", "abstract": "As a promising architectural design for future Internet, named data networking (NDN) relies on in-network caching to efficiently deliver name-based content. However, the in-network caching is vulnerable to cache pollution attacks (CPA), which can reduce cache hits by violating cache locality and significantly degrade the overall performance of NDN. To defend against CPA attacks, the most effective way is to first detect the attacks and then throttle them. Since the CPA attack itself has already imposed a huge burden on victims, to avoid exhausting the remaining resources on the victims for detection purpose, we expect a lightweight detection solution. We thus propose ELDA, an Efficient and Lightweight Detection scheme against cache pollution Attacks, in which we design a Lightweight Flajolet-Martin (LFM) sketch to monitor the interest traffic. Our analysis and simulations demonstrate that, by consuming a few computation and memory resources, ELDA can effectively and efficiently detect CPA attacks.", "venue": "2015 IEEE 40th Conference on Local Computer Networks (LCN)", "authors": ["Zhiwei  Xu", "Bo  Chen", "Ninghan  Wang", "Yujun  Zhang", "Zhongcheng  Li"], "year": 2015, "n_citations": 17}
{"id": 2935601, "s2_id": "a6c80335acb9736298817b99e05f78eb958d7f3e", "title": "Integration of the Packetdrill Testing Tool in INET", "abstract": "Google released in 2013 a script-based tool called packetdrill, which allows to test transport protocols like UDP and TCP on Linux and BSD-based operating systems. The scripts defining a test-case allow to inject packets to the implementation under test, perform operations at the API controlling the transport protocol and verify the sending of packets, all at specified times. This paper describes a port of packetdrill to the INET framework for the OMNeT++ simulation environment providing a simple and powerful method of testing the transport protocols implemented in INET.", "venue": "ArXiv", "authors": ["Irene  R\u00fcngeler", "Michael  T\u00fcxen"], "year": 2015, "n_citations": 2}
{"id": 2940048, "s2_id": "d782341a0d975e58a8a1625202181f4299b1a836", "title": "Enhanced Innovized Repair Operator for Evolutionary Multi- and Many-objective Optimization", "abstract": "\"Innovization\" is a task of learning common relationships among some or all of the Pareto-optimal (PO) solutions in multi- and many-objective optimization problems. Recent studies have shown that a chronological sequence of non-dominated solutions obtained in consecutive iterations during an optimization run also possess salient patterns that can be used to learn problem features to help create new and improved solutions. In this paper, we propose a machine-learning- (ML-) assisted modelling approach that learns the modifications in design variables needed to advance population members towards the Pareto-optimal set. We then propose to use the resulting ML model as an additional innovized repair (IR2) operator to be applied on offspring solutions created by the usual genetic operators, as a novel mean of improving their convergence properties. In this paper, the well-known random forest (RF) method is used as the ML model and is integrated with various evolutionary multi- and many-objective optimization algorithms, including NSGA-II, NSGA-III, and MOEA/D. On several test problems ranging from two to five objectives, we demonstrate improvement in convergence behaviour using the proposed IR2-RF operator. Since the operator does not demand any additional solution evaluations, instead using the history of gradual and progressive improvements in solutions over generations, the proposed ML-based optimization opens up a new direction of optimization algorithm development with advances in AI and ML approaches.", "venue": "ArXiv", "authors": ["Sukrit  Mittal", "Dhish Kumar Saxena", "Kalyanmoy  Deb", "Erik  Goodman"], "year": 2020, "n_citations": 1}
{"id": 2944150, "s2_id": "0e45ad2b01c5cdebad0bdba1db4e2d4f95503988", "title": "Location Aggregation of Spatial Population CTMC Models", "abstract": "In this paper we focus on spatial Markov population models, describing the stochastic evolution of populations of agents, explicitly modelling their spatial distribution, representing space as a discrete, finite graph. More specifically, we present a heuristic approach to aggregating spatial locations, which is designed to preserve the dynamical behaviour of the model whilst reducing the computational cost of analysis. Our approach combines stochastic approximation ideas (moment closure, linear noise), with computational statistics (spectral clustering) to obtain an efficient aggregation, which is experimentally shown to be reasonably accurate on two case studies: an instance of epidemic spreading and a London bike sharing scenario.", "venue": "QAPL", "authors": ["Luca  Bortolussi", "Cheng  Feng"], "year": 2016, "n_citations": 0}
{"id": 2946009, "s2_id": "25cdbcbb89e9f277b0c50676105e7f5ded6426f2", "title": "Infinite Servers Queue Systems Busy Period Time Length Distribution and Parameters Study through Computational Simulation", "abstract": "A FORTRAN program to simulate the operation of infinite servers\u2019 queues is presented in this work. Poisson arrivals processes are considered but not only. For many parameters of interest in queuing systems study or application, either there are not theoretical results or, existing, they are mathematically intractable what makes their utility doubtful. In this case a possible issue is to use simulation methods in order to get more useful results. Indeed, using simulation, some experiences may be performed and the respective results used to conjecture about certain queue systems interesting quantities. In this paper this procedure is followed to learn something more about quantities of interest for those infinite servers queue systems, in particular about busy period parameters and probability distributions.", "venue": "ArXiv", "authors": ["Manuel Alberto M. Ferreira"], "year": 2021, "n_citations": 0}
{"id": 2949946, "s2_id": "683e0cf68992fdf7d5c95c1e84178f63a82f7fdb", "title": "A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions", "abstract": "The field of machine programming (MP), the automation of the development of software, is making notable research advances. This is, in part, due to the emergence of a wide range of novel techniques in machine learning. In this paper, we apply MP to the automation of software performance regression testing. A performance regression is a software performance degradation caused by a code change. We present AutoPerf - a novel approach to automate regression testing that utilizes three core techniques: (i) zero-positive learning, (ii) autoencoders, and (iii) hardware telemetry. We demonstrate AutoPerf's generality and efficacy against 3 types of performance regressions across 10 real performance bugs in 7 benchmark and open-source programs. On average, AutoPerf exhibits 4% profiling overhead and accurately diagnoses more performance bugs than prior state-of-the-art approaches. Thus far, AutoPerf has produced no false negatives.", "venue": "NeurIPS", "authors": ["Mejbah  Alam", "Justin  Gottschlich", "Nesime  Tatbul", "Javier  Turek", "Timothy  Mattson", "Abdullah  Muzahid"], "year": 2019, "n_citations": 21}
{"id": 2950533, "s2_id": "91d5661fb5af651144884c747a49da0e956a3616", "title": "An In-Depth Analysis of the Slingshot Interconnect", "abstract": "The interconnect is one of the most critical components in large scale computing systems, and its impact on the performance of applications is going to increase with the system size. In this paper, we will describe Slingshot, an interconnection network for large scale computing systems. Slingshot is based on high-radix switches, which allow building exascale and hyperscale datacenters networks with at most three switch-to-switch hops. Moreover, Slingshot provides efficient adaptive routing and congestion control algorithms, and highly tunable traffic classes. Slingshot uses an optimized Ethernet protocol, which allows it to be interoperable with standard Ethernet devices while providing high performance to HPC applications. We analyze the extent to which Slingshot provides these features, evaluating it on microbenchmarks and on several applications from the datacenter and AI worlds, as well as on HPC applications. We find that applications running on Slingshot are less affected by congestion compared to previous generation networks.", "venue": "SC", "authors": ["Daniele De Sensi", "Salvatore Di Girolamo", "Kim H. McMahon", "Duncan  Roweth", "Torsten  Hoefler"], "year": 2020, "n_citations": 13}
{"id": 2952128, "s2_id": "2012e5549a60e0e7d969088a96f2780745e10b54", "title": "Parametric Estimation of the Ultimate Size of Hypercomputers", "abstract": "The performance of the emerging petaflops-scale supercomputers of the nearest future (hypercomputers) will be governed not only by the clock frequency of the processing nodes or by the width of the system bus, but also by such factors as the overall power consumption and the geometric size. In this paper, we study the influence of such parameters on one of the most important characteristics of a general purpose computer - on the degree of multithreading that must be present in an application to make the use of the hypercomputer justifiable. Our major finding is that for the class of applications with purely random memory access patterns \"super-fast computing\" and \"high-performance computing\" are essentially synonyms for \"massively-parallel computing.\"", "venue": "ArXiv", "authors": ["Dmitry  Zinoviev"], "year": 2011, "n_citations": 0}
{"id": 2952598, "s2_id": "cc2dfc09191fa3947fe3013600df88d05fb1c00b", "title": "Low-Latency Software Polar Decoders", "abstract": "Polar codes are a new class of capacity-achieving error-correcting codes with low encoding and decoding complexity. Their low-complexity decoding algorithms rendering them attractive for use in software-defined radio applications where computational resources are limited. In this work, we present low-latency software polar decoders that exploit modern processor capabilities. We show how adapting the algorithm at various levels can lead to significant improvements in latency and throughput, yielding polar decoders that are suitable for high-performance software-defined radio applications on modern desktop processors and embedded-platform processors. These proposed decoders have an order of magnitude lower latency and memory footprint compared to state-of-the-art decoders, while maintaining comparable throughput. In addition, we present strategies and results for implementing polar decoders on graphical processing units. Finally, we show that the energy efficiency of the proposed decoders is comparable to state-of-the-art software polar decoders.", "venue": "J. Signal Process. Syst.", "authors": ["Pascal  Giard", "Gabi  Sarkis", "Camille  Leroux", "Claude  Thibeault", "Warren J. Gross"], "year": 2018, "n_citations": 21}
{"id": 2961369, "s2_id": "fd83261c79bd001414de031e3f006d573cf468b8", "title": "High Performance Direct Gravitational N-body Simulations on Graphics Processing Units", "abstract": "We present the results of gravitational direct N-body simulations using the graphics processing unit (GPU) on a commercial NVIDIA GeForce 8800GTX designed for gaming computers. The force evaluation of the N-body problem is implemented in \u2018\u2018Compute Unified Device Architecture\u2019\u2019 (CUDA) using the GPU to speedup the calculations. We tested the implementation on three different N-body codes: two direct N-body integration codes, using the 4th order predictor\u2013corrector Hermite integrator with block time-steps, and one Barnes-Hut treecode, which uses a 2nd order leapfrog integration scheme. The integration of the equations of motions for all codes is performed on the host CPU. We find that for N > 512 particles the GPU outperforms the GRAPE-6Af, if some softening in the force calculation is accepted. Without softening and for very small integration time-steps the GRAPE still outperforms the GPU. We conclude that modern GPUs offer an attractive alternative to GRAPE-6Af special purpose hardware. Using the same time-step criterion, the total energy of the N-body system was conserved better than to one in 10 6 on the GPU, only about an order of magnitude worse than obtained with GRAPE-6Af. For N J 10 5 the 8800GTX outperforms the host CPU by a factor of about 100 and runs at about the same speed as the GRAPE-6Af.", "venue": "ArXiv", "authors": ["Simon Portegies Zwart", "Robert G. Belleman", "Peter  Geldof"], "year": 2007, "n_citations": 187}
{"id": 2961422, "s2_id": "e16cd7c43c43486c454292e21545e0665bccda31", "title": "A Multi-Stage CUDA Kernel for Floyd-Warshall", "abstract": "We present a new implementation of the Floyd-Warshall All-Pairs Shortest Paths algorithm on CUDA. Our algorithm runs approximately 5 times faster than the previously best reported algorithm. In order to achieve this speedup, we applied a new technique to reduce usage of on-chip shared memory and allow the CUDA scheduler to more effectively hide instruction latency.", "venue": "ArXiv", "authors": ["Ben D. Lund", "Justin W. Smith"], "year": 2010, "n_citations": 26}
{"id": 2965367, "s2_id": "d15d6271293893fe208f194ee72858ded95093d2", "title": "The Implications of Diverse Applications and Scalable Data Sets in Benchmarking Big Data Systems", "abstract": "Now we live in an era of big data, and big data applications are becoming more and more pervasive. How to benchmark data center computer systems running big data applications in short big data systems is a hot topic. In this paper, we focus on measuring the performance impacts of diverse applications and scalable volumes of data sets on big data systems. For four typical data analysis applications--an important class of big data applications, we find two major results through experiments: first, the data scale has a significant impact on the performance of big data systems, so we must provide scalable volumes of data sets in big data benchmarks. Second, for the four applications, even all of them use the simple algorithms, the performance trends are different with increasing data scales, and hence we must consider not only variety of data sets but also variety of applications in benchmarking big data systems.", "venue": "WBDB", "authors": ["Zhen  Jia", "Runlin  Zhou", "Chunge  Zhu", "Lei  Wang", "Wanling  Gao", "Yingjie  Shi", "Jianfeng  Zhan", "Lixin  Zhang"], "year": 2012, "n_citations": 15}
{"id": 2966343, "s2_id": "5a4266ce2cea7b9ed14ee2a703d754f83c30d9f0", "title": "Queues with Small Advice", "abstract": "Motivated by recent work on scheduling with predicted job sizes, we consider the performance of scheduling algorithms with minimal advice, namely a single bit. Besides demonstrating the power of very limited advice, such schemes are quite natural. In the prediction setting, one bit of advice can be used to model a simple prediction as to whether a job is \"large\" or \"small\"; that is, whether a job is above or below a given threshold. Further, one-bit advice schemes can correspond to mechanisms that tell whether to put a job at the front or the back for the queue, a limitation which may be useful in many implementation settings. Finally, queues with a single bit of advice have a simple enough state that they can be analyzed in the limiting mean-field analysis framework for the power of two choices. Our work follows in the path of recent work by showing that even small amounts of even possibly inaccurate information can greatly improve scheduling performance.", "venue": "ACDA", "authors": ["Michael  Mitzenmacher"], "year": 2021, "n_citations": 4}
{"id": 2974940, "s2_id": "5e0b9af562d8c29984f779b0708f079181f658a9", "title": "Latency of Concatenating Unlicensed LPWAN with Cellular IoT: An Experimental QoE Study", "abstract": "Developing low-power wide-area network (LPWAN) solutions that are efficient to adopt, deploy and maintain are vital for smart cities. The poor quality-of-service of unlicensed LPWAN, and the high service cost of LTE-M/NB-IoT are key disadvantages of these technologies. Concatenating unlicensed with licensed LPWANs can overcome these limitations and harness their benefits. However, a concatenated LPWAN architecture will inevitably result in excess latency which may impact users\u2019 quality-of-experience (QoE). To evaluate the real-life feasibility of this system, we first propose a concatenated LPWAN architecture and experimentally measure the statistics of end-to-end (E2E) latencies. The concatenated delay margin is determined by benchmarking the latencies with different LPWAN architecture schemes, namely with unlicensed IoT (standalone LoRa), cellular IoT (standalone LTE-M), and concatenated IoT (LoRa interfaced with LTE-M). Through extensive experimental measurement campaigns of 30,000 data points of E2E latencies, we show that the excess delay due to LPWAN interfacing introduces on average less than 300 milliseconds. With a users\u2019 QoE satisfaction of 95%, we also found that concatenated LPWAN outperforms unlicensed IoT by roughly 1.5 s. Overall, the result suggests that a concatenated LPWAN is technically feasible and offers an affordable alternative for real-world smart city deployment.", "venue": "2021 IEEE 94th Vehicular Technology Conference (VTC2021-Fall)", "authors": ["Alvin  Ramoutar", "Zohreh  Motamedi", "Mouhamed  Abdulla"], "year": 2021, "n_citations": 0}
{"id": 2976721, "s2_id": "dafbf5ceacb8ea8dae6c3fced7f94c9c59b4bc83", "title": "A Performance Analysis Tool for Nokia Mobile Phone Software", "abstract": "Performance problems are often observed in embedded software systems. The reasons for poor performance are frequently not obvious. Bottlenecks can occur in any of the software components along the execution path. Therefore it is important to instrument and monitor the different components contributing to the runtime behavior of an embedded software system. Performance analysis tools can help locate performance bottlenecks in embedded software systems by monitoring the software's execution and producing easily understandable performance data. We maintain and further develop a tool for analyzing the performance of Nokia mobile phone software. The user can select among four performance analysis reports to be generated: average processor load, processor utilization, task execution time statistics, and task execution timeline. Each of these reports provides important information about where execution time is being spent. The demo will show how the tool helps to identify performance bottlenecks in Nokia mobile phone software and better understand areas of poor performance.", "venue": "ArXiv", "authors": ["Edu  Metz", "Raimondas  Lencevicius"], "year": 2003, "n_citations": 2}
{"id": 2977402, "s2_id": "c9a8baa4e0f6deadb3e6aafbf0be030b3bd5c677", "title": "Characterizing and subsetting big data workloads", "abstract": "Big data benchmark suites must include a diversity of data and workloads to be useful in fairly evaluating big data systems and architectures. However, using truly comprehensive benchmarks poses great challenges for the architecture community. First, we need to thoroughly understand the behaviors of a variety of workloads. Second, our usual simulation-based research methods become prohibitively expensive for big data. As big data is an emerging field, more and more software stacks are being proposed to facilitate the development of big data applications, which aggravates these challenges. In this paper, we first use Principle Component Analysis (PCA) to identify the most important characteristics from 45 metrics to characterize big data workloads from BigDataBench, a comprehensive big data benchmark suite. Second, we apply a clustering technique to the principle components obtained from the PCA to investigate the similarity among big data workloads, and we verify the importance of including different software stacks for big data benchmarking. Third, we select seven representative big data workloads by removing redundant ones and release the BigDataBench simulation version, which is publicly available from http://prof.ict.ac.cn/BigDataBench/simulatorversion/.", "venue": "2014 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Zhen  Jia", "Jianfeng  Zhan", "Lei  Wang", "Rui  Han", "Sally A. McKee", "Qiang  Yang", "Chunjie  Luo", "Jingwei  Li"], "year": 2014, "n_citations": 74}
{"id": 2978907, "s2_id": "a161b7794d1e917bfc8c5287de847980c8c07313", "title": "Asymptotic and Numerical Analysis of Multiserver Retrial Queue with Guard Channel for Cellular Networks", "abstract": "This paper considers a retrial queueing model for a base station in cellular networks where fresh calls and handover calls are available. Fresh calls are initiated from the cell of the base station. On the other hand, a handover call has been connecting to a base station and moves to another one. In order to keep the continuation of the communication, it is desired that an available channel in the new base station is immediately assigned to the handover call. To this end, a channel is reserved as the guard channel for handover calls in base stations. Blocked fresh and handover calls join a virtual orbit and repeat their attempts in a later time. We assume that a base station can recognize retrial calls and give them the same priority as that of handover calls. We model a base station by a multiserver retrial queue with priority customers for which a level-dependent QBD process is formulated. We obtain Taylor series expansion for the nonzero elements of the rate matrices of the level-dependent QBD. Using the expansion results, we obtain an asymptotic upper bound for the joint stationary distribution of the number of busy channels and that of customers in the orbit. Furthermore, we derive an efficient numerical algorithm to calculate the joint stationary distribution.", "venue": "ArXiv", "authors": ["Kazuki  Kajiwara", "Tuan  Phung-Duc"], "year": 2014, "n_citations": 1}
{"id": 2979612, "s2_id": "5a809c27f52993abd4fd48d46376483dfa223846", "title": "A Repairable System Supported by Two Spare Units and Serviced by Two Types of Repairers", "abstract": "We study a one-unit repairable system, supported by two identical spare units on cold standby, and serviced by two types of repairers. The model applies, for instance, to ANSI (American National Standard Institute) centrifugal pumps in a chemical plant. The failed unit undergoes repair either by an in-house repairer within a random or deterministic patience time, or else by a visiting expert repairer. The expert repairs one or all failed units before leaving, and does so faster but at a higher cost rate than the regular repairer. Four models arise depending on the number of repairs done by the expert and the nature of the patience time. We compare these models based on the limiting availability $A_{\\infty}$, and the limiting profit per unit time $\\omega$, using semi-Markov processes, when all distributions are exponential. As anticipated, to maximize $A_{\\infty}$, the expert should repair all failed units. To maximize $\\omega$, a suitably chosen deterministic patience time is better than a random patience time. Furthermore, given all cost parameters, we determine the optimum number of repairs the expert should complete, and the optimum patience time given to the regular repairer in order to maximize $\\omega$.", "venue": "ArXiv", "authors": ["Vahid  Andalib", "Jyotirmoy  Sarkar"], "year": 2019, "n_citations": 0}
{"id": 2983722, "s2_id": "489d31524040c43f43f5c05d06d343c9debeba35", "title": "Performance Acceleration of Kernel Polynomial Method Applying Graphics Processing Units", "abstract": "The Kernel Polynomial Method (KPM) is one of the fast diagonalization methods used for simulations of quantum systems in research fields of condensed matter physics and chemistry. The algorithm has a difficulty to be parallelized on a cluster computer or a supercomputer due to the fine-gain recursive calculations. This paper proposes an implementation of the KPM on the recent graphics processing units (GPU) where the recursive calculations are able to be parallelized in the massively parallel environment. This paper also illustrates performance evaluations regarding the cases when the actual simulation parameters are applied, the one for increased intensive calculations and the one for increased amount of memory usage. Finally, it concludes that the performance on GPU promises very high performance compared to the one on CPU and reduces the overall simulation time.", "venue": "2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum", "authors": ["Shixun  Zhang", "Shinichi  Yamagiwa", "Masahiko  Okumura", "Seiji  Yunoki"], "year": 2011, "n_citations": 6}
{"id": 2986003, "s2_id": "f2d7de4cc14600e10442f98df23c4e752d513d47", "title": "Analytical models for energy consumption in infrastructure WLAN STAs carrying TCP traffic", "abstract": "We develop analytical models for estimating the energy spent by stations (STAs) in infrastructure WLANs when performing TCP controlled file downloads. We focus on the energy spent in radio communication when the STAs are in the Continuously Active Mode (CAM), or in the static Power Save Mode (PSM). Our approach is to develop accurate models for obtaining the fraction of times the STA radios spend in idling, receiving and transmitting. We discuss two traffic models for each mode of operation: (i) each STA performs one large file download, and (ii) the STAs perform short file transfers. We evaluate the rate of STA energy expenditure with long file downloads, and show that static PSM is worse than just using CAM. For short file downloads we compute the number of file downloads that can be completed with given battery capacity, and show that PSM performs better than CAM for this case. We provide a validation of our analytical models using the NS-2 simulator.", "venue": "2010 Second International Conference on COMmunication Systems and NETworks (COMSNETS 2010)", "authors": ["Pranav  Agrawal", "A.  Kumar", "J.  Kuri", "M.  Panda", "V.  Navda", "R.  Ramjee", "V. N. Padmanabhan"], "year": 2010, "n_citations": 28}
{"id": 2988165, "s2_id": "b0063f40fb10dc79ab36bef565c65975146fec12", "title": "Improving Seek Time for Column Store Using MMH Algorithm", "abstract": "Hash based search has, proven excellence on large data warehouses stored in column store. Data distribution has significant impact on hash based search. To reduce impact of data distribution, we have proposed Memory Managed Hash (MMH) algorithm that uses shift XOR group for Queries and Transactions in column store. Our experiments show that MMH improves read and write throughput by 22% for TPC-H distribution.", "venue": "ArXiv", "authors": ["Tejaswini  Apte", "Maya  Ingle", "A. K. Goyal"], "year": 2012, "n_citations": 0}
{"id": 2993479, "s2_id": "b83d1b8e4e54a890dc0a5d172cc65ebb336c3bc8", "title": "An architecture-based dependability modeling framework using AADL", "abstract": "For efficiency reasons, the software system designers' will is to use an integrated set of methods and tools to describe specifications and designs, and also to perform analyses such as dependability, schedulability and performance. AADL (Architecture Analysis and Design Language) has proved to be efficient for software architecture modeling. In addition, AADL was designed to accommodate several types of analyses. This paper presents an iterative dependency-driven approach for dependability modeling using AADL. It is illustrated on a small example. This approach is part of a complete framework that allows the generation of dependability analysis and evaluation models from AADL models to support the analysis of software and system architectures, in critical application domains.", "venue": "ICSE 2007", "authors": ["Ana-Elena  Rugina", "Karama  Kanoun", "Mohamed  Ka\u00e2niche"], "year": 2007, "n_citations": 39}
{"id": 2999165, "s2_id": "1cbfb8b65746cd49723234fb6692809cb638f353", "title": "A Design of Endurance Queue for Co-Existing Systems in Multi-Programmed Environments", "abstract": "These days enterprise applications try to integrate online processing and batch jobs into a common software stack for seamless monitoring and driverless operations. Continuous integration of these systems results in choking of the poorly performing sub-systems, when the service demand and throughput are not synchronized. A poorly performing sub-system may become a serious performance bottleneck for the entire system if its serviceability and the capacity is over utilized by increased service demand from upstream systems. From all the integrated sub-systems, queuing systems are majorly categorized as choking elements due to their limited service length and lack of processing details. The situation becomes more pronounced in multiprogramming environments where the queue performance exponentially degrades with increased degree of multiprogramming at upstream levels. This paper presents an approach to compute the queue length and devise a distribution model such that the queue length is dynamically adjusted depending on the sudden growth or decline of transmission packets. The idea is to design a heat map of the memory and correlate it with the queue length distribution. With each degree of multi-programmability, the data processing logic is adjusted by the distribution model to arrive at an endurance level queue for long term service under variable load conditions. It will take away the current implementation of using delayed processing logic and/or batch processing of data at downstream systems.", "venue": "ArXiv", "authors": ["Subrata  Ashe"], "year": 2015, "n_citations": 0}
{"id": 3001633, "s2_id": "605a540c211a79c27fcc4139c0b640069b83e491", "title": "Testing Docker Performance for HPC Applications", "abstract": "The main goal for this article is to compare performance penalties when using KVM virtualization and Docker containers for creating isolated environments for HPC applications. The article provides both data obtained using commonly accepted synthetic tests (High Performance Linpack) and real life applications (OpenFOAM). The article highlights the influence on resulting application performance of major infrastructure configuration options: CPU type presented to VM, networking connection type used.", "venue": "ArXiv", "authors": ["Alexey  Ermakov", "Alexey  Vasyukov"], "year": 2017, "n_citations": 3}
{"id": 3002950, "s2_id": "eb1f6cb75fd74ed708ee371d1270f9f671d6b05c", "title": "Sharp Waiting-Time Bounds for Multiserver Jobs", "abstract": "Multiserver jobs, which are jobs that occupy multiple servers simultaneously during service, are prevalent in today\u2019s computing clusters. But little is known about the delay performance of systems with multiserver jobs. We consider queueing models for multiserver jobs in a scaling regime where the total number of servers in the system becomes large and meanwhile both the system load and the number of servers that a job needs scale with the total number of servers. Prior work has derived upper bounds on the queueing probability in this scaling regime. However, without proper lower bounds, the existing results cannot be used to differentiate between policies. In this paper, we study the delay performance by establishing sharp bounds on the mean waiting time of multiserver jobs, where the waiting time of a job is the time spent in queueing rather than in service. We first consider the commonly used First-Come-First-Serve (FCFS) policy and characterize the exact order of its mean waiting time. We then prove a lower bound on the mean waiting time of all policies, and demonstrate that there is an order gap between this lower bound and the mean waiting time under FCFS. We finally complement the lower bound with an achievability result: we show that under a priority policy that we call P-Priority, the mean waiting time achieves the order of the lower bound. This achievability result implies the tightness of the lower bound, the asymptotic optimality of P-Priority, and the strict suboptimality of FCFS.", "venue": "ArXiv", "authors": ["Yige  Hong", "Weina  Wang"], "year": 2021, "n_citations": 1}
{"id": 3008015, "s2_id": "78b0633e2d1ae35f393b7b5667e59370d3cb2a11", "title": "Energy-Efficient Real-Time Heart Monitoring on Edge-Fog-Cloud Internet-of-Medical-Things", "abstract": "The recent developments in wearable devices and the Internet of Medical Things (IoMT) allow real-time monitoring and recording of electrocardiogram (ECG) signals. However, continuous monitoring of ECG signals is challenging in lowpower wearable devices due to energy and memory constraints. Therefore, in this paper, we present a novel and energy-efficient methodology for continuously monitoring the heart for lowpower wearable devices. The proposed methodology is composed of three different layers: 1) a Noise/Artifact detection layer to grade the quality of the ECG signals; 2) a Normal/Abnormal beat classification layer to detect the anomalies in the ECG signals, and 3) an Abnormal beat classification layer to detect diseases from ECG signals. Moreover, a distributed multi-output Convolutional Neural Network (CNN) architecture is used to decrease the energy consumption and latency between the edgefog/cloud. Our methodology reaches an accuracy of 99.2% on the well known MIT-BIH Arrhythmia dataset. Evaluation on real hardware shows that our methodology is suitable for devices having a minimum RAM of 32KB. Moreover, the proposed methodology achieves 7\u00d7 more energy efficiency compared to state-of-the-art works.", "venue": "IEEE Internet of Things Journal", "authors": ["Berken Utku Demirel", "Islam Abdelsalam Bayoumy", "Mohammad Abdullah Al Faruque"], "year": 2021, "n_citations": 0}
{"id": 3008096, "s2_id": "f528cb81841538be2e60aba3fcbc058131d97572", "title": "Sage: Using Unsupervised Learning for Scalable Performance Debugging in Microservices", "abstract": "Cloud applications are increasingly shifting from large monolithic services to complex graphs of loosely-coupled microservices. Despite the advantages of modularity and elasticity microservices offer, they also complicate cluster management and performance debugging, as dependencies between tiers introduce backpressure and cascading QoS violations. \nWe present Sage, a machine learning-driven root cause analysis system for interactive cloud microservices. Sage leverages unsupervised ML models to circumvent the overhead of trace labeling, captures the impact of dependencies between microservices to determine the root cause of unpredictable performance online, and applies corrective actions to recover a cloud service's QoS. In experiments on both dedicated local clusters and large clusters on Google Compute Engine we show that Sage consistently achieves over 93% accuracy in correctly identifying the root cause of QoS violations, and improves performance predictability.", "venue": "ArXiv", "authors": ["Yu  Gan", "Mingyu  Liang", "Sundar  Dev", "David  Lo", "Christina  Delimitrou"], "year": 2021, "n_citations": 0}
{"id": 3008764, "s2_id": "c32dea6f666e82f9ef2dbffe5eb627f3766c7919", "title": "End-to-End QoS Improvement of HSDPA End-User Multi-Flow Traffic Using RAN Buffer Management", "abstract": "High speed downlink packet access (HSDPA) was introduced to UMTS radio access segment to provide higher capacity for new packet switched services. As a result, packet switched sessions with multiple diverse traffic flows such as concurrent voice and data, or video and data being transmitted to the same user are a likely commonplace cellular packet data scenario. In HSDPA, radio access network (RAN) buffer management schemes are essential to support the end-to-end QoS of such sessions. Hence in this paper we present the end-to-end performance study of a proposed RAN buffer management scheme for multi-flow sessions via dynamic system-level HSDPA simulations. The scheme is an enhancement of a time-space priority (TSP) queuing strategy applied to the node B MAC-hs buffer allocated to an end user with concurrent real-time (RT) and non-real-time (NRT) flows during a multi-flow session. The experimental multi- flow scenario is a packet voice call with concurrent TCP-based file download to the same user. Results show that with the proposed enhancements to the TSP-based RAN buffer management, end-to-end QoS performance gains accrue to the NRT flow without compromising RT flow QoS of the same end user session.", "venue": "2008 New Technologies, Mobility and Security", "authors": ["Suleiman Y. Yerima", "Khalid  Al-Begain"], "year": 2008, "n_citations": 2}
{"id": 3010146, "s2_id": "15be6ed36368fc2b6fed3842ae79595318b54073", "title": "Parsing gigabytes of JSON per second", "abstract": "JavaScript Object Notation or JSON is a ubiquitous data exchange format on the web. Ingesting JSON documents can become a performance bottleneck due to the sheer volume of data. We are thus motivated to make JSON parsing as fast as possible. Despite the maturity of the problem of JSON parsing, we show that substantial speedups are possible. We present the first standard-compliant JSON parser to process gigabytes of data per second on a single core, using commodity processors. We can use a quarter or fewer instructions than a state-of-the-art reference parser like RapidJSON. Unlike other validating parsers, our software (simdjson) makes extensive use of single instruction and multiple data instructions. To ensure reproducibility, simdjson is freely available as open-source software under a liberal license.", "venue": "The VLDB Journal", "authors": ["Geoff  Langdale", "Daniel  Lemire"], "year": 2019, "n_citations": 23}
{"id": 3011726, "s2_id": "18261a6dd0c1c050e08bebc6e2cb3e3551fb5f3e", "title": "10 Years Later: Cloud Computing is Closing the Performance Gap", "abstract": "Can cloud computing infrastructures provide HPC-competitive performance for scientific applications broadly? Despite prolific related literature, this question remains open. Answers are crucial for designing future systems and democratizing high-performance computing. We present a multi-level approach to investigate the performance gap between HPC and cloud computing, isolating different variables that contribute to this gap. Our experiments are divided into (i) hardware and system microbenchmarks and (ii) user application proxies. The results show that today's high-end cloud computing can deliver HPC-competitive performance not only for computationally intensive applications, but also for memory- and communication-intensive applications -- at least at modest scales -- thanks to the high-speed memory systems and interconnects and dedicated batch scheduling now available on some cloud platforms.", "venue": "ICPE", "authors": ["Giulia  Guidi", "Marquita  Ellis", "Aydin  Buluc", "Katherine  Yelick", "David  Culler"], "year": 2021, "n_citations": 0}
{"id": 3011853, "s2_id": "a3578def9f15bf44a9d66585c33999eab76c6f52", "title": "Power Gradient Descent", "abstract": "The development of machine learning is promoting the search for fast and stable minimization algorithms. To this end, we suggest a change in the current gradient descent methods that should speed up the motion in flat regions and slow it down in steep directions of the function to minimize. It is based on a \"power gradient\", in which each component of the gradient is replaced by its versus-preserving $H$-th power, with $0<H<1$. We test three modern gradient descent methods fed by such variant and by standard gradients, finding the new version to achieve significantly better performances for the Nesterov accelerated gradient and AMSGrad. We also propose an effective new take on the ADAM algorithm, which includes power gradients with varying $H$.", "venue": "ArXiv", "authors": ["Marco  Baiesi"], "year": 2019, "n_citations": 1}
{"id": 3012872, "s2_id": "601a30bb20aa5dc5176e84cda4c339b17dd92aea", "title": "Performance Evaluation of Multiple TCP connections in iSCSI", "abstract": "Storage area networks (SANs) based on fibre channel have been used extensively in the last decade while iSCSI is fast becoming a serious contender due to its reduced costs and unified infrastructure. This work examines the performance of iSCSI with multiple TCP connections. Multiple TCP connections are often used to realize higher bandwidth but there may be no fairness in how bandwidth is distributed. We propose a mechanism to share congestion information across multiple flows in \"Fair-TCP\" for improved performance. Our results show that Fair-TCP significantly improves the performance for I/O intensive workloads.", "venue": "24th IEEE Conference on Mass Storage Systems and Technologies (MSST 2007)", "authors": ["K  BhargavaKumar", "Ganesh M. Narayan", "K.  Gopinath"], "year": 2007, "n_citations": 14}
{"id": 3015338, "s2_id": "edd11d0f7621dae3d7bc3093215cf7961c2c491b", "title": "Novel Matrix Hit and Run for Sampling Polytopes and Its GPU Implementation", "abstract": "We propose and analyze a new Markov Chain Monte Carlo algorithm that generates a uniform sample over full and non-full dimensional polytopes. This algorithm, termed \u201dMatrix Hit and Run\u201d (MHAR), is a modification of the Hit and Run framework. For the regime n 1 3 m, MHAR has a lower asymptotic cost per sample in terms of soft-O notation (O\u2217) than do existing sampling algorithms after a warm start. MHAR is designed to take advantage of matrix multiplication routines that require less computational and memory resources. Our tests show this implementation to be substantially faster than the hitandrun R package, especially for higher dimensions. Finally, we provide a python library based on Pytorch and a Colab notebook with the implementation ready for deployment in architectures with GPU or just CPU.", "venue": "ArXiv", "authors": ["Mario Vazquez Corte", "Luis V. Montiel"], "year": 2021, "n_citations": 0}
{"id": 3015556, "s2_id": "073cd649ec411dcd0b469a9e48ab72cd53c8c866", "title": "The Multi-branched Method of Moments for Queueing Networks", "abstract": "We propose a new exact solution algorithm for closed multiclass product-form queueing networks that is several orders of magnitude faster and less memory consuming than established methods for multiclass models, such as the Mean Value Analysis (MVA) algorithm. The technique generalizes the recently proposed Method of Moments (MoM) which, differently from MVA, recursively computes {higher-order} moments of queue lengths instead of mean values. The main contribution of this paper is to show that the information used in the MoM recursion can be increased by considering multiple recursive branches that evaluate models with fewer queues. This reformulation allows to define a simpler matrix difference equation for computing normalizing constants which leads to large computational savings with respect to the MoM recursion. Computational analysis shows many cases where the proposed algorithm is between 1,000 and 10,000 times faster and less memory consuming than MoM, thus extending the range of multiclass models where exact solutions are feasible.", "venue": "2009 Sixth International Conference on the Quantitative Evaluation of Systems", "authors": ["Giuliano  Casale"], "year": 2009, "n_citations": 1}
{"id": 3015579, "s2_id": "2e839d8bd3428d64e603544176c773fe66b12f90", "title": "A JSON Token-Based Authentication and Access Management Schema for Cloud SaaS Applications", "abstract": "Cloud computing is significantly reshaping the computing industry built around core concepts such as virtualization, processing power, connectivity and elasticity to store and share IT resources via a broad network. It has emerged as the key technology that unleashes the potency of Big Data, Internet of Things, Mobile and Web Applications, and other related technologies; but it also comes with its challenges &#x2013; such as governance, security, and privacy. This paper is focused on the security and privacy challenges of cloud computing with specific reference to user authentication and access management for cloud SaaS applications. The suggested model uses a framework that harnesses the stateless and secure nature of JWT for client authentication and session management. Furthermore, authorized access to protected cloud SaaS resources have been efficiently managed. Accordingly, a Policy Match Gate (PMG) component and a Policy Activity Monitor (PAM) component have been introduced. In addition, other subcomponents such as a Policy Validation Unit (PVU) and a Policy Proxy DB (PPDB) have also been established for optimized service delivery. A theoretical analysis of the proposed model portrays a system that is secure, lightweight and highly scalable for improved cloud resource security and management.", "venue": "2017 IEEE 5th International Conference on Future Internet of Things and Cloud (FiCloud)", "authors": ["Obinna  Ethelbert", "Faraz Fatemi Moghaddam", "Philipp  Wieder", "Ramin  Yahyapour"], "year": 2017, "n_citations": 17}
{"id": 3016962, "s2_id": "4eb236a8d78f3c8b19d6335884f4bff4864cea85", "title": "BatteryLab, A Distributed Power Monitoring Platform For Mobile Devices", "abstract": "Recent advances in cloud computing have simplified the way that both software development and testing are performed. Unfortunately, this is not true for battery testing for which state of the art test-beds simply consist of one phone attached to a power meter. These test-beds have limited resources, access, and are overall hard to maintain; for these reasons, they often sit idle with no experiment to run. In this paper, we propose to share existing battery testing setups and build BatteryLab, a distributed platform for battery measurements. Our vision is to transform independent battery testing setups into vantage points of a planetary-scale measurement platform offering heterogeneous devices and testing conditions. In the paper, we design and deploy a combination of hardware and software solutions to enable BatteryLab's vision. We then preliminarily evaluate BatteryLab's accuracy of battery reporting, along with some system benchmarking. We also demonstrate how BatteryLab can be used by researchers to investigate a simple research question.", "venue": "HotNets", "authors": ["Matteo  Varvello", "Kleomenis  Katevas", "Mihai  Plesa", "Hamed  Haddadi", "Benjamin  Livshits"], "year": 2019, "n_citations": 5}
{"id": 3020775, "s2_id": "671105275a2e49536ea7db09d2acf0b3e3b8539d", "title": "Automatic Performance Debugging of SPMD Parallel Programs", "abstract": "Automatic performance debugging of parallel applications usually involves two steps: automatic detection of performance bottlenecks and uncovering their root causes for performance optimization. Previous work fails to resolve this challenging issue in several ways: first, several previous efforts automate analysis processes, but present the results in a confined way that only identifies performance problems with apriori knowledge; second, several tools take exploratory or confirmatory data analysis to automatically discover relevant performance data relationships. However, these efforts do not focus on locating performance bottlenecks or uncovering their root causes. In this paper, we design and implement an innovative system, AutoAnalyzer, to automatically debug the performance problems of single program multi-data (SPMD) parallel programs. Our system is unique in terms of two dimensions: first, without any apriori knowledge, we automatically locate bottlenecks and uncover their root causes for performance optimization; second, our method is lightweight in terms of size of collected and analyzed performance data. Our contribution is three-fold. First, we propose a set of simple performance metrics to represent behavior of different processes of parallel programs, and present two effective clustering and searching algorithms to locate bottlenecks. Second, we propose to use the rough set algorithm to automatically uncover the root causes of bottlenecks. Third, we design and implement the AutoAnalyzer system, and use two production applications to verify the effectiveness and correctness of our methods. According to the analysis results of AutoAnalyzer, we optimize two parallel programs with performance improvements by minimally 20% and maximally 170%.", "venue": "ArXiv", "authors": ["Xu  Liu", "Lin  Yuan", "Jianfeng  Zhan", "Bibo  Tu", "Dan  Meng"], "year": 2010, "n_citations": 1}
{"id": 3025165, "s2_id": "e6059dbbb280f0983f6fed1732e7ca6ff0eb7cf9", "title": "Wireless Performance Evaluation of Building Layouts: Closed-Form Computation of Figures of Merit", "abstract": "This paper presents a part of our ground-breaking work on evaluation of buildings in terms of wireless friendliness at the building-design stage. The main goal is to devise building design practices that provide for a good performance of wireless networks deployed in buildings. In this paper, the interference gain (IG) and power gain (PG) are defined as two figures of merit (FoM) of the wireless performance of buildings. The FoMs bridge the gap between building design and wireless communications industries. An approach to derive exact closed-form equations for these FoMs is proposed for the first time. The derived analytic expressions facilitate straightforward and more computationally efficient numerical evaluation of the proposed FoMs as compared to Monte Carlo simulations for well-known indoor propagation models. It is shown that the derived closed-form expression can be readily employed to evaluate the impact of building properties, such as the sizes and the aspect ratios (ARs) of rooms, on the wireless performance. The proposed approach sheds light to architects on evaluation and design of wireless-friendly building layouts.", "venue": "IEEE Transactions on Communications", "authors": ["Jiliang  Zhang", "Andr\u00e9s Alay\u00f3n Glazunov", "Jie  Zhang"], "year": 2021, "n_citations": 2}
{"id": 3028939, "s2_id": "ee3a3e2671168384c86385cbb3102c14b12f69c5", "title": "Performance Analysis of Software to Hardware Task Migration in Codesign", "abstract": "The complexity of multimedia applications in terms of intensity of computation and heterogeneity of treated data led the designers to embark them on multiprocessor systems on chip. The complexity of these systems on one hand and the expectations of the consumers on the other hand complicate the designers job to conceive and supply strong and successful systems in the shortest deadlines. They have to explore the different solutions of the design space and es timate their performances in order to deduce the solution that respects their design constraints. In this context, we propose the modeling of one of the design space possible solutions : the software to hardware task migration. This modeling exploits the synchronous dataflow graphs to take into account the different migration impacts and estimate their performances in terms of throughput.", "venue": "ArXiv", "authors": ["Dorsaf  Sebai", "Abderrazak  Jemai", "Imed  Bennour"], "year": 2010, "n_citations": 0}
{"id": 3031959, "s2_id": "ee2ea2495a8aff833cb901e28ede9f4d17574a6e", "title": "A Non-Ideal NOMA-based mmWave D2D Networks with Hardware and CSI Imperfections", "abstract": "This letter investigates a non-orthogonal multiple access (NOMA) assisted millimeter-wave device-to-device (D2D) network practically limited by multiple interference noises, transceiver hardware impairments, imperfect successive interference cancellation, and channel state information mismatch. Generalized outage probability expressions for NOMA-D2D users are deduced and achieved results, validated by Monte Carlo simulations, are compared with the orthogonal multiple access to show the superior performance of the proposed network model", "venue": "ArXiv", "authors": ["Leila  Tlebaldiyeva", "Galymzhan  Nauryzbayev", "Sultangali  Arzykulov", "Yerassyl  Akhmetkaziyev", "Mohammad S. Hashmi", "Ahmed M. Eltawil"], "year": 2020, "n_citations": 1}
{"id": 3034279, "s2_id": "f20f3234716cc5b3f022d36ad586edbbd7df15f2", "title": "Threshold-based rerouting and replication for resolving job-server affinity relations", "abstract": "We consider a system with several job types and two parallel server pools. Within the pools the servers are homogeneous, but across pools possibly not in the sense that the service speed of a job may depend on its type as well as the server pool. Immediately upon arrival, jobs are assigned to a server pool, possibly based on (partial) knowledge of their type. In case such knowledge is not available upon arrival, it can however be obtained while the job is in service; as the service progresses, the likelihood that the service speed of this job type is low increases, creating an incentive to execute the job on different, possibly faster, server(s). Two policies are considered: reroute the job to the other server pool, or replicate it there.We determine the effective load per server under both the rerouting and replication policy for completely unknown as well as partly known job types. We also examine the impact of these policies on the stability bound, which is defined as the maximum arrival rate of jobs for which the effective load per server is smaller than one. We demonstrate that the uncertainty in job types may significantly reduce the stability bound, and that for (highly) unbalanced service speeds full replication achieves the largest stability bound. Finally, we discuss how the use of threshold-based policies can help improve the expected latency for completely or partly unknown job types.", "venue": "IEEE INFOCOM 2021 - IEEE Conference on Computer Communications", "authors": ["Youri  Raaijmakers", "Sem  Borst", "Onno  Boxma"], "year": 2021, "n_citations": 0}
{"id": 3040448, "s2_id": "1b37263e8c7e2cd2523e0bcc2551760c0318c7ed", "title": "A Microservices Architecture for Distributed Complex Event Processing in Smart Cities", "abstract": "A considerable volume of data is collected from sensors today and needs to be processed in real time. Complex Event Processing (CEP) is one of the most important techniques developed for this purpose. In CEP, each new sensor measurement is considered an event and new event types can be defined based on other events occurrence. There exists several open-source CEP implementations currently available, but all of them use orchestration to distribute event processing. This kind of architectural organization may harm system resilience, since it relies on a central core (i.e. the orchestrator). Any failures in the core might impact the whole system. Moreover, the core can become a bottleneck on system performance. In this work, a choreography-based microservices architecture is proposed for distributed CEP, in order to benefit from the low coupling and greater horizontal scalability this kind of architecture provides.", "venue": "ArXiv", "authors": ["Fernando Freire Scattone", "Kelly Rosa Braghetto"], "year": 2020, "n_citations": 1}
{"id": 3040525, "s2_id": "dbda42679c708d8d30a338e66e9322d7bccd2596", "title": "An Empirical Analysis of Internet Protocol Version 6 (IPv6)", "abstract": "Although the current Internet Protocol known as IPv4 has served its purpose for over 20 years, its days are numbered. With IPv6 reaching a mature enough level, there is a need to evaluate the performance benefits or drawbacks that the new IPv6 protocol will have in comparison to the well established IPv4 protocol. Theoretically, the overhead between the two different protocols should be directly proportional to the difference in the packet's header size, however according to our findings, the empirical performance difference between IPv4 and IPv6, especially when the transition mechanisms are taken into consideration, is much larger than anticipated. We first examine the performance of each protocol independently. We then examined two transition mechanisms which perform the encapsulation at various points in the network: host-to-host and router-to-router (tunneling). Our experiments were conducted using two dual stack (IPv4/IPv6) routers using end nodes running both Windows 2000 and Solaris 8.0 in order to compare two different IPv6 implementations side by side. Our tests were written in C++ and utilized metrics such as latency, throughput, CPU utilization, socket creation time, socket connection time, web server simulation, and a video client/server application for TCP/UDP in IPv4/IPv6 under both Windows 2000 and Solaris 8.0. Our empirical evaluation proved that IPv6 is not yet a mature enough technology and that it is still years away from having consistent and good enough implementations, as the performance of IPv6 in many cases proved to be significantly worse than IPv4.", "venue": "ArXiv", "authors": ["Ioan  Raicu"], "year": 2004, "n_citations": 14}
{"id": 3042544, "s2_id": "30f291f5008d4da8b012b9c00e8fc7e199e18b02", "title": "Devito: Automated Fast Finite Difference Computation", "abstract": "Domain specific languages have successfully been used in a variety of fields to cleanly express scientific problems as well as to simplify implementation and performance optimization on different computer architectures. Although a large number of stencil languages are available, finite difference domain specific languages have proved challenging to design because most practical use cases require additional features that fall outside the finite difference abstraction. Inspired by the complexity of real-world seismic imaging problems, we introduce Devito, a domain specific language in which high level equations are expressed using symbolic expressions from the SymPy package. Complex equations are automatically manipulated, optimized, and translated into highly optimized C code that aims to perform comparably or better than hand-tuned code. All this is transparent to users, who only see concise symbolic mathematical expressions.", "venue": "2016 Sixth International Workshop on Domain-Specific Languages and High-Level Frameworks for High Performance Computing (WOLFHPC)", "authors": ["Navjot  Kukreja", "Mathias  Louboutin", "Felippe  Vieira", "Fabio  Luporini", "Michael  Lange", "Gerard  Gorman"], "year": 2016, "n_citations": 23}
{"id": 3051392, "s2_id": "54b28b41cae9f5c20222a0e4290da99cfe7d5a65", "title": "A Recursive Algebraic Coloring Technique for Hardware-efficient Symmetric Sparse Matrix-vector Multiplication", "abstract": "The symmetric sparse matrix-vector multiplication (SymmSpMV) is an important building block for many numerical linear algebra kernel operations or graph traversal applications. Parallelizing SymmSpMV on today\u2019s multicore platforms with up to 100 cores is difficult due to the need to manage conflicting updates on the result vector. Coloring approaches can be used to solve this problem without data duplication, but existing coloring algorithms do not take load balancing and deep memory hierarchies into account, hampering scalability and full-chip performance. In this work, we propose the recursive algebraic coloring engine (RACE), a novel coloring algorithm and open-source library implementation that eliminates the shortcomings of previous coloring methods in terms of hardware efficiency and parallelization overhead. We describe the level construction, distance-k coloring, and load balancing steps in RACE, use it to parallelize SymmSpMV, and compare its performance on 31 sparse matrices with other state-of-the-art coloring techniques and Intel MKL on two modern multicore processors. RACE outperforms all other approaches substantially. By means of a parameterized roofline model, we analyze the SymmSpMV performance in detail and discuss outliers. While we focus on SymmSpMV in this article, our algorithm and software are applicable to any sparse matrix operation with data dependencies that can be resolved by distance-k coloring.", "venue": "ACM Trans. Parallel Comput.", "authors": ["Christie L. Alappat", "Georg  Hager", "Olaf  Schenk", "Jonas  Thies", "Achim  Basermann", "Alan R. Bishop", "Holger  Fehske", "Gerhard  Wellein"], "year": 2020, "n_citations": 22}
{"id": 3051898, "s2_id": "c45e4feb511c39f011ccdbc6b345cb3064c463a4", "title": "Performance Analysis of Dual-Hop Mixed PLC/RF Communication Systems", "abstract": "In this paper, we study a dual-hop mixed power line communication and radio-frequency communication (PLC/RF) system, where the connection between the PLC link and the RF link is made by a decode-and-forward (DF) or amplify-and-forward (AF) relay. Assume that the PLC channel is affected by both additive background noise and impulsive noise suffers from Log-normal fading, while the RF link undergoes Rician fading. Based on this model, analytical expressions of the outage probability (OP), average bit error rate (BER), and the average channel capacity (ACC) are derived. Furthermore, an asymptotic analysis for the OP and average BER, as well as an upper bound expression for the ACC are presented. At last, numerical results are developed to validate our analytical results, and in-depth discussions are conducted.", "venue": "ArXiv", "authors": ["Liang  Yang", "Xiaoqin  Yan", "Sai  Li", "Daniel Benevides da Costa", "Mohamed-Slim  Alouini"], "year": 2020, "n_citations": 3}
{"id": 3052242, "s2_id": "26e9a421eb5d89b0e80e5bfa5b1c85f2e1eec067", "title": "The Fast Fibonacci Decompression Algorithm", "abstract": "Data compression has been widely applied in many data processing areas. Compression methods use variable-size codes with the shorter codes assigned to symbols or groups of symbols that appear in the data frequently. Fibonacci coding, as a representative of these codes, is used for compressing small numbers. Time consumption of a decompression algorithm is not usually as important as the time of a compression algorithm. However, efficiency of the decompression may be a critical issue in some cases. For example, a real-time compression of tree data structures follows this issue. Tree's pages are decompressed during every reading from a secondary storage into the main memory. In this case, the efficiency of a decompression algorithm is extremely important. We have developed a Fast Fibonacci decompression for this purpose. Our approach is up to $3.5\\times$ faster than the original implementation.", "venue": "ArXiv", "authors": ["Radim  Baca", "V\u00e1clav  Sn\u00e1sel", "Jan  Platos", "Michal  Kr\u00e1tk\u00fd", "Eyas  El-Qawasmeh"], "year": 2007, "n_citations": 4}
{"id": 3052292, "s2_id": "9404dc5c41f96a16bf9ca16f6088682d23eb4b30", "title": "Optimizing data-intensive computations in existing libraries with split annotations", "abstract": "Data movement between main memory and the CPU is a major bottleneck in parallel data-intensive applications. In response, researchers have proposed using compilers and intermediate representations (IRs) that apply optimizations such as loop fusion under existing high-level APIs such as NumPy and TensorFlow. Even though these techniques generally do not require changes to user applications, they require intrusive changes to the library itself: often, library developers must rewrite each function using a new IR. In this paper, we propose a new technique called split annotations (SAs) that enables key data movement optimizations over unmodified library functions. SAs only require developers to annotate functions and implement an API that specifies how to partition data in the library. The annotation and API describe how to enable cross-function data pipelining and parallelization, while respecting each function's correctness constraints. We implement a parallel runtime for SAs in a system called Mozart. We show that Mozart can accelerate workloads in libraries such as Intel MKL and Pandas by up to 15x, with no library modifications. Mozart also provides performance gains competitive with solutions that require rewriting libraries, and can sometimes outperform these systems by up to 2x by leveraging existing hand-optimized code.", "venue": "SOSP", "authors": ["Shoumik  Palkar", "Matei  Zaharia"], "year": 2019, "n_citations": 11}
{"id": 3052952, "s2_id": "ccc2c33f2cb3c70008f783fb80a427d1b9481bde", "title": "Support vector regression model for BigData systems", "abstract": "Nowadays Big Data are becoming more and more important. Many sectors of our economy are now guided by data-driven decision processes. Big Data and business intelligence applications are facilitated by the MapReduce programming model while, at infrastructural layer, cloud computing provides flexible and cost effective solutions for allocating on demand large clusters. In such systems, capacity allocation, which is the ability to optimally size minimal resources for achieve a certain level of performance, is a key challenge to enhance performance for MapReduce jobs and minimize cloud resource costs. In order to do so, one of the biggest challenge is to build an accurate performance model to estimate job execution time of MapReduce systems. Previous works applied simulation based models for modeling such systems. Although this approach can accurately describe the behavior of Big Data clusters, it is too computationally expensive and does not scale to large system. We try to overcome these issues by applying machine learning techniques. More precisely we focus on Support Vector Regression (SVR) which is intrinsically more robust w.r.t other techniques, like, e.g., neural networks, and less sensitive to outliers in the training set. To better investigate these benefits, we compare SVR to linear regression.", "venue": "ArXiv", "authors": ["Alessandro Maria Rizzi"], "year": 2016, "n_citations": 5}
{"id": 3054841, "s2_id": "7e98cbfe7705c4acd5beb31ec6a6caa4a139e253", "title": "On Minimizing the Completion Times of Long Flows Over Inter-Datacenter WAN", "abstract": "Long flows contribute huge volumes of traffic over inter-datacenter WAN. The flow completion time (FCT) is a vital network performance metric that affects the running time of distributed applications and the users\u2019 quality of experience. Flow routing techniques based on propagation or queuing latency or instantaneous link utilization are insufficient for minimization of the long flows\u2019 FCT. We propose a routing approach that uses the remaining sizes and paths of all ongoing flows to minimize the worst case completion time of incoming flows assuming no knowledge of future flow arrivals. Our approach can be formulated as an NP-Hard graph optimization problem. We propose <italic>BWRH</italic>, a heuristic to quickly generate an approximate solution. We evaluate <italic>BWRH</italic> against several real-WAN topologies and two different traffic patterns. We see that <italic>BWRH</italic> provides solutions with an average optimality gap of less than 0.25%. Furthermore, we show that compared with other popular routing heuristics, <italic>BWRH</italic> reduces the mean and tail FCT by up to <inline-formula> <tex-math notation=\"LaTeX\">$1.46\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$1.53\\times $ </tex-math></inline-formula>, respectively.", "venue": "IEEE Communications Letters", "authors": ["Mohammad  Noormohammadpour", "Ajitesh  Srivastava", "Cauligi S. Raghavendra"], "year": 2018, "n_citations": 3}
{"id": 3059250, "s2_id": "65c87cabcaf958cabc9c390160c07c0913b75fb9", "title": "A Distributed Application Placement and Migration Management Techniques for Edge and Fog Computing Environments", "abstract": "Fog/Edge computing model allows harnessing of resources in the proximity of the Internet of Things (IoT) devices to support various types of latency-sensitive IoT applications. However, due to the mobility of users and a wide range of IoT applications with different resource requirements, it is a challenging issue to satisfy these applications\u2019 requirements. The execution of IoT applications exclusively on one fog/edge server may not be always feasible due to limited resources, while the execution of IoT applications on different servers requires further collaboration and management among servers. Moreover, considering user mobility, some modules of each IoT application may require migration to other servers for execution, leading to service interruption and extra execution costs. In this article, we propose a new weighted cost model for hierarchical fog computing environments, in terms of the response time of IoT applications and energy consumption of IoT devices, to minimize the cost of running IoT applications and potential migrations. Besides, a distributed clustering technique is proposed to enable the collaborative execution of tasks, emitted from application modules, among servers. Also, we propose an application placement technique to minimize the overall cost of executing IoT applications on multiple servers in a distributed manner. Furthermore, a distributed migration management technique is proposed for the potential migration of applications\u2019 modules to other remote servers as the users move along their path. Besides, failure recovery methods are embedded in the clustering, application placement, and migration management techniques to recover from unpredicted failures. The performance results demonstrate that our technique significantly improves its counterparts in terms of placement deployment time, average execution cost of tasks, the total number of migrations, the total number of interrupted tasks, and cumulative migration cost.", "venue": "2021 16th Conference on Computer Science and Intelligence Systems (FedCSIS)", "authors": ["Mohammad  Goudarzi", "Marimuthu  Palaniswami", "Rajkumar  Buyya"], "year": 2021, "n_citations": 2}
{"id": 3063575, "s2_id": "1bca3b27e6b2c9fbb635e472cfbd72fa2cbaeab1", "title": "High-performance Passive Eigen-model-based Detectors of Single Emitter Using Massive MIMO Receivers", "abstract": "For a passive direction of arrival (DoA) measurement system using massive multiple input multiple output (MIMO), it is mandatory to infer whether the emitter exists or not before performing DOA estimation operation. Inspired by the detection idea from radio detection and ranging (radar), three high-performance detectors are proposed to infer the existence of single passive emitter from the eigen-space of sample covariance matrix of receive signal vector. The test statistic (TS) of the first method is defined as the ratio of maximum eigen-value (MaxEV) to minimum eigen-value (R-MaxEV-MinEV) while that of the second one is defined as the ratio of Max-EV to noise variance (R-MaxEV-NV). The TS of the third method is the mean of maximum eigen-value (EV) and minimum EV(M-MaxEVMinEV). Their closed-form expressions are presented and the corresponding detection performance is given. Simulation results show that the proposed M-MaxEV-MinEV and R-MaxEV-NV methods can approximately achieve the same detection performance that is better than the traditional generalized likelihood ratio test method with false alarm probability being less than 0.3.", "venue": "ArXiv", "authors": ["Qijuan  Jie", "Xichao  Zhan", "Feng  Shu", "Yaohui  Ding", "Baihua  Shi", "Yifan  Li", "Jiangzhou  Wang"], "year": 2021, "n_citations": 1}
{"id": 3065439, "s2_id": "e4d28d726c51672de04976037497ff8ecb5e78e2", "title": "Comparing the performance of different x86 SIMD instruction sets for a medical imaging application on modern multi- and manycore chips", "abstract": "Single Instruction, Multiple Data (SIMD) vectorization is a major driver of performance in current architectures, and is mandatory for achieving good performance with codes that are limited by instruction throughput. We investigate the efficiency of different SIMD-vectorized implementations of the RabbitCT benchmark. RabbitCT performs 3D image reconstruction by back projection, a vital operation in computed tomography applications. The underlying algorithm is a challenge for vectorization because it consists, apart from a streaming part, also of a bilinear interpolation requiring scattered access to image data. We analyze the performance of SSE (128 bit), AVX (256 bit), AVX2 (256 bit), and IMCI (512 bit) implementations on recent Intel x86 systems. A special emphasis is put on the vector gather implementation on Intel Haswell and Knights Corner microarchitectures. Finally we discuss why GPU implementations perform much better for this specific algorithm.", "venue": "WPMVP '14", "authors": ["Johannes  Hofmann", "Jan  Treibig", "Georg  Hager", "Gerhard  Wellein"], "year": 2014, "n_citations": 41}
{"id": 3071546, "s2_id": "21bff953e078c9d395936a70c78f119c85344456", "title": "A framework for the analytical performance assessment of matrix and tensor-based ESPRIT-type algorithms", "abstract": "In this paper we present a generic framework for the asymptotic performance analysis of subspace-based parameter estimation schemes. It is based on earlier results on an explicit first-order expansion of the estimation error in the signal subspace obtained via an SVD of the noisy observation matrix. We extend these results in a number of aspects. Firstly, we derive an explicit first-order expansion of the Higher- Order SVD (HOSVD)-based subspace estimate. Secondly, we show how to obtain explicit first-order expansions of the estimation error of ESPRIT-type algorithms and provide the expressions for matrix-based and tensor-based Standard ESPRIT and Unitary ESPRIT. Thirdly, we derive closed-form expressions for the mean square error (MSE) and show that they only depend on the second-order moments of the noise. Hence, we only need the noise to be zero mean and possess finite second order moments. Fourthly, we investigate the effect of using Structured Least Squares (SLS) to solve the overdetermined shift invariance equations in ESPRIT and provide an explicit first-order expansion as well as a closed-form MSE expression. Finally, we simplify the MSE for the special case of a single source and compute the asymptotic efficiency of the investigated ESPRIT-type algorithms in compact closed-form expressions which only depend on the array size and the effective SNR. Our results are more general than existing results on the performance analysis of ESPRIT-type algorithms since (a) we do not need any assumptions about the noise except for the mean to be zero and the second-order moments to be finite (in contrast to earlier results that require Gaussianity or second-order circular symmetry); (b) our results are asymptotic in the effective SNR, i.e., we do not require the number of samples to be large; (c) we present a framework that incorporates various ESPRIT-type algorithms in one unified manner.", "venue": "ArXiv", "authors": ["Florian  Roemer", "Martin  Haardt"], "year": 2012, "n_citations": 14}
{"id": 3077670, "s2_id": "6258b37b8d517f121c844ebad226da472761adc6", "title": "Eigenvector Component Calculation Speedup over NumPy for High Performance Computing", "abstract": "Applications related to artificial intelligence, machine learning, and system identification simulations essentially use eigenvectors. Calculating eigenvectors for very large matrices using conventional methods is compute-intensive and renders the applications slow. Recently, Eigenvector-Eigenvalue Identity formula promising significant speedup was identified. We study the algorithmic implementation of the formula against the existing state-of-the-art algorithms and their implementations to evaluate the performance gains. We provide a first of its kind systematic study of the implementation of the formula. We demonstrate further improvements using high-performance computing concepts over native NumPy eigenvector implementation which uses LAPACK and BLAS.", "venue": "ArXiv", "authors": ["Shrey  Dabhi", "Manojkumar  Parmar"], "year": 2020, "n_citations": 0}
{"id": 3081371, "s2_id": "7e8a6f37fed946c1a9587107ea4e0eb9c2fb75cb", "title": "Deadline-aware Power Management in Data Centers", "abstract": "We study the dynamic power optimization problem in data centers. We formulate and solve the following offline problem: in which slot which server has to be assigned to which job; and in which slot which server has to be switched ON or OFF so that the total power is optimal for some time horizon. We show that the offline problem is a new version of generalized assignment problem including new constraints issuing from deadline characteristics of jobs and difference of activation energy of servers. We propose an online algorithm that solves the problem heuristically and compare it to randomized routing.", "venue": "ArXiv", "authors": ["Cengis  Hasan", "Zygmunt J. Haas"], "year": 2015, "n_citations": 0}
{"id": 3083213, "s2_id": "9a5c3d04cde121acd808b8a7d6ef7a4014b2cb72", "title": "Dependability Analysis of Data Storage Systems in Presence of Soft Errors", "abstract": "In recent years, high availability and reliability of data storage systems (DSS) have been significantly threatened by soft errors occurring in storage controllers. Due to their specific functionality and hardware\u2013software stack, error propagation and manifestation in DSS is quite different from general-purpose computing architectures. To the best of our knowledge, no previous study has examined the system-level effects of soft errors on the availability and reliability of DSS. In this paper, we first analyze the effects of soft errors occurring in the server processors of storage controllers on the entire storage system dependability. To this end, we implement the major functions of a typical data storage system controller, running on a full stack of storage system operating system, and develop a framework to perform fault injection experiments using a full system simulator. We then propose a new metric, storage system vulnerability factor (SSVF), to accurately capture the impact of soft errors in storage systems. By conducting extensive experiment, it is revealed that depending on the controller configuration, up to 40% of cache memory contains end-user data in which any unrecoverable soft errors will result in data loss (DL) in an irreversible manner. However, soft errors in the rest of cache memory filled by operating system and storage applications will result in data unavailability (DU) at the storage system level. Our analysis also shows that detectable unrecoverable errors on the cache data field are the major cause of DU in storage systems, while silent data corruptions in the cache tag and data fields are mainly the cause of DL in storage systems.", "venue": "IEEE Transactions on Reliability", "authors": ["Mostafa  Kishani", "Mehdi  Tahoori", "Hossein  Asadi"], "year": 2019, "n_citations": 12}
{"id": 3085891, "s2_id": "23c38bc983a7473f38f76a68fae06201e6da2832", "title": "Analysis of the Symmetric Join the Shortest Orbit Queue", "abstract": "This work introduces the join the shortest queue policy in the retrial setting. We consider a Markovian single server retrial system with two infinite capacity orbits. An arriving job finding the server busy, it is forwarded to the least loaded orbit. Otherwise, it is forwarded to an orbit randomly. Orbiting jobs of either type retry to access the server independently. We investigate the stability condition, the stationary tail decay rate, and obtain the equilibrium distribution by using the compensation method.", "venue": "Oper. Res. Lett.", "authors": ["Ioannis  Dimitriou"], "year": 2021, "n_citations": 2}
{"id": 3086203, "s2_id": "419581700910c6bff67e30699310c9c326dfb339", "title": "Analytical Performance Models for NoCs with Multiple Priority Traffic Classes", "abstract": "Networks-on-chip (NoCs) have become the standard for interconnect solutions in industrial designs ranging from client CPUs to many-core chip-multiprocessors. Since NoCs play a vital role in system performance and power consumption, pre-silicon evaluation environments include cycle-accurate NoC simulators. Long simulations increase the execution time of evaluation frameworks, which are already notoriously slow, and prohibit design-space exploration. Existing analytical NoC models, which assume fair arbitration, cannot replace these simulations since industrial NoCs typically employ priority schedulers and multiple priority classes. To address this limitation, we propose a systematic approach to construct priority-aware analytical performance models using micro-architecture specifications and input traffic. Our approach decomposes the given NoC into individual queues with modified service time to enable accurate and scalable latency computations. Specifically, we introduce novel transformations along with an algorithm that iteratively applies these transformations to decompose the queuing system. Experimental evaluations using real architectures and applications show high accuracy of 97% and up to 2.5\u00d7 speedup in full-system simulation.", "venue": "ACM Trans. Embed. Comput. Syst.", "authors": ["Sumit K. Mandal", "Raid  Ayoub", "Michael  Kishinevsky", "\u00dcmit Y. Ogras"], "year": 2019, "n_citations": 14}
{"id": 3086256, "s2_id": "34e7adb16781eb7e30131b7a9c046d720c6ea3d6", "title": "Fast semi-supervised discriminant analysis for binary classification of large data-sets", "abstract": "Abstract High-dimensional data requires scalable algorithms. We propose and analyze four scalable and related algorithms for semi-supervised discriminant analysis (SDA). These methods are based on Krylov subspace methods and therefore exploit data sparsity and the shift-invariance of Krylov subspaces. In addition, centralization was derived for the semi-supervised setting. The proposed methods are evaluated on an industry-scale data set from a pharmaceutical company to predict compound activity on target proteins. The results show that our methods only require a few seconds, significantly improving computation time on the state of the art.", "venue": "Pattern Recognit.", "authors": ["Joris  Tavernier", "Jaak  Simm", "Karl  Meerbergen", "J\u00f6rg K. Wegner", "Hugo  Ceulemans", "Yves  Moreau"], "year": 2019, "n_citations": 8}
{"id": 3087681, "s2_id": "6efed1ec73487678c7f479abd7308d66da339c73", "title": "Accelerating PageRank using Partition-Centric Processing", "abstract": "PageRank is a fundamental link analysis algorithm and a key representative of the performance of other graph algorithms and Sparse Matrix Vector (SpMV) multiplication. Calculating PageRank on sparse graphs generates large amount of random memory accesses resulting in low cache line utilization and poor use of memory bandwidth. In this paper, we present a novel Partition-Centric Processing Methodology (PCPM) that drastically reduces the amount of communication with DRAM and achieves high memory bandwidth. Similar to the state of the art Binning with Vertex-centric Gather-Apply-Scatter (BVGAS) method, PCPM performs partition wise scatter and gather of updates with both phases enjoying full cache line utilization. However, BVGAS suffers from random memory accesses and redundant read/write of update values from nodes to their neighbors. In contrast, PCPM propagates single update from source node to all destinations in a partition, thus decreasing redundancy effectively. We make use of this characteristic to develop a novel bipartite Partition-Node Graph (PNG) data layout for PCPM, that enables streaming memory accesses, with very little generation overhead. We perform detailed analysis of PCPM and provide theoretical bounds on the amount of communication and random DRAM accesses. We experimentally evaluate our approach using 6 large graph datasets and demonstrate an average 2.7x speedup in execution time and 1.7x reduction in communication, compared to the state of the art. We also show that unlike the BVGAS implementation, PCPM is able to take advantage of intelligent node labeling that enhances locality in graphs, by further reducing the amount of communication with DRAM. Although we use PageRank as the target application in this paper, our approach can be applied to generic SpMV computation.", "venue": "USENIX Annual Technical Conference", "authors": ["Kartik  Lakhotia", "Rajgopal  Kannan", "Viktor K. Prasanna"], "year": 2018, "n_citations": 19}
{"id": 3088075, "s2_id": "65cb4ed01dbaa6b3d0f0f0ebdf3bdd15e9caf33b", "title": "Cross-Layer Chase Combining With Selective Retransmission, Analysis, and Throughput Optimization for OFDM Systems", "abstract": "In this paper, we present a bandwidth efficient retransmission method employing selective retransmission approach at a modulation layer under orthogonal frequency division multiplexing (OFDM) signaling. The proposed cross-layer design embeds a selective retransmission sub-layer in physical layer (PHY) that targets the retransmission of information symbols transmitted over poor quality OFDM sub-carriers. Most of the times, a few errors in decoded bit stream result in packet failure at medium access control (MAC) layer. The unnecessary retransmission of good quality information symbols of a failed packet has detrimental effect on the overall throughput of transceiver. We propose a cross-layer Chase combining with selective retransmission (CCSR) method by blending Chase combining approach at MAC layer and selective retransmission in PHY. The selective retransmission in PHY targets the poor quality information symbols prior to decoding, which results in lower hybrid automatic repeat reQuest retransmissions at MAC layer. We also present bit-error rate upper bound and throughput lower bound for the CCSR method. In order to maximize the throughput, we formulate optimization problem with respect to the amount of information to be retransmitted in selective retransmission. We also present an impact of selective retransmission on latency. The proposed CCSR method achieves a significant throughput gain as compared with the conventional Chase combining method.", "venue": "IEEE Transactions on Communications", "authors": ["Taniya  Shafique", "Muhammad  Zia", "Huy-Dung  Han", "Hasan  Mahmood"], "year": 2016, "n_citations": 8}
{"id": 3088850, "s2_id": "69a5f37945b3758bacc6772def5aba830453adca", "title": "Even Faster SNN Simulation with Lazy+Event-driven Plasticity and Shared Atomics", "abstract": "We present two novel optimizations that accelerate clock-based spiking neural network (SNN) simulators. The first one targets spike timing dependent plasticity (STDP). It combines lazy- with event-driven plasticity and efficiently facilitates the computation of pre- and post-synaptic spikes using bitfields and integer intrinsics. It offers higher bandwidth than event-driven plasticity alone and achieves a 1.5\u00d7\u20132\u00d7 speedup over our closest competitor. The second optimization targets spike delivery. We partition our graph representation in a way that bounds the number of neurons that need be updated at any given time which allows us to perform said update in shared memory instead of global memory. This is 2\u00d7\u20132.5\u00d7 faster than our closest competitor. Both optimizations represent the final evolutionary stages of years of iteration on STDP and spike delivery inside \"Spice\" (/spaIk/), our state of the art SNN simulator. The proposed optimizations are not exclusive to our graph representation or pipeline but are applicable to a multitude of simulator designs. We evaluate our performance on three well-established models and compare ourselves against three other state of the art simulators.", "venue": "2021 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Dennis  Bautembach", "Iason  Oikonomidis", "Antonis  Argyros"], "year": 2021, "n_citations": 0}
{"id": 3089049, "s2_id": "59bbf35cdb6a3601c17453bc71cd920ed7f1557a", "title": "Adaptive Performance Optimization under Power Constraint in Multi-thread Applications with Diverse Scalability", "abstract": "Energy consumption has become a core concern in computing systems. In this context, power capping is an approach that aims at ensuring that the power consumption of a system does not overcome a predefined threshold. Although various power capping techniques exist in the literature, they do not fit well the nature of multi-threaded workloads with shared data accesses and non-minimal thread-level concurrency. For these workloads, scalability may be limited by thread contention on hardware resources and/or data, to the point that performance may even decrease while increasing the thread-level parallelism, indicating scarce ability to exploit the actual computing power available in highly parallel hardware. In this paper, we consider the problem of maximizing the performance of multi-thread applications under a power cap by dynamically tuning the thread-level parallelism and the power state of CPU-cores in combination. Based on experimental observations, we design a technique that adaptively identifies, in linear time within a bi-dimensional space, the optimal parallelism and power state setting. We evaluated the proposed technique with different benchmark applications, and using different methods for synchronizing threads when accessing shared data, and we compared it with other state-of-the-art power capping techniques.", "venue": "ICPE", "authors": ["Stefano  Conoci", "Pierangelo di Sanzo", "Bruno  Ciciani", "Francesco  Quaglia"], "year": 2018, "n_citations": 4}
{"id": 3092320, "s2_id": "0b3d554eb04e5f6a842a020fcc0817caefc0f564", "title": "Heavy-tailed limits for medium size jobs and comparison scheduling", "abstract": "We study the conditional sojourn time distributions of processor sharing (PS), foreground background processor sharing (FBPS) and shortest remaining processing time first (SRPT) scheduling disciplines on an event where the job size of a customer arriving in stationarity is smaller than exactly k\u22650 out of the preceding m\u2265k arrivals. Then, conditioning on the preceding event, the sojourn time distribution of this newly arriving customer behaves asymptotically the same as if the customer were served in isolation with a server of rate (1\u2212\u03c1)/(k+1) for PS/FBPS, and (1\u2212\u03c1) for SRPT, respectively, where \u03c1 is the traffic intensity. Hence, the introduced notion of conditional limits allows us to distinguish the asymptotic performance of the studied schedulers by showing that SRPT exhibits considerably better asymptotic behavior for relatively smaller jobs than PS/FBPS.Inspired by the preceding results, we propose an approximation to the SRPT discipline based on a novel adaptive job grouping mechanism that uses relative size comparison of a newly arriving job to the preceding m arrivals. Specifically, if the newly arriving job is smaller than k and larger than m\u2212k of the previous m jobs, it is routed into class k. Then, the classes of smaller jobs are served with higher priorities using the static priority scheduling. The good performance of this mechanism, even for a small number of classes m+1, is demonstrated using the asymptotic queueing analysis under the heavy-tailed job requirements. We also discuss refinements of the comparison grouping mechanism that improve the accuracy of job classification at the expense of a small additional complexity.", "venue": "Ann. Oper. Res.", "authors": ["Predrag R. Jelenkovic", "Xiaozhu  Kang", "Jian  Tan"], "year": 2009, "n_citations": 3}
{"id": 3100242, "s2_id": "779d6a28fed4b2480abc9782d287193a07a1cdac", "title": "Parallel Rendering and Large Data Visualization", "abstract": "We are living in the big data age: An ever increasing amount of data is being produced through data acquisition and computer simulations. While large scale analysis and simulations have received significant attention for cloud and high-performance computing, software to efficiently visualise large data sets is struggling to keep up. \nVisualization has proven to be an efficient tool for understanding data, in particular visual analysis is a powerful tool to gain intuitive insight into the spatial structure and relations of 3D data sets. Large-scale visualization setups are becoming ever more affordable, and high-resolution tiled display walls are in reach even for small institutions. Virtual reality has arrived in the consumer space, making it accessible to a large audience. \nThis thesis addresses these developments by advancing the field of parallel rendering. We formalise the design of system software for large data visualization through parallel rendering, provide a reference implementation of a parallel rendering framework, introduce novel algorithms to accelerate the rendering of large amounts of data, and validate this research and development with new applications for large data visualization. Applications built using our framework enable domain scientists and large data engineers to better extract meaning from their data, making it feasible to explore more data and enabling the use of high-fidelity visualization installations to see more detail of the data.", "venue": "ArXiv", "authors": ["Stefan  Eilemann"], "year": 2019, "n_citations": 1}
{"id": 3102122, "s2_id": "e5c9b980e87b13cc964c7aaf5b4b47f065cc54b3", "title": "Gossip-based Search in Multipeer Communication Networks", "abstract": "We study a gossip-based algorithm for searching data objects in a multipeer communication network. All of the nodes in the network are able to communicate with each other. There exists an initiator node that starts a round of searches by randomly querying one or more of its neighbours for a desired object. The queried nodes can also be activated and look for the object. We examine several behavioural patterns of nodes with respect to their willingness to cooperate in the search. We derive mathematical models for the search process based on the balls and bins model, as well as known approximations for the rumour-spreading problem. All models are validated with simulations. We also evaluate the performance of the algorithm and examine the impact of search parameters.", "venue": "ArXiv", "authors": ["Eva  Jaho", "Ioannis Z. Koukoutsidis", "Siyu  Tang", "Ioannis  Stavrakakis", "Piet Van Mieghem"], "year": 2009, "n_citations": 0}
{"id": 3102945, "s2_id": "46362fe84e0f3bb86491f876a6a78f6468de0eb7", "title": "Validation of hardware events for successful performance pattern identification in High Performance Computing", "abstract": "Hardware performance monitoring (HPM) is a crucial ingredient of performance analysis tools. While there are interfaces like LIKWID, PAPI or the kernel interface perf_event which provide HPM access with some additional features, many higher level tools combine event counts with results retrieved from other sources like function call traces to derive (semi-)automatic performance advice. However, although HPM is available for x86 systems since the early 90s, only a small subset of the HPM features is used in practice. Performance patterns provide a more comprehensive approach, enabling the identification of various performance-limiting effects. Patterns address issues like bandwidth saturation, load imbalance, non-local data access in ccNUMA systems, or false sharing of cache lines. This work defines HPM event sets that are best suited to identify a selection of performance patterns on the Intel Haswell processor. We validate the chosen event sets for accuracy in order to arrive at a reliable pattern detection mechanism and point out shortcomings that cannot be easily circumvented due to bugs or limitations in the hardware.", "venue": "ArXiv", "authors": ["Thomas  R\u00f6hl", "Jan  Eitzinger", "Georg  Hager", "Gerhard  Wellein"], "year": 2017, "n_citations": 6}
{"id": 3104964, "s2_id": "cf4a0dfb7d6e0ff626cd2765c5dc0b005579e77e", "title": "CROFT: A scalable three-dimensional parallel Fast Fourier Transform (FFT) implementation for High Performance Clusters", "abstract": "The FFT of three dimensional (3D) input data is an important computational kernel of numerical simulations and is widely used in High Performance Computing (HPC) codes running on large number of processors. Although the efficient parallelization of 3D FFT has been largely investigated over the last few decades, performance and scalability of parallel 3D FFT methods on new generation hardware architecture for HPC is a major challenge. Looking at upcoming exascale cluster architectures, the conventional parallel 3D FFT calculations on HPC needs improvement for better performance. In this paper, we present CDACs three dimensional Fast Fourier Transform (CROFT) library which implements three dimensional parallel FFT using pencil decomposition. To exploit the multithreading capabilities of hardware without affecting performance, CROFT is designed to use hybrid programming model of OpenMP and MPI. CROFT implementation has a feature of overlapping compute and memory I/O with MPI communication. Depending on the number of processes used, CROFT shows performance improvement of about 51 to 42 percent as compared to FFTW3 library.", "venue": "ArXiv", "authors": ["Vivek  Gavane", "Supriya  Prabhugawankar", "Shivam  Garg", "Archana  Achalere", "Rajendra  Joshi"], "year": 2020, "n_citations": 0}
{"id": 3107319, "s2_id": "a7e9ba75ba6cd349adc64c159e559b86578ba88e", "title": "Deterministic Memory Abstraction and Supporting Multicore System Architecture", "abstract": "Poor time predictability of multicore processors has been a long-standing challenge in the real-time systems community. In this paper, we make a case that a fundamental problem that prevents efficient and predictable real-time computing on multicore is the lack of a proper memory abstraction to express memory criticality, which cuts across various layers of the system: the application, OS, and hardware. We, therefore, propose a new holistic resource management approach driven by a new memory abstraction, which we call Deterministic Memory. The key characteristic of deterministic memory is that the platform - the OS and hardware - guarantees small and tightly bounded worst-case memory access timing. In contrast, we call the conventional memory abstraction as best-effort memory in which only highly pessimistic worst-case bounds can be achieved. We propose to utilize both abstractions to achieve high time predictability but without significantly sacrificing performance. We present deterministic memory-aware OS and architecture designs, including OS-level page allocator, hardware-level cache, and DRAM controller designs. We implement the proposed OS and architecture extensions on Linux and gem5 simulator. Our evaluation results, using a set of synthetic and real-world benchmarks, demonstrate the feasibility and effectiveness of our approach.", "venue": "ECRTS", "authors": ["Farzad  Farshchi", "Prathap Kumar Valsan", "Renato  Mancuso", "Heechul  Yun"], "year": 2018, "n_citations": 11}
{"id": 3111455, "s2_id": "8dd9117c691fbff7653150df5668f1756e45a31b", "title": "On the joint impact of bias and power control on downlink spectral efficiency in cellular networks", "abstract": "Cell biasing and downlink transmit power are two controls that may be used to improve the spectral efficiency of cellular networks. With cell biasing, each mobile user associates with the base station offering, say, the highest biased signal to interference plus noise ratio. Biasing affects the cell association decisions of mobile users, but not the received instantaneous downlink transmission rates. Adjusting the collection of downlink transmission powers can likewise affect the cell associations, but in contrast with biasing, it also directly affects the instantaneous rates. This paper investigates the joint use of both cell biasing and transmission power control and their (individual and joint) effects on the statistical properties of the collection of per-user spectral efficiencies. Our analytical results and numerical investigations demonstrate in some cases a significant performance improvement in the Pareto efficient frontiers of both a mean-variance and throughput-fairness tradeoff from using both bias and power controls over using either control alone.", "venue": "ArXiv", "authors": ["Alex  Fridman", "Jeffrey  Wildman", "Steven P. Weber"], "year": 2015, "n_citations": 0}
{"id": 3114284, "s2_id": "2d9040d7e33a038d9784f36924e35710674a6edc", "title": "An Artificial Immune Based Approach for Detection and Isolation Misbehavior Attacks in Wireless Networks", "abstract": "MANETs (Mobile Ad-hoc Networks) is a temporal network, which is managed by autonomous nodes, which have the ability to communicate with each other without having fixed network infrastructure or any central base station. Due to some reasons such as dynamic changes of the network topology, trusting the nodes to each other, lack of fixed substructure for the analysis of nodes behaviors and loss of specific offensive lines, this type of networks is not supportive against malicious nodes attacks. One of these attacks is black hole attack. In this attack, the malicious nodes absorb data packets and destroy them. Thus, it is essential to present an algorithm against the black hole attacks. This paper proposed a new approach, which improvement the security of DSR routing protocol to encounter the black hole attacks. This schema tries to identify malicious nodes according to nodes behaviors in a MANETs and isolate them from routing. The proposed protocol, called AIS-DSR (Artificial Immune System DSR) employ AIS (Artificial Immune System) to defend against black hole attacks. AIS-DSR is evaluated through extensive simulations in the ns-2 environment. The results show that AIS-DSR outperforms other existing solutions in terms of throughput, end-to-end delay, packets loss ratio and packets drop ratio.", "venue": "J. Comput.", "authors": ["Shahram  Behzad", "Reza  Fotohi", "Jaber Hosseini Balov", "Mohammad Javad Rabipour"], "year": 2018, "n_citations": 23}
{"id": 3115406, "s2_id": "7ad410bc802e093a4b4429b1ca88801d1b21d65c", "title": "Asynchronous MPI for the Masses", "abstract": "We present a simple library which equips MPI implementations with truly asynchronous non-blocking point-to-point operations, and which is independent of the underlying communication infrastructure. It utilizes the MPI profiling interface (PMPI) and the MPI_THREAD_MULTIPLE thread compatibility level, and works with current versions of Intel MPI, Open MPI, MPICH2, MVAPICH2, Cray MPI, and IBM MPI. We show performance comparisons on a commodity InfiniBand cluster and two tier-1 systems in Germany, using low-level and application benchmarks. Issues of thread/process placement and the peculiarities of different MPI implementations are discussed in detail. We also identify the MPI libraries that already support asynchronous operations. Finally we show how our ideas can be extended to MPI-IO.", "venue": "ArXiv", "authors": ["Markus  Wittmann", "Georg  Hager", "Thomas  Zeiser", "Gerhard  Wellein"], "year": 2013, "n_citations": 29}
{"id": 3115853, "s2_id": "e162f4307f5707f54fc1c1993c83e4b06e96df7a", "title": "Evaluation as a Service architecture and crowdsourced problems solving implemented in Optil.io platform", "abstract": "Reliable and trustworthy evaluation of algorithms is a challenging process. Firstly, each algorithm has its strengths and weaknesses, and the selection of test instances can significantly influence the assessment process. Secondly, the measured performance of the algorithm highly depends on the test environment architecture, i.e., CPU model, available memory, cache configuration, operating system's kernel, and even compilation flags. Finally, it is often difficult to compare algorithm with software prepared by other researchers. Evaluation as a Service (EaaS) is a cloud computing architecture that tries to make assessment process more reliable by providing online tools and test instances dedicated to the evaluation of algorithms. One of such platforms is Optil.io which gives the possibility to define problems, store evaluation data and evaluate solutions submitted by researchers in almost real time. In this paper, we briefly present this platform together with four challenges that were organized with its support.", "venue": "ArXiv", "authors": ["Szymon  Wasik", "Maciej  Antczak", "Jan  Badura", "Artur  Laskowski"], "year": 2018, "n_citations": 1}
{"id": 3116027, "s2_id": "58a407092379f487084b0397fa456ace56e421ba", "title": "IoT-based Emergency Evacuation Systems", "abstract": "Fires, earthquakes, floods, hurricanes, overcrowding, or and even pandemic viruses endanger human lives. Hence, designing infrastructures to handle possible emergencies has become an ever-increasing need. The safe evacuation of occupants from the building takes precedence when dealing with the necessary mitigation and disaster risk management. This thesis deals with designing an IoT system to provide safe and quick evacuation suggestions. The IoT-based evacuation system provides optimal evacuation paths that can be continuously updated based on run-time sensory data, so evacuation guidelines can be adjusted according to visitors occupants that evolve over time. This thesis makes the following main contributions: i) Addressing an up to date state of the art class for IoT architectural styles and patterns; ii) Proposing a set of self-adaptive IoT patterns and assessing their specific quality attributes (fault-tolerance, energy consumption, and performance); iii) Designing an IoT infrastructure and testing its performance in both real-time and design-time applications; iv) Developing a network flow algorithm that facilitates minimizing the time necessary to evacuate people from a scene of a disaster; v) Modeling various social agents and their interactions during an emergency to improve the IoT system accordingly; vi) Evaluating the system by using empirical and real case studies.", "venue": "ArXiv", "authors": ["Mahyar Tourchi Moghaddam"], "year": 2020, "n_citations": 0}
{"id": 3116223, "s2_id": "2b0d00054ba31e4296b494f32264bbbcf942ecd8", "title": "A Non-Stationary Service Curve Model for Estimation of Cellular Sleep Scheduling", "abstract": "While steady-state solutions of backlog and delay have been derived for wireless systems, the analysis of transient phases still poses significant challenges. Considering the majority of short-lived and interactive flows, transient startup effects, as caused by sleep scheduling in cellular networks, have, however, a substantial impact on the performance. To facilitate reasoning about the transient behavior of systems, this paper contributes a notion of non-stationary service curves. Models of systems with sleep scheduling are derived and transient backlogs and delays are analyzed. Further, measurement methods that estimate the service of an unknown system from observations of selected probe traffic are developed. Fundamental limitations of existing measurement methods are explained by the non-convexity of the transient service and further difficulties are shown to be due to the super-additivity of network service processes. A novel two-phase probing technique is devised that first determines the shape of a minimal probe and subsequently obtains an accurate estimate of the unknown service. In a comprehensive measurement campaign, the method is used to evaluate the service of cellular networks with sleep scheduling (2G, 3G, and 4G), revealing considerable transient backlog and delay overshoots that persist for long relaxation times.", "venue": "IEEE Transactions on Mobile Computing", "authors": ["Nico  Becker", "Markus  Fidler"], "year": 2019, "n_citations": 4}
{"id": 3121861, "s2_id": "a2f58e4da4abffa3fd4c92f07ef8f9d05306a28d", "title": "A fluid simulation system based on the MPS method", "abstract": "Abstract Fluid flow simulation is a highly active area with applications in a wide range of engineering problems and interactive systems. Meshless methods like the Moving Particle Semi-implicit (MPS) are a great alternative to deal efficiently with large deformations and free-surface flow. However, mesh-based approaches can achieve higher numerical precision than particle-based techniques with a performance cost. This paper presents a numerically stable and parallelized system that benefits from advances in the literature and parallel computing to obtain an adaptable MPS method. The proposed technique can simulate liquids using different approaches, such as two ways to calculate the particles\u2019 pressure, turbulent flow, and multiphase interaction. The method is evaluated under traditional tests cases presenting comparable results to recent techniques. This work integrates the previously mentioned advances into a single solution, which can switch on improvements, such as better momentum conservation and less spurious pressure oscillations, through a graphical interface. The code is entirely open-source under the GPLv3 free software license. The GPU-accelerated code reached speedups ranging from 3 to 43 times, depending on the total number of particles. The simulation runs at one fps for a case with approximately 200,000 particles. Program summary Program Title: Voxar MPS CPC Library link to program files: http://dx.doi.org/10.17632/49f6djvhjk.1 Licensing provisions: GNU General Public License version 3 Programming language: C++ and CUDA Nature of problem: The Voxar MPS code has been developed to study the flow of incompressible fluids that requires high computational cost. Solution method: Voxar MPS is an implementation of the Moving Particle Semi-implicit, a Lagrangian meshless particle method for incompressible fluids.", "venue": "Comput. Phys. Commun.", "authors": ["Andre Luiz Buarque Vieira e Silva", "Caio Jose dos Santos Brito", "Francisco  Sim\u00f5es", "Veronica  Teichrieb"], "year": 2021, "n_citations": 2}
{"id": 3122469, "s2_id": "e29bb8a400381754c9d8868c113575ddd04e99f4", "title": "Comparison of the FCFS and PS discipline in Redundancy Systems", "abstract": "We consider the c.o.c. redundancy system with N parallel servers where incoming jobs are immediately replicated to d servers chosen uniformly at random (without replacement). A job finishes service as soon as the first replica is completed, after which all the remaining replicas are abandoned. We compare the performance of the first-come first-served (FCFS) and processor-sharing (PS) discipline based on the stability condition, the tail behavior of the latency and the expected latency.", "venue": "ArXiv", "authors": ["Youri  Raaijmakers"], "year": 2021, "n_citations": 0}
{"id": 3123158, "s2_id": "8564c6a79726ebe387a363d7ca956493bef0d40b", "title": "On the Performance Analysis of Epidemic Routing in Non-Sparse Delay Tolerant Networks", "abstract": "We study the behavior of epidemic routing in a delay tolerant network as a function of node density. Focusing on the probability of successful delivery to a destination within a deadline (PS), we show that PS experiences a phase transition as node density increases. Specifically, we prove that PS exhibits a phase transition when nodes are placed according to a Poisson process and allowed to move according to independent and identical processes with limited speed. We then propose four fluid models to evaluate the performance of epidemic routing in non-sparse networks. A model is proposed for supercritical networks based on approximation of the infection rate as a function of time. Other models are based on the approximation of the pairwise infection rate. Two of them, one for subcritical networks and another for supercritical networks, use the pairwise infection rate as a function of the number of infected nodes. The other model uses pairwise infection rate as a function of time, and can be applied for both subcritical and supercritical networks achieving good accuracy. The model for subcritical networks is accurate when density is not close to the percolation critical density. Moreover, the models that target only supercritical regime are accurate.", "venue": "ArXiv", "authors": ["Leila  Rashidi", "Donald F. Towsley", "Arman  Mohseni-Kabir", "Ali  Movaghar"], "year": 2020, "n_citations": 4}
{"id": 3124977, "s2_id": "5aedff352b947128c04988907c069f1ebbad57fa", "title": "Efficient lock-free durable sets", "abstract": "Non-volatile memory is expected to co-exist or replace DRAM in upcoming architectures. Durable concurrent data structures for non-volatile memories are essential building blocks for constructing adequate software for use with these architectures. In this paper, we propose a new approach for durable concurrent sets and use this approach to build the most efficient durable hash tables available today. Evaluation shows a performance improvement factor of up to 3.3x over existing technology.", "venue": "Proc. ACM Program. Lang.", "authors": ["Yoav  Zuriel", "Michal  Friedman", "Gali  Sheffi", "Nachshon  Cohen", "Erez  Petrank"], "year": 2019, "n_citations": 26}
{"id": 3128576, "s2_id": "5f3f45233270d296292f68b3f4b230db90e2d1ed", "title": "Pinpointing the Memory Behaviors of DNN Training", "abstract": "The training of deep neural networks (DNNs) is usually memory-hungry due to the limited device memory capacity of DNN accelerators. Characterizing the memory behaviors of DNN training is critical to optimize the device memory pressures. In this work, we pinpoint the memory behaviors of each device memory block of GPU during training by instrumenting the memory allocators of the runtime system. Our results show that the memory access patterns of device memory blocks are stable and follow an iterative fashion. These observations are useful for the future optimization of memory-efficient training from the perspective of raw memory access patterns.", "venue": "2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Jiansong  Li", "Xiao  Dong", "Guangli  Li", "Peng  Zhao", "Xueying  Wang", "Xiaobing  Chen", "Xianzhi  Yu", "Yongxin  Yang", "Zihan  Jiang", "Wei  Cao", "Lei  Liu", "Xiaobing  Feng"], "year": 2021, "n_citations": 0}
{"id": 3130422, "s2_id": "92e247b6c18ef5d3c539c281fcf33b27388bb854", "title": "Generic pipelined processor modeling and high performance cycle-accurate simulator generation", "abstract": "Detailed modeling of processors and high performance cycle-accurate simulators are essential for today's hardware and software design. These problems are challenging enough by themselves and have seen many previous research efforts. Addressing both simultaneously is even more challenging, with many existing approaches focusing on one over another. In this paper, we propose the reduced colored Petri net (RCPN) model that has two advantages: first, it offers a very simple and intuitive way of modeling pipelined processors: second, it can generate high performance cycle-accurate simulators. RCPN benefits from all the useful features of colored Petri nets without suffering from their exponential growth in complexity. RCPN processor models are very intuitive since they are a mirror image of the processor pipeline block diagram. Furthermore, in our experiments on the generated cycle-accurate simulators for XScale and StrongArm processor models, we achieved an order of magnitude (/spl sim/15 times) speedup over the popular SimpleScalar ARM simulator.", "venue": "Design, Automation and Test in Europe", "authors": ["Mehrdad  Reshadi", "Nikil D. Dutt"], "year": 2005, "n_citations": 24}
{"id": 3136353, "s2_id": "3cf01ee4d3da8db113d09a50a969f8865288211b", "title": "Parallel algorithms development for programmable logic devices", "abstract": "Programmable logic devices (PLDs) continue to grow in size and currently contain several millions of gates. At the same time, research effort is going into higher-level hardware synthesis methodologies for reconfigurable computing that can exploit PLD technology. In this paper, we explore the effectiveness and extend one such formal methodology in the design of massively parallel algorithms. We take a step-wise refinement approach to the development of correct reconfigurable hardware circuits from formal specifications. A functional programming notation is used for specifying algorithms and for reasoning about them. The specifications are realised through the use of a combination of function decomposition strategies, data refinement techniques, and off-the-shelf refinements based upon higher-order functions. The off-the-shelf refinements are inspired by the operators of communicating sequential processes (CSP) and map easily to programs in Handel-C (a hardware description language). The Handel-C descriptions are directly compiled into reconfigurable hardware. The practical realisation of this methodology is evidenced by a case studying the matrix multiplication algorithm as it is relatively simple and well known. In this paper, we obtain several hardware implementations with different performance characteristics by applying different refinements to the algorithm. The developed designs are compiled and tested under Celoxica's RC-1000 reconfigurable computer with its 2 million gates Virtex-E FPGA. Performance analysis and evaluation of these implementations are included.", "venue": "Adv. Eng. Softw.", "authors": ["Issam W. Damaj"], "year": 2006, "n_citations": 10}
{"id": 3138716, "s2_id": "c0886957c05e5d1e61509bbadc52c0c67b66fc79", "title": "HPC AI500: The Methodology, Tools, Roofline Performance Models, and Metrics for Benchmarking HPC AI Systems", "abstract": "The recent years witness a trend of applying large-scale distributed deep learning in both business and scientific computing areas, whose goal is to speed up the training time to achieve a state-of-the-art quality. The HPC community feels a great interest in building the HPC AI systems that are dedicated to running those workloads. The HPC AI benchmarks accelerate the process. Unfortunately, benchmarking HPC AI systems at scale raises serious challenges. None of previous HPC AI benchmarks achieve the goal of being equivalent, relevant, representative, affordable, and repeatable. This paper presents a comprehensive methodology, tools, Roofline performance models, and innovative metrics for benchmarking, optimizing, and ranking HPC AI systems, which we call HPC AI500 V2.0. We abstract the HPC AI system into nine independent layers, and present explicit benchmarking rules and procedures to assure equivalence of each layer, repeatability, and replicability. On the basis of AIBench -- by far the most comprehensive AI benchmarks suite, we present and build two HPC AI benchmarks from both business and scientific computing: Image Classification, and Extreme Weather Analytics, achieving both representativeness and affordability. To rank the performance and energy-efficiency of HPC AI systems, we propose Valid FLOPS, and Valid FLOPS per watt, which impose a penalty on failing to achieve the target quality. We propose using convolution and GEMM -- the two most intensively-used kernel functions to measure the upper bound performance of the HPC AI systems, and present HPC AI roofline models for guiding performance optimizations. The evaluations show our methodology, benchmarks, performance models, and metrics can measure, optimize, and rank the HPC AI systems in a scalable, simple, and affordable way. HPC AI500 V2.0 are publicly available from this http URL.", "venue": "ArXiv", "authors": ["Zihan  Jiang", "Lei  Wang", "Xingwang  Xiong", "Wanling  Gao", "Chunjie  Luo", "Fei  Tang", "Chuanxin  Lan", "Hongxiao  Li", "Jianfeng  Zhan"], "year": 2020, "n_citations": 9}
{"id": 3144163, "s2_id": "bd0f6cb1547e8c8496bc5fcc19f922b5c2f69151", "title": "Block-structured supermarket models", "abstract": "Supermarket models are a class of parallel queueing networks with an adaptive control scheme that play a key role in the study of resource management of, such as, computer networks, manufacturing systems and transportation networks. When the arrival processes are non-Poisson and the service times are non-exponential, analysis of such a supermarket model is always limited, interesting, and challenging.This paper describes a supermarket model with non-Poisson inputs: Markovian Arrival Processes (MAPs) and with non-exponential service times: Phase-type (PH) distributions, and provides a generalized matrix-analytic method which is first combined with the operator semigroup and the mean-field limit. When discussing such a more general supermarket model, this paper makes some new results and advances as follows: (1) Providing a detailed probability analysis for setting up an infinite-dimensional system of differential vector equations satisfied by the expected fraction vector, where the invariance of environment factors is given as an important result. (2) Introducing the phase-type structure to the operator semigroup and to the mean-field limit, and a Lipschitz condition can be established by means of a unified matrix-differential algorithm. (3) The matrix-analytic method is used to compute the fixed point which leads to performance computation of this system. Finally, we use some numerical examples to illustrate how the performance measures of this supermarket model depend on the non-Poisson inputs and on the non-exponential service times. Thus the results of this paper give new highlight on understanding influence of non-Poisson inputs and of non-exponential service times on performance measures of more general supermarket models.", "venue": "Discret. Event Dyn. Syst.", "authors": ["Quan-Lin  Li", "John C. S. Lui"], "year": 2016, "n_citations": 15}
{"id": 3146724, "s2_id": "558482a2eca647952a62de694e34a9fdf4d8a224", "title": "Non-equilibrium Surface Growth and Scalability of Parallel Algorithms for Large Asynchronous Systems", "abstract": "The scalability of massively parallel algorithms is a fundamental question in computer science. We study the scalability and the efficiency of a conservative massively parallel algorithm for discrete-event simulations where the discrete events are Poisson arrivals. The parallel algorithm is applicable to a wide range of problems, including dynamic Monte Carlo simulations for large asynchronous systems with short-range interactions. The evolution of the simulated time horizon is analogous to a growing and fluctuating surface, and the efficiency of the algorithm corresponds to the density of local minima of this surface. In one dimension we find that the steady state of the macroscopic landscape is governed by the Edwards-Wilkinson Hamiltonian, which implies that the algorithm is scalable. Preliminary results for higher-dimensional logical topologies are discussed.", "venue": "ArXiv", "authors": ["G.  Korniss", "M. A. Novotny", "Zolt\u00e1n  Toroczkai", "Per Arne Rikvold"], "year": 2000, "n_citations": 7}
{"id": 3148758, "s2_id": "1e9ab371d22ee57c002ac3d355bb4d8bdef02827", "title": "Modeling multi-cell IEEE 802.11 WLANs with application to channel assignment", "abstract": "We provide a simple and accurate analytical model for multi-cell IEEE 802.11 WLANs. Our model applies if the cell radius, R, is much smaller than the carrier sensing range, Rcs. We argue that, the condition Rcs \u226b\u226b R is likely to hold in a dense deployment of Access Points (APs). We develop a scalable cell level model for such WLANs with saturated nodes as well as for TCP-controlled long file downloads. The accuracy of our model is demonstrated by comparison with ns-2 simulations. Based on the insights provided by our analytical model, we propose a simple channel assignment algorithm which provides static assignments that are Nash equilibria in pure strategies for the objective of maximizing normalized network throughput, and requires only as many steps as there are channels. Furthermore, our channel assignment algorithm does not require any a priori knowledge of topology and can be implemented in a decentralized manner. In contrast to prior work, our approach to channel assignment is based on the throughput metric.", "venue": "2009 7th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks", "authors": ["Manoj K. Panda", "Anurag  Kumar"], "year": 2009, "n_citations": 11}
{"id": 3150263, "s2_id": "5984a0b6957b0e9ed8b030c3784f3ec367fbfd8c", "title": "SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems", "abstract": "Simple graph algorithms such as PageRank have been the target of numerous hardware accelerators. Yet, there also exist much more complex graph mining algorithms for problems such as clustering or maximal clique listing. These algorithms are memory-bound and thus could be accelerated by hardware techniques such as Processing-in-Memory (PIM). However, they also come with non-straightforward parallelism and complicated memory access patterns. In this work, we address this problem with a simple yet surprisingly powerful observation: operations on sets of vertices, such as intersection or union, form a large part of many complex graph mining algorithms, and can offer rich and simple parallelism at multiple levels. This observation drives our cross-layer design, in which we (1) expose set operations using a novel programming paradigm, (2) express and execute these operations efficiently with carefully designed set-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA instructions. The key design idea is to alleviate the bandwidth needs of SISA instructions by mapping set operations to two types of PIM: in-DRAM bulk bitwise computing for bitvectors representing high-degree vertices, and near-memory logic layers for integer arrays representing low-degree vertices. Set-centric SISA-enhanced algorithms are efficient and outperform hand-tuned baselines, offering more than 10 \u00d7 speedup over the established Bron-Kerbosch algorithm for listing maximal cliques. We deliver more than 10 SISA set-centric algorithm formulations, illustrating SISA\u2019s wide applicability.", "venue": "MICRO", "authors": ["Maciej  Besta", "Raghavendra  Kanakagiri", "Grzegorz  Kwasniewski", "Rachata  Ausavarungnirun", "Jakub  Ber\u00e1nek", "Konstantinos  Kanellopoulos", "Kacper  Janda", "Zur  Vonarburg-Shmaria", "Lukas  Gianinazzi", "Ioana  Stefan", "Juan  G\u00f3mez-Luna", "Marcin  Copik", "Lukas  Kapp-Schwoerer", "Salvatore Di Girolamo", "Marek  Konieczny", "Onur  Mutlu", "Torsten  Hoefler"], "year": 2021, "n_citations": 6}
{"id": 3150303, "s2_id": "3c910501a0c58d35be487fccccf4ceb27ea0f4af", "title": "Performance of network and service monitoring frameworks", "abstract": "The efficiency and the performance of management systems is becoming a hot research topic within the networks and services management community. This concern is due to the new challenges of large scale managed systems, where the management plane is integrated within the functional plane and where management activities have to carry accurate and up-to-date information.", "venue": "2009 IFIP/IEEE International Symposium on Integrated Network Management", "authors": ["Abdelkader  Lahmadi", "Laurent  Andrey", "Olivier  Festor"], "year": 2009, "n_citations": 6}
{"id": 3151735, "s2_id": "edfc40fd66c767f4c9ba15d916781b4b3407233b", "title": "DV-DVFS: merging data variety and DVFS technique to manage the energy consumption of big data processing", "abstract": "Data variety is one of the most important features of Big Data. Data variety is the result of aggregating data from multiple sources and uneven distribution of data. This feature of Big Data causes high variation in the consumption of processing resources such as CPU consumption. This issue has been overlooked in previous works. To overcome the mentioned problem, in the present work, we used Dynamic Voltage and Frequency Scaling (DVFS) to reduce the energy consumption of computation. To this goal, we consider two types of deadlines as our constraint. Before applying the DVFS technique to computer nodes, we estimate the processing time and the frequency needed to meet the deadline. In the evaluation phase, we have used a set of data sets and applications. The experimental results show that our proposed approach surpasses the other scenarios in processing real datasets. Based on the experimental results in this paper, DV-DVFS can achieve up to 15% improvement in energy consumption.", "venue": "J. Big Data", "authors": ["Hossein  Ahmadvand", "Fouzhan  Foroutan", "Mahmood  Fathy"], "year": 2021, "n_citations": 1}
{"id": 3152039, "s2_id": "bbbd8b51ead1ca0f6637e586a611d9ff3b362c41", "title": "A polling model with multiple priority levels", "abstract": "In this paper we consider a single-server cyclic polling system. Between visits to successive queues, the server is delayed by a random switch-over time. The order in which customers are served in each queue is determined by a priority level that is assigned to each customer at his arrival. For this situation the following service disciplines are considered: gated, exhaustive, and globally gated. We study the cycle time distribution, the waiting times for each customer type, the joint queue length distribution of all priority classes at all queues at polling epochs, and the steady-state marginal queue length distributions for each customer type.", "venue": "Perform. Evaluation", "authors": ["Marko A. A. Boon", "Ivo J. B. F. Adan", "Onno J. Boxma"], "year": 2010, "n_citations": 30}
{"id": 3154956, "s2_id": "91db231a3191e8f1a1ef64d8a8cd62b832ab0004", "title": "A Preliminary Proposal for an Analytical Model for Evaluating the Impact on Performance of Data Access Patterns in Transaction Execution", "abstract": "We present a preliminary proposal for an analytical model for evaluating the impact on performance of data access patterns in concurrent transaction execution. We consider the case of concurrency control protocols that use locking to ensure isolation in the execution of transactions. We analyse scenarios where transactions access one or more sets of data items in the same order or in different order.", "venue": "ArXiv", "authors": ["Pierangelo Di Sanzo"], "year": 2021, "n_citations": 0}
{"id": 3155929, "s2_id": "d808dbdd6a53259bea908266397b3983f246aabc", "title": "Dissecting the Graphcore IPU Architecture via Microbenchmarking", "abstract": "This report focuses on the architecture and performance of the Intelligence Processing Unit (IPU), a novel, massively parallel platform recently introduced by Graphcore and aimed at Artificial Intelligence/Machine Learning (AI/ML) workloads. We dissect the IPU's performance behavior using microbenchmarks that we crafted for the purpose. We study the IPU's memory organization and performance. We study the latency and bandwidth that the on-chip and off-chip interconnects offer, both in point-to-point transfers and in a spectrum of collective operations, under diverse loads. We evaluate the IPU's compute power over matrix multiplication, convolution, and AI/ML primitives. We discuss actual performance in comparison with its theoretical limits. Our findings reveal how the IPU's architectural design affects its performance. Moreover, they offer simple mental models to predict an application's performance on the IPU, on the basis of the computation and communication steps it involves. This report is the natural extension to a novel architecture of a continuing effort of ours that focuses on the microbenchmark-based discovery of massively parallel architectures.", "venue": "ArXiv", "authors": ["Zhe  Jia", "Blake  Tillman", "Marco  Maggioni", "Daniele Paolo Scarpazza"], "year": 2019, "n_citations": 36}
{"id": 3159511, "s2_id": "e244ca7e29348023d4d75d987b50678d63ab2ec1", "title": "Formally Verified Argument Reduction with a Fused Multiply-Add", "abstract": "The Cody and Waite argument reduction technique works perfectly for reasonably large arguments, but as the input grows, there are no bits left to approximate the constant with enough accuracy. Under mild assumptions, we show that the result computed with a fused multiply-add provides a fully accurate result for many possible values of the input with a constant almost accurate to the full working precision. We also present an algorithm for a fully accurate second reduction step to reach full double accuracy (all the significand bits of two numbers are accurate) even in the worst cases of argument reduction. Our work recalls the common algorithms and presents proofs of correctness. All the proofs are formally verified using the Coq automatic proof checker.", "venue": "IEEE Transactions on Computers", "authors": ["Sylvie  Boldo", "Marc  Daumas", "Ren-Cang  Li"], "year": 2009, "n_citations": 14}
{"id": 3163793, "s2_id": "498ac2e02021a9429ba11279f1da55294da9942b", "title": "A Characterization of the COVID-19 Pandemic Impact on a Mobile Network Operator Traffic", "abstract": "During early 2020, the SARS-CoV-2 virus rapidly spread worldwide, forcing many governments to impose strict lock-down measures to tackle the pandemic. This significantly changed peoples mobility and habits, subsequently impacting how they use telecommunication networks. In this paper, we investigate the effects of the COVID-19 emergency on a UK Mobile Network Operator (MNO). We quantify the changes in users mobility and investigate how this impacted the cellular network usage and performance. Our analysis spans from the entire country to specific regions, and geodemographic area clusters. We also provide a detailed analysis for London. Our findings bring insights at different geotemporal granularity on the status of the cellular network, from the decrease in data traffic volume in the cellular network and lower load on the radio network, counterposed to a surge in the conversational voice traffic volume.", "venue": "Internet Measurement Conference", "authors": ["Andra  Lutu", "Diego  Perino", "Marcelo  Bagnulo", "Enrique  Frias-Martinez", "Javad  Khangosstar"], "year": 2020, "n_citations": 38}
{"id": 3165224, "s2_id": "d5dc53508c5a97e810d6be8c32af9dbce1afcc57", "title": "Optimal Load Balancing in Bipartite Graphs", "abstract": "Applications in cloud platforms motivate the study of efficient load balancing under job-server constraints and server heterogeneity. In this paper, we study load balancing on a bipartite graph where left nodes correspond to job types and right nodes correspond to servers, with each edge indicating that a job type can be served by a server. Thus edges represent locality constraints, i.e., each job can only be served at servers which contained certain data and/or machine learning (ML) models. Servers in this system can have heterogeneous service rates. In this setting, we investigate the performance of two policies named Join-the-Fastest-of-the-Shortest-Queue (JFSQ) and Join-the-Fastest-of-the-Idle-Queue (JFIQ), which are simple variants of Join-the-Shortest-Queue and Join-the-Idle-Queue, where ties are broken in favor of the fastest servers. Under a \"well-connected\" graph condition, we show that JFSQ and JFIQ are asymptotically optimal in the mean response time when the number of servers goes to infinity. In addition to asymptotic optimality, we also obtain upper bounds on the mean response time for finite-size systems. We further show that the well-connectedness condition can be satisfied by a random bipartite graph construction with relatively sparse connectivity.", "venue": "ArXiv", "authors": ["Wentao  Weng", "Xingyu  Zhou", "R.  Srikant"], "year": 2020, "n_citations": 4}
{"id": 3166426, "s2_id": "48b2b54d27898017b48f158eacabe4952a24a870", "title": "Accuracy and Performance Comparison of Video Action Recognition Approaches", "abstract": "Over the past few years, there has been significant interest in video action recognition systems and models. However, direct comparison of accuracy and computational performance results remain clouded by differing training environments, hardware specifications, hyperparameters, pipelines, and inference methods. This article provides a direct comparison between fourteen \u201coff-the-shelf\u201d and state-of-the-art models by ensuring consistency in these training characteristics in order to provide readers with a meaningful comparison across different types of video action recognition algorithms. Accuracy of the models is evaluated using standard Top-1 and Top-5 accuracy metrics in addition to a proposed new accuracy metric. Additionally, we compare computational performance of distributed training from two to sixty-four GPUs on a state-of-the-art HPC system.", "venue": "2020 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Matthew  Hutchinson", "Siddharth  Samsi", "William  Arcand", "David  Bestor", "Bill  Bergeron", "Chansup  Byun", "Micheal  Houle", "Matthew  Hubbell", "Micheal  Jones", "Jeremy  Kepner", "Andrew  Kirby", "Peter  Michaleas", "Lauren  Milechin", "Julie  Mullen", "Andrew  Prout", "Antonio  Rosa", "Albert  Reuther", "Charles  Yee", "Vijay  Gadepally"], "year": 2020, "n_citations": 3}
{"id": 3168247, "s2_id": "a0d083bb1d23e3e4f23373ab46990c5d7c12df0c", "title": "Throughput Optimal Scheduling Policies in Networks of Interacting Queues", "abstract": "This report considers a fairly general model of constrained queuing networks that allows us to represent both MMBP (Markov Modulated Bernoulli Processes) arrivals and time-varying service constraints. We derive a set of sufficient conditions for throughput optimality of scheduling policies that encompass and generalize all the previously obtained results in the field. This leads to the definition of new classes of (non diagonal) throughput optimal scheduling policies. We prove the stability of queues by extending the traditional Lyapunov drift criteria methodology.", "venue": "ArXiv", "authors": ["Emilio  Leonardi"], "year": 2013, "n_citations": 0}
{"id": 3172957, "s2_id": "84e14a974bf1e95bb71e4d998be4fafa6ce4a330", "title": "Inferring Catchment in Internet Routing", "abstract": "BGP is the de-facto Internet routing protocol for interconnecting Autonomous Systems (AS). Each AS selects its preferred routes based on its routing policies, which are typically not disclosed. Due to the distributed route selection and information hiding, answering questions such as \"what is the expected catchment of the anycast sites of a content provider at the AS-level, if new sites are deployed?\", or \"how will load-balancing behave if an ISP changes its routing policy for a prefix?\", is a hard challenge. In this work, we propose a framework and methodology to infer the routing behavior in existing or hypothetical routing configurations, and provide new capabilities and insights for informative route inference (e.g., isolating the effect of randomness that is present in prior simulation-based approaches). The proposed framework can be useful in a number of applications: measurements/monitoring, traffic engineering, network planning, Internet routing models, etc.", "venue": "PERV", "authors": ["Pavlos  Sermpezis", "Vasileios  Kotronis"], "year": 2019, "n_citations": 0}
{"id": 3174436, "s2_id": "a1fa25ea78618fb254fb0010982e58bb0b1347aa", "title": "Poly-symmetry in processor-sharing systems", "abstract": "We consider a system of processor-sharing queues with state-dependent service rates. These are allocated according to balanced fairness within a polymatroid capacity set. Balanced fairness is known to be both insensitive and Pareto-efficient in such systems, which ensures that the performance metrics, when computable, will provide robust insights into the real performance of the system considered. We first show that these performance metrics can be evaluated with a complexity that is polynomial in the system size if the system is partitioned into a finite number of parts, so that queues are exchangeable within each part and asymmetric across different parts. This in turn allows us to derive stochastic bounds for a larger class of systems which satisfy less restrictive symmetry assumptions. These results are applied to practical examples of tree data networks, such as backhaul networks of Internet service providers, and computer clusters.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Thomas  Bonald", "C\u00e9line  Comte", "Virag  Shah", "Gustavo de Veciana"], "year": 2017, "n_citations": 6}
{"id": 3179790, "s2_id": "22a018fbee5488f14578909143fda6b243b49685", "title": "Performance Evaluation of Python Parallel Programming Models: Charm4Py and mpi4py", "abstract": "Python is rapidly becoming the lingua franca of machine learning and scientific computing. With the broad use of frameworks such as Numpy, SciPy, and TensorFlow, scientific computing and machine learning are seeing a productivity boost on systems without a requisite loss in performance. While highperformance libraries often provide adequate performance within a node, distributed computing is required to scale Python across nodes and make it genuinely competitive in large-scale highperformance computing. Many frameworks, such as Charm4Py, DaCe, Dask, Legate Numpy, mpi4py, and Ray, scale Python across nodes. However, little is known about these frameworks\u2019 relative strengths and weaknesses, leaving practitioners and scientists without enough information about which frameworks are suitable for their requirements. In this paper, we seek to narrow this knowledge gap by studying the relative performance of two such frameworks: Charm4Py and mpi4py. We perform a comparative performance analysis of Charm4Py and mpi4py using CPU and GPU-based microbenchmarks other representative mini-apps for scientific computing.", "venue": "ArXiv", "authors": ["Zane  Fink", "Simeng  Liu", "Jaemin  Choi", "Matthias  Diener", "Laxmikant V. Kale"], "year": 2021, "n_citations": 0}
{"id": 3179901, "s2_id": "713c7125caa2914de6c72e7b1ee75a917f79b93b", "title": "CARMA: Collective Adaptive Resource-sharing Markovian Agents", "abstract": "In this paper we present CARMA, a language recently defined to support specification and analysis of collective adaptive systems. CARMA is a stochastic process algebra equipped with linguistic constructs specifically developed for modelling and programming systems that can operate in open-ended and unpredictable environments. This class of systems is typically composed of a huge number of interacting agents that dynamically adjust and combine their behaviour to achieve specific goals. A CARMA model, termed a collective, consists of a set of components, each of which exhibits a set of attributes. To model dynamic aggregations, which are sometimes referred to as ensembles, CARMA provides communication primitives that are based on predicates over the exhibited attributes. These predicates are used to select the participants in a communication. Two communication mechanisms are provided in the CARMA language: multicast-based and unicast-based. In this paper, we first introduce the basic principles of CARMA and then we show how our language can be used to support specification with a simple but illustrative example of a socio-technical collective adaptive system.", "venue": "QAPL", "authors": ["Luca  Bortolussi", "Rocco De Nicola", "Vashti  Galpin", "Stephen  Gilmore", "Jane  Hillston", "Diego  Latella", "Michele  Loreti", "Mieke  Massink"], "year": 2015, "n_citations": 44}
{"id": 3180660, "s2_id": "d480bb1728557669889b9443dedb65d354f53ad5", "title": "Fair and Efficient Gossip in Hyperledger Fabric", "abstract": "Permissioned blockchains are supported by identified but individually untrustworthy nodes, collectively maintaining a replicated ledger whose content is trusted. The Hyperledger Fabric permissioned blockchain system targets high-throughput transaction processing. Fabric uses a set of nodes tasked with the ordering of transactions using consensus. Additional peers endorse and validate transactions, and maintain a copy of the ledger. The ability to quickly disseminate new transaction blocks from ordering nodes to all peers is critical for both performance and consistency. Broadcast is handled by a gossip protocol, using randomized exchanges of blocks between peers.We show that the current implementation of gossip in Fabric leads to heavy tail distributions of block propagation latencies, impacting performance, consistency, and fairness. We contribute a novel design for gossip in Fabric that simultaneously optimizes propagation time, tail latency and bandwidth consumption. Using a 100-node cluster, we show that our enhanced gossip allows the dissemination of blocks to all peers more than 10 times faster than with the original implementation, while decreasing the overall network bandwidth consumption by more than 40%. With a high throughput and concurrent application, this results in 17% to 36% fewer invalidated transactions for different block sizes.", "venue": "2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)", "authors": ["Nicolae  Berendea", "Hugues  Mercier", "Emanuel  Onica", "Etienne  Riviere"], "year": 2020, "n_citations": 4}
{"id": 3184727, "s2_id": "a1df5d48a7e0a078b289957f42ea505056a748af", "title": "The Anatomy of Large-Scale Distributed Graph Algorithms", "abstract": "The increasing complexity of the software/hardware stack of modern supercomputers results in explosion of parameters. The performance analysis becomes a truly experimental science, even more challenging in the presence of massive irregularity and data dependency. We analyze how the existing body of research handles the experimental aspect in the context of distributed graph algorithms (DGAs). We distinguish algorithm-level contributions, often prioritized by authors, from runtime-level concerns that are harder to place. We show that the runtime is such an integral part of DGAs that experimental results are difficult to interpret and extrapolate without understanding the properties of the runtime used. We argue that in order to gain understanding about the impact of runtimes, more information needs to be gathered. To begin this process, we provide an initial set of recommendations for describing DGA results based on our analysis of the current state of the field.", "venue": "ArXiv", "authors": ["Jesun Sahariar Firoz", "Thejaka Amila Kanewala", "Marcin  Zalewski", "Martina  Barnas", "Andrew  Lumsdaine"], "year": 2015, "n_citations": 1}
{"id": 3185181, "s2_id": "7b99feba181fccdf6c2576b971b5b77a7b2c7008", "title": "MRRR-based eigensolvers for multi-core processors and supercomputers", "abstract": "The real symmetric tridiagonal eigenproblem is of outstanding importance in numerical computations; it arises frequently as part of eigensolvers for standard and generalized dense Hermitian eigenproblems that are based on a reduction to tridiagonal form. For its solution, the algorithm of Multiple Relatively Robust Representations (MRRR or MR3 in short) - introduced in the late 1990s - is among the fastest methods. To compute k eigenpairs of a real n-by-n tridiagonal T, MRRR only requires O(kn) arithmetic operations; in contrast, all the other practical methods require O(k^2 n) or O(n^3) operations in the worst case. This thesis centers around the performance and accuracy of MRRR.", "venue": "ArXiv", "authors": ["Matthias  Petschow"], "year": 2013, "n_citations": 1}
{"id": 3188904, "s2_id": "a7660f22a8bfbc93d103efe30b5b7041f8d12a19", "title": "Mosaic flows: A transferable deep learning framework for solving PDEs on unseen domains", "abstract": "Physics-informed neural networks (PINNs) are increasingly employed to replace/augment traditional numerical methods in solving partial differential equations (PDEs). While state-of-the-art PINNs have many attractive features, they approximate a specific realization of a PDE system and hence are problem-specific. That is, the model needs to be re-trained each time the boundary conditions (BCs) and domain shape/size change. This limitation prohibits the application of PINNs to realistic or large-scale engineering problems especially since the costs and efforts associated with their training are considerable. We introduce a transferable framework for solving boundary value problems (BVPs) via deep neural networks which can be trained once and used forever for various unseen domains and BCs. We first introduce genomic flow network (GFNet), a neural network that can infer the solution of a BVP across arbitrary BCs on a small square domain called genome. Then, we propose mosaic flow (MF) predictor, a novel iterative algorithm that assembles the GFNet\u2019s inferences for BVPs on large domains with unseen sizes/shapes and BCs while preserving the spatial regularity of the solution. We demonstrate that our framework can estimate the solution of Laplace and Navier-Stokes equations in domains of unseen shapes and BCs that are, respectively, 1200 and 12 times larger than the training domains. Since our framework eliminates the need to re-train models for unseen domains and BCs, it demonstrates up to 3 orders-of-magnitude speedups compared to the state-of-the-art.", "venue": "Computer Methods in Applied Mechanics and Engineering", "authors": ["Hengjie  Wang", "Robert  Planas", "Aparna  Chandramowlishwaran", "Ramin  Bostanabad"], "year": 2022, "n_citations": 0}
{"id": 3190790, "s2_id": "ea799ce1ba20836023ad46143f840853b6e2bf07", "title": "Software Performance Analysis", "abstract": "The key to speeding up applications is often understanding where the elapsed time is spent, and why. This document reviews in depth the full array of performance analysis tools and techniques available on Linux for this task, from the traditional tools like gcov and gprof, to the more advanced tools still under development like oprofile and the Linux Trace Toolkit. The focus is more on the underlying data collection and processing algorithms, and their overhead and precision, than on the cosmetic details of the graphical user interface frontends.", "venue": "ArXiv", "authors": ["Michel  Dagenais", "Karim  Yaghmour", "Charles  Levert", "Makan  Pourzandi"], "year": 2005, "n_citations": 6}
{"id": 3191138, "s2_id": "6cfc6179bce37db64bc03e15317c09b3cb84ad1e", "title": "Visualizing a Moving Target: A Design Study on Task Parallel Programs in the Presence of Evolving Data and Concerns", "abstract": "Common pitfalls in visualization projects include lack of data availability and the domain users' needs and focus changing too rapidly for the design process to complete. While it is often prudent to avoid such projects, we argue it can be beneficial to engage them in some cases as the visualization process can help refine data collection, solving a \u201cchicken and egg\u201d problem of having the data and tools to analyze it. We found this to be the case in the domain of task parallel computing where such data and tooling is an open area of research. Despite these hurdles, we conducted a design study. Through a tightly-coupled iterative design process, we built Atria, a multi-view execution graph visualization to support performance analysis. Atria simplifies the initial representation of the execution graph by aggregating nodes as related to their line of code. We deployed Atria on multiple platforms, some requiring design alteration. We describe how we adapted the design study methodology to the \u201cmoving target\u201d of both the data and the domain experts' concerns and how this movement kept both the visualization and programming project healthy. We reflect on our process and discuss what factors allow the project to be successful in the presence of changing data and user needs.", "venue": "IEEE Transactions on Visualization and Computer Graphics", "authors": ["Katy  Williams", "Alex  Bigelow", "Kate  Isaacs"], "year": 2020, "n_citations": 9}
{"id": 3191145, "s2_id": "8d990e5a42e37e5ccbdbada32a94e9ca7be58141", "title": "A Call-Graph Profiler for GNU Octave", "abstract": "We report the design and implementation of a call-graph profiler for GNU Octave, a numerical computing platform. GNU Octave simplifies matrix computation for use in modeling or simulation. Our work provides a call-graph profiler, which is an improvement on the flat profiler. We elaborate design constraints of building a profiler for numerical computation, and benchmark the profiler by comparing it to the rudimentary timer start-stop (tic-toc) measurements, for a similar set of programs. The profiler code provides clean interfaces to internals of GNU Octave, for other (newer) profiling tools on GNU Octave.", "venue": "ArXiv", "authors": ["Muthiah  Annamalai", "R. Leela Velusamy"], "year": 2008, "n_citations": 0}
{"id": 3191532, "s2_id": "2ff67b74411bccd6084d0d6dbaadbe77081f1128", "title": "Reflection Resource Management for Intelligent Reflecting Surface Aided Wireless Networks", "abstract": "In this paper, the adoption of an intelligent reflecting surface (IRS) for multiple user pairs in two-hop networks is investigated. Different from the existing studies on IRS that mainly focused on tuning the reflection coefficients of all elements, we consider the implementation of true reflection resource management (RRM) through the identification of the best triggered module subset. More precisely, the implementation of true RRM builds on the premise of our proposed modular IRS structure consisting of multiple independent and controllable modules. In the context of modular IRS structure, we investigate the signal-to-interference-plus-noise ratio (SINR)-based max-min problem subject to per source terminals (STs) power budgets and module size constraint, via joint triggered module subset identification, transmit power allocation, and the corresponding passive beamforming. Whereas this problem is NP-hard due to the module size constraint, which can be addressed by the convex sparsity-inducing approximation to the hard module size constraint using mixed $\\ell _{1,F}\\text {-norm}$ , where it yields a suitable semidefinite relaxation. Using techniques from separable convex programming, we provide a two-block alternating direction method of multipliers (ADMM) algorithm for the approximated problem. Numerical simulations are used to validate the analysis and assess the performance of the proposed algorithm as a function of the system parameters. Further energy efficiency (EE) performance comparison demonstrates the necessity and meaningfulness of the introduced modular IRS structure. Specifically, for a given network setting, there is an optimal value of the number of triggered modules for system, when the EE is considered.", "venue": "IEEE Transactions on Communications", "authors": ["Yulan  Gao", "Chao  Yong", "Zehui  Xiong", "Jun  Zhao", "Yue  Xiao", "Dusit  Niyato"], "year": 2021, "n_citations": 8}
{"id": 3193045, "s2_id": "81ed5d9ddf15e2a8b2bcd748ca0e4d02242da8da", "title": "On the maximal shortest path in a connected component in V2V", "abstract": "In this work, a VANET (Vehicular Ad-hoc NETwork) is considered to operate on a simple lane, without infrastructure. The arrivals of vehicles are assumed to be general with any traffic and speed assumptions. The vehicles communicate through the shortest path. In this paper, we study the probability distribution of the number of hops on the maximal shortest path in a connected component of vehicles. The general formulation is given for any assumption of road traffic. Then, it is applied to calculate the z-transform of this distribution for medium and dense networks in the Poisson case. Our model is validated with the Madrid road traces of the Universitat Polit\\`ecnica de Catalunya. These results may be useful for example when evaluating diffusion protocols through the shortest path in a VANET, where not only the mean but also the other moments are needed to derive accurate results.", "venue": "Perform. Evaluation", "authors": ["Michel  Marot", "Adel Mounir Sareh Said", "Hossam  Afifi"], "year": 2015, "n_citations": 4}
{"id": 3194275, "s2_id": "f899f59f0f97cfb150bf4b5394db3776e8d3c73d", "title": "Combinatorial Sleeping Bandits With Fairness Constraints", "abstract": "The multi-armed bandit (MAB) model has been widely adopted for studying many practical optimization problems (network resource allocation, ad placement, crowdsourcing, etc.) with unknown parameters. The goal of the player (i.e., the decision maker) here is to maximize the cumulative reward in the face of uncertainty. However, the basic MAB model neglects several important factors of the system in many real-world applications, where multiple arms (i.e., actions) can be simultaneously played and an arm could sometimes be \u201csleeping\u201d (i.e., unavailable). Besides reward maximization, ensuring <italic>fairness</italic> is also a key design concern in practice. To that end, we propose a new <italic>Combinatorial Sleeping MAB model with Fairness constraints</italic>, called <italic>CSMAB-F</italic>, aiming to address the aforementioned crucial modeling issues. The objective is now to maximize the reward while satisfying the fairness requirement of a minimum selection fraction for each individual arm. To tackle this new problem, we extend an online learning algorithm, called <italic>Upper Confidence Bound (UCB)</italic>, to deal with a critical tradeoff between <italic>exploitation</italic> and <italic>exploration</italic> and employ the virtual queue technique to properly handle the fairness constraints. By carefully integrating these two techniques, we develop a new algorithm, called <italic>Learning with Fairness Guarantee (LFG)</italic>, for the CSMAB-F problem. Further, we rigorously prove that not only LFG is <italic>feasibility-optimal</italic>, but it also has a time-average <italic>regret</italic> upper bounded by <inline-formula><tex-math notation=\"LaTeX\">$\\frac{N}{2 \\eta } + \\frac{\\beta _1 \\sqrt{m N T \\log {T}}+ \\beta _2~N}{T}$</tex-math></inline-formula>, where <inline-formula><tex-math notation=\"LaTeX\">$N$</tex-math></inline-formula> is the total number of arms, <inline-formula><tex-math notation=\"LaTeX\">$m$</tex-math></inline-formula> is the maximum number of arms that can be simultaneously played, <inline-formula><tex-math notation=\"LaTeX\">$T$</tex-math></inline-formula> is the time horizon, <inline-formula><tex-math notation=\"LaTeX\">$\\beta _1$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$\\beta _2$</tex-math></inline-formula> are constants, and <inline-formula><tex-math notation=\"LaTeX\">$\\eta$</tex-math></inline-formula> is a design parameter that we can tune. Finally, we perform extensive simulations to corroborate the effectiveness of the proposed algorithm. Interestingly, the simulation results reveal an important tradeoff between the regret and the speed of convergence to a point satisfying the fairness constraints.", "venue": "IEEE Transactions on Network Science and Engineering", "authors": ["Fengjiao  Li", "Jia  Liu", "Bo  Ji"], "year": 2020, "n_citations": 2}
{"id": 3195895, "s2_id": "fd1c32e473ad9a5cc8a4cce2fae7d9bea2d42b2a", "title": "Using Machine Learning to Optimize Web Interactions on Heterogeneous Mobile Multi-cores", "abstract": "The web has become a ubiquitous application development platform for mobile systems. Yet, energy-efficient mobile web browsing remains an outstanding challenge. Prior work in the field mainly focuses on the initial page loading stage but fails to exploit the opportunities for energy-efficiency optimization while the user is interacting with a loaded page. This paper presents a novel approach for performing energy optimization for interactive mobile web browsing. At the heart of our approach is a set of machine learning models, which estimate \\emph{at runtime} the frames per second for a given user interaction input by running the computation-intensive web render engine on a specific processor core under a given clock speed. We use the learned predictive models as a utility function to quickly search for the optimal processor setting to carefully trade responsive time for reduced energy consumption. We integrate our techniques to the open-source Chromium browser and apply it to two representative mobile user events: scrolling and pinching (i.e., zoom in and out). We evaluate the developed system on the landing pages of the top-100 hottest websites and two big.LITTLE heterogeneous mobile platforms. Our extensive experiments show that the proposed approach reduces system-wide energy consumption by over 36\\% on average and up to 70\\%. This translates to an over 10\\% improvement on energy-efficiency over a state-of-the-art event-based web browser scheduler, but with significantly fewer violations on the quality of service.", "venue": "ArXiv", "authors": ["Lu  Yuan", "Jie  Ren", "Ling  Gao", "Zhanyong  Tang", "Zheng  Wang"], "year": 2019, "n_citations": 1}
{"id": 3196185, "s2_id": "c5d73d3a257b0037d0fa1857c16b1c1f3c960aed", "title": "A hardware time manager implementation for the Xenomai real-time kernel of embedded Linux", "abstract": "Nowadays, the use of embedded operating systems in different embedded projects is subject to a tremendous growth. Embedded Linux is becoming one of those most popular EOSs due to its modularity, efficiency, reliability, and cost. One way to make it hard real-time is to include a real-time kernel like Xenomai. One of the key characteristics of a Real-Time Operating System (RTOS) is its ability to meet execution time deadlines deterministically. So, the more precise and flexible the time management can be, the better it can handle efficiently the determinism for different embedded applications. RTOS time precision is characterized by a specific periodic interrupt service controlled by a software time manager. The smaller the period of the interrupt, the better the precision of the RTOS, the more it overloads the CPU, and though reduces the overall efficiency of the RTOS. In this paper, we propose to drastically reduce these overheads by migrating the time management service of Xenomai into a configurable hardware component to relieve the CPU. The hardware component is implemented in a Field Programmable Gate Array coupled to the CPU. This work was achieved in a Master degree project where students could apprehend many fields of embedded systems: RTOS programming, hardware design, performance evaluation, etc.", "venue": "SIGBED", "authors": ["Pierre  Olivier", "Jalil  Boukhobza"], "year": 2012, "n_citations": 5}
{"id": 3200881, "s2_id": "d24408cbaab0254f0c89055a4433c99c5faaefc9", "title": "Staffing for many-server systems facing non-standard arrival processes", "abstract": "Arrival processes to service systems often display (i) larger than anticipated fluctuations, (ii) a time-varying rate, and (iii) temporal correlation. Motivated by this, we introduce a specific non-homogeneous Poisson process that incorporates these three features. The resulting arrival process is fed into an infinite-server system, which is then used as a proxy for its many-server counterpart. This leads to a staffing rule based on the square-root staffing principle that acknowledges the three features. After a slight rearrangement of servers over the time slots, we succeed to stabilize system performance even under highly varying and strongly correlated conditions. We fit the arrival stream model to real data from an emergency department and demonstrate (by simulation) the performance of the novel staffing rule.", "venue": "Eur. J. Oper. Res.", "authors": ["M.  Heemskerk", "M.  Mandjes", "B.  Mathijsen"], "year": 2022, "n_citations": 0}
{"id": 3201540, "s2_id": "97d7c323228e01e39f0e6add5dd494893fc2adf0", "title": "Using constraint programming to resolve the multi-source/multi-site data movement paradigm on the Grid", "abstract": "In order to achieve both fast and coordinated data transfer t o collaborative sites as well as to create a distribution of data over multiple sites, efficient data mo vement is one of the most essential aspects in distributed environment. With such capabilities a t hand, truly distributed task scheduling with minimal latencies would be reachable by internationally distributed collaborations (such as ones in HENP) seeking for scavenging or maximizing on geographically spread computational resources. But it is often not all clear (a) how to move data when available from multiple sources or (b) how to move data to multiple compute resources to achieve an optimal usage of available resources. Constraint programming (CP) is a technique from artificial intelligence and operations research allowing to find solutions in a multi-dimensi onal space of variables. We present a method of creating a CP model consisting of sites, links and their attributes such as bandwidth for grid network data transfer also considering user tasks a s part of the objective function for an optimal solution. We will explore and explain trade-off between schedule generation time and divergence from the optimal solution and show how to improve and render viable the solution\u2019s finding time by using search tree time limit, approximations , restrictions such as symmetry breaking or grouping similar tasks together, or generating sequence of optimal schedules by splitting the input problem. Results of data transfer simulation for e ach case will also include a well known Peer-2-Peer model, and time taken to generate a schedule as well as time needed for a schedule execution will be compared to a CP optimal solution. We will additionally present a possible implementation aimed to bring a distributed datasets (multiple sources) to a given site in a minimal time.", "venue": "ArXiv", "authors": ["Michal  Zerola", "J\u00e9r\u00f4me  Lauret", "Roman  Bart\u00e1k", "Michal  Sumbera"], "year": 2009, "n_citations": 4}
{"id": 3203944, "s2_id": "92fcf4d4236861c8f81aadc26ef76ea57137849e", "title": "Dwarfs on Accelerators: Enhancing OpenCL Benchmarking for Heterogeneous Computing Architectures", "abstract": "For reasons of both performance and energy efficiency, high performance computing (HPC) hardware is becoming increasingly heterogeneous. The OpenCL framework supports portable programming across a wide range of computing devices and is gaining influence in programming next-generation accelerators. To characterize the performance of these devices across a range of applications requires a diverse, portable and configurable benchmark suite, and OpenCL is an attractive programming model for this purpose. We present an extended and enhanced version of the OpenDwarfs OpenCL benchmark suite, with a strong focus placed on the robustness of applications, curation of additional benchmarks with an increased emphasis on correctness of results and choice of problem size. Preliminary results and analysis are reported for eight benchmark codes on a diverse set of architectures -- three Intel CPUs, five Nvidia GPUs, six AMD GPUs and a Xeon Phi.", "venue": "ICPP Workshops", "authors": ["Beau  Johnston", "Josh  Milthorpe"], "year": 2018, "n_citations": 8}
{"id": 3207666, "s2_id": "46838e5d26c4c60f041efcf52372b6a85ef2835e", "title": "ProCal: A Low-Cost and Programmable Calibration Tool for IoT Devices", "abstract": "Calibration is an important step towards building reliable IoT systems. For example, accurate sensor reading requires ADC calibration, and power monitoring chips must be calibrated before being used for measuring the energy consumption of IoT devices. In this paper, we present ProCal, a low-cost, accurate, and scalable power calibration tool. ProCal is a programmable platform which provides dynamic voltage and current output for calibration. The basic idea is to use a digital potentiometer connected to a parallel resistor network controlled through digital switches. The resistance and output frequency of ProCal is controlled by a software communicating with the board through the SPI interface. Our design provides a simple synchronization mechanism which prevents the need for accurate time synchronization. We present mathematical modeling and validation of the tool by incorporating the concept of Fibonacci sequence. Our extensive experimental studies show that this tool can significantly improve measurement accuracy. For example, for ATMega2560, the ADC error reduces from 0.2% to 0.01%. ProCal not only costs less than 2\\% of the current commercial solutions, it is also highly accurate by being able to provide extensive range of current and voltage values.", "venue": "ICIOT", "authors": ["Chia-Chi  Li", "Behnam  Dezfouli"], "year": 2018, "n_citations": 2}
{"id": 3207786, "s2_id": "9bfa73a3e2e14be4fe17925e46d266e604524e7e", "title": "Distributed link scheduling with constant overhead", "abstract": "This paper proposes a new class of simple, distributed algorithms for scheduling in multihop wireless networks under the primary interference model. The class is parameterized by integers k \u2265 1. We show that algorithm kof our class achieves k/(k + 2) of the capacity region, for every k \u2265 1. The algorithms have small and constant worst-case overheads. In particular, algorithm k generates a new schedule using a) time less than 4k + 2 round-trip times between neighboring nodes in the network and b) at most three control transmissions by any given node for any k. The control signals are explicitly specified and face the same interference effects as normal data transmissions. Our class of distributed wireless scheduling algorithms are the first ones guaranteed to achieve any fixed fraction of the capacity region while using small and constant overheads that do not scale with network size. The parameter k explicitly captures the tradeoff between control overhead and throughput performance and provides a tuning-knob protocol designers can use to harness this tradeoff in practice.", "venue": "TNET", "authors": ["Loc X. Bui", "Sujay  Sanghavi", "R.  Srikant"], "year": 2009, "n_citations": 15}
{"id": 3209151, "s2_id": "f113cbe0df270fef2b69e93f10889b82f9d0c9eb", "title": "Hardware Versus Software Fault Injection of Modern Undervolted SRAMs", "abstract": "To improve power efficiency, researchers are experimenting with dynamically adjusting the supply voltage of systems below the nominal operating points. However, production systems are typically not allowed to function on voltage settings that is below the reliable limit. Consequently, existing software fault tolerance studies are based on fault models, which inject faults on random fault locations using fault injection techniques. In this work we study whether random fault injection is accurate to simulate the behavior of undervolted SRAMs. \nOur study extends the Gem5 simulator to support fault injection on the caches of the simulated system. The fault injection framework uses fault maps, which describe the faulty bits of SRAMs, as inputs. To compare random fault injection and hardware guided fault injection, we use two types of fault maps. The first type of maps are created through undervolting real SRAMs and observing the location of the erroneous bits, whereas the second type of maps are created by corrupting random bits of the SRAMs. During our study we corrupt the L1-Dcache of the simulated system and we monitor the behavior of the two types of fault maps on the resiliency of six benchmarks. The difference among the resiliency of a benchmark when tested with the different fault maps can be up to 24%.", "venue": "ArXiv", "authors": ["Muhammet Abdullah Soyturk", "Konstantinos  Parasyris", "Behzad  Salami", "Osman S. Unsal", "Gulay  Yalcin", "Leonardo  Bautista-Gomez"], "year": 2019, "n_citations": 0}
{"id": 3210318, "s2_id": "e87937e88a78c00ef64d0eece5ba6cadf116af09", "title": "Space-efficient scheduling of stochastically generated tasks", "abstract": "We study the problem of scheduling tasks for execution by a processor when the tasks can stochastically generate new tasks. Tasks can be of different types, and each type has a fixed, known probability of generating other tasks. We present results on the random variable S^@s modeling the maximal space needed by the processor to store the currently active tasks when acting under the scheduler @s. We obtain tail bounds for the distribution of S^@s for both offline and online schedulers, and investigate the expected value E[S^@s].", "venue": "Inf. Comput.", "authors": ["Tom\u00e1s  Br\u00e1zdil", "Javier  Esparza", "Stefan  Kiefer", "Michael  Luttenberger"], "year": 2012, "n_citations": 4}
{"id": 3211336, "s2_id": "c683c532ea27c3b9866517da6adc554511241251", "title": "Importance of explicit vectorization for CPU and GPU software performance", "abstract": "Much of the current focus in high-performance computing is on multi-threading, multi-computing, and graphics processing unit (GPU) computing. However, vectorization and non-parallel optimization techniques, which can often be employed additionally, are less frequently discussed. In this paper, we present an analysis of several optimizations done on both central processing unit (CPU) and GPU implementations of a particular computationally intensive Metropolis Monte Carlo algorithm. Explicit vectorization on the CPU and the equivalent, explicit memory coalescing, on the GPU are found to be critical to achieving good performance of this algorithm in both environments. The fully-optimized CPU version achieves a 9x to 12x speedup over the original CPU version, in addition to speedup from multi-threading. This is 2x faster than the fully-optimized GPU version, indicating the importance of optimizing CPU implementations.", "venue": "J. Comput. Phys.", "authors": ["Neil G. Dickson", "Kamran  Karimi", "Firas  Hamze"], "year": 2011, "n_citations": 27}
{"id": 3213905, "s2_id": "5502e9bc4aa62a7e33d9cecc1773bc764aa67e07", "title": "Compiler Phase Ordering as an Orthogonal Approach for Reducing Energy Consumption", "abstract": "Compiler writers typically focus primarily on the performance of the generated program binaries when selecting the passes and the order in which they are applied in the standard optimization levels, such as GCC -O3. In some domains, such as embedded systems and High-Performance Computing (HPC), it might be sometimes acceptable to slowdown computations if the energy consumed can be significantly decreased. Embedded systems often rely on a battery and besides energy also have power dissipation limitations, while HPC centers have a growing concern with electricity and cooling costs. Relying on power policies to apply frequency/voltage scaling and/or change the CPU to idle states (e.g., alternate between power levels in bursts) as the main method to reduce energy leaves potential for improvement using other orthogonal approaches. In this work we evaluate the impact of compiler pass sequences specialization (also known as compiler phase ordering) as a means to reduce the energy consumed by a set of programs/functions when comparing with the use of the standard compiler phase orders provided by, e.g., -OX flags. We use our phase selection and ordering framework to explore the design space in the context of a Clang+LLVM compiler targeting a multicore ARM processor in an ODROID board and a dual x86 desktop representative of a node in a Supercomputing center. Our experiments with a set of representative kernels show that there we can reduce energy consumption by up to 24% and that some of these improvements can only be partially explained by improvements to execution time. The experiments show cases where applications that run faster consume more energy. Additionally, we make an effort to characterize the compiler sequence exploration space in terms of their impact on performance and energy.", "venue": "ArXiv", "authors": ["Ricardo  Nobre", "Lu\u00eds  Reis", "Jo\u00e3o M. P. Cardoso"], "year": 2018, "n_citations": 15}
{"id": 3216484, "s2_id": "169c5b4c26e0a44c4752fc2828ca2bb020420fe2", "title": "A Model Predictive Control Approach for Low-Complexity Electric Vehicle Charging Scheduling: Optimality and Scalability", "abstract": "With the increasing adoption of plug-in electric vehicles (PEVs), it is critical to develop efficient charging coordination mechanisms that minimize the cost and impact of PEV integration to the power grid. In this paper, we consider the optimal PEV charging scheduling, where the noncausal information about future PEV arrivals is not known in advance, but its statistical information can be estimated. This leads to an \u201conline\u201d charging scheduling problem that is naturally formulated as a finite-horizon dynamic programming with continuous state space and action space. To avoid the prohibitively high complexity of solving such a dynamic programming problem, we provide a model predictive control (MPC)-based algorithm with computational complexity <inline-formula> <tex-math notation=\"LaTeX\">$O(T^3)$</tex-math></inline-formula>, where <inline-formula><tex-math notation=\"LaTeX\">$T$ </tex-math></inline-formula> is the total number of time stages. We rigorously analyze the performance gap between the near-optimal solution of the MPC-based approach and the optimal solution for any distributions of exogenous random variables. Furthermore, our rigorous analysis shows that when the random process describing the arrival of charging demands is first-order periodic, the complexity of the proposed algorithm can be reduced to <inline-formula> <tex-math notation=\"LaTeX\">$O(1)$</tex-math></inline-formula>, which is independent of <inline-formula> <tex-math notation=\"LaTeX\">$T$</tex-math></inline-formula>. Extensive simulations show that the proposed online algorithm performs very closely to the optimal online algorithm. The performance gap is smaller than <inline-formula> <tex-math notation=\"LaTeX\">$0.4\\%$</tex-math></inline-formula> in most cases.", "venue": "IEEE Transactions on Power Systems", "authors": ["Wanrong  Tang", "Ying Jun (Angela) Zhang"], "year": 2017, "n_citations": 88}
{"id": 3216607, "s2_id": "266e9599df642791b5188cc9c3561e59cdec1831", "title": "An Improved Accurate Solver for the Time-Dependent RTE in Underwater Optical Wireless Communications", "abstract": "Underwater optical wireless communication (UOWC) has been widely advocated as a viable way to satisfy these high-speed links constraints in the marine medium through the use of the visible spectrum. Nevertheless, UOWC faces several limitations, such as the path-loss due to the absorption and scattering phenomena, caused by underwater particles. Thus, quantifying this path-loss is of paramount importance in the design of futuristic UOWC systems. To this end, several approaches have been used in this regard, namely the Beer\u2013Lambert\u2019s law, Monte Carlo simulation, as well as radiative transfer equation (RTE). This last mentioned evaluates the optical path-loss of the light wave in an underwater channel in terms of the absorption and scattering coefficients as well as the scattering phase function (SPF). In this paper, an improved numerical solver to evaluate the time-dependent RTE for UOWC is proposed. The proposed numerical algorithm was improved based on the previously proposed ones, by making use of an improved finite difference scheme, a modified scattering angular discretization, as well as an enhancement of the quadrature method by involving a more accurate seven-point quadrature scheme in order to calculate the weight coefficients corresponding to the RTE integral term. Importantly, we applied the RTE solver to three different volume scattering functions, namely the single-term Henyey\u2013Greenstein (HG) phase function, the two-term HG phase function, and the Fournier\u2013Forand phase function, over both Harbor-I and Harbor-II water types. Based on the normalized received power evaluated through the proposed algorithm, the bit error rate performance of the UOWC system is investigated in terms of system and channel parameters. The enhanced algorithm gives a tightly close performance to its Monte Carlo counterpart by adjusting the numerical cumulative distribution function computation method as well as optimizing the number of scattering angles.", "venue": "IEEE Access", "authors": ["Elmehdi  Illi", "Faissal El Bouanani", "Ki-Hong  Park", "Fouad  Ayoub", "Mohamed-Slim  Alouini"], "year": 2019, "n_citations": 6}
{"id": 3217518, "s2_id": "3b773a1d6d9451abaffa5818df1f012a213d9c92", "title": "An analytic performance model for overlapping execution of memory-bound loop kernels on multicore CPUs", "abstract": "Complex applications running on multicore processors show a rich performance phenomenology. The growing number of cores per ccNUMA domain complicates performance analysis of memory-bound code since system noise, load imbalance, or task-based programming models can lead to thread desynchronization. Hence, the simplifying assumption that all cores execute the same loop can not be upheld. Motivated by observations on plain and modified versions of the HPCG benchmark, we construct a performance model of execution of memory-bound loop kernels. It can predict the memory bandwidth share per kernel on a memory contention domain depending on the number of active cores and which other workload the kernel is paired with. The only code features required are the single-thread cache line access frequency per kernel, which is directly related to the single-thread memory bandwidth, and its saturated bandwidth. It can either be measured directly or predicted using the Execution-Cache-Memory (ECM) performance model. The computational intensity of the kernels and the detailed structure of the code is of no significance. We validate our model on Intel Broadwell, Intel Cascade Lake, and AMD Rome processors pairing various streaming and stencil kernels. The error in predicting the bandwidth share per kernel is less than 8%.", "venue": "ArXiv", "authors": ["Ayesha  Afzal", "Georg  Hager", "Gerhard  Wellein"], "year": 2020, "n_citations": 1}
{"id": 3223230, "s2_id": "ad23a89930428525e751806308e33d148167f824", "title": "Store-forward and its implications for proportional scheduling", "abstract": "The Proportional Scheduler was recently proposed as a scheduling algorithm for multi-hop switch networks. For these networks, the BackPressure scheduler is the classical benchmark. For networks with fixed routing, the Proportional Scheduler is maximum stable, myopic and, furthermore, will alleviate certain scaling issues found in BackPressure for large networks. Nonetheless, the equilibrium and delay properties of the Proportional Scheduler have not been fully characterized. In this article, we postulate on the equilibrium behaviour of the Proportional Scheduler through the analysis of an analogous rule called the Store-Forward allocation. It has been shown that Store-Forward allocates asymptotically according to the Proportional Scheduler. Further, for Store-Forward networks, numerous equilibrium quantities are explicitly calculable. For FIFO networks under Store-Forward, we calculate the policies stationary distribution and end-to-end route delay. We discuss network topologies where the stationary distribution is product-form, a phenomenon which we call product form resource pooling. We extend this product form notion to independent set scheduling on perfect graphs, where we show that non-neighbouring queues are statistically independent. Finally, we analyse the large deviations behaviour of the equilibrium distribution of Store-Forward networks in order to construct Lyapunov functions for FIFO switch networks.", "venue": "2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "authors": ["Neil S. Walton"], "year": 2014, "n_citations": 3}
{"id": 3225733, "s2_id": "7317dcd52ea4dee743ca377dc0497397b0df2d15", "title": "SleepScale: Runtime joint speed scaling and sleep states management for power efficient data centers", "abstract": "Power consumption in data centers has been growing significantly in recent years. To reduce power, servers are being equipped with increasingly sophisticated power management mechanisms. Different mechanisms offer dramatically different trade-offs between power savings and performance penalties. Considering the complexity, variety, and temporally-varying nature of the applications hosted in a typical data center, intelligently determining which power management policy to use and when is a complicated task. In this paper we analyze a system model featuring both performance scaling and low-power states. We reveal the interplay between performance scaling and low-power states via intensive simulation and analytic verification. Based on the observations, we present SleepScale, a runtime power management tool designed to efficiently exploit existing power control mechanisms. At run time, SleepScale characterizes power consumption and quality-of-service (QoS) for each low-power state and frequency setting, and selects the best policy for a given QoS constraint. We evaluate SleepScale using workload traces from data centers and achieve significant power savings relative to conventional power management strategies.", "venue": "2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)", "authors": ["Yanpei  Liu", "Stark C. Draper", "Nam Sung Kim"], "year": 2014, "n_citations": 53}
{"id": 3232431, "s2_id": "594ffb43f4724fd18b9d95d0edcf3c7df5a2c957", "title": "Proceedings of the 1st OMNeT++ Community Summit, Hamburg, Germany, September 2, 2014", "abstract": "This is the Proceedings of the 1st OMNeT++ Community Summit, which was held in Hamburg, Germany, September 2, 2014.", "venue": "ArXiv", "authors": ["Anna  F\u00f6rster", "Christoph  Sommer", "Till  Steinbach", "Matthias  W\u00e4hlisch"], "year": 2014, "n_citations": 1}
{"id": 3232827, "s2_id": "d925c09c6f80f9a55ac2876bbc33b5e0f71538e6", "title": "Comparing Broadband ISP Performance using Big Data from M-Lab", "abstract": "Comparing ISPs on broadband speed is challenging, since measurements can vary due to subscriber attributes such as operation system and test conditions such as access capacity, server distance, TCP window size, time-of-day, and network segment size. In this paper, we draw inspiration from observational studies in medicine, which face a similar challenge in comparing the effect of treatments on patients with diverse characteristics, and have successfully tackled this using \u201ccausal inference\u201d techniques for post facto analysis of medical records. Our first contribution is to develop a tool to pre-process and visualize the millions of data points in M-Lab at various timeand space-granularities to get preliminary insights on factors affecting broadband performance. Next, we analyze 24 months of data pertaining to twelve ISPs across three countries, and demonstrate that there is observational bias in the data due to disparities amongst ISPs in their attribute distributions. For our third contribution, we apply a multi-variate matching method to identify suitable cohorts that can be compared without bias, which reveals that ISPs are closer in performance than thought before. Our final contribution is to refine our model by developing a method for estimating speed-tier and re-apply matching for comparison of ISP performance. Our results challenge conventional rankings of ISPs, and pave the way towards data-driven approaches for unbiased comparisons of ISPs world-wide.", "venue": "ArXiv", "authors": ["Xiaohong  Deng", "Yun  Feng", "Thanchanok  Sutjarittham", "Hassan Habibi Gharakheili", "Blanca  Gallego", "Vijay  Sivaraman"], "year": 2021, "n_citations": 0}
{"id": 3241009, "s2_id": "25c0cce689a043aad88dfd77e21893a6e69d4125", "title": "Sensitivity of mean-field fluctuations in Erlang loss models with randomized routing", "abstract": "Abstract In this paper we study a large system of N servers, each with capacity to process at most C simultaneous jobs; an incoming job is routed to a server if it has the lowest occupancy amongst d (out of N) randomly selected servers. A job that is routed to a server with no vacancy is assumed to be blocked and lost. Such randomized policies are referred to JSQ(d) (Join the Shortest Queue out of d) policies. Under the assumption that jobs arrive according to a Poisson process with rate \n$N\\lambda^{(N)}$\n where \n$\\lambda^{(N)}=\\sigma-\\frac{\\beta}{\\sqrt{N}\\,}$\n , \n$\\sigma\\in\\mathbb{R}_+$\n and \n$\\beta\\in\\mathbb{R}$\n , we establish functional central limit theorems for the fluctuation process in both the transient and stationary regimes when service time distributions are exponential. In particular, we show that the limit is an Ornstein\u2013Uhlenbeck process whose mean and variance depend on the mean field of the considered model. Using this, we obtain approximations to the blocking probabilities for large N, where we can precisely estimate the accuracy of first-order approximations.", "venue": "Journal of Applied Probability", "authors": ["Thirupathaiah  Vasantam", "Ravi R. Mazumdar"], "year": 2021, "n_citations": 0}
{"id": 3242803, "s2_id": "da808878c42ad7e3170e78288d585b80282c8996", "title": "Integrating State of the Art Compute, Communication, and Autotuning Strategies to Multiply the Performance of the Application Programm CPMD for Ab Initio Molecular Dynamics Simulations", "abstract": "We present our recent code modernizations of the of the ab initio molecular dynamics program CPMD (this http URL) with a special focus on the ultra-soft pseudopotential (USPP) code path. Following the internal instrumentation of CPMD, all time critical routines have been revised to maximize the computational throughput and to minimize the communication overhead for optimal performance. Throughout the program missing hybrid MPI+OpenMP parallelization has been added to optimize scaling. For communication intensive routines, as the multiple distributed 3d FFTs of the electronic states and distributed matrix-matrix multiplications related to the $\\beta$-projectors of the pseudopotentials, this MPI+OpenMP parallelization now overlaps computation and communication. The necessary partitioning of the workload is optimized by an auto-tuning algorithm. In addition, the largest global MPI_Allreduce operation has been replaced by highly tuned node-local parallelized operations using MPI shared-memory windows to avoid inter-node communication. A batched algorithm for the multiple 3d FFTs improves the throughput of the MPI_Alltoall communication and, thus, the scalability of the implementation, both for USPP and for the frequently used norm-conserving pseudopotential code path. The enhanced performance and scalability is demonstrated on a mid-sized benchmark system of 256 water molecules and further water systems of from 32 up to 2048 molecules.", "venue": "ArXiv", "authors": ["Tobias  Kl\u00f6ffel", "Gerald  Mathias", "Bernd  Meyer"], "year": 2020, "n_citations": 1}
{"id": 3243833, "s2_id": "9616f2d895183b9adcbd47100cde08edf021c484", "title": "Measuring the Optimality of Hadoop Optimization", "abstract": "In recent years, much research has focused on how to optimize Hadoop jobs. Their approaches are diverse, ranging from improving HDFS and Hadoop job scheduler to optimizing parameters in Hadoop configurations. Despite their success in improving the performance of Hadoop jobs, however, very little is known about the limit of their optimization performance. That is, how optimal is a given Hadoop optimization? When a Hadoop optimization method X improves the performance of a job by Y %, how do we know if this improvement is as good as it can be? To answer this question, in this paper, we first examine the ideal best case, the lower bound, of running time for Hadoop jobs and develop a measure to accurately estimate how optimal a given Hadoop optimization is with respect to the lower bound. Then, we demonstrate how one may exploit the proposed measure to improve the optimization of Hadoop jobs.", "venue": "ArXiv", "authors": ["Woo-Cheol  Kim", "Changryong  Baek", "Dongwon  Lee"], "year": 2013, "n_citations": 3}
{"id": 3245526, "s2_id": "1fe88c30a359ed4173524871cfa30bb106d86bc9", "title": "Performance Evaluation of Secure Multi-party Computation on Heterogeneous Nodes", "abstract": "Secure multi-party computation (MPC) is a broad cryptographic concept that can be adopted for privacy-preserving computation. With MPC, a number of parties can collaboratively compute a function, without revealing the actual input or output of the plaintext to others. The applications of MPC range from privacy-preserving voting, arithmetic calculation, and large-scale data analysis. From the system perspective, each party in MPC can run on one compute node. The compute nodes of multiple parties could be either homogeneous or heterogeneous; however, the distributed workloads from the MPC protocols tend to be always homogeneous (symmetric). In this work, we study a representative MPC framework and a set of MPC applications from the system performance perspective. We show the detailed online computation workflow of a state-of-the-art MPC protocol and analyze the root cause of its stall time and performance bottleneck on homogeneous and heterogeneous compute nodes.", "venue": "ArXiv", "authors": ["Zhou  Ni", "Rujia  Wang"], "year": 2020, "n_citations": 0}
{"id": 3248092, "s2_id": "79553b184b733403dadb1b61cf168ca09913c395", "title": "Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for Emerging Storage Models", "abstract": "With data durability, high access speed, low power efficiency and byte addressability, NVMe and SSD, which are acknowledged representatives of emerging storage technologies, have been applied broadly in many areas. However, one key issue with high-performance adoption of these technologies is how to properly define intelligent cache layers such that the performance gap between emerging technologies and main memory can be well bridged. To this end, we propose Phoebe, a reuse-aware reinforcement learning framework for the optimal online caching that is applicable for a wide range of emerging storage models. By continuous interacting with the cache environment and the data stream, Phoebe is capable to extract critical temporal data dependency and relative positional information from a single trace, becoming ever smarter over time. To reduce training overhead during online learning, we utilize periodical training to amortize costs. Phoebe is evaluated on a set of Microsoft cloud storage workloads. Experiment results show that Phoebe is able to close the gap of cache miss rate from LRU and a state-of-the-art online learning based cache policy to the Belady's optimal policy by 70.3% and 52.6%, respectively.", "venue": "ArXiv", "authors": ["Nan  Wu", "Pengcheng  Li"], "year": 2020, "n_citations": 2}
{"id": 3250564, "s2_id": "8bdc6eb995d1cfe33c21d2a3d4576d2ed00839aa", "title": "SLA-Driven Load Scheduling in Multi-Tier Cloud Computing: Financial Impact Considerations", "abstract": "A cloud service provider strives to effectively provide a high Quality of Service (QoS) to client jobs. Such jobs vary in computational and Service-Level-Agreement (SLA) obligations, as well as differ with respect to tolerating delays and SLA violations. The job scheduling plays a critical role in servicing cloud demands by allocating appropriate resources to execute client jobs. The response to such jobs is optimized by the cloud service provider on a multi-tier cloud computing environment. Typically, the complex and dynamic nature of multi-tier environments incurs difficulties in meeting such demands, because tiers are dependent on each others which in turn makes bottlenecks of a tier shift to escalate in subsequent tiers. However, the optimization process of existing approaches produces single-tier-driven schedules that do not employ the differential impact of SLA violations in executing client jobs. Furthermore, the impact of schedules optimized at the tier level on the performance of schedules formulated in subsequent tiers tends to be ignored, resulting in a less than optimal performance when measured at the multi-tier level. Thus, failing in committing job obligations incurs SLA penalties that often take the form of either financial compensations, or losing future interests and motivations of unsatisfied clients in the service provided. Therefore, tolerating the risk of such delays on the operational performance of a cloud service provider is vital to meet SLA expectations and mitigate their associated commercial penalties. Such situations demand the cloud service provider to employ scalable service mechanisms that efficiently manage the execution of resource loads in accordance to their financial influence on the system performance, so as to ensure system reliability and cost reduction. In this paper, a scheduling and allocation approach is proposed to formulate schedules that account for differential impacts of SLA violation penalties and, thus, produce schedules that are optimal in financial performance. A queue virtualization scheme is designed to facilitate the formulation of optimal schedules at the tier and multi-tier levels of the cloud environment. Because the scheduling problem is NPhard, a biologically inspired approach is proposed to mitigate the complexity of finding optimal schedules. The reported results in this paper demonstrate the efficacy of the proposed approach in formulating costoptimal schedules that reduce SLA penalties of jobs at various architectural granularities of the multi-tier cloud environment.", "venue": "International Journal on Cloud Computing: Services and Architecture", "authors": ["Husam  Suleiman", "Otman A. Basir"], "year": 2020, "n_citations": 1}
{"id": 3251508, "s2_id": "15f5b6e00e596525fcc507ef7739f7f8e83e2042", "title": "Performance analysis and optimization of the JOREK code for many-core CPUs", "abstract": "This report investigates the performance of the JOREK code on the Intel Knights Landing and Skylake processor architectures. The OpenMP scaling of the matrix construction part of the code was analyzed and improved synchronization methods were implemented. A new switch was implemented to control the number of threads used for the linear equation solver independently from other parts of the code. The matrix construction subroutine was vectorized, and the data locality was also improved. These steps led to a factor of two speedup for the matrix construction.", "venue": "ArXiv", "authors": ["Tamas B. Feh\u00e9r", "Matthias  Hoelzl", "Guillaume  Latu", "G. T. A. Huijsmans"], "year": 2018, "n_citations": 2}
{"id": 3253075, "s2_id": "a72866ed068cf83e39d9b6360f709b8dfd378757", "title": "Performance Analysis for Bandwidth Allocation in IEEE 802.16 Broadband Wireless Networks using BMAP Queueing", "abstract": "This paper presents a performance analysis for the bandwidth allocation in IEEE 802.16 broadband wireless access (BWA) networks considering the packet-level quality-of-service (QoS) constraints. Adaptive Modulation and Coding (AMC) rate based on IEEE 802.16 standard is used to adjust the transmission rate adaptively in each frame time according to channel quality in order to obtain multiuser diversity gain. To model the arrival process and the traffic source we use the Batch Markov Arrival Process (BMAP), which enables more realistic and more accurate traffic modelling. We determine analytically different performance parameters, such as average queue length, packet dropping probability, queue throughput and average packet delay. Finally, the analytical results are validated numerically.", "venue": "ArXiv", "authors": ["Said El Kafhali", "Abdelali El Bouchti", "Mohamed  Hanini", "Abdelkrim  Haqiq"], "year": 2012, "n_citations": 11}
{"id": 3258612, "s2_id": "240d907731c9c7aaf98b9fce5ad74cad9bb1607b", "title": "The Cost of OSCORE and EDHOC for Constrained Devices", "abstract": "Many modern IoT applications rely on the Constrained Application Protocol (CoAP). Recently, the Internet Engineering Task Force (IETF) proposed two novel protocols for securing it. These are: 1) Object Security for Constrained RESTful Environments (OSCORE) providing authenticated encryption for the CoAP's payload data and 2) Ephemeral Diffie-Hellman Over COSE (EDHOC) providing the symmetric session keys required for OSCORE. In this paper, we present the design of four firmware libraries for these protocols which are especially targeted for constrained microcontrollers and their detailed evaluation. More precisely, we present the design of uOSCORE and \u03bcEDHOC libraries for regular microcontrollers and \u03bcOSCORE-TEE and \u03bcEDHOC-TEE libraries for microcontrollers with a Trusted Execution Environment (TEE), such as microcontrollers featuring ARM TrustZone-M. Our firmware design for the latter class of devices concerns the fact that attackers may exploit common software vulnerabilities, e.g., buffer overflows in the protocol logic, OS or application to compromise the protocol security. We present an evaluation of our implementations in terms of RAM/FLASH requirements and execution speed on a broad range of microcontrollers. Our implementations are available as open-source software.", "venue": "CODASPY", "authors": ["Stefan  Hristozov", "Manuel  Huber", "Lei  Xu", "Jaro  Fietz", "Marco  Liess", "Georg  Sigl"], "year": 2021, "n_citations": 2}
{"id": 3261100, "s2_id": "4a93dc34dc4a5b1c03ae0a5b23a88bece568c28a", "title": "Optimal DNN primitive selection with partitioned boolean quadratic programming", "abstract": "Deep Neural Networks (DNNs) require very large amounts of computation, and many different algorithms have been proposed to implement their most expensive layers, each of which has a large number of variants with different trade-offs of parallelism, locality, memory footprint, and execution time. In addition, specific algorithms operate much more efficiently on specialized data layouts. We state the problem of optimal primitive selection in the presence of data layout transformations, and show that it is NP-hard by demonstrating an embedding in the Partitioned Boolean Quadratic Assignment problem (PBQP). We propose an analytic solution via a PBQP solver, and evaluate our approach experimentally by optimizing several popular DNNs using a library of more than 70 DNN primitives, on an embedded platform and a general purpose platform. We show experimentally that significant gains are possible versus the state of the art vendor libraries by using a principled analytic solution to the problem of primitive selection in the presence of data layout transformations.", "venue": "CGO", "authors": ["Andrew  Anderson", "David  Gregg"], "year": 2018, "n_citations": 22}
{"id": 3262758, "s2_id": "bcb5619768cf8e0132eccd8698e365453eb39cbf", "title": "Multi-objective Evolutionary Approach for Efficient Kernel Size and Shape for CNN", "abstract": "While state-of-the-art development in Convolutional Neural Networks (CNNs) topology, such as VGGNet and ResNet, have become increasingly accurate, these networks are computationally expensive involving billions of arithmetic operations and parameters. In order to improve the classification accuracy, state-of-the-art CNNs usually involve large and complex convolutional layers. However, for certain applications, e.g. Internet of Things (IoT), where such CNNs are to be implemented on resource-constrained platforms, the CNN architectures have to be small and efficient. To deal with this problem, reducing the resource consumption in convolutional layers has become one of the most significant solutions. In this work, a multi-objective optimisation approach is proposed to trade-off between the amount of computation and network accuracy by using Multi-Objective Evolutionary Algorithms (MOEAs). The number of convolution kernels and the size of these kernels are proportional to computational resource consumption of CNNs. Therefore, this paper considers optimising the computational resource consumption by reducing the size and number of kernels in convolutional layers. Additionally, the use of unconventional kernel shapes has been investigated and results show these clearly outperform the commonly used square convolution kernels. The main contributions of this paper are therefore a methodology to significantly reduce computational cost of CNNs, based on unconventional kernel shapes, and provide different trade-offs for specific use cases. The experimental results further demonstrate that the proposed method achieves large improvements in resource consumption with no significant reduction in network performance. Compared with the benchmark CNN, the best trade-off architecture shows a reduction in multiplications of up to 6X and with slight increase in classification accuracy on CIFAR-10 dataset.", "venue": "ArXiv", "authors": ["Ziwei  Wang", "Martin A. Trefzer", "Simon J. Bale", "Andy M. Tyrrell"], "year": 2021, "n_citations": 0}
{"id": 3265582, "s2_id": "21553fe69bb631b9acd23b1cee113a58da974c6d", "title": "A Methodology for Optimizing Multithreaded System Scalability on Multi-cores", "abstract": "We show how to quantify scalability with the Universal Scalability Law (USL) by applying it to performance measurements of memcached, J2EE, and Weblogic on multi-core platforms. Since commercial multicores are essentially black-boxes, the accessible performance gains are primarily available at the application level. We also demonstrate how our methodology can identify the most significant performance tuning opportunities to optimize application scalability, as well as providing an easy means for exploring other aspects of the multi-core system design space.", "venue": "ArXiv", "authors": ["Neil J. Gunther", "Shanti  Subramanyam", "Stefan  Parvu"], "year": 2011, "n_citations": 12}
{"id": 3267785, "s2_id": "c13d92685fdd2f217ff8808599a69b72f446308f", "title": "Towards a workload for evolutionary analytics", "abstract": "Emerging data analysis involves the ingestion and exploration of new data sets, application of complex functions, and frequent query revisions based on observing prior query answers. We call this new type of analysis evolutionary analytics and identify its properties. This type of analysis is not well represented by current benchmark workloads. In this paper, we present a workload and identify several metrics to test system support for evolutionary analytics. Along with our metrics, we present methodologies for running the workload that capture this analytical scenario.", "venue": "DanaC '13", "authors": ["Jeff  LeFevre", "Jagan  Sankaranarayanan", "Hakan  Hacig\u00fcm\u00fcs", "Jun'ichi  Tatemura", "Neoklis  Polyzotis"], "year": 2013, "n_citations": 9}
{"id": 3267838, "s2_id": "00cfff020b5674f709ce4efdea93e85ffafd8605", "title": "Computational Petri Nets: Adjunctions Considered Harmful", "abstract": "We review some of the endeavors in trying to connect Petri nets with free symmetric monoidal categories. We give a list of requirement such connections should respect if they are meant to be useful for practical/implementation purposes. We show how previous approaches do not satisfy them, and give compelling evidence that this depends on trying to make the correspondence functorial in the direction from nets to free symmetric monoidal categories, in order to produce an adjunction. We show that dropping this immediately honors our desiderata, and conclude by introducing an Idris library which implements them.", "venue": "ArXiv", "authors": ["Fabrizio  Genovese", "Alex  Gryzlov", "Jelle  Herold", "Marco  Perone", "Erik  Post", "Andr\u00e9  Videla"], "year": 2019, "n_citations": 13}
{"id": 3273400, "s2_id": "07fa2422258c602c1b06e86f86d0315e933a4ebf", "title": "Near-Memory Computing: Past, Present, and Future", "abstract": "The conventional approach of moving data to the CPU for computation has become a significant performance bottleneck for emerging scale-out data-intensive applications due to their limited data reuse. At the same time, the advancement in 3D integration technologies has made the decade-old concept of coupling compute units close to the memory --- called near-memory computing (NMC) --- more viable. Processing right at the \"home\" of data can significantly diminish the data movement problem of data-intensive applications. \nIn this paper, we survey the prior art on NMC across various dimensions (architecture, applications, tools, etc.) and identify the key challenges and open issues with future research directions. We also provide a glimpse of our approach to near-memory computing that includes i) NMC specific microarchitecture independent application characterization ii) a compiler framework to offload the NMC kernels on our target NMC platform and iii) an analytical model to evaluate the potential of NMC.", "venue": "Microprocess. Microsystems", "authors": ["Gagandeep  Singh", "Lorenzo  Chelini", "Stefano  Corda", "Ahsan Javed Awan", "Sander  Stuijk", "Roel  Jordans", "Henk  Corporaal", "Albert-Jan  Boonstra"], "year": 2019, "n_citations": 25}
{"id": 3275963, "s2_id": "1a7143367b575f7536bd07e530c67e568e954162", "title": "Markovian Model for Broadcast in Wireless Body Area Networks", "abstract": "Wireless body area networks became recently a vast field of investigation. A large amount of research in this field is dedicated to the evaluation of various communication protocols, e.g., broadcast or convergecast, against human body mobility. Most of the time this evaluation is done via simulations and in many situations only synthetic data is used for the human body mobility. In this paper we propose for the first time in Wireless Body Area Networks a Markovian analytical model specifically designed for WBAN networks. The main objective of the model is to evaluate the efficiency of a multi-hop transmission in the case of a diffusion-based broadcast protocol, with respect to various performance parameters (e.g., cover probability, average cover number, hitting probability or average cover time). We validate our model by comparing its results to simulation and show its accuracy. Finally, but not least, we show how our model can be used to analytically evaluate the trade-off between transmission power and redundancy, when the same message is broadcasted several times in order to increase the broadcast reliability while maintaining a low transmission power.", "venue": "MobiWac", "authors": ["Bruno  Baynat", "Gewu  Bu", "Maria Gradinariu Potop-Butucaru"], "year": 2019, "n_citations": 2}
{"id": 3281403, "s2_id": "a6a906ae9727a33409eaa207253a6ad32a871c11", "title": "Mira: A Framework for Static Performance Analysis", "abstract": "The performance model of an application can provide understanding about its runtime behavior on particular hardware. Such information can be analyzed by developers for performance tuning. However, model building and analyzing is frequently ignored during software development until performance problems arise because they require significant expertise and can involve many time-consuming application runs. In this paper, we propose a fast, accurate, flexible and user-friendly tool, Mira, for generating performance models by applying static program analysis, targeting scientific applications running on supercomputers. We parse both the source code and binary to estimate performance attributes with better accuracy than considering just source or just binary code. Because our analysis is static, the target program does not need to be executed on the target architecture, which enables users to perform analysis on available machines instead of conducting expensive experiments on potentially expensive resources. Moreover, statically generated models enable performance prediction on nonexistent or unavailable architectures. In addition to flexibility, because model generation time is significantly reduced compared to dynamic analysis approaches, our method is suitable for rapid application performance analysis and improvement. We present empirical validation results to demonstrate the current capabilities of our approach on small benchmarks and a mini application.", "venue": "2017 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Kewen  Meng", "Boyana  Norris"], "year": 2017, "n_citations": 7}
{"id": 3281458, "s2_id": "0c50a1547757b57c3d36e3f3901922018d977ed3", "title": "Efficient Replication for Straggler Mitigation in Distributed Computing", "abstract": "The potential of distributed computing to improve the performance of big data processing engines is contingent on mitigation of several challenges. In particular, by relying on multiple commodity servers, the performance of a distributed computing engine is dictated by the slowest servers, known as stragglers. Redundancy could mitigate stragglers by reducing the dependence of the computing engine on every server. Nevertheless, redundancy could yet be a burden to the system and aggravate stragglers. In this paper, we consider task replication as the redundancy technique and study the optimum redundancy planning to improve the performance of a master-worker distributed computing system. We start with optimum assignment policy of a given set of redundant tasks to a set of workers. Using the results from majorization theory, we show that if the service time of workers is a stochastically (decreasing and) convex random variable, a balanced assignment of non-overlapping batches of tasks minimizes the average job compute time. With that results, we then study the optimum level of redundancy from the perspective of average job compute time and compute time predictability. We derive the efficient redundancy level as a function of tasks' service time distribution. We observe that, the redundancy level that minimizes average compute time is not necessarily the same as the redundancy level that maximized compute time predictability. Finally, by running experiments on Google cluster traces, we show that a careful planning of redundancy according to the tasks' service time distribution can speed up the computing job by an order of magnitude.", "venue": "ArXiv", "authors": ["Amir  Behrouzi-Far", "Emina  Soljanin"], "year": 2020, "n_citations": 5}
{"id": 3282080, "s2_id": "d88142793cd2a4d22d2c42388d29b09525180091", "title": "SPSC: a new execution policy for exploring discrete-time stochastic simulations", "abstract": "In this paper, we introduce a new method called SPSC (Simulation, Partitioning, Selection, Cloning) to estimate efficiently the probability of possible solutions in stochastic simulations. This method can be applied to any type of simulation, however it is particularly suitable for multi-agent-based simulations (MABS). Therefore, its performance is evaluated on a well-known MABS and compared to the classical approach, i.e., Monte Carlo.", "venue": "PRIMA", "authors": ["Yu-Lin  Huang", "Gildas  Morvan", "Fr'ed'eric  Pichon", "David  Mercier"], "year": 2019, "n_citations": 0}
{"id": 3286128, "s2_id": "143ef26db4e8f0ec881398129b9af0cf3509b343", "title": "NetCut: Real-Time DNN Inference Using Layer Removal", "abstract": "Deep Learning plays a significant role in assisting humans in many aspects of their lives. As these networks tend to get deeper over time, they extract more features to increase accuracy at the cost of additional inference latency. This accuracy-performance trade-off makes it more challenging for Embedded Systems, as resource-constrained processors with strict deadlines, to deploy them efficiently. This can lead to selection of networks that can prematurely meet a specified deadline with excess slack time that could have potentially contributed to increased accuracy. In this work, we propose: (i) the concept of layer removal as a means of constructing TRimmed Networks (TRNs) that are based on removing problem-specific features of a pretrained network used in transfer learning, and (ii) NetCut, a methodology based on an empirical or an analytical latency estimator, which only proposes and retrains TRNs that can meet the application's deadline, hence reducing the exploration time significantly. We demonstrate that TRNs can expand the Pareto frontier that trades off latency and accuracy to provide networks that can meet arbitrary deadlines with potential accuracy improvement over off-the-shelf networks. Our experimental results show that such utilization of TRNs, while transferring to a simpler dataset, in combination with NetCut, can lead to the proposal of networks that can achieve relative accuracy improvement of up to 10.43% among existing off-the-shelf neural architectures while meeting a specific deadline, and 27x speedup in exploration time.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Mehrshad  Zandigohar", "Deniz  Erdogmus", "Gunar  Schirner"], "year": 2021, "n_citations": 0}
{"id": 3289332, "s2_id": "30528c0d9fb604649ec3cbe60f755171bfe79f08", "title": "Scalability of high-performance PDE solvers", "abstract": "Performance tests and analyses are critical to effective high-performance computing software development and are central components in the design and implementation of computational algorithms for achieving faster simulations on existing and future computing architectures for large-scale application problems. In this article, we explore performance and space-time trade-offs for important compute-intensive kernels of large-scale numerical solvers for partial differential equations (PDEs) that govern a wide range of physical applications. We consider a sequence of PDE-motivated bake-off problems designed to establish best practices for efficient high-order simulations across a variety of codes and platforms. We measure peak performance (degrees of freedom per second) on a fixed number of nodes and identify effective code optimization strategies for each architecture. In addition to peak performance, we identify the minimum time to solution at 80% parallel efficiency. The performance analysis is based on spectral and p-type finite elements but is equally applicable to a broad spectrum of numerical PDE discretizations, including finite difference, finite volume, and h-type finite elements.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Paul  Fischer", "Misun  Min", "Thilina  Rathnayake", "Som  Dutta", "Tzanio  Kolev", "Veselin  Dobrev", "Jean-Sylvain  Camier", "Martin  Kronbichler", "Tim  Warburton", "Kasia  \u015awirydowicz", "Jed  Brown"], "year": 2020, "n_citations": 30}
{"id": 3293361, "s2_id": "6698f5ecc576ee93e11c1555cbdf1362f0099984", "title": "Secure Compressed Reading in Smart Grids", "abstract": "Smart Grids measure energy usage in real-time and tailor supply and delivery accordingly, in order to improve power transmission and distribution. For the grids to operate effectively, it is critical to collect readings from massively-installed smart meters to control centers in an efficient and secure manner. In this paper, we propose a secure compressed reading scheme to address this critical issue. We observe that our collected real-world meter data express strong temporal correlations, indicating they are sparse in certain domains. We adopt Compressed Sensing technique to exploit this sparsity and design an efficient meter data transmission scheme. Our scheme achieves substantial efficiency offered by compressed sensing, without the need to know beforehand in which domain the meter data are sparse. This is in contrast to traditional compressed-sensing based scheme where such sparse-domain information is required a priori. We then design specific dependable scheme to work with our compressed sensing based data transmission scheme to make our meter reading reliable and secure. We provide performance guarantee for the correctness, efficiency, and security of our proposed scheme. Through analysis and simulations, we demonstrate the effectiveness of our schemes and compare their performance to prior arts.", "venue": "ArXiv", "authors": ["Sheng  Cai", "Jihang  Ye", "Minghua  Chen", "Jianxin  Yan", "Sidharth  Jaggi"], "year": 2012, "n_citations": 3}
{"id": 3293530, "s2_id": "48663a79038b4eb956da245ad3b4ba3fa6eae4f7", "title": "Irregular-Mapped Protograph LDPC-Coded Modulation: A Bandwidth-Efficient Solution for 5G Networks with Massive Data-Storage Requirement", "abstract": "The huge amount of data produced in the fifthgeneration (5G) networks not only brings new challenges to the reliability and efficiency of mobile devices but also drives rapid development of new storage techniques. With the benefits of fast access speed and high reliability, NAND flash memory has become a promising storage solution for the 5G networks. In this paper, we investigate a protograph-coded bit-interleaved coded modulation with iterative detection and decoding (BICM-ID) utilizing irregular mapping (IM) in the multi-level-cell (MLC) NAND flashmemory systems. First, we propose an enhanced protograph-based extrinsic information transfer (EPEXIT) algorithm to facilitate the analysis of protograph codes in the IM-BICM-ID systems. With the use of EPEXIT algorithm, a simple design method is conceived for the construction of a family of high-rate protograph codes, called irregular-mapped accumulate-repeat-accumulate (IMARA) codes, which possess both excellent decoding thresholds and linearminimum-distance-growth property. Furthermore, motivated by the voltage-region iterative gain characteristics of IM-BICM-ID systems, a novel read-voltage optimization scheme is developed to acquire accurate read-voltage levels, thus minimizing the decoding thresholds of protograph codes. Theoretical analyses and errorrate simulations indicate that the proposed IMARA-aided IMBICM-ID scheme and the proposed read-voltage optimization scheme remarkably improve the convergence and decoding performance of flash-memory systems. Thus, the proposed protographcoded IM-BICM-ID flash-memory systems can be viewed as a reliable and efficient storage solution for the new-generation mobile networks with massive data-storage requirement.", "venue": "ArXiv", "authors": ["Yi  Fang", "Yingcheng  Bu", "Pingping  Chen", "Shahid  Mumtaz", "Francis C. M. Lau", "Sattam Al Otaibi"], "year": 2021, "n_citations": 1}
{"id": 3295027, "s2_id": "5bc03a2d44c2f00f724da77652705dfab5afcb52", "title": "Generating a Performance Stochastic Model from UML Specifications", "abstract": "Since its initiation by Connie Smith, the process of Software Performance Engineering (SPE) is becoming a growing concern. The idea is to bring performance evaluation into the software design process. This suitable methodology allows software designers to determine the performance of software during design. Several approaches have been proposed to provide such techniques. Some of them propose to derive from a UML (Unified Modeling Language) model a performance model such as Stochastic Petri Net (SPN) or Stochastic process Algebra (SPA) models. Our work belongs to the same category. We propose to derive from a UML model a Stochastic Automata Network (SAN) in order to obtain performance predictions. Our approach is more flexible due to the SAN modularity and its high resemblance to UML' state-chart diagram.", "venue": "ArXiv", "authors": ["Ihab  Sbeity", "Leonardo  Brenner", "Mohamed  Dbouk"], "year": 2012, "n_citations": 4}
{"id": 3295963, "s2_id": "441ef97b953e26a68b82a34e56a0758479c51380", "title": "An End-to-End ML System for Personalized Conversational Voice Models in Walmart E-Commerce", "abstract": "Searching for and making decisions about products is becoming increasingly easier in the e-commerce space, thanks to the evolution of recommender systems. Personalization and recommender systems have gone hand-in-hand to help customers fulfill their shopping needs and improve their experiences in the process. With the growing adoption of conversational platforms for shopping, it has become important to build personalized models at scale to handle the large influx of data and perform inference in real-time. In this work, we present an end-to-end machine learning system for personalized conversational voice commerce. We include components for implicit feedback to the model, model training, evaluation on update, and a real-time inference engine. Our system personalizes voice shopping for Walmart Grocery customers and is currently available via Google Assistant, Siri and Google Home devices.", "venue": "ArXiv", "authors": ["Rahul Radhakrishnan Iyer", "Praveenkumar  Kanumala", "Stephen  Guo", "Kannan  Achan"], "year": 2020, "n_citations": 0}
{"id": 3298316, "s2_id": "8693080526ae9936b7a3cd0bbb9c66cfdea27f7c", "title": "In cloud, do MTC or HTC service providers benefit from the economies of scale?", "abstract": "Cloud computing, which is advocated as an economic platform for daily computing, has become a hot topic for both industrial and academic communities in the last couple of years. The basic idea behind cloud computing is that resource providers, which own the cloud platform, offer elastic resources to end users. In this paper, we intend to answer one key question to the success of cloud computing: in cloud, do many task computing (MTC) or high throughput computing (HTC) service providers, which offer the corresponding computing service to end users, benefit from the economies of scale? To the best of our knowledge, no previous work designs and implements the enabling system to consolidate MTC and HTC workloads on the cloud platform and no one answers the above question. Our research contributions are threefold: first, we propose an innovative usage model, called dynamic service provision (DSP) model, for MTC or HTC service providers. In the DSP model, the resource provider provides the service of creating and managing runtime environments for MTC or HTC service providers, and consolidates heterogeneous MTC or HTC workloads on the cloud platform; second, based on the DSP model, we design and implement Dawningcloud, which provides automatic management for heterogeneous workloads; third, a comprehensive evaluation of Dawningcloud has been performed in an emulatation experiment. We found that for typical workloads, in comparison with the previous two cloud solutions, Dawningcloud saves the resource consumption maximally by 46.4% (HTC) and 74.9% (MTC) for the service providers, and saves the total resource consumption maximally by 29.7% for the resource provider. At the same time, comparing with the traditional solution that provides MTC or HTC services with dedicated systems, Dawningcloud is more cost-effective. To this end, we conclude that for typical MTC and HTC workloads, on the cloud platform, MTC and HTC service providers and the resource service provider can benefit from the economies of scale.", "venue": "MTAGS '09", "authors": ["Lei  Wang", "Jianfeng  Zhan", "Weisong  Shi", "Yi  Liang", "Lin  Yuan"], "year": 2009, "n_citations": 33}
{"id": 3298865, "s2_id": "1df3e40f708b05d6073ddf176ff3cb4c036dbed9", "title": "Affinity Scheduling and the Applications on Data Center Scheduling with Data Locality", "abstract": "MapReduce framework is the de facto standard in Hadoop. Considering the data locality in data centers, the load balancing problem of map tasks is a special case of affinity scheduling problem. There is a huge body of work on affinity scheduling, proposing heuristic algorithms which try to increase data locality in data centers like Delay Scheduling and Quincy. However, not enough attention has been put on theoretical guarantees on throughput and delay optimality of such algorithms. In this work, we present and compare different algorithms and discuss their shortcoming and strengths. To the best of our knowledge, most data centers are using static load balancing algorithms which are not efficient in any ways and results in wasting the resources and causing unnecessary delays for users.", "venue": "ArXiv", "authors": ["Mohammadamir  Kavousi"], "year": 2017, "n_citations": 5}
{"id": 3300176, "s2_id": "649d939d1349ae3c4b0afd82984953b67f48baf0", "title": "Block-Diagonal and LT Codes for Distributed Computing With Straggling Servers", "abstract": "We propose two coded schemes for the distributed computing problem of multiplying a matrix by a set of vectors. The first scheme is based on partitioning the matrix into submatrices and applying maximum distance separable (MDS) codes to each submatrix. For this scheme, we prove that up to a given number of partitions the communication load and the computational delay (not including the encoding and decoding delay) are identical to those of the scheme recently proposed by Li et al., based on a single, long MDS code. However, due to the use of shorter MDS codes, our scheme yields a significantly lower overall computational delay when the delay incurred by encoding and decoding is also considered. We further propose a second coded scheme based on Luby transform (LT) codes under inactivation decoding. Interestingly, LT codes may reduce the delay over the partitioned scheme at the expense of an increased communication load. We also consider distributed computing under a deadline and show numerically that the proposed schemes outperform other schemes in the literature, with the LT code-based scheme yielding the best performance for the scenarios considered.", "venue": "IEEE Transactions on Communications", "authors": ["Albin  Severinson", "Alexandre  Graell i Amat", "Eirik  Rosnes"], "year": 2019, "n_citations": 60}
{"id": 3300302, "s2_id": "f2b1477766e752addf49332fc42c6e4d64e212a7", "title": "Do the Hard Stuff First: Scheduling Dependent Computations in Data-Analytics Clusters", "abstract": "We present a scheduler that improves cluster utilization and job completion times by packing tasks having multi-resource requirements and inter-dependencies. While the problem is algorithmically very hard, we achieve near-optimality on the job DAGs that appear in production clusters at a large enterprise and in benchmarks such as TPC-DS. A key insight is that carefully handling the long-running tasks and those with tough-to-pack resource needs will produce good-enough schedules. However, which subset of tasks to treat carefully is not clear (and intractable to discover). Hence, we offer a search procedure that evaluates various possibilities and outputs a preferred schedule order over tasks. An online component enforces the schedule orders desired by the various jobs running on the cluster. In addition, it packs tasks, overbooks the fungible resources and guarantees bounded unfairness for a variety of desirable fairness schemes. Relative to the state-of-the art schedulers, we speed up 50% of the jobs by over 30% each.", "venue": "ArXiv", "authors": ["Robert  Grandl", "Srikanth  Kandula", "Sriram  Rao", "Aditya  Akella", "Janardhan  Kulkarni"], "year": 2016, "n_citations": 7}
{"id": 3301138, "s2_id": "72c561b2d0672b489a67d132ee72bcc32d5bde69", "title": "Persistent Stochastic Non-Interference", "abstract": "In this paper, we study an information flow security property for systems specified as terms of a quantitative Markovian process algebra, namely the Performance Evaluation Process Algebra (PEPA). We propose a quantitative extension of the Non-Interference property used to secure systems from the functional point view by assuming that the observers are able to measure also the timing properties of the system, e.g., the response time of certain actions or its throughput. We introduce the notion of Persistent Stochastic Non-Interference (PSNI) based on the idea that every state reachable by a process satisfies a basic Stochastic Non-Interference (SNI) property. The structural operational semantics of PEPA allows us to give two characterizations of PSNI: one based on a bisimulation-like equivalence relation inducing a lumping on the underlying Markov chain, and another one based on unwinding conditions which demand properties of individual actions. These two different characterizations naturally lead to efficient methods for the verification and construction of secure systems. A decision algorithm for PSNI is presented and an application of PSNI to a queueing system is discussed.", "venue": "Fundam. Informaticae", "authors": ["Jane  Hillston", "Andrea  Marin", "Carla  Piazza", "Sabina  Rossi"], "year": 2021, "n_citations": 0}
{"id": 3302471, "s2_id": "0480e396a75886ab81173e44ac4df3b2896e7e6c", "title": "On the BMAP_1, BMAP_2/PH/g, c retrial queueing system", "abstract": "In this paper, we analyze a retrial queueing system with Batch Markovian Arrival Processes and two types of customers. The rate of individual repeated attempts from the orbit is modulated according to a Markov Modulated Poisson Process. Using the theory of multi-dimensional asymptotically quasi-Toeplitz Markov chain, we obtain the stability condition and the algorithm for calculating the stationary state distribution of the system. Main performance measures are presented. Furthermore, we investigate some optimization problems. The algorithm for determining the optimal number of guard servers and total servers is elaborated. Finally, this queueing system is applied to the cellular wireless network. Numerical results to illustrate the optimization problems and the impact of retrial on performance measures are provided. We find that the performance measures are mainly affected by the two types of customers' arrivals and service patterns, but the retrial rate plays a less crucial role.", "venue": "ArXiv", "authors": ["Jinbiao  Wu", "Yi  Peng", "Zaiming  Liu"], "year": 2015, "n_citations": 0}
{"id": 3303087, "s2_id": "8f95bd01a6926f07edf46deaf008d13b0a977660", "title": "DynIMS: A Dynamic Memory Controller for In-memory Storage on HPC Systems", "abstract": "In order to boost the performance of data-intensive computing on HPC systems, in-memory computing frameworks, such as Apache Spark and Flink, use local DRAM for data storage. Optimizing the memory allocation to data storage is critical to delivering performance to traditional HPC compute jobs and throughput to data-intensive applications sharing the HPC resources. Current practices that statically configure in-memory storage may leave inadequate space for compute jobs or lose the opportunity to utilize more available space for data-intensive applications. In this paper, we explore techniques to dynamically adjust in-memory storage and make the right amount of space for compute jobs. We have developed a dynamic memory controller, DynIMS, which infers memory demands of compute tasks online and employs a feedback-based control model to adapt the capacity of in-memory storage. We test DynIMS using mixed HPCC and Spark workloads on a HPC cluster. Experimental results show that DynIMS can achieve up to 5X performance improvement compared to systems with static memory allocations.", "venue": "ArXiv", "authors": ["Pengfei  Xuan", "Feng  Luo", "Rong  Ge", "Pradip K. Srimani"], "year": 2016, "n_citations": 1}
{"id": 3308280, "s2_id": "4516e7c5d26d73deea98fc0f6494847ba5af661b", "title": "Analyzing the performance of probabilistic algorithm in noisy manets", "abstract": "Probabilistic broadcast has been widely used as a flooding optimization mechanism to alleviate the effect of broadcast storm problem (BSP) in mobile ad hoc networks (MANETs). Many research studies have been carried-out to develop and evaluate the performance of this mechanism in an error-free (noiseless) environment. In reality, wireless communication channels in MANETs are an error-prone and suffer from high packet-loss due to presence of noise, i.e., noisy environment. In this paper, we propose a simulation model that can be used to evaluate the performance of probabilistic broadcast for flooding in noisy environment. In the proposed model, the noise-level is represented by a generic name, probability of reception (pc) (0<=pc<=1), where pc=1 for noiseless and <1 for noisy environment. The effect of noise is determined randomly by generating a random number \\zeta (0<=\\zeta<1); if \\zeta<=pc means the packet is successfully delivered to the receiving node, otherwise, unsuccessful delivery occurs. The proposed model is implemented on a MANET simulator, namely, MANSim. The effect of noise on the performance of probabilistic algorithm was investigated in four scenarios. The main conclusions of these scenarios are: the performance of probabilistic algorithm suffers in presence of noise. However, this suffering is less in high density networks, or if the nodes characterized by high retransmission probability or large radio transmission range. The nodes' speed has no or insignificant effect on the performance.", "venue": "ArXiv", "authors": ["Hussein  Al-Bahadili", "Khalid  Kaabneh"], "year": 2010, "n_citations": 19}
{"id": 3310449, "s2_id": "bdc582b9673887bbe3f859cd26dcd633ef01644c", "title": "Performance Evaluation of Packet-to-Cell Segmentation Schemes in Input Buffered Packet Switches", "abstract": "Most input buffered packet switches internally segment variable-length packets into fixed-length cells. The last cell in a segmented packet will contain overhead bytes if the packet length is not evenly divisible by the cell length. Switch speed-up is used to compensate for this overhead. In this paper, we develop an analytical model of a single-server queue where an input stream of packets is segmented into cells for service. Analytical models are developed for M/M/1, M/H2/1, and M/E2/1 queues with a discretized (or quantized) service time. These models and simulation using real packet traces are used to evaluate the effect of speed-up on mean queue length. We propose and evaluate a new method of segmenting a packet trailer and subsequent packet header into a single cell. This cell merging method reduces the required speed-up. No changes to switch-matrix scheduling algorithms are needed. Simulation with a packet trace shows a reduction in the needed speed-up for an iSLIP scheduled input buffered switch.", "venue": "ArXiv", "authors": ["Kenneth J. Christensen", "Kenji  Yoshigoe", "Allen  Roginsky", "Neil J. Gunther"], "year": 2004, "n_citations": 6}
{"id": 3310565, "s2_id": "23e8a3f9904b71144437acb5630f62557031b7f8", "title": "Outage Analysis of Cognitive Electric Vehicular Networks Over Mixed RF/VLC Channels", "abstract": "Modern transportation infrastructures are considered as one of the main sources of the greenhouse gases emitted into the atmosphere. This situation requires the decision-making players to enact the mass use of electric vehicles (EVs) which, in turn, highly demand novel secure communication technologies robust to various cyber-attacks. Therefore, in this paper, a novel jamming-robust communication method is proposed for different outdoor cognitive EV-enabled network scenarios over mixed radio-frequency (RF)/visible light communication (VLC) channels. One EV is designated to act as a relay enabling an aggregator to communicate with a jammed vehicle. This relay operates in both RF and VLC spectrum bands while meeting the interference restrictions defined by the primary network. Considering perfect and imperfect channel state information, exact closed-form analytical expressions are derived for the outage probability and their asymptotic analysis is provided. Moreover, we quantify the outage reduction achievable by deploying such mixed VLC/RF channels. Finally, analytical results are validated by Monte Carlo simulations.", "venue": "IEEE Transactions on Cognitive Communications and Networking", "authors": ["Galymzhan  Nauryzbayev", "Mohamed  Abdallah", "Naofal  Al-Dhahir"], "year": 2020, "n_citations": 3}
{"id": 3311239, "s2_id": "a80278337b6d3950b24171ccfbd135da8791f1b3", "title": "Acceleration of tensor-product operations for high-order finite element methods", "abstract": "This article is devoted to graphics processing unit (GPU) kernel optimization and performance analysis of three tensor-product operations arising in finite element methods. We provide a mathematical background to these operations and implementation details. Achieving close to peak performance for these operators requires extensive optimization because of the operators\u2019 properties: low arithmetic intensity, tiered structure, and the need to store intermediate results during the kernel execution. We give a guided overview of optimization strategies and we present a performance model that allows us to compare the efficacy of these optimizations against an empirically calibrated roofline.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Kasia  Swirydowicz", "Noel  Chalmers", "Ali  Karakus", "Timothy C. Warburton"], "year": 2019, "n_citations": 29}
{"id": 3312606, "s2_id": "63fdd06a3469c05f4075731607ece590e4c8b24e", "title": "Approximation of LRU Caches Miss Rate: Application to Power-law Popularities", "abstract": "Building on the 1977 pioneering work of R. Fagin, we give a closed-form expression for the approximated Miss Rate (MR) of LRU Caches assuming a power-law popularity. Asymptotic behavior of this expression is an already known result when power-law parameter is above 1. It is extended to any value of the parameter. In addition, we bring a new analysis of the conditions (cache relative size, popularity parameter) under which the ratio of LRU MR to Static MR is worst-case.", "venue": "ArXiv", "authors": ["Christian  Berthet"], "year": 2017, "n_citations": 8}
{"id": 3313928, "s2_id": "c0ce6f68a3258e11f6aa7e6b44257dbecf4fe5ee", "title": "Platform Independent Software Analysis for Near Memory Computing", "abstract": "Near-memory Computing (NMC) promises improved performance for the applications that can exploit the features of emerging memory technologies such as 3D-stacked memory. However, it is not trivial to find such applications and specialized tools are needed to identify them. In this paper, we present PISA-NMC, which extends a state-of-the-art hardware agnostic profiling tool with metrics concerning memory and parallelism, which are relevant for NMC. The metrics include memory entropy, spatial locality, data-level, and basic-block-level parallelism. By profiling a set of representative applications and correlating the metrics with the application's performance on a simulated NMC system, we verify the importance of those metrics. Finally, we demonstrate which metrics are useful in identifying applications suitable for NMC architectures.", "venue": "2019 22nd Euromicro Conference on Digital System Design (DSD)", "authors": ["Stefano  Corda", "Gagandeep  Singh", "Ahsan Javed Awan", "Roel  Jordans", "Henk  Corporaal"], "year": 2019, "n_citations": 4}
{"id": 3318153, "s2_id": "c8f70aaa5c6d6d985bdb8491c14aadb3aa7c1fc3", "title": "HPC AI500: A Benchmark Suite for HPC AI Systems", "abstract": "In recent years, with the trend of applying deep learning (DL) in high performance scientific computing, the unique characteristics of emerging DL workloads in HPC raise great challenges in designing, implementing HPC AI systems. The community needs a new yard stick for evaluating the future HPC systems. In this paper, we propose HPC AI500 --- a benchmark suite for evaluating HPC systems that running scientific DL workloads. Covering the most representative scientific fields, each workload from HPC AI500 is based on real-world scientific DL applications. Currently, we choose 14 scientific DL benchmarks from perspectives of application scenarios, data sets, and software stack. We propose a set of metrics for comprehensively evaluating the HPC AI systems, considering both accuracy, performance as well as power and cost. We provide a scalable reference implementation of HPC AI500. HPC AI500 is a part of the open-source AIBench project, the specification and source code are publicly available from \\url{this http URL}.", "venue": "Bench", "authors": ["Zihan  Jiang", "Wanling  Gao", "Lei  Wang", "Xingwang  Xiong", "Yuchen  Zhang", "Xu  Wen", "Chunjie  Luo", "Hainan  Ye", "Yunquan  Zhang", "Shengzhong  Feng", "Kenli  Li", "Weijia  Xu", "Jianfeng  Zhan"], "year": 2018, "n_citations": 30}
{"id": 3318606, "s2_id": "3c044528eb62ffc2f38814fe40acf4dace4846b8", "title": "Linux-Tomcat Application Performance on Amazon AWS", "abstract": "The need for Linux system administrators to do performance management has returned with a vengeance. Why? The cloud. Resource consumption in the cloud is all about pay-as-you-go. This article shows you how performance models can find the most cost-effective deployment of an application on Amazon's cloud.", "venue": "ArXiv", "authors": ["Neil J. Gunther", "Mohit  Chawla"], "year": 2018, "n_citations": 1}
{"id": 3323253, "s2_id": "530a62aa798168558739852b80a292e76022646c", "title": "Straggler Mitigation by Delayed Relaunch of Tasks", "abstract": "Quantum computers have usually been considered unattainable devices, but quantum computing has come a long way since its inception in the 1980s. Indeed, universal quantum computers of small size are now available for general use in the case of IBM\u2019s Quantum Experience, even for free. So what is a quantum computer, and what can it do di erently from classical computers? In this tutorial we will describe the paradigm underpinning the behavior of quantum computers, treating quantum computers as purely mathematical objects. We will use a simple gate model and show the basic ideas underlying quantum algorithms. This introduction does not require any prior knowledge of physics, and will, in fact, not discuss any quantum physics at all. However, it requires some familiarity with linear algebra on complex vector spaces.", "venue": "PERV", "authors": ["Giacomo  Nannicini"], "year": 2018, "n_citations": 1}
{"id": 3323726, "s2_id": "1bd5ac925e9a26aced4ce6c59b4cb3d87de31df5", "title": "A64FX \u2013 Your Compiler You Must Decide!", "abstract": "The current number one of the TOP500 list, Supercomputer Fugaku, has demonstrated that CPU-only HPC systems aren\u2019t dead and CPUs can be used for more than just being the host controller for a discrete accelerators. While the specifications of the chip and overall system architecture, and benchmarks submitted to various lists, like TOP500 and Green500, etc., are clearly highlighting the potential, the proliferation of Arm into the HPC business is rather recent and hence the software stack might not be fully matured and tuned, yet. We test 3 state-of-the-art compiler suite against a broad set of benchmarks. Our measurements show that orders of magnitudes in performance can be gained by deviating from the recommended usage model of the A64FX compute nodes.", "venue": "2021 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Jens  Domke"], "year": 2021, "n_citations": 1}
{"id": 3324143, "s2_id": "28e4ae18a652e7d67df3e3fa6f4703ae9ef930e9", "title": "BestConfig: tapping the performance potential of systems via automatic configuration tuning", "abstract": "An ever increasing number of configuration parameters are provided to system users. But many users have used one configuration setting across different workloads, leaving untapped the performance potential of systems. A good configuration setting can greatly improve the performance of a deployed system under certain workloads. But with tens or hundreds of parameters, it becomes a highly costly task to decide which configuration setting leads to the best performance. While such task requires the strong expertise in both the system and the application, users commonly lack such expertise. To help users tap the performance potential of systems, we present Best Config, a system for automatically finding a best configuration setting within a resource limit for a deployed system under a given application workload. BestConfig is designed with an extensible architecture to automate the configuration tuning for general systems. To tune system configurations within a resource limit, we propose the divide-and-diverge sampling method and the recursive bound-and-search algorithm. BestConfig can improve the throughput of Tomcat by 75%, that of Cassandra by 63%, that of MySQL by 430%, and reduce the running time of Hive join job by about 50% and that of Spark join job by about 80%, solely by configuration adjustment.", "venue": "SoCC", "authors": ["Yuqing  Zhu", "Jianxun  Liu", "Mengying  Guo", "Yungang  Bao", "Wenlong  Ma", "Zhuoyue  Liu", "Kunpeng  Song", "Yingchun  Yang"], "year": 2017, "n_citations": 98}
{"id": 3325115, "s2_id": "fff22d4e5a5aeb2bcb6ba902f35a321accbdf9d0", "title": "inbaverSim: An OMNeT++ Model Framework for Content Centric Networking", "abstract": "Today\u2019s networks are used primarily to move content. To cater to this requirement Information Centric Networks (ICN) were introduced. One of the main architectures of ICN is Content Centric Networking (CCN) and its derivative, Named Data Networking (NDN). CCN is standardized at the Internet Engineering Task Force (IETF) and is envisaged to replace the current Internet over time. To evaluate large scale deployments of CCN, a model framework called the inbaverSim is developed in OMNeT++. This work presents the architecture of this model framework together with an example evaluation using the model framework. The code is open source and is available at GitHub.", "venue": "ArXiv", "authors": ["Asanga  Udugama"], "year": 2021, "n_citations": 0}
{"id": 3330036, "s2_id": "f9ac8dc7cc32242c95156cd326808d3058e777b5", "title": "Delay and Backlog Analysis for 60 GHz Wireless Networks", "abstract": "To meet the ever-increasing demands on higher throughput and better network delay performance, 60 GHZ networking is proposed as a promising solution for the next generation of wireless communications. To successfully deploy such networks, its important to understand their performance first. However, due to the unique fading characteristic of the 60 GHz channel, the characterization of the corresponding service process, offered by the channel, using the conventional methodologies may not be tractable. In this work, we provide an alternative approach to derive a closed-form expression that characterizes the cumulative service process of the 60 GHz channel in terms of the moment generating function (MGF) of its instantaneous channel capacity. We then use this expression to derive probabilistic upper bounds on the backlog and delay that are experienced by a flow traversing this network, using results from the MGF-based network calculus. The computed bounds are validated using simulation. We provide numerical results for different networking scenarios and for different traffic and channel parameters and we show that the 60 GHz wireless network is capable of satisfying stringent quality-of-Service (QoS) requirements, in terms of network delay and reliability. With this analysis approach at hand, a larger scale 60 GHz network design and optimization is possible.", "venue": "2016 IEEE Global Communications Conference (GLOBECOM)", "authors": ["Guang  Yang", "Ming  Xiao", "James  Gross", "Hussein  Al-Zubaidy", "Yongming  Huang"], "year": 2016, "n_citations": 16}
{"id": 3330677, "s2_id": "5015117984f0b0bccfc01fec1e64381be4aa5cfb", "title": "AIS for misbehavior detection in wireless sensor networks: Performance and design principles", "abstract": "A sensor network is a collection of wireless devices that are able to monitor physical or environmental conditions. These devices are expected to operate autonomously, be battery powered and have very limited computational capabilities. This makes the task of protecting a sensor network against misbehavior or possible malfunction a challenging problem. In this document we discuss performance of Artificial immune systems (AIS) when used as the mechanism for detecting misbehavior. We concentrate on performance of respective genes; genes are necessary to measure a network's performance from a sensor's viewpoint. We conclude that the choice of genes has a profound influence on the performance of the AIS. We identified a specific MAC layer based gene that showed to be especially useful for detection. We also discuss implementation details of AIS when used with sensor networks.", "venue": "2007 IEEE Congress on Evolutionary Computation", "authors": ["Martin  Drozda", "Sven  Schaust", "Helena  Szczerbicka"], "year": 2007, "n_citations": 60}
{"id": 3337155, "s2_id": "0199c82db10df4c548bbc02e16885e30f8a14c0a", "title": "Software Autotuning for Sustainable Performance Portability", "abstract": "Scientific software applications are increasingly developed by large interdiscplinary teams operating on functional modules organized around a common software framework, which is capable of integrating new functional capabilities without modifying the core of the framework. In such environment, software correctness and modularity take precedence at the expense of code performance, which is an important concern during execution on supercomputing facilities, where the allocation of core-hours is a valuable resource. To alleviate the performance problems, we propose automated performance tuning (autotuning) of software to extract the maximum performance on a given hardware platform and to enable performance portability across heterogeneous hardware platforms. The resulting code remains generic without committing to a particular software stack and yet is compile-time specializable for maximal sustained performance.", "venue": "ArXiv", "authors": ["Azamat  Mametjanov", "Boyana  Norris"], "year": 2013, "n_citations": 0}
{"id": 3338460, "s2_id": "bb6df03d12603f5a679b37085da7d4b8fa1ec50c", "title": "Is Big Data Performance Reproducible in Modern Cloud Networks?", "abstract": "Performance variability has been acknowledged as a problem for over a decade by cloud practitioners and performance engineers. Yet, our survey of top systems conferences reveals that the research community regularly disregards variability when running experiments in the cloud. Focusing on networks, we assess the impact of variability on cloud-based big-data workloads by gathering traces from mainstream commercial clouds and private research clouds. Our data collection consists of millions of datapoints gathered while transferring over 9 petabytes of data. We characterize the network variability present in our data and show that, even though commercial cloud providers implement mechanisms for quality-of-service enforcement, variability still occurs, and is even exacerbated by such mechanisms and service provider policies. We show how big-data workloads suffer from significant slowdowns and lack predictability and replicability, even when state-of-the-art experimentation techniques are used. We provide guidelines for practitioners to reduce the volatility of big data performance, making experiments more repeatable.", "venue": "NSDI", "authors": ["Alexandru  Uta", "Alexandru  Custura", "Dmitry  Duplyakin", "Ivo  Jimenez", "Jan  Rellermeyer", "Carlos  Maltzahn", "Robert  Ricci", "Alexandru  Iosup"], "year": 2020, "n_citations": 28}
{"id": 3344169, "s2_id": "b209d125684c5153b35ad9a2d30d98e25fd62e81", "title": "Energy-Efficient Strategies for Cooperative Multichannel MAC Protocols", "abstract": "Distributed Information SHaring (DISH) is a new cooperative approach to designing multichannel MAC protocols. It aids nodes in their decision making processes by compensating for their missing information via information sharing through neighboring nodes. This approach was recently shown to significantly boost the throughput of multichannel MAC protocols. However, a critical issue for ad hoc communication devices, viz. energy efficiency, has yet to be addressed. In this paper, we address this issue by developing simple solutions that reduce the energy consumption without compromising the throughput performance and meanwhile maximize cost efficiency. We propose two energy-efficient strategies: in-situ energy conscious DISH, which uses existing nodes only, and altruistic DISH, which requires additional nodes called altruists. We compare five protocols with respect to these strategies and identify altruistic DISH to be the right choice in general: it 1) conserves 40-80 percent of energy, 2) maintains the throughput advantage, and 3) more than doubles the cost efficiency compared to protocols without this strategy. On the other hand, our study also shows that in-situ energy conscious DISH is suitable only in certain limited scenarios.", "venue": "IEEE Transactions on Mobile Computing", "authors": ["Tie  Luo", "Mehul  Motani", "Vikram  Srinivasan"], "year": 2012, "n_citations": 17}
{"id": 3347495, "s2_id": "e0e852b8f4dd03a7a6994762855e3d33afcb8b20", "title": "Online Advertisement, Optimization and Stochastic Networks", "abstract": "In this paper, we propose a stochastic model to describe how search service providers charge client companies based on users' queries for the keywords related to these companies' ads by using certain advertisement assignment strategies. We formulate an optimization problem to maximize the long-term average revenue for the service provider under each client's long-term average budget constraint and design an online algorithm which captures the stochastic properties of users' queries and click-through behaviors. We solve the optimization problem by making connections to scheduling problems in wireless networks, queueing theory and stochastic networks. Unlike prior models, we do not assume that the number of query arrivals is known. Due to the stochastic nature of the arrival process considered here, either temporary \u201cfree\u201d service, i.e., service above the specified budget (which we call \u201coverdraft\u201d) or under-utilization of the budget (which we call \u201cunderdraft\u201d) is unavoidable. We prove that our online algorithm can achieve a revenue that is within O(\u03b5) of the optimal revenue while ensuring that the overdraft or underdraft is O(1/\u03b5) , where \u03b5 can be arbitrarily small. With a view towards practice, we can show that one can always operate strictly under the budget. In addition, we extend our results to a click-through rate maximization model and also show how our algorithm can be modified to handle non-stationary query arrival processes and clients with short-term contracts. Our algorithm also allows us to quantify the effect of errors in click-through rate estimation on the achieved revenue. We show that we lose at most \u0394/1+\u0394 fraction of the revenue if \u0394 is the relative error in click-through rate estimation. We also show that in the long run, an expected overdraft level of \u03a9(log(1/\u03b5)) is unavoidable (a universal lower bound) under any stationary ad assignment algorithm which achieves a long-term average revenue within O(\u03b5) of the offline optimum.", "venue": "IEEE Transactions on Automatic Control", "authors": ["Bo  Tan", "R.  Srikant"], "year": 2012, "n_citations": 45}
{"id": 3349794, "s2_id": "631f621ef04e55ad2b545ce2721740b2e9940d8c", "title": "Quantum-resistant digital signatures schemes for low-power IoT", "abstract": "Quantum computers are on the horizon to get to a sufficient size that will then be able to break pretty much all the encryption and signature schemes we currently use. This is the case for human interface devices as well as for IoT nodes. In this paper i am comparing some signature schemes currently in the process of standardization by the NIST. After explaining the underlying basis on why some schemes are different in some aspects compared to others i will evaluate which currently available implementations are better suited for usage in IoT usecases. We will come to further focus on the most promising schemes FALCON and Dilithium, which differ in one signifiant aspect that makes FALCON worse for signing but very good for verification purposes.", "venue": "ArXiv", "authors": ["Hannes  Hattenbach"], "year": 2021, "n_citations": 0}
{"id": 3352520, "s2_id": "c889abf559040a6c7bfb92a2baf16f52ef0f2de7", "title": "A Queuing Model for CPU Functional Unit and Issue Queue Configuration", "abstract": "Abstract In a superscalar processor, instructions of various types flow through an execution pipeline, traversing hardware resources which are mostly shared among many different instruction types. A notable exception to shared pipeline resources is the collection of functional units, the hardware that performs specific computations. In a trade-off of cost versus performance, a pipeline designer must decide how many of each type of functional unit to place in a processor\u2019s pipeline. In this paper, we model a superscalar processor\u2019s issue queue and functional units as a novel queuing network. We treat the issue queue as a finite-sized waiting area and the functional units as servers. In addition to common queuing problems, customers of the network share the queue but wait for specific servers to become ready (e.g., addition instructions wait for adders). Furthermore, the customers in this queue are not necessary ready for service, since instructions may be waiting for operands. In this paper we model a novel queuing network that provides a solution to the expected queue length of each type of instruction. This network and its solution can also be generalized to other problems, notably other resource-allocation issues that arise in superscalar pipelines.", "venue": "Simul. Model. Pract. Theory", "authors": ["Shane  Carroll", "Wei-Ming  Ling"], "year": 2018, "n_citations": 0}
{"id": 3356062, "s2_id": "bf967c0ea15bec7047017516c6d60c8cfa824710", "title": "Performance Analysis of Sequential Method for Handover in Cognitive Radio Systems", "abstract": "Powerful spectrum handover schemes enable cognitive radios (CRs) to use transmission opportunities in primary users' channels appropriately. In this paper, we consider the cognitive access of primary channels by a secondary user. We evaluate the average detection time and the maximum achievable average throughput of the secondary user when the sequential method for hand-over (SMHO) is used. We assume that a prior knowledge of the primary users' presence and absence probabilities are available. When investigating the maximum achievable throughput of the secondary user, we end into an optimization problem, in which the optimum value of sensing time must be selected. In our optimization problem, we take into account the spectrum hand over due to false detection of the primary user. We also propose a weighted based hand-over (WBHO) scheme in which the impacts of channels conditions and primary users' presence probability are considered. This Spectrum handover scheme provides higher average throughput for the SU than the SMHO method. The tradeoff between the maximum achievable throughput and consumed energy is discussed, and finally an energy efficient optimization formulation for finding a proper sensing time is provided.", "venue": "ArXiv", "authors": ["Hossein Shokri Ghadikolaei", "Mohammad  Mozaffari", "Masoumeh  Nasiri-Kenari"], "year": 2011, "n_citations": 0}
{"id": 3356852, "s2_id": "b6168546565d3052176b27b5a76c56046c7830b9", "title": "ParaPlan: A Tool for Parallel Reachability Analysis of Planar Polygonal Differential Inclusion Systems", "abstract": "Andrei Sandler, and Olga Tveretina, \u2018ParaPlan: A Tool for Parallel Reachability Analysis of Planar Polygonal Differential Inclusion Systems\u2019, in Patricia Bouyer, Andrea Orlandini and Pierluigi San Pietro, eds. Proceedings Eight International Symposium on Games, Automata, Logics and Formal Verification (GandALF 2017), Rome, Italy, 20-22 September 2017, Electronic Proceedings in Theoretical Computer Science, Vol. 256: 283-296, September 2017. \u00a9 2017 The Author(s). This work is licensed under the Creative Commons Attribution License CC BY 4.0 https://creativecommons.org/licenses/by/4.0/", "venue": "GandALF", "authors": ["Andrei  Sandler", "Olga  Tveretina"], "year": 2017, "n_citations": 0}
{"id": 3358107, "s2_id": "6c6f657d079944f29740c6974f21697bcfcca5fc", "title": "Sharp bounds in stochastic network calculus", "abstract": "The practicality of the stochastic network calculus (SNC) is often questioned on grounds of potential looseness of its performance bounds. In this paper it is uncovered that for bursty arrival processes (specifically Markov-Modulated On-Off (MMOO)), whose amenability to per-flow analysis is typically proclaimed as a highlight of SNC, the bounds can unfortunately indeed be very loose (e.g., by several orders of magnitude off). In response to this uncovered weakness of SNC, the (Standard) per-flow bounds are herein improved by deriving a general sample-path bound, using martingale based techniques, which accommodates FIFO, SP, and EDF scheduling disciplines. The obtained (Martingale) bounds capture an additional exponential decay factor of O(e-\u03b1 n) in the number of flows $n$, and are remarkably accurate even in multiplexing scenarios with few flows.", "venue": "SIGMETRICS '13", "authors": ["Florin  Ciucu", "Felix  Poloczek", "Jens B. Schmitt"], "year": 2013, "n_citations": 13}
{"id": 3358444, "s2_id": "6ef0b5213196b48d12a34111bd20c6902f4cb2c0", "title": "Efficient MPI-based Communication for GPU-Accelerated Dask Applications", "abstract": "Dask is a popular parallel and distributed computing framework, which rivals Apache Spark to enable task-based scalable processing of big data. The Dask Distributed library forms the basis of this computing engine and provides support for adding new communication devices. It currently has two communication devices: one for TCP and the other for high-speed networks using UCX-Py\u2014a Cython wrapper to UCX. This paper presents the design and implementation of a new communication backend for Dask\u2014called MPI4Dask\u2014that is targeted for modern HPC clusters built with GPUs. MPI4Dask exploits mpi4py over MVAPICH2-GDR, which is a GPU-aware implementation of the Message Passing Interface (MPI) standard. MPI4Dask provides point-to-point asynchronous I/O communication coroutines, which are non-blocking concurrent operations defined using the async/await keywords from the Python\u2019s asyncio framework. Our latency and throughput comparisons suggest that MPI4Dask outperforms UCX by 6\u00d7 for 1 Byte message and 4\u00d7 for large messages (2 MBytes and beyond) respectively. We also conduct comparative performance evaluation of MPI4Dask with UCX using two benchmark applications: 1) sum of cuPy array with its transpose, and 2) cuDF merge. MPI4Dask speeds up the overall execution time of the two applications by an average of 3.47\u00d7 and 3.11\u00d7 respectively on an in-house cluster built with NVIDIA Tesla V100 GPUs for 1 \u2013 6 Dask workers. We also perform scalability analysis of MPI4Dask against UCX for these applications on TACC\u2019s Frontera (GPU) system with upto 32 Dask workers on 32 NVIDIA Quadro RTX 5000 GPUs and 256 CPU cores. MPI4Dask speeds up the execution time for cuPy and cuDF applications by an average of 1.71\u00d7 and 2.91\u00d7 respectively for 1 \u2212 32 Dask workers on the Frontera (GPU) system.", "venue": "2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)", "authors": ["Aamir  Shafi", "Jahanzeb Maqbool Hashmi", "Hari  Subramoni", "Dhabaleswar K. Panda"], "year": 2021, "n_citations": 0}
{"id": 3361922, "s2_id": "b6522de0053f50bca840bfeb5a69a1aa145342db", "title": "Fast and Efficient Bulk Multicasting over Dedicated Inter-Datacenter Networks", "abstract": "Several organizations have built multiple datacenters connected via dedicated wide area networks over which large inter-datacenter transfers take place. This includes tremendous volumes of bulk multicast traffic generated as a result of data and content replication. Although one can perform these transfers using a single multicast forwarding tree, that can lead to poor performance as the slowest receiver on each tree dictates the completion time for all receivers. Using multiple trees per transfer each connected to a subset of receivers alleviates this concern. The choice of multicast trees also determines the total bandwidth usage. To further improve the performance, bandwidth over dedicated inter-datacenter networks can be carved for different multicast trees over specific time periods to avoid congestion and minimize the average receiver completion times. \nIn this paper, we break this problem into the three sub-problems of partitioning, tree selection, and rate allocation. We present an algorithm called QuickCast which is computationally fast and allows us to significantly speed up multiple receivers per bulk multicast transfer with control over extra bandwidth consumption. We evaluate QuickCast against a variety of synthetic and real traffic patterns as well as real WAN topologies. Compared to performing bulk multicast transfers as separate unicast transfers, QuickCast achieves up to $3.64\\times$ reduction in mean completion times while at the same time using $0.71\\times$ the bandwidth. Also, QuickCast allows the top $50\\%$ of receivers to complete between $3\\times$ to $35\\times$ faster on average compared with when a single forwarding multicast tree is used for data delivery.", "venue": "ArXiv", "authors": ["Mohammad  Noormohammadpour", "Cauligi S. Raghavendra", "Srikanth  Kandula", "Sriram  Rao"], "year": 2018, "n_citations": 1}
{"id": 3362543, "s2_id": "2ffd59da4a926f6ae8afb839388644b1f4551e02", "title": "Flexible Performant GEMM Kernels on GPUs", "abstract": "General Matrix Multiplication or GEMM kernels take center place in high performance computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as NVIDIA's Tensor Cores. Their exploitation is hampered by the two-language problem: it requires either low-level programming which implies low programmer productivity or using libraries that only offer a limited set of components. Because rephrasing algorithms in terms of established components often introduces overhead, the libraries' lack of flexibility limits the freedom to explore new algorithms. Researchers using GEMMs can hence not enjoy programming productivity, high performance, and research flexibility at once. \nIn this paper we solve this problem. We present three sets of abstractions and interfaces to program GEMMs within the scientific Julia programming language. The interfaces and abstractions are co-designed for researchers' needs and Julia's features to achieve sufficient separation of concerns and flexibility to easily extend basic GEMMs in many different ways without paying a performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS and CUTLASS, we demonstrate that our performance is mostly on par with, and in some cases even exceeds, the libraries, without having to write a single line of code in CUDA C++ or assembly, and without facing flexibility limitations.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Thomas  Faingnaert", "Tim  Besard", "Bjorn De Sutter"], "year": 2021, "n_citations": 1}
{"id": 3368099, "s2_id": "f66d1bd99478b133deddd201c519fe7a0bf1ccfd", "title": "Reactive User Behavior and Mobility Models", "abstract": "In this paper, we present a set of simulation models to more realistically mimic the behaviour of users reading messages. We propose a User Behaviour Model, where a simulated user reacts to a message by a flexible set of possible reactions (e.g. ignore, read, like, save, etc.) and a mobility-based reaction (visit a place, run away from danger, etc.). We describe our models and their implementation in OMNeT++. We strongly believe that these models will significantly contribute to the state of the art of simulating realistically opportunistic networks.", "venue": "ArXiv", "authors": ["Anna  F\u00f6rster", "Anas Bin Muslim", "Asanga  Udugama"], "year": 2017, "n_citations": 3}
{"id": 3370810, "s2_id": "e14ba8c67235a0163f4877519c3b0ae2f786b95e", "title": "TabulaROSA: Tabular Operating System Architecture for Massively Parallel Heterogeneous Compute Engines", "abstract": "The rise in computing hardware choices is driving a reevaluation of operating systems. The traditional role of an operating system controlling the execution of its own hardware is evolving toward a model whereby the controlling processor is distinct from the compute engines that are performing most of the computations. In this context, an operating system can be viewed as software that brokers and tracks the resources of the compute engines and is akin to a database management system. To explore the idea of using a database in an operating system role, this work defines key operating system functions in terms of rigorous mathematical semantics (associative array algebra) that are directly translatable into database operations. These operations possess a number of mathematical properties that are ideal for parallel operating systems by guaranteeing correctness over a wide range of parallel operations. The resulting operating system equations provide a mathematical specification for a Tabular Operating System Architecture (TabulaROSA) that can be implemented on any platform. Simulations of forking in TabularROSA are performed using an associative array implementation and compared to Linux on a 32,000+ core supercomputer. Using over 262,000 forkers managing over 68,000,000,000 processes, the simulations show that TabulaROSA has the potential to perform operating system functions on a massively parallel scale. The TabulaROSA simulations show 20x higher performance as compared to Linux while managing 2000x more processes in fully searchable tables.", "venue": "2018 IEEE High Performance extreme Computing Conference (HPEC)", "authors": ["Jeremy  Kepner", "Ron  Brightwell", "Alan  Edelman", "Vijay  Gadepally", "Hayden  Jananthan", "Michael  Jones", "Samuel  Madden", "Peter  Michaleas", "Hamed  Okhravi", "Kevin T. Pedretti", "Albert  Reuther", "Thomas L. Sterling", "Michael  Stonebraker"], "year": 2018, "n_citations": 6}
{"id": 3373419, "s2_id": "7531b4f3eeeeb155f63ed3e2727100b1fc520873", "title": "PCNM: A New Platform for Cellular Networks Measurements and Optimization", "abstract": "In this paper, we present PCNM, a new mobile platform for cellular networks measurements. PCNM is based on a set of techniques that tailors theoretical calculations and simulations to the real cellular network environment. It includes: (a) modules that measure different parameters of a base station such as localization, cell identification, time advance information, reception level and quality, (b) a new protocol that optimizes the task of network measurement by monitoring a set of mobile nodes and finally (c) the ability to extend an existing cellular network by adding new base stations. We evaluate our genetic algorithm used to reduce node mobility and optimize the measurement extraction of N base stations using k mobile sensors (k ges 1). We show how connecting real measurements (using mobile sensors in a collaborative way) to theoretical and prediction methods is of high benefit for cellular networks maintenance, extension and performance evaluation.", "venue": "2007 International Conference on Wireless Communications, Networking and Mobile Computing", "authors": ["Tayeb  Lemlouma", "Yoann  Lefebvre", "Fr\u00e9d\u00e9ric  Cespedes"], "year": 2007, "n_citations": 1}
{"id": 3373704, "s2_id": "386aa6d99359610397cb24ef9847e2885ae97835", "title": "High-Performance Simultaneous Multiprocessing for Heterogeneous System-on-Chip", "abstract": "This paper presents a methodology for simultaneous heterogeneous computing, named ENEAC, where a quad core ARM Cortex-A53 CPU works in tandem with a preprogrammed on-board FPGA accelerator. A heterogeneous scheduler distributes the tasks optimally among all the resources and all compute units run asynchronously, which allows for improved performance for irregular workloads. ENEAC achieves up to 17\\% performance improvement \\ignore{and 14\\% energy usage reduction,} when using all platform resources compared to just using the FPGA accelerators and up to 865\\% performance increase \\ignore{and up to 89\\% energy usage decrease} when using just the CPU. The workflow uses existing commercial tools and C/C++ as a single programming language for both accelerator design and CPU programming for improved productivity and ease of verification.", "venue": "ArXiv", "authors": ["Kris  Nikov", "Mohammad  Hosseinabady", "Rafael  Asenjo", "Andr\u00e9s  Rodr\u00edguez", "Angeles G. Navarro", "Jos\u00e9 L. N\u00fa\u00f1ez-Y\u00e1\u00f1ez"], "year": 2020, "n_citations": 0}
{"id": 3374913, "s2_id": "8b4cfeeefca48f1e834ac73355706099b76db6b8", "title": "Enabling Reproducible Analysis of Complex Workflows on the Edge-to-Cloud Continuum", "abstract": "Distributed digital infrastructures for computation and analytics are now evolving towards an interconnected ecosystem allowing complex applications to be executed from IoT Edge devices to the HPC Cloud (aka the Computing Continuum, the Digital Continuum, or the Transcontinuum). Understanding end-to-end performance in such a complex continuum is challenging. This breaks down to reconciling many, typically contradicting application requirements and constraints with low-level infrastructure design choices. One important challenge is to accurately reproduce relevant behaviors of a given application workflow and representative settings of the physical infrastructure underlying this complex continuum. We introduce a rigorous methodology for such a process and validate it through E2Clab. It is the first platform to support the complete experimental cycle across the Computing Continuum: deployment, analysis, optimization. Preliminary results with real-life use cases show that E2Clab allows one to understand and improve performance, by correlating it to the parameter settings, the resource usage and the specifics of the underlying infrastructure.", "venue": "ArXiv", "authors": ["Daniel  Rosendo", "Alexandru  Costan", "Gabriel  Antoniu", "Patrick  Valduriez"], "year": 2021, "n_citations": 0}
{"id": 3375323, "s2_id": "ad49b232c04dcbd8eaec8e56d2955e50b0be3902", "title": "Datacenter Traffic Control: Understanding Techniques and Tradeoffs", "abstract": "Datacenters provide cost-effective and flexible access to scalable compute and storage resources necessary for today\u2019s cloud computing needs. A typical datacenter is made up of thousands of servers connected with a large network and usually managed by one operator. To provide quality access to the variety of applications and services hosted on datacenters and maximize performance, it deems necessary to use datacenter networks effectively and efficiently. Datacenter traffic is often a mix of several classes with different priorities and requirements. This includes user-generated interactive traffic, traffic with deadlines, and long-running traffic. To this end, custom transport protocols and traffic management techniques have been developed to improve datacenter network performance. In this tutorial paper, we review the general architecture of datacenter networks, various topologies proposed for them, their traffic properties, general traffic control challenges in datacenters and general traffic control objectives. The purpose of this paper is to bring out the important characteristics of traffic control in datacenters and not to survey all existing solutions (as it is virtually impossible due to massive body of existing research). We hope to provide readers with a wide range of options and factors while considering a variety of traffic control mechanisms. We discuss various characteristics of datacenter traffic control, including management schemes, transmission control, traffic shaping, prioritization, load balancing, multipathing, and traffic scheduling. Next, we point to several open challenges as well as new and interesting networking paradigms. At the end of this paper, we briefly review inter-datacenter networks that connect geographically dispersed datacenters, which have been receiving increasing attention recently and pose interesting and novel research problems. To measure the performance of datacenter networks, different performance metrics have been used, such as flow completion times, deadline miss rate, throughput, and fairness. Depending on the application and user requirements, some metrics may need more attention. While investigating different traffic control techniques, we point out the tradeoffs involved in terms of costs, complexity, and performance. We find that a combination of different traffic control techniques may be necessary at particular entities and layers in the network to improve the variety of performance metrics. We also find that despite significant research efforts, there are still open problems that demand further attention from the research community.", "venue": "IEEE Communications Surveys & Tutorials", "authors": ["Mohammad  Noormohammadpour", "Cauligi S. Raghavendra"], "year": 2018, "n_citations": 73}
{"id": 3377869, "s2_id": "683ba52e1d987303af0ac369bd9d38cb22667e28", "title": "Queuing Theoretic Models for Multicast and Coded-Caching in Downlink Wireless Systems", "abstract": "We consider a server connected to $L$ users over a shared finite capacity link. Each user is equipped with a cache. File requests at the users are generated as independent Poisson processes according to a popularity profile from a library of $M$ files. The server has access to all the files in the library. Users can store parts of the files or full files from the library in their local caches. The server should send missing parts of the files requested by the users. The server attempts to fulfill the pending requests with minimal transmissions exploiting multicasting and coding opportunities among the pending requests. We study the performance of this system in terms of queuing delays for the naive multicasting and several coded multicasting schemes proposed in the literature. We also provide approximate expressions for the mean queuing delay for these models and establish their effectiveness with simulations.", "venue": "ArXiv", "authors": ["Mahadesh  Panju", "Ramkumar  Raghu", "Vinod  Sharma", "Ramachandran  Rajesh"], "year": 2018, "n_citations": 3}
{"id": 3387394, "s2_id": "7c6e86641679d6d3de061a1365a2772e151afa1f", "title": "Rapid Exploration of Optimization Strategies on Advanced Architectures using TestSNAP and LAMMPS", "abstract": "The exascale race is at an end with the announcement of the Aurora and Frontier machines. This next generation of supercomputers utilize diverse hardware architectures to achieve their compute performance, providing an added onus on the performance portability of applications. An expanding fragmentation of programming models would provide a compounding optimization challenge were it not for the evolution of performance-portable frameworks, providing unified models for mapping abstract hierarchies of parallelism to diverse architectures. A solution to this challenge is the evolution of performance-portable frameworks, providing unified models for mapping abstract hierarchies of parallelism to diverse architectures. Kokkos is one such performance portable programming model for C++ applications, providing back-end implementations for each major HPC platform. Even with a performance portable framework, restructuring algorithms to expose higher degrees of parallelism is non-trivial. The Spectral Neighbor Analysis Potential (SNAP) is a machine-learned inter-atomic potential utilized in cutting-edge molecular dynamics simulations. Previous implementations of the SNAP calculation showed a downward trend in their performance relative to peak on newer-generation CPUs and low performance on GPUs. In this paper we describe the restructuring and optimization of SNAP as implemented in the Kokkos CUDA backend of the LAMMPS molecular dynamics package, benchmarked on NVIDIA GPUs. We identify novel patterns of hierarchical parallelism, facilitating a minimization of memory access overheads and pushing the implementation into a compute-saturated regime. Our implementation via Kokkos enables recompile-and-run efficiency on upcoming architectures. We find a $\\sim$22x time-to-solution improvement relative to an existing implementation as measured on an NVIDIA Tesla V100-16GB for an important benchmark.", "venue": "ArXiv", "authors": ["Rahulkumar  Gayatri", "Stan  Moore", "Evan  Weinberg", "Nicholas  Lubbers", "Sarah  Anderson", "Jack  Deslippe", "Danny  Perez", "Aidan P. Thompson"], "year": 2020, "n_citations": 4}
{"id": 3390412, "s2_id": "726b9b981e0ce4db74fe44d68ca7ce2c7e5235cf", "title": "Performance Benefits of DataMPI: A Case Study with BigDataBench", "abstract": "Apache Hadoop and Spark are gaining prominence in Big Data processing and analytics. Both of them are widely deployed in Internet companies. On the other hand, high-performance data analysis requirements are causing academical and industrial communities to adopt state-of-the-art technologies in HPC to solve Big Data problems. Recently, we have proposed a key-value pair based communication library, DataMPI, which is extending MPI to support Hadoop/Spark-like Big Data Computing jobs. In this paper, we use BigDataBench, a Big Data benchmark suite, to do comprehensive studies on performance and resource utilization characterizations of Hadoop, Spark and DataMPI. From our experiments, we observe that the job execution time of DataMPI has up to 57 % and 50 % speedups compared with those of Hadoop and Spark, respectively. Most of the benefits come from the high-efficiency communication mechanisms in DataMPI. We also notice that the resource (CPU, memory, disk and network I/O) utilizations of DataMPI are also more efficient than those of the other two frameworks.", "venue": "BPOE@ASPLOS/VLDB", "authors": ["Fan  Liang", "Chen  Feng", "Xiaoyi  Lu", "Zhiwei  Xu"], "year": 2014, "n_citations": 37}
{"id": 3390558, "s2_id": "48d098b4d956dfe48313985b8ce62fe630fe070a", "title": "Productivity, portability, performance: data-centric Python", "abstract": "Python has become the de facto language for scientific computing. Programming in Python is highly productive, mainly due to its rich science-oriented software ecosystem built around the NumPy module. As a result, the demand for Python support in High Performance Computing (HPC) has skyrocketed. However, the Python language itself does not necessarily offer high performance. In this work, we present a workflow that retains Python's high productivity while achieving portable performance across different architectures. The workflow's key features are HPC-oriented language extensions and a set of automatic optimizations powered by a data-centric intermediate representation. We show performance results and scaling across CPU, GPU, FPGA, and the Piz Daint supercomputer (up to 23,328 cores), with 2.47x and 3.75x speedups over previous-best solutions, first-ever Xilinx and Intel FPGA results of annotated Python, and up to 93.16% scaling efficiency on 512 nodes.", "venue": "SC", "authors": ["Alexandros Nikolaos Ziogas", "Timo  Schneider", "Tal  Ben-Nun", "Alexandru  Calotoiu", "Tiziano De Matteis", "Johannes de Fine Licht", "Luca  Lavarini", "Torsten  Hoefler"], "year": 2021, "n_citations": 0}
{"id": 3391710, "s2_id": "493305b5b3529cb338a75920ceb93ec822eb47af", "title": "Exploring Oracle RDBMS latches using Solaris DTrace", "abstract": "Rise of hundreds cores technologies bring again to the first plan the problem of interprocess synchronization in database engines. Spinlocks are widely used in contemporary DBMS to synchronize processes at microsecond timescale. Latches are Oracle RDBMS specific spinlocks. The latch contention is common to observe in contemporary high concurrency OLTP environments. \nIn contrast to system spinlocks used in operating systems kernels, latches work in user context. Such user level spinlocks are influenced by context preemption and multitasking. Until recently there were no direct methods to measure effectiveness of user spinlocks. This became possible with the emergence of Solaris 10 Dynamic Tracing framework. DTrace allows tracing and profiling both OS and user applications. \nThis work investigates the possibilities to diagnose and tune Oracle latches. It explores the contemporary latch realization and spinning-blocking strategies, analyses corresponding statistic counters. \nA mathematical model developed to estimate analytically the effect of tuning _SPIN_COUNT value.", "venue": "ArXiv", "authors": ["Andrey  Nikolaev"], "year": 2011, "n_citations": 2}
{"id": 3391792, "s2_id": "eaca5bc1dde94e548f24f427553d17a020fbbd3b", "title": "Minimizing Total Busy Time for Energy-Aware Virtual Machine Allocation Problems", "abstract": "This paper investigates the energy-aware virtual machine (VM) allocation problems in clouds along characteristics: multiple resources, fixed interval time and non-preemption of virtual machines. Many previous works have been proposed to use a minimum number of physical machines; however, this is not necessarily a good solution to minimize total energy consumption in the VM placement with multiple resources, fixed interval time and non-preemption. We observed that minimizing the sum of total busy time of all physical machines implies minimizing total energy consumption of physical machines. In addition to, if mapping of a VM onto physical machines have the same total busy time then the best mapping has physical machine's remaining available resource minimizing. Based on these observations, we proposed heuristic-based EM algorithm to solve the energy-aware VM allocation with fixed starting time and duration time. In addition, this work studies some heuristics for sorting the list of virtual machines (e.g., sorting by the earliest starting time, or latest finishing time, or the longest duration time first, etc.) to allocate VM. We evaluate the EM using CloudSim toolkit and jobs log-traces in the Feitelson's Parallel Workloads Archive. Simulation's results show that all of EM-ST, EM-LFT and EM-LDTF algorithms could reduce total energy consumption compared to state-of-the-art of power-aware VM allocation algorithms. (e.g. Power-Aware Best-Fit Decreasing (PABFD) [7])).", "venue": "SoICT", "authors": ["Nguyen  Quang-Hung", "Nam  Thoai"], "year": 2015, "n_citations": 2}
{"id": 3391928, "s2_id": "b2e925e3a5ff9d98f8bf9b8354c2d6c45cc2ca8f", "title": "Infrastructure for Artificial Intelligence, Quantum and High Performance Computing", "abstract": "High Performance Computing (HPC), Artificial Intelligence (AI)/Machine Learning (ML), and Quantum Computing (QC) and communications offer immense opportunities for innovation and impact on society. Researchers in these areas depend on access to computing infrastructure, but these resources are in short supply and are typically siloed in support of their research communities, making it more difficult to pursue convergent and interdisciplinary research. Such research increasingly depends on complex workflows that require different resources for each stage. This paper argues that a more-holistic approach to computing infrastructure, one that recognizes both the convergence of some capabilities and the complementary capabilities from new computing approaches, be it commercial cloud to Quantum Computing, is needed to support computer science research.", "venue": "ArXiv", "authors": ["William  Gropp", "Sujata  Banerjee", "Ian  Foster"], "year": 2020, "n_citations": 0}
{"id": 3392675, "s2_id": "8b37dd944fc8be4cc4e1d400bb0f1a1d69c12ee6", "title": "Study on State-of-the-art Cloud Services Integration Capabilities with Autonomous Ground Vehicles", "abstract": "Computing and intelligence are substantial requirements for the accurate performance of autonomous ground vehicles (AGVs). In this context, the use of cloud services in addition to onboard computers enhances computing and intelligence capabilities of AGVs. In addition, the vast amount of data processed in a cloud system contributes to overall performance and capabilities of the onboard system. This research study entails a qualitative analysis to gather insights on the applicability of the leading cloud service providers in AGV operations. These services include Google Cloud, Microsoft Azure, Amazon AWS, and IBM Cloud. The study begins with a brief review of AGV technical requirements that are necessary to determine the rationale for identifying the most suitable cloud service. The qualitative analysis studies and addresses the applicability of the cloud service over the proposed generalized AGV's architecture integration, performance, and manageability. Our findings conclude that a generalized AGV architecture can be supported by state-of-the-art cloud service, but there should be a clear line of separation between the primary and secondary computing needs. Moreover, our results show significant lags while using cloud services and preventing their use in real-time AGV operation.", "venue": "2018 IEEE 88th Vehicular Technology Conference (VTC-Fall)", "authors": ["Praveen  Damacharla", "Dhwani  Mehta", "Ahmad Y. Javaid", "Vijay K. Devabhaktuni"], "year": 2018, "n_citations": 4}
{"id": 3393619, "s2_id": "a638e3cdb9e84c17ab5113c4adefb91e23666083", "title": "Evolutionary Optimisation of Real-Time Systems and Networks", "abstract": "The design space of networked embedded systems is very large, posing challenges to the optimisation of such platforms when it comes to support applications with real-time guarantees. Recent research has shown that a number of inter-related optimisation problems have a critical influence over the schedulability of a system, i.e. whether all its application components can execute and communicate by their respective deadlines. Examples of such optimization problems include task allocation and scheduling, communication routing and arbitration, memory allocation, and voltage and frequency scaling. In this paper, we advocate the use of evolutionary approaches to address such optimization problems, aiming to evolve individuals of increased fitness over multiple generations of potential solutions. We refer to plentiful evidence that existing real-time schedulability tests can be used effectively to guide evolutionary optimisation, either by themselves or in combination with other metrics such as energy dissipation or hardware overheads. We then push that concept one step further and consider the possibility of using evolutionary techniques to evolve the schedulability tests themselves, aiming to support the verification and optimisation of systems which are too complex for state-of-the-art (manual) derivation of schedulability tests.", "venue": "ArXiv", "authors": ["Leandro Soares Indrusiak", "Robert I. Davis", "Piotr  Dziurzanski"], "year": 2019, "n_citations": 2}
{"id": 3395994, "s2_id": "136519c487c7b0448a54878d895a9eecc8f78352", "title": "Collecting and Presenting Reproducible Intranode Stencil Performance: INSPECT", "abstract": "Stencil algorithms have been receiving considerable interest in HPC research for decades. The techniques used to approach multi-core stencil performance modeling and engineering span basic runtime measurements, elaborate performance models, detailed hardware counter analysis, and thorough scaling behavior evaluation. Due to the plurality of approaches and stencil patterns, we set out to develop a generalizable methodology for reproducible measurements accompanied by state-of-the-art performance models. Our open-source toolchain, and collected results are publicly available in the \"Intranode Stencil Performance Evaluation Collection\" (INSPECT). We present the underlying methodologies, models and tools involved in gathering and documenting the performance behavior of a collection of typical stencil patterns across multiple architectures and hardware configuration options. Our aim is to endow performance-aware application developers with reproducible baseline performance data and validated models to initiate a well-defined process of performance assessment and optimization.", "venue": "Supercomput. Front. Innov.", "authors": ["Julian  Hornich", "Julian  Hammer", "Georg  Hager", "Thomas  Gruber", "Gerhard  Wellein"], "year": 2019, "n_citations": 4}
{"id": 3396012, "s2_id": "a2bebb842ed27e89b0f3daeaf6f057b78191dfa4", "title": "SplitFS: reducing software overhead in file systems for persistent memory", "abstract": "We present SplitFS, a file system for persistent memory (PM) that reduces software overhead significantly compared to state-of-the-art PM file systems. SplitFS presents a novel split of responsibilities between a user-space library file system and an existing kernel PM file system. The user-space library file system handles data operations by intercepting POSIX calls, memory-mapping the underlying file, and serving the read and overwrites using processor loads and stores. Metadata operations are handled by the kernel PM file system (ext4 DAX). SplitFS introduces a new primitive termed relink to efficiently support file appends and atomic data operations. SplitFS provides three consistency modes, which different applications can choose from, without interfering with each other. SplitFS reduces software overhead by up-to 4x compared to the NOVA PM file system, and 17x compared to ext4 DAX. On a number of micro-benchmarks and applications such as the LevelDB key-value store running the YCSB benchmark, SplitFS increases application performance by up to 2x compared to ext4 DAX and NOVA while providing similar consistency guarantees.", "venue": "SOSP", "authors": ["Rohan  Kadekodi", "Se Kwon Lee", "Sanidhya  Kashyap", "Taesoo  Kim", "Aasheesh  Kolli", "Vijay  Chidambaram"], "year": 2019, "n_citations": 60}
{"id": 3405521, "s2_id": "815fb19f947816b79220cbb01df50a39cb9fbba6", "title": "HS06 Benchmark for an ARM Server", "abstract": "We benchmarked an ARM cortex-A9 based server system with a four-core CPU running at 1.1 GHz. The system used Ubuntu 12.04 as operating system and the HEPSPEC 2006 (HS06) benchmarking suite was compiled natively with gcc-4.4 on the system. The benchmark was run for various settings of the relevant gcc compiler options. We did not find significant influence from the compiler options on the benchmark result. The final HS06 benchmark result is 10.4.", "venue": "ArXiv", "authors": ["Stefan  Kluth"], "year": 2013, "n_citations": 0}
{"id": 3406859, "s2_id": "cb449c58779129789751f8f7af19eefacd148f3d", "title": "A matrix math facility for Power ISA(TM) processors", "abstract": "Power ISA(TM) Version 3.1 has introduced a new family of matrix math instructions, collectively known as the Matrix-Multiply Assist (MMA) facility. The instructions in this facility implement numerical linear algebra operations on small matrices and are meant to accelerate computation-intensive kernels, such as matrix multiplication, convolution and discrete Fourier transform. These instructions have led to a power- and area-efficient implementation of a high throughput math engine in the future POWER10 processor. Performance per core is 4 times better, at constant frequency, than the previous generation POWER9 processor. We also advocate the use of compiler built-ins as the preferred way of leveraging these instructions, which we illustrate through case studies covering matrix multiplication and convolution.", "venue": "ArXiv", "authors": ["Jos\u00e9 E. Moreira", "Kit  Barton", "Steven  Battle", "Peter  Bergner", "Ramon Bertran Monfort", "Puneeth  Bhat", "Pedro  Caldeira", "David  Edelsohn", "Gordon  Fossum", "Brad  Frey", "Nemanja  Ivanovic", "Chip  Kerchner", "Vincent  Lim", "Shakti  Kapoor", "Tulio Machado Filho", "Silvia Melitta Mueller", "Brett  Olsson", "Satish  Sadasivam", "Baptiste  Saleil", "Bill  Schmidt", "Rajalakshmi  Srinivasaraghavan", "Shricharan  Srivatsan", "Brian W. Thompto", "Andreas  Wagner", "Nelson  Wu"], "year": 2021, "n_citations": 1}
{"id": 3412434, "s2_id": "4ea4dd4a781300e18483a89c19120a1ad1c88a4e", "title": "Comparing the behavior of OpenMP Implementations with various Applications on two different Fujitsu A64FX platforms", "abstract": "The development of the A64FX processor by Fujitsu has been a massive innovation in vectorized processors and led to Fugaku: the current world\u2019s fastest supercomputer. We use a variety of tools to analyze the behavior and performance of several OpenMP applications with different compilers, and how these applications scale on the different A64FX processors on clusters at Stony Brook University and RIKEN.", "venue": "PEARC", "authors": ["Benjamin  Michalowicz", "Eric  Raut", "Yan  Kang", "Tony  Curtis", "Barbara  Chapman", "Dossay  Oryspayev"], "year": 2021, "n_citations": 3}
{"id": 3413854, "s2_id": "a6dd31a1fcf0a4f0e6b896b16b91f41116bf26aa", "title": "RL-QN: A Reinforcement Learning Framework for Optimal Control of Queueing Systems", "abstract": "With the rapid advance of information technology, network systems have become increasingly complex and hence the underlying system dynamics are often unknown or difficult to characterize. Finding a good network control policy is of significant importance to achieve desirable network performance (e.g., high throughput or low delay). In this work, we consider using model-based reinforcement learning (RL) to learn the optimal control policy for queueing networks so that the average job delay (or equivalently the average queue backlog) is minimized. Traditional approaches in RL, however, cannot handle the unbounded state spaces of the network control problem. To overcome this difficulty, we propose a new algorithm, called Reinforcement Learning for Queueing Networks (RL-QN), which applies model-based RL methods over a finite subset of the state space, while applying a known stabilizing policy for the rest of the states. We establish that the average queue backlog under RL-QN with an appropriately constructed subset can be arbitrarily close to the optimal result. We evaluate RL-QN in dynamic server allocation, routing and switching problems. Simulation results show that RL-QN minimizes the average queue backlog effectively.", "venue": "ArXiv", "authors": ["Bai  Liu", "Qiaomin  Xie", "Eytan  Modiano"], "year": 2020, "n_citations": 1}
{"id": 3415029, "s2_id": "0aa30fd430b39f54f4a3aefe07a06273e31b7269", "title": "Efficient analysis of caching strategies under dynamic content popularity", "abstract": "In this paper we develop a novel technique to analyze both isolated and interconnected caches operating under different caching strategies and realistic traffic conditions. The main strength of our approach is the ability to consider dynamic contents which are constantly added into the system catalogue, and whose popularity evolves over time according to desired profiles. We do so while preserving the simplicity and computational efficiency of models developed under stationary popularity conditions, which are needed to analyze several caching strategies. Our main achievement is to show that the impact of content popularity dynamics on cache performance can be effectively captured into an analytical model based on a fixed content catalogue (i.e., a catalogue whose size and objects' popularity do not change over time).", "venue": "2015 IEEE Conference on Computer Communications (INFOCOM)", "authors": ["Michele  Garetto", "Emilio  Leonardi", "Stefano  Traverso"], "year": 2015, "n_citations": 64}
{"id": 3415258, "s2_id": "c67cee5f371268e5407e7c3906aa7e985cb0862d", "title": "Introducing Fast and Secure Deterministic Stash Free Write Only Oblivious RAMs for Demand Paging in Keystone", "abstract": "Keystone is a trusted execution environment, based on RISC-V architecture. It divides the memory into a secure Keystone private memory and an unsecure non-Keystone memory, and allows code that lies inside the Keystone private memory to execute securely. Simple demand paging in Keystone ends up leaking sensitive access patterns of Keystone application to the Operating System(OS), that is assumed to be malicious. This is because, to access the unsecure non-Keystone memory, Keystone needs support of the OS. To mitigate this, Keystone needs to implement oblivious demand paging while obfuscating its page access patterns by using Oblivious RAM(ORAM) techniques. This causes substantial slowdown in the application execution. In this paper, we bridge the performance gap between application execution time with unsecure and secure demand paging in Keystone by using Deterministic, stash free, Write only ORAM (DetWoORAM) for oblivious demand paging. We also show why DetWoORAM, that is a write-only ORAM, is sufficient for oblivious demand paging. DetWoORAM logically partitions the memory into a main area and a holding area. The actual pages are stored in main area. We propose two enhancements over DetWoORAM that improves the application execution slowdown. The first enhancement, which we call the Eager DetWoORAM, involves page preloading that exploits the deterministic access pattern of DetWoORAM, and tries to hide the ORAM latency. The second enhancement, which we call the Parallel DetWoORAM, involves spawning multiple threads and each thread performs a part of the DetWoORAM memory access algorithm. Compared to DetWoORAM that shows slowdown of [1.4x, 2x, and 3.24x], Eager DetWoORAM and Parallel DetWoORAM provide slowdown of [1.2x, 1.8x, and 3.2x] and [1.1x, 1.1x, and 1.4x], for k= 3, 7, and 15, respectively.", "venue": "ArXiv", "authors": ["Mriganka Shekhar Chakravarty", "Biswabandan  Panda"], "year": 2021, "n_citations": 0}
{"id": 3417910, "s2_id": "342487b583353d89e16c8bb89f2d5ecae681e7c1", "title": "Channel assignment in dense MC-MR wireless networks: Scaling laws and algorithms", "abstract": "We investigate optimal channel assignment algorithms that maximize per node throughput in dense multi-channel multi-radio (MC-MR) wireless networks. Specifically, we consider an MC-MR network where all nodes are within the transmission range of each other. This situation is encountered in many real-life settings such as students in a lecture hall, delegates attending a conference, or soldiers in a battlefield. In this scenario, we show that intelligent assignment of the available channels results in a significantly higher per node throughput. We first propose a class of channel assignment algorithms, parameterized by T (the number of transceivers per node), that can achieve \u0398(1/N1/T) per node throughput using \u0398(TN1-1/T ) channels. In view of practical constraints on T, we then propose another algorithm that can achieve \u0398((1/log2n)2) per node throughput using only two transceivers per node. Finally, we identify a fundamental relationship between the achievable per node throughput, the total number of channels used, and the network size under any strategy. Using analysis and simulations, we show that our algorithms achieve close to optimal performance at different operating points on this curve. Our work has several interesting implications on the optimal network design for dense MC-MR wireless networks.", "venue": "2013 Proceedings IEEE INFOCOM", "authors": ["Rahul  Urgaonkar", "Ram  Ramanathan", "Jason  Redi", "William N. Tetteh"], "year": 2013, "n_citations": 9}
{"id": 3418919, "s2_id": "8e2b620becebd4023950668254eeeb5585cd267c", "title": "Effect of Human Learning on the Transient Performance of Cloud-based Tiered Applications", "abstract": "Cloud based tiered applications are increasingly becoming popular, be it on phones or on desktops. End users of these applications range from novice to expert depending on how experienced they are in using them. With repeated usage (practice) of an application, a user's think time gradually decreases, known as learning phenomenon. In contrast to the popular notion of constant mean think time of users across all practice sessions, decrease in mean think time over practice sessions does occur due to learning. This decrease gives rise to a different system workload thereby affecting the application's short-term performance. However, such impact of learning on performance has never been accounted for. In this work we propose a model that accounts for human learning behavior in analyzing the transient (short-term) performance of a 3-tier cloud based application. Our approach is based on a closed queueing network model. We solve the model using discrete event simulation. In addition to the overall mean System Response Time (SRT), our model solution also generates the mean SRTs for various types (novice, intermediate, expert) of requests submitted by users at various levels of their expertise. We demonstrate that our model can be used to evaluate various what-if scenarios to decide the number of VMs we need for each tier-a VM configuration-that would meet the response time SLA. The results show that the lack of accountability of learning may lead to a selection of an inappropriate VM configuration. The results further show that the mean SRTs for various types of requests are better measures to consider in VM allocation process in comparison to the overall mean SRT.", "venue": "ArXiv", "authors": ["Arindam  Das", "Olivia  Das"], "year": 2016, "n_citations": 0}
{"id": 3420201, "s2_id": "0c5345500ff34046bbde06eb7b90d575c9e9408a", "title": "Towards Demystifying Intra-Function Parallelism in Serverless Computing", "abstract": "Serverless computing offers a pay-per-use model with high elasticity and automatic scaling for a wide range of applications. Since cloud providers abstract most of the underlying infrastructure, these services work similarly to black-boxes. As a result, users can influence the resources allocated to their functions, but might not be aware that they have to parallelize them to profit from the additionally allocated virtual CPUs (vCPUs). In this paper, we analyze the impact of parallelization within a single function and container instance for AWS Lambda, Google Cloud Functions (GCF), and Google Cloud Run (GCR). We focus on compute-intensive workloads since they benefit greatly from parallelization. Furthermore, we investigate the correlation between the number of allocated CPU cores and vCPUs in serverless environments. Our results show that the number of available cores to a function/container instance does not always equal the number of allocated vCPUs. By parallelizing serverless workloads, we observed cost savings up to 81% for AWS Lambda, 49% for GCF, and 69.8% for GCR.", "venue": "WOSC@Middleware", "authors": ["Michael  Kiener", "Mohak  Chadha", "Michael  Gerndt"], "year": 2021, "n_citations": 1}
{"id": 3425576, "s2_id": "aa94d0b0fe1ae925a592de27548636b06074db55", "title": "EMPIOT: An Energy Measurement Platform for Wireless IoT Devices", "abstract": "Profiling and minimizing the energy consumption of IoT devices is an essential step towards employing IoT in various application domains. In this paper we propose EMPIOT, an accurate, low-cost, easy to build, and flexible, power measurement platform. We present the hardware and software components of this platform, and study the effect of various design parameters on accuracy. In particular, we analyze the effect of driver, bus speed, input voltage, and buffering mechanism, on sampling rate, measurement accuracy and processing demand. These extensive experimental studies enable us to configure the system in order to achieve its highest performance. We also propose a novel calibration technique and report the calibration parameters under various settings. Using five different IoT devices performing four types of workloads, we evaluate the performance of EMPIOT against the ground truth obtained from high-accuracy devices. Our results show that, for very low-power devices that utilize 802.15.4 wireless standard, measurement error is less than 4%. In addition, for 802.11-based devices that generate short and high power spikes, error is less than 3%.", "venue": "J. Netw. Comput. Appl.", "authors": ["Behnam  Dezfouli", "Immanuel  Amirtharaj", "Chia-Chi  Li"], "year": 2018, "n_citations": 44}
{"id": 3429620, "s2_id": "475ef64765d3f589e4c9814f050e9772f5b011fc", "title": "Scale MLPerf-0.6 models on Google TPU-v3 Pods", "abstract": "The recent submission of Google TPU-v3 Pods to the industry wide MLPerf v0.6 training benchmark demonstrates the scalability of a suite of industry relevant ML models. MLPerf defines a suite of models, datasets and rules to follow when benchmarking to ensure results are comparable across hardware, frameworks and companies. Using this suite of models, we discuss the optimizations and techniques including choice of optimizer, spatial partitioning and weight update sharding necessary to scale to 1024 TPU chips. Furthermore, we identify properties of models that make scaling them challenging, such as limited data parallelism and unscaled weights. These optimizations contribute to record performance in transformer, Resnet-50 and SSD in the Google MLPerf-0.6 submission.", "venue": "ArXiv", "authors": ["Sameer  Kumar", "Victor  Bitorff", "Dehao  Chen", "Chiachen  Chou", "Blake  Hechtman", "HyoukJoong  Lee", "Naveen  Kumar", "Peter  Mattson", "Shibo  Wang", "Tao  Wang", "Yuanzhong  Xu", "Zongwei  Zhou"], "year": 2019, "n_citations": 24}
{"id": 3429899, "s2_id": "742c05a640bf63317a541b92422afa25547d81c4", "title": "Temporal vectorization for stencils", "abstract": "Stencil computations represent a very common class of nested loops in scientific and engineering applications. Exploiting vector units in modern CPUs is crucial to achieving peak performance. Previous vectorization approaches often consider the data space, in particular the innermost unit-strided loop. It leads to the well-known data alignment conflict problem that vector loads are overlapped due to the data sharing between continuous stencil computations. This paper proposes a novel temporal vectorization scheme for stencils. It vectorizes the stencil computation in the iteration space and assembles points with different time coordinates in one vector. The temporal vectorization leads to a small fixed number of vector reorganizations that is irrelevant to the vector length, stencil order, and dimension. Furthermore, it is also applicable to Gauss-Seidel stencils, whose vectorization is not well-studied. The effectiveness of the temporal vectorization is demonstrated by various Jacobi and Gauss-Seidel stencils.", "venue": "SC", "authors": ["Liang  Yuan", "Hang  Cao", "Yunquan  Zhang", "Kun  Li", "Pengqi  Lu", "Yue  Yue"], "year": 2021, "n_citations": 0}
{"id": 3430216, "s2_id": "680f70767d33c48abc60006b87c5a21f3cbaa28b", "title": "Integrating Queuing Regime into Cognitive Radio Channel Aggregation Policies: A Performance Evaluation", "abstract": "Channel aggregation (CA) is one of the newest concept which cognitive radio network is bringing to bear for the smooth role out of fifth/next generation wireless networks. This is the combining of several unused primary user spectrum holes into a logic usable channel. However, several of these strategies have been investigated considering the varying nature of wireless link and adaptive modulation and coding (AMC). Examples are the instant blocking strategy (IBS) and readjustment based strategy (RBS). This paper develops and compares two CA policies with queue, which are the IBS with queue (IBS + Q), and the RBS with queue (RBS+Q). This is in furtherance of previous proposed work. The aim is to identifying the impact of a queuing regime on the performance of the secondary network such that any secondary user (SU) that has not completed its service, as an alternative to dropping or forcibly terminating the service, it is queued in order to get another opportunity to access the primary user (PU) channels. The performance is evaluated through a simulation framework. The results validate that with a welldesigned queuing regime, capacity, access and other metrics can be improved with significant reduction in blocking and forced termination probabilities respectively.", "venue": "ArXiv", "authors": ["Ebenezer  Esenogho", "Elie Ngomseu Mambou"], "year": 2017, "n_citations": 1}
{"id": 3430464, "s2_id": "f628077ddf8fc136db2103815050daf6ce73e273", "title": "Impact of GPU uncertainty on the training of predictive deep neural networks", "abstract": "retracted] We found out that the difference was dependent on the Chainer library, and does not replicate with another library (PyTorch) which indicates that the results are probably due to a bug in Chainer, rather than being hardware-dependent. \u2013 old abstract Deep neural networks often present uncertainties such as hardwareand software-derived noise and randomness. We studied the effects of such uncertainty on learning outcomes, with a particular focus on the function of graphics processing units (GPUs), and found that GPU-induced uncertainty increased learning accuracy of a certain deep neural network. When training a predictive deep neural network using only the CPU without the GPU, the learning error is higher than when training the same number of epochs using the GPU, suggesting that the GPU plays a different role in the learning process than just increasing the computational speed. Because this effect cannot be observed in learning by a simple autoencoder, it could be a phenomenon specific to certain types of neural networks. GPU-specific computational processing is more indeterminate than that by CPUs, and hardware-derived uncertainties, which are often considered obstacles that need to be eliminated, might, in some cases, be successfully incorporated into the training of deep neural networks. Moreover, such uncertainties might be interesting phenomena to consider in brain-related computational processing, which comprises a large mass of uncertain signals. 1 SPECIAL NOTE We followed up the experiments we had been doing with the Chainer version of PredNet with a PredNet written in PyTorch. In particular, we carefully examined the difference between CPU and \u2217also affiliated to Department of Basic Biology, The Graduate University for Advanced Studies (SOKENDAI), Miura, Kanagawa 240-0193 1 ar X iv :2 10 9. 01 45 1v 3 [ cs .L G ] 2 5 Se p 20 21 GPU, which is the most impactful data in this paper. Then, we found that the PyTorch version did not show the dramatic difference between CPU and GPU (Supplemental Figure 7: Fig. 15) . Although the cause of the differences between the Chainer and PyTorch versions is not clear, we speculate that the CPU-GPU differences observed in the Chainer version may be due to some bug in the Chainer version. The initial value dependency, cuDNN dependency, and minor hardware dependency of the training results were reproduced in the PyTorch version. However, since the generality of the data, which we consider to be the most important part of this manuscript, has been lost, we have decided to retract this manuscript after discussion among the co-authors. We would like to thank the Internet community for their helpful discussions. The PyTorch version used for the test is available at the following URL: https://github.com/eijwat/prednet_in_pytorch 2 INTRODUCTION Large systems often present uncertainties such as hardwareand software-derived noise and randomness, which designers do not prefer. Deep neural networks, which have recently been used in the development of artificial intelligence (AI), are no exception. The randomness associated with the initial node-weight values and the nondeterminism of computational processes involving graphics processing units (GPUs) (Nagarajan et al., 2018) (e.g., float operations (Morin & Willetts, 2020) and matrix computations when optimizing computational speed (Pham et al., 2020)) are related to the uncertainty of training results from deep neural networks and often destabilize the learning results and hinder reproducibility. However, uncertainty does not have only negative effects on learning outcomes. For example, the dropout method randomly introduces unlearned nodes during the training process while suppressing overfitting during network learning (Srivastava et al., 2014). Alternatively, randomly initialized feed-forward networks can be used for efficient learning by neural networks Frankle & Carbin (2018). Although a theoretical understanding of the effects of uncertainty has not been achieved, investigating the effect of uncertainty on neural networks is beneficial for further development of AI. Therefore, in this study, we investigated the effect of uncertainty, with a specific focus on uncertainties associated with GPU processing, on network training using an autoencoder (Hinton & Salakhutdinov, 2006) and predictive deep neural networks (PredNet, Lotter et al. (2017)). 3 METHODS 3.1 TRAINING An autoencoder and PredNet were used in this study (Supplemental Figure 1: Fig. 9), with both networks trained using an image dataset. After training, the networks output appropriate images (y) against input images (x). The labeling data associated with the images were not used for training, i.e., unsupervised learning was used. For the autoencoder, the training progressed such that one image was input, followed by output of the exact same image. For the PredNet, a sequential video image was used as input, followed by learning and subsequent output of an image relative to the input image. PredNet is based on a hypothesis used to explain the operation of the cerebral cortex (predictive coding, Rao & Ballard (1999)). The neural network trained for prediction emulates the diverse features of biological neurons (Lotter et al., 2020) and perception, including visual illusions (Watanabe et al., 2018; Lotter et al., 2020). The autoencoder and PredNet were both written in Chainer (Tokui et al., 2015) and embedded into interface code (https://doi.org/10.6084/m9. figshare.12318953 and https://doi.org/10.6084/m9.figshare.5483710, respectively) that was also used in a previous experiment (Watanabe et al., 2018). The autoencoder was trained using the Modified National Institute of Standards and Technology (MNIST) dataset comprising handwritten digits (LeCun & Cortes, 2010). A single image (224 \u00d7 224 pixels, Supplemental Figure 2: Fig. 10] was backpropagated as a single batch, with a total of 5000 batches performed for training using 5000 images. PredNet was trained with the first-person social interactions (FPSI) dataset (Fathi et al., 2012), which contains day-long self-view motion videos of subjects at the Disney World Resort in Orlando, Florida. Image size was 160 \u00d7 120 pixels (Supplemental Figure 3: Fig. 11). Twenty consecutive images were backpropagated as a single", "venue": "ArXiv", "authors": ["Maciej  Pietrowski", "Andrzej  Gajda", "Takuto  Yamamoto", "Taisuke  Kobayashi", "Lana  Sinapayen", "Eiji  Watanabe"], "year": 2021, "n_citations": 0}
{"id": 3431207, "s2_id": "a9ae1dd2fb261751f033e52970de208afc2ce582", "title": "Evaluation of Time-Critical Communications for IEC 61850-Substation Network Architecture", "abstract": "Present-day developments, in electrical power transmission and distribution, require considerations of the status quo. In other meaning, international regulations enforce increasing of reliability and reducing of environment impact, correspondingly they motivate developing of dependable systems. Power grids especially intelligent (smart grids) ones become industrial solutions that follow standardized development. The International standardization, in the field of power transmission and distribution, improve technology influences. The rise of dedicated standards for SAS (Substation Automation Systems) communications, such as the leading International Electro-technical Commission standard IEC 61850, enforces modern technological trends in this field. Within this standard, a constraint of low ETE (End-to-End) latency should be respected, and time-critical status transmission must be achieved. This experimental study emphasis on IEC 61850 SAS communication standard, e.g. IEC 61850 GOOSE (Generic Object Oriented Substation Events), to implement an investigational method to determine the protection communication delay. This method observes GOOSE behaviour by adopting monitoring and analysis capabilities. It is observed by using network test equipment, i.e. SPAN (Switch Port Analyser) and TAP (Test Access Point) devices, with on-the-shelf available hardware and software solutions.", "venue": "ArXiv", "authors": ["Ahmed  Altaher", "St\u00e9phane  Mocanu", "Jean-Marc  Thiriet"], "year": 2015, "n_citations": 3}
{"id": 3432554, "s2_id": "3a2efa9fb9ad9d2f2dc6cfbd5f8bb37cb69e2276", "title": "Gadget3 on GPUs with OpenACC", "abstract": "We present preliminary results of a GPU porting of all main Gadget3 modules (gravity computation, SPH density computation, SPH hydrodynamic force, and thermal conduction) using OpenACC directives. Here we assign one GPU to each MPI rank and exploit both the host and accellerator capabilities by overlapping computations on the CPUs and GPUs: while GPUs asynchronously compute interactions between particles within their MPI ranks, CPUs perform tree-walks and MPI communications of neighbouring particles. We profile various portions of the code to understand the origin of our speedup, where we find that a peak speedup is not achieved because of time-steps with few active particles. We run a hydrodynamic cosmological simulation from the Magneticum project, with $2\\cdot10^{7}$ particles, where we find a final total speedup of $\\approx 2.$ We also present the results of an encouraging scaling test of a preliminary gravity-only OpenACC porting, run in the context of the EuroHack17 event, where the prototype of the porting proved to keep a constant speedup up to $1024$ GPUs.", "venue": "PARCO", "authors": ["Antonio  Ragagnin", "Klaus  Dolag", "Mathias  Wagner", "Claudio  Gheller", "Conradin  Roffler", "David  Goz", "David  Hubber", "Alexander  Arth"], "year": 2019, "n_citations": 1}
{"id": 3432804, "s2_id": "c9c72500eac3c89aa2d458071f869a06b556777d", "title": "Optimizing the Linear Fascicle Evaluation Algorithm for Multi-core and Many-core Systems", "abstract": "Sparse matrix-vector multiplication (SpMV) operations are commonly used in various scientific and engineering applications. The performance of the SpMV operation often depends on exploiting regularity patterns in the matrix. Various representations and optimization techniques have been proposed to minimize the memory bandwidth bottleneck arising from the irregular memory access pattern involved. Among recent representation techniques, tensor decomposition is a popular one used for very large but sparse matrices. Post sparse-tensor decomposition, the new representation involves indirect accesses, making it challenging to optimize for multi-cores and even more demanding for the massively parallel architectures, such as on GPUs. Computational neuroscience algorithms often involve sparse datasets while still performing long-running computations on them. The Linear Fascicle Evaluation (LiFE) application is a popular neuroscience algorithm used for pruning brain connectivity graphs. The datasets employed herein involve the Sparse Tucker Decomposition (STD)\u2014a widely used tensor decomposition method. Using this decomposition leads to multiple indirect array references, making it very difficult to optimize on both multi-core and many-core systems. Recent implementations of the LiFE algorithm show that its SpMV operations are the key bottleneck for performance and scaling. In this work, we first propose target-independent optimizations to optimize the SpMV operations of LiFE decomposed using the STD technique, followed by target-dependent optimizations for CPU and GPU systems. The target-independent techniques include: (1) standard compiler optimizations to prevent unnecessary and redundant computations, (2) data restructuring techniques to minimize the effects of indirect array accesses, and (3) methods to partition computations among threads to obtain coarse-grained parallelism with low synchronization overhead. Then, we present the target-dependent optimizations for CPUs such as: (1) efficient synchronization-free thread mapping and (2) utilizing BLAS calls to exploit hardware-specific speed. Following that, we present various GPU-specific optimizations to optimally map threads at the granularity of warps, thread blocks, and grid. Furthermore, to automate the CPU-based optimizations developed for this algorithm, we also extend the PolyMage domain-specific language, embedded in Python. Our highly optimized and parallelized CPU implementation obtains a speedup of 6.3\u00d7 over the naive parallel CPU implementation running on 16-core Intel Xeon Silver (Skylake-based) system. In addition to that, our optimized GPU implementation achieves a speedup of 5.2\u00d7 over a reference-optimized GPU code version on NVIDIA\u2019s GeForce RTX 2080 Ti GPU and a speedup of 9.7\u00d7 over our highly optimized and parallelized CPU implementation.", "venue": "ACM Trans. Parallel Comput.", "authors": ["Karan  Aggarwal", "Uday  Bondhugula", "Varsha  Sreenivasan", "Devarajan  Sridharan"], "year": 2020, "n_citations": 1}
{"id": 3432850, "s2_id": "2c1e3f09d20368e71cf7d2d4c6fb33340247c125", "title": "Understanding the paradoxical effects of power control on the capacity of wireless networks", "abstract": "Recent works show conflicting results: network capacity may increase or decrease with higher transmission power under different scenarios. In this work, we want to understand this paradox. Specifically, we address the following questions: (1)Theoretically, should we increase or decrease transmission power to maximize network capacity? (2) Theoretically, how much network capacity gain can we achieve by power control? (3) Under realistic situations, how do power control, link scheduling and routing interact with each other? Under which scenarios can we expect a large capacity gain by using higher transmission power? To answer these questions, firstly, we prove that the optimal network capacity is a non-decreasing function of transmission power. Secondly, we prove that the optimal network capacity can be increased unlimitedly by higher transmission power in some network configurations. However, when nodes are distributed uniformly, the gain of optimal network capacity by higher transmission power is upper-bounded by a positive constant. Thirdly, we discuss why network capacity may increase or decrease with higher transmission power under different scenarios using carrier sensing and the minimum hop-count routing. Extensive simulations verify our analysis.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Yue  Wang", "John C. S. Lui", "Dah-Ming  Chiu"], "year": 2009, "n_citations": 18}
{"id": 3433495, "s2_id": "67c0af58399342674efc18f5bab06a85ec407b2e", "title": "A Method for the Characterisation of Observer Effects and its Application to OML", "abstract": "In all measurement campaigns, one needs to assert that the instrumentation tools do not significantly impact the system being monitored. This is critical to future claims based on the collected data and is sometimes overseen in experimental studies. We propose a method to evaluate the potential \"observer effect\" of an instrumentation system, and apply it to the OMF Measurement Library (OML). OML allows the instrumentation of almost any software to collect any type of measurements. As it is increasingly being used in networking research, it is important to characterise possible biases it may introduce in the collected metrics. Thus, we study its effect on multiple types of reports from various applications commonly used in wireless research. To this end, we designed experiments comparing OML-instrumented software with their original flavours. Our analyses of the results from these experiments show that, with an appropriate reporting setup, OML has no significant impact on the instrumented applications, and may even improve some of their performances in specifics cases. We discuss our methodology and the implication of using OML, and provide guidelines on instrumenting off-the-shelf software.", "venue": "ArXiv", "authors": ["Olivier  Mehani", "Guillaume  Jourjon", "Thierry  Rakotoarivelo"], "year": 2012, "n_citations": 2}
{"id": 3443260, "s2_id": "1fc3478310bbea2e5043f593755def1c9d0f5929", "title": "iotools: High-Performance I/O Tools for R", "abstract": "The iotools package provides a set of tools for Input/Output (I/O) intensive datasets processing in R (R Core Team, 2014). Efficent parsing methods are included which minimize copying and avoid the use of intermediate string representations whenever possible. Functions for applying chunk-wise operations allow for computing on streaming input as well as arbitrarily large files. We present a set of example use cases for iotools, as well as extensive benchmarks comparing comparable functions provided in both core-R as well as other contributed packages.", "venue": "R J.", "authors": ["Taylor  Arnold", "Michael J. Kane", "Simon  Urbanek"], "year": 2017, "n_citations": 3}
{"id": 3466779, "s2_id": "aeb89c3190a875aeff5e2e0d1416d521ef78e69f", "title": "Design of a High-Performance GEMM-like Tensor\u2013Tensor Multiplication", "abstract": "We present \u201cGEMM-like Tensor\u2013Tensor multiplication\u201d (GETT), a novel approach for dense tensor contractions that mirrors the design of a high-performance general matrix\u2013matrix multiplication (GEMM). The critical insight behind GETT is the identification of three index sets, involved in the tensor contraction, which enable us to systematically reduce an arbitrary tensor contraction to loops around a highly tuned \u201cmacro-kernel.\u201d This macro-kernel operates on suitably prepared (\u201cpacked\u201d) sub-tensors that reside in a specified level of the cache hierarchy. In contrast to previous approaches to tensor contractions, GETT exhibits desirable features such as unit-stride memory accesses, cache-awareness, as well as full vectorization, without requiring auxiliary memory. We integrate GETT alongside the so-called Transpose\u2013Transpose-GEMM-Transpose and Loops-over-GEMM approaches into an open source \u201cTensor Contraction Code Generator.\u201d The performance results for a wide range of tensor contractions suggest that GETT has the potential of becoming the method of choice: While GETT exhibits excellent performance across the board, its effectiveness for bandwidth-bound tensor contractions is especially impressive, outperforming existing approaches by up to 12.4\u00d7. More precisely, GETT achieves speedups of up to 1.41\u00d7 over an equivalent-sized GEMM for bandwidth-bound tensor contractions while attaining up to 91.3% of peak floating-point performance for compute-bound tensor contractions.", "venue": "ACM Trans. Math. Softw.", "authors": ["Paul  Springer", "Paolo  Bientinesi"], "year": 2018, "n_citations": 59}
{"id": 3467340, "s2_id": "7f8d4af54337b935ec94291bb643a1d53ac783b3", "title": "The Vectorization of the Tersoff Multi-body Potential: An Exercise in Performance Portability", "abstract": "Molecular dynamics simulations, an indispensable research tool in computational chemistry and materials science, consume a significant portion of the supercomputing cycles around the world. We focus on multi-body potentials and aim at achieving performance portability. Compared with well-studied pair potentials, multibody potentials deliver increased simulation accuracy but are too complex for effective compiler optimization. Because of this, achieving cross-platform performance remains an open question. By abstracting from target architecture and computing precision, we develop a vectorization scheme applicable to both CPUs and accelerators. We present results for the Tersoff potential within the molecular dynamics code LAMMPS on several architectures, demonstrating efficiency gains not only for computational kernels, but also for large-scale simulations. On a cluster of Intel Xeon Phi's, our optimized solver is between 3 and 5 times faster than the pure MPI reference.", "venue": "SC16: International Conference for High Performance Computing, Networking, Storage and Analysis", "authors": ["Markus  H\u00f6hnerbach", "Ahmed E. Ismail", "Paolo  Bientinesi"], "year": 2016, "n_citations": 32}
{"id": 3473578, "s2_id": "3e7332c0c6b0199696a4931d024e52022811c956", "title": "tinyMD: A Portable and Scalable Implementation for Pairwise Interactions Simulations", "abstract": "This paper investigates the suitability of the AnyDSL partial evaluation framework to implement tinyMD: an efficient, scalable, and portable simulation of pairwise interactions among particles. We compare tinyMD with the miniMD proxy application that scales very well on parallel supercomputers. We discuss the differences between both implementations and contrast miniMD's performance for single-node CPU and GPU targets, as well as its scalability on SuperMUC-NG and Piz Daint supercomputers. Additionaly, we demonstrate tinyMD's flexibility by coupling it with the waLBerla multi-physics framework. This allow us to execute tinyMD simulations using the load-balancing mechanism implemented in waLBerla.", "venue": "ArXiv", "authors": ["Rafael Ravedutti L. Machado", "Jonas  Schmitt", "Sebastian  Eibl", "Jan  Eitzinger", "Roland  Leissa", "Sebastian  Hack", "Arsene  P'erard-Gayot", "Richard  Membarth", "Harald Kostler Chair for System Simulation at University  Erlangen-Nurnberg", "Regional Computer Center Erlangen at University of Erlangen-Nurnberg", "Saarland Informatics Campus at Saarland University", "German Research Center for Artificial Intelligence at Saa Campus"], "year": 2020, "n_citations": 0}
{"id": 3479989, "s2_id": "e86a3e25c50a25d96704311c84ce27940a510dff", "title": "Investigation of the relationship between code change set n-grams and change in energy consumption", "abstract": "The amount of software running on mobile devices is constantly growing as consumers and industry purchase more battery powered devices. On the other hand, tools that provide developers with feed- back on how their software changes affect battery life are not widely available. This work employs Green Mining, the study of the rela- tionship between energy consumption and software changesets, and n-gram language models to evaluate if source code changeset perplex- ity correlates with change in energy consumption. A correlation be- tween perplexity and change in energy consumption would permit the development of a tool that predicts the impact a code changeset may have on a software applications energy consumption. The case study results show that there is weak to no correlation between cross en- tropy and change in energy consumption. Therefore, future areas of investigation are proposed.", "venue": "ArXiv", "authors": ["Stephen  Romansky"], "year": 2014, "n_citations": 0}
{"id": 3484773, "s2_id": "641ab680f5d3a9104391cbb695f5cd469dcfe3aa", "title": "A review of analytical performance modeling and its role in computer engineering and science", "abstract": "This article is a review of analytical performance modeling for computer systems. It discusses the motivation for this area of research, examines key issues, introduces some ideas, illustrates how it is applied, and points out a role that it can play in developing Computer Science.", "venue": "ArXiv", "authors": ["Y. C. Tay"], "year": 2020, "n_citations": 0}
{"id": 3496001, "s2_id": "28f028f48cbd99fe7bdab902361ba7e928e864ae", "title": "Weaves: A Novel Direct Code Execution Interface for Parallel High Performance Scientific Codes", "abstract": "Scientific codes are increasingly being used in compositional settings, especially problem solving environments (PSEs). Typical compositional modeling frameworks require significant buy-in, in the form of commitment to a particular style of programming (e.g., distributed object components). While this solution is feasible for newer generations of component-based scientific codes, large legacy code bases present a veritable software engineering nightmare. We introduce Weaves a novel framework that enables modeling, composition, direct code execution, performance characterization, adaptation, and control of unmodified high performance scientific codes. Weaves is an efficient generalized framework for parallel compositional modeling that is a proper superset of the threads and processes models of programming. In this paper, our focus is on the transparent code execution interface enabled by Weaves. We identify design constraints, their impact on implementation alternatives, configuration scenarios, and present results from a prototype implementation on Intel x86 architectures.", "venue": "ArXiv", "authors": ["Srinidhi  Varadarajan", "Joy  Mukherjee", "Naren  Ramakrishnan"], "year": 2002, "n_citations": 4}
{"id": 3498565, "s2_id": "d356b348f526ebb1ae2ac4c098d5fa24bc5087a9", "title": "Delay Bounds for Multiclass FIFO", "abstract": "FIFO is perhaps the simplest scheduling discipline. For single-class FIFO, its delay guarantee performance has been extensively studied: The well-known results include a stochastic delay bound for $GI/GI/1$ by Kingman and a deterministic delay bound for $D/D/1$ by Cruz. However, for multiclass FIFO, few such results are available. To fill the gap, we prove delay bounds for multiclass FIFO in this work, considering both deterministic and stochastic cases. Specifically, delay bounds are presented for $D/D/1$, $G/D/1$, $GI/D/1$, and $GI/GI/1$, all under multiclass FIFO.", "venue": "ArXiv", "authors": ["Yuming  Jiang", "Vishal  Misra"], "year": 2016, "n_citations": 2}
{"id": 3499580, "s2_id": "429fbc357950f5ac37a6ce49bd982b5ce848cea8", "title": "Scheduling Distributed Resources in Heterogeneous Private Clouds", "abstract": "We first consider the static problem of allocating resources to (i.e., scheduling) multiple distributed application frameworks, possibly with different priorities and server preferences, in a private cloud with heterogeneous servers. Several fair scheduling mechanisms have been proposed for this purpose. We extend prior results on max-min fair (MMF) and proportional fair (PF) scheduling to this constrained multiresource and multiserver case for generic fair scheduling criteria. The task efficiencies (a metric related to proportional fairness) of max-min fair allocations found by progressive filling are compared by illustrative examples. In the second part of this paper, we consider the online problem (with framework churn) by implementing variants of these schedulers in Apache Mesos using progressive filling to dynamically approximate max-min fair allocations. We evaluate the implemented schedulers in terms of overall execution time of realistic distributed Spark workloads. Our experiments show that resource efficiency is improved and execution times are reduced when the scheduler is \"server specific\" or when it leverages characterized required resources of the workloads (when known).", "venue": "2018 IEEE 26th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)", "authors": ["George  Kesidis", "Yuquan  Shan", "Aman  Jain", "Bhuvan  Urgaonkar", "Jalal  Khamse-Ashari", "Ioannis  Lambadaris"], "year": 2018, "n_citations": 5}
{"id": 3501482, "s2_id": "674270e002ddd30726565b2cbed287cea3907594", "title": "Software-defined Radios: Architecture, State-of-the-art, and Challenges", "abstract": "Software-defined Radio (SDR) is a programmable transceiver with the capability of operating various wireless communication protocols without the need to change or update the hardware. Progress in the SDR field has led to the escalation of protocol development and a wide spectrum of applications, with more emphasis on programmability, flexibility, portability, and energy efficiency, in cellular, WiFi, and M2M communication. Consequently, SDR has earned a lot of attention and is of great significance to both academia and industry. SDR designers intend to simplify the realization of communication protocols while enabling researchers to experiment with prototypes on deployed networks. This paper is a survey of the state-of-the-art SDR platforms in the context of wireless communication protocols. We offer an overview of SDR architecture and its basic components, then discuss the significant design trends and development tools. In addition, we highlight key contrasts between SDR architectures with regards to energy, computing power, and area, based on a set of metrics. We also review existing SDR platforms and present an analytical comparison as a guide to developers. Finally, we recognize a few of the related research topics and summarize potential solutions.", "venue": "Comput. Commun.", "authors": ["Rami  Akeela", "Behnam  Dezfouli"], "year": 2018, "n_citations": 44}
{"id": 3503135, "s2_id": "4173a664cf0f6e816ae1110658a07486b0c71cb6", "title": "Efficiency Near the Edge: Increasing the Energy Efficiency of FFTs on GPUs for Real-Time Edge Computing", "abstract": "The Square Kilometer Array (SKA) is an international initiative for developing the world\u2019s largest radio telescope with a total collecting area of over a million square meters. The scale of the operation, combined with the remote location of the telescope, requires the use of energy-efficient computational algorithms. This, along with the extreme data rates that will be produced by the SKA and the requirement for a real-time observing capability, necessitates in-situ data processing in an edge style computing solution. More generally, energy efficiency in the modern computing landscape is becoming of paramount concern. Whether it be the power budget that can limit some of the world\u2019s largest supercomputers, or the limited power available to the smallest Internet-of-Things devices. In this article, we study the impact of hardware frequency scaling on the energy consumption and execution time of the Fast Fourier Transform (FFT) on NVIDIA GPUs using the cuFFT library. The FFT is used in many areas of science and it is one of the key algorithms used in radio astronomy data processing pipelines. Through the use of frequency scaling, we show that we can lower the power consumption of the NVIDIA A100 GPU when computing the FFT by up to 47% compared to the boost clock frequency, with less than a 10% increase in the execution time. Furthermore, using one common core clock frequency for all tested FFT lengths, we show on average a 43% reduction in power consumption compared to the boost core clock frequency with an increase in the execution time still below 10%. We demonstrate how these results can be used to lower the power consumption of existing data processing pipelines. These savings, when considered over years of operation, can yield significant financial savings, but can also lead to a significant reduction of greenhouse gas emissions.", "venue": "IEEE Access", "authors": ["Karel  Ad\u00e1mek", "Jan  Novotn\u00fd", "Jeyarajan  Thiyagalingam", "Wesley  Armour"], "year": 2021, "n_citations": 0}
{"id": 3504119, "s2_id": "3b043b019dd7b4f2c4724f26dc1f5c18e1806fa8", "title": "Optimization of an Electromagnetics Code with Multicore Wavefront Diamond Blocking and Multi-dimensional Intra-Tile Parallelization", "abstract": "Understanding and optimizing the properties of solar cells is becoming a key issue in the search for alternatives to nuclear and fossil energy sources. A theoretical analysis via numerical simulations involves solving Maxwell's Equations in discretized form and typically requires substantial computing effort. We start from a hybrid-parallel (MPI+OpenMP) production code that implements the Time Harmonic Inverse Iteration Method (THIIM) with Finite-Difference Frequency Domain (FDFD) discretization. Although this algorithm has the characteristics of a strongly bandwidth-bound stencil update scheme, it is significantly different from the popular stencil types that have been exhaustively studied in the high performance computing literature to date. We apply a recently developed stencil optimization technique, multicore wavefront diamond tiling with multi-dimensional cache block sharing, and describe in detail the peculiarities that need to be considered due to the special stencil structure. Concurrency in updating the components of the electric and magnetic fields provides an additional level of parallelism. The dependence of the cache size requirement of the optimized code on the blocking parameters is modeled accurately, and an auto-tuner searches for optimal configurations in the remaining parameter space. We were able to completely decouple the execution from the memory bandwidth bottleneck, accelerating the implementation by a factor of three to four compared to an optimal implementation with pure spatial blocking on an 18-core Intel Haswell CPU.", "venue": "2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Tareq M. Malas", "Julian  Hornich", "Georg  Hager", "Hatem  Ltaief", "Christoph  Pflaum", "David E. Keyes"], "year": 2016, "n_citations": 16}
{"id": 3508409, "s2_id": "5cb475910bfce89f0248317ea23a8a322c1ea0f9", "title": "F3ORNITS: A Flexible Variable Step Size Non-Iterative Co-simulation Method handling Subsystems with Hybrid Advanced Capabilities", "abstract": "This paper introduces the F3ORNITS non-iterative co-simulation algorithm in which F3 stands for the 3 flexible aspects of the method: flexible polynomial order representation of coupling variables, flexible time-stepper applying variable co-simulation step size rules on subsystems allowing it and flexible scheduler orchestrating the meeting times among the subsystems and capable of asynchronousness when subsystems\u2019 constraints requires it. The motivation of the F3ORNITS method is to accept any kind of co-simulation model, including any kind of subsystem, regardless on their available capabilities. Indeed, one the major problems in industry is that the subsystems usually have constraints or lack of advanced capabilities making it impossible to implement most of the advanced co-simulation algorithms on them. The method makes it possible to preserve the dynamics of the coupling constraints when necessary as well as to avoid breaking C1 smoothness at communication times, and also to adapt the co-simulation step size in a way that is robust both to zero-crossing variables (contrary to classical relative error-based criteria) and to jumps. Two test cases are presented to illustrate the robustness of the F3ORNITS method as well as its higher accuracy than the non-iterative Jacobi coupling algorithm (the most commonly used method in industry) for a smaller number of co-simulation steps.", "venue": "ArXiv", "authors": ["Yohan  Eguillon", "Bruno  Lacabanne", "Damien  Tromeur-Dervout"], "year": 2021, "n_citations": 0}
{"id": 3509896, "s2_id": "23dc8cb0bf86202e9faf7140f491718d4975e3d1", "title": "Benchmarking and implementation of probability-based simulations on programmable graphics cards", "abstract": "The latest graphics processing units (GPUs) are reported to reach up to 200 billion floating point operations per second (200Gflops (Spode's Abode, GeForce FX Preview (NV30), Spode, November (2002), Internet address (accessed on 10/2003): http://www.spodesabode.com/content/article/geforcefx)) and to have price performance of 0.1 cents per Mflop. These facts raise great interest in the plausibility of extending the GPUs' use to non-graphics applications, in particular numerical simulations on structured grids (lattice). In this paper we (1) review previous works on using GPUs for non-graphics applications, (2) implement probability-based simulations on the GPU, namely the Ising and percolation models, (3) implement vector operation benchmarks for the GPU, and finally (4) compare the CPU's and GPU's performance. Original contribution of this work is implementing Monte Carlo type simulations on the GPU. Such simulations have a wide area of applications. They are computationally intensive and, as we show in the paper, lend themselves naturally to implementation on GPUs, therefore allowing us to better use the GPU's computational power and speed up the computation. A general conclusion from the results obtained is that moving computations from the CPU to the GPU is feasible, yielding good time and price performance, for certain lattice computations. Preliminary results also show that it is feasible to use them in parallel.", "venue": "Comput. Graph.", "authors": ["Stanimire  Tomov", "Michael D. McGuigan", "Robert  Bennett", "Gordon  Smith", "John  Spiletic"], "year": 2005, "n_citations": 57}
{"id": 3510414, "s2_id": "2abb99d82d53d2844bb95d584ddec9967e4f678d", "title": "RZBENCH: Performance evaluation of current HPC architectures using low-level and application benchmarks", "abstract": "RZBENCH is a benchmark suite that was specifically developed to reflect the requirements of scientific supercomputer users at the University of Erlangen-Nuremberg (FAU). It comprises a number of application and low-level codes under a common build infrastructure that fosters maintainability and expandability. This paper reviews the structure of the suite and briefly introduces the most relevant benchmarks. In addition, some widely known standard benchmark codes are reviewed in order to emphasize the need for a critical review of often-cited performance results. Benchmark data is presented for the HLRB-II at LRZ Munich and a local InfiniBand Woodcrest cluster as well as two uncommon system architectures: A bandwidth-optimized InfiniBand cluster based on single socket nodes (\u201cPort Townsend\u201d) and an early version of Sun\u2019s highly threaded T2 architecture (\u201cNiagara 2\u201d).", "venue": "ArXiv", "authors": ["Georg  Hager", "Holger  Stengel", "Thomas  Zeiser", "Gerhard  Wellein"], "year": 2007, "n_citations": 7}
{"id": 3513113, "s2_id": "b1a6ada5b4e8d94541abea0e19c9034b8725a6f8", "title": "Improved repeatability measures for evaluating performance of feature detectors", "abstract": "The most frequently employed measure for performance characterisation of local feature detectors is repeatability, but it has been observed that this does not necessarily mirror actual performance. Presented are improved repeatability formulations which correlate much better with the true performance of feature detectors. Comparative results for several state-of-the-art feature detectors are presented using these measures; it is found that Hessian-based detectors are generally superior at identifying features when images are subject to various geometric and photometric transformations.", "venue": "ArXiv", "authors": ["Shoaib  Ehsan", "Nadia  Kanwal", "Adrian F. Clark", "Klaus D. McDonald-Maier"], "year": 2015, "n_citations": 18}
{"id": 3514490, "s2_id": "5b969e91e6d2036fb13dbd8691246b8780992009", "title": "On the Performance of RIS-Assisted Dual-Hop Mixed RF-UWOC Systems", "abstract": "In this article, we investigate the performance of a reconfigurable intelligent surface (RIS)-assisted dual-hop mixed radio-frequency underwater wireless optical communication (RF-UWOC) system. An RIS is an emerging and low-cost technology that aims to enhance the strength of the received signal, thus improving the system performance. In the considered system setup, a ground source does not have a reliable direct link to a given marine buoy and communicates with it through an RIS installed on a building. In particular, the fixed buoy acts as a relay that sends the signal to an underwater destination. In this context, analytical expressions for the outage probability (OP), average bit error rate (ABER), and average channel capacity (ACC) are derived assuming fixed-gain amplify-and-forward (AF) and decode-and-forward (DF) relaying protocols at the marine buoy. Moreover, asymptotic analyses of the OP and ABER are carried out in order to gain further insights from the analytical frameworks. In particular, the system diversity order is derived and it is shown to depend on the RF link parameters and on the detection schemes of the UWOC link. Finally, it is demonstrated that RIS-assisted systems can effectively improve the performance of mixed dual-hop RF-UWOC systems.", "venue": "IEEE Transactions on Cognitive Communications and Networking", "authors": ["Sai  Li", "Liang  Yang", "Daniel Benevides da Costa", "Marco Di Renzo", "Mohamed-Slim  Alouini"], "year": 2021, "n_citations": 3}
{"id": 3516431, "s2_id": "d5c6ec1c4c65bb6c851442d0979902dbc59577ea", "title": "BSF: A parallel computation model for scalability estimation of iterative numerical algorithms on cluster computing systems", "abstract": "This paper examines a new parallel computation model called bulk synchronous farm (BSF) that focuses on estimating the scalability of compute-intensive iterative algorithms aimed at cluster computing systems. In the BSF model, a computer is a set of processor nodes connected by a network and organized according to the mas-ter/slave paradigm. A cost metric of the BSF model is presented. This cost metric requires the algorithm to be represented in the form of operations on lists. This allows us to derive an equation that predicts the scalability boundary of a parallel program: the maximum number of processor nodes after which the speedup begins to de-crease. The paper includes several examples of applying the BSF model to designing and analyzing parallel nu-merical algorithms. The large-scale computational experiments conducted on a cluster computing system confirm the adequacy of the analytical estimations obtained using the BSF model.", "venue": "J. Parallel Distributed Comput.", "authors": ["Leonid B. Sokolinsky"], "year": 2021, "n_citations": 5}
{"id": 3518459, "s2_id": "8253c248ebb1e4ed8b57112d4445f65f288d8574", "title": "FaaSter Troubleshooting - Evaluating Distributed Tracing Approaches for Serverless Applications", "abstract": "Serverless applications can be particularly difficult to troubleshoot, as these applications are often composed of various managed and partly managed services. Faults are often unpredictable and can occur at multiple points, even in simple compositions. Each additional function or service in a serverless composition introduces a new possible fault source and a new layer to obfuscate faults. Currently, serverless platforms offer only limited support for identifying runtime faults. Developers looking to observe their serverless compositions often have to rely on scattered logs and ambiguous error messages to pinpoint root causes. In this paper, we investigate the use of distributed tracing for improving the observability of faults in serverless applications. To this end, we first introduce a model for characterizing fault observability, then provide a prototypical tracing implementation-specifically, a developer-driven and a platform-supported tracing approach. We compare both approaches with our model, measure associated trade-offs (execution latency, resource utilization), and contribute new insights for troubleshooting serverless compositions.", "venue": "2021 IEEE International Conference on Cloud Engineering (IC2E)", "authors": ["Maria C. Borges", "Sebastian  Werner", "Ahmet  Kilic"], "year": 2021, "n_citations": 1}
{"id": 3519522, "s2_id": "cab37e4ab84312f8428ac2ec183ca3a8dbaa7d37", "title": "Understanding memory access patterns using the BSC performance tools", "abstract": "The growing gap between processor and memory speeds results in complex memory hierarchies as processors evolve to mitigate such divergence by taking advantage of the locality of reference. In this direction, the BSC performance analysis tools have been recently extended to provide insight relative to the application memory accesses depicting their temporal and spatial characteristics, correlating with the source-code and the achieved performance simultaneously. These extensions rely on the Precise Event-Based Sampling (PEBS) mechanism available in recent Intel processors to capture information regarding the application memory accesses. The sampled information is later combined with the Folding technique to represent a detailed temporal evolution of the memory accesses and in conjunction with the achieved performance and the source-code counterpart. The results obtained from the combination of these tools help not only application developers but also processor architects to understand better how the application behaves and how the system performs. In this paper, we describe a tighter integration of the sampling mechanism into the monitoring package. We also demonstrate the value of the complete workflow by exploring already optimized state--of--the--art benchmarks, providing detailed insight of their memory access behavior. We have taken advantage of this insight to apply small modifications that improve the applications' performance.", "venue": "Parallel Comput.", "authors": ["Harald  Servat", "Jes\u00fas  Labarta", "Hans-Christian  Hoppe", "Judit  Gim\u00e9nez", "Antonio J. Pe\u00f1a"], "year": 2018, "n_citations": 3}
{"id": 3519627, "s2_id": "9fab3687c7eed01fbe915b57ca33c3a6e071ed2e", "title": "Power Flow Analysis Using Graph based Combination of Iterative Methods and Vertex Contraction Approach", "abstract": "Compared with relational database (RDB), graph database (GDB) is a more intuitive expression of the real world. Each node in the GDB is a both storage and logic unit. Since it is connected to its neighboring nodes through edges, and its neighboring information could be easily obtained in one-step graph traversal. It is able to conduct local computation independently and all nodes can do their local work in parallel. Then the whole system can be maximally analyzed and assessed in parallel to largely improve the computation performance without sacrificing the precision of final results. This paper firstly introduces graph database, power system graph modeling and potential graph computing applications in power systems. Two iterative methods based on graph database and PageRank are presented and their convergence are discussed. Vertex contraction is proposed to improve the performance by eliminating zero-impedance branch. A combination of the two iterative methods is proposed to make use of their advantages. Testing results based on a provincial 1425-bus system demonstrate that the proposed comprehensive approach is a good candidate for power flow analysis.", "venue": "2018 International Conference on Power System Technology (POWERCON)", "authors": ["Chen  Yuan", "Guangyi  Liu", "Renchang  Dai", "Zhiwei  Wang"], "year": 2018, "n_citations": 1}
{"id": 3521311, "s2_id": "424b71ed746317352adca4960f80128f05a07e3d", "title": "Exact maximal reduction of stochastic reaction networks by species lumping", "abstract": "MOTIVATION\nStochastic reaction networks are a widespread model to describe biological systems where the presence of noise is relevant, such as in cell regulatory processes. Unfortunately, in all but simplest models the resulting discrete state-space representation hinders analytical tractability and makes numerical simulations expensive. Reduction methods can lower complexity by computing model projections that preserve dynamics of interest to the user.\n\n\nRESULTS\nWe present an exact lumping method for stochastic reaction networks with mass-action kinetics. It hinges on an equivalence relation between the species, resulting in a reduced network where the dynamics of each macro-species is stochastically equivalent to the sum of the original species in each equivalence class, for any choice of the initial state of the system. Furthermore, by an appropriate encoding of kinetic parameters as additional species, the method can establish equivalences that do not depend on specific values of the parameters. The method is supported by an efficient algorithm to compute the largest species equivalence, thus the maximal lumping. The effectiveness and scalability of our lumping technique, as well as the physical interpretability of resulting reductions, is demonstrated in several models of signaling pathways and epidemic processes on complex networks.\n\n\nAVAILABILITY\nThe algorithms for species equivalence have been implemented in the software tool ERODE, freely available for download from https://www.erode.eu.", "venue": "Bioinform.", "authors": ["Luca  Cardelli", "Isabel Cristina Perez-Verona", "Mirco  Tribastone", "Max  Tschaikowski", "Andrea  Vandin", "Tabea  Waizmann"], "year": 2021, "n_citations": 3}
{"id": 3522283, "s2_id": "601ac87028e44ed48dd7a82a1bc2e566e139b5c2", "title": "High-level Modeling of Manufacturing Faults in Deep Neural Network Accelerators", "abstract": "The advent of data-driven real-time applications requires the implementation of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google\u2019s Tensor Processing Unit (TPU) is one such neural network accelerator that uses systolic array-based matrix multiplication hardware for computation in its crux. Manufacturing faults at any state element of the matrix multiplication unit can cause unexpected errors in these inference networks. In this paper, we propose a formal model of permanent faults and their propagation in a TPU using the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed using the probabilistic model checking technique to reason about the likelihood of faulty outputs. The obtained quantitative results show that the classification accuracy is sensitive to the type of permanent faults as well as their location, bit position and the number of layers in the neural network. The conclusions from our theoretical model have been validated using experiments on a digit recognition-based DNN.", "venue": "2020 IEEE 26th International Symposium on On-Line Testing and Robust System Design (IOLTS)", "authors": ["Shamik  Kundu", "Ahmet  Soyyigit", "Khaza Anuarul Hoque", "Kanad  Basu"], "year": 2020, "n_citations": 3}
{"id": 3522920, "s2_id": "b2f842f29e92976e12a8b40280c2f3edbb176327", "title": "An Overview of Self-Similar Traffic: Its Implications in the Network Design", "abstract": "The knowledge about the true nature of the traffic in computer networking is a key requirement in the design of such networks. The phenomenon of self-similarity is a characteristic of the traffic of current client/server packet networks in LAN/WAN environments dominated by network technologies such as Ethernet and the TCP/IP protocol stack. The development of networks traffic simulators, which take into account this attribute, is necessary for a more realistic description the traffic on these networks and their use in the design of resources (contention elements) and protocols of flow control and network congestion. In this scenario it is recommended do not adopt standard traffic models of the Poisson type.", "venue": "ArXiv", "authors": ["Ernande F. Melo", "H. M. de Oliveira"], "year": 2020, "n_citations": 2}
{"id": 3525224, "s2_id": "75bdb27008ece2def8e877c465b74565e49f81f0", "title": "Reproducibility Report for the Paper: QN-based Modeling and Analysis of Software Performance Antipatterns for Cyber-Physical Systems", "abstract": "The authors have uploaded their artifact to Zenodo, which ensures a long-term retention of the artifact. The artifact allows to re-run the experiments very smoothly, and the dependencies are well documented. The process to regenerate data for the gures and tables in the paper completes, and all results are reproducible. This paper can thus receive the Artifacts Available badge. The software in the artifact runs correctly with no trouble, and is relevant to the paper, thus deserving the Artifacts Evaluated\u2014Functional badge. Given the successful reproduction of all gures and tables, the Results Reproduced badge can be assigned.", "venue": "ArXiv", "authors": ["Alessandro  Pellegrini"], "year": 2021, "n_citations": 0}
{"id": 3528055, "s2_id": "8c8e5a757cb9667f5c677d39b4fba238ad7f7354", "title": "An Efficient Architecture for Information Retrieval in P2P Context Using Hypergraph", "abstract": "Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of Internet traffic. P2P systems have emerged as an accepted way to share enormous volumes of data. Needs for widely distributed information systems supporting virtual organizations have given rise to a new category of P2P systems called schema-based. In such systems each peer is a database management system in itself, ex-posing its own schema. In such a setting, the main objective is the efficient search across peer databases by processing each incoming query without overly consuming bandwidth. The usability of these systems depends on successful techniques to find and retrieve data; however, efficient and effective routing of content-based queries is an emerging problem in P2P networks. This work was attended as an attempt to motivate the use of mining algorithms in the P2P context may improve the significantly the efficiency of such methods. Our proposed method based respectively on combination of clustering with hypergraphs. We use ECCLAT to build approximate clustering and discovering meaningful clusters with slight overlapping. We use an algorithm MTMINER to extract all minimal transversals of a hypergraph (clusters) for query routing. The set of clusters improves the robustness in queries routing mechanism and scalability in P2P Network. We compare the performance of our method with the baseline one considering the queries routing problem. Our experimental results prove that our proposed methods generate impressive levels of performance and scalability with with respect to important criteria such as response time, precision and recall.", "venue": "ArXiv", "authors": ["Anis  Ismail", "Mohamed  Quafafou", "Nicolas  Durand", "Mohammad  Hajjar"], "year": 2011, "n_citations": 2}
{"id": 3532008, "s2_id": "3781b4ac165070c1be676d9b8fcc32d5db5d2fe7", "title": "Hierarchical Beamforming: Resource Allocation, Fairness and Flow Level Performance", "abstract": "We consider hierarchical beamforming in wireless networks. For a given population of flows, we propose computationally efficient algorithms for fair rate allocation including proportional fairness and max-min fairness. We further propose closed-form formulas for flow level performance, for both elastic and streaming traffic (1).", "venue": "PERV", "authors": ["Julien  Floquet", "Richard  Combes", "Zwi  Altman"], "year": 2019, "n_citations": 0}
{"id": 3533561, "s2_id": "321ccac8e298a5b53e3df3becff4e79b0b216870", "title": "A Machine Learning Pipeline Stage for Adaptive Frequency Adjustment", "abstract": "A machine learning (ML) design framework is proposed for adaptively adjusting clock frequency based on propagation delay of individual instructions. A random forest model is trained to classify propagation delays in real time, utilizing current operation type, current operands, and computation history as ML features. The trained model is implemented in Verilog as an additional pipeline stage within a baseline processor. The modified system is experimentally tested at the gate level in 45 nm CMOS technology, exhibiting a speedup of 70% and energy reduction of 30% with coarse-grained ML classification. A speedup of 89% is demonstrated with finer granularities with 15.5% reduction in energy consumption.", "venue": "ArXiv", "authors": ["Arash Fouman Ajirlou", "Inna  Partin-Vaisband"], "year": 2020, "n_citations": 2}
{"id": 3538049, "s2_id": "bfd023b10ac63d24d41ecaabd351e32d5a67e9f9", "title": "On the Power-of-d-choices with Least Loaded Server Selection", "abstract": "Motivated by distributed schedulers that combine the power-of-d-choices with late binding and systems that use replication with cancellation-on-start, we study the performance of the LL(d) policy which assigns a job to a server that currently has the least workload among d randomly selected servers in large-scale homogeneous clusters. We consider general job size distributions and propose a partial integro-differential equation to describe the evolution of the system. This equation relies on the earlier proven ansatz for LL(d) which asserts that the workload distribution of any finite set of queues becomes independent of one another as the number of servers tends to infinity. Based on this equation we propose a fixed point iteration for the limiting workload distribution and study its convergence. For exponential job sizes we present a simple closed form expression for the limiting workload distribution that is valid for any work-conserving service discipline as well as for the limiting response time distribution in case of first-come-first-served scheduling. We further show that for phase-type distributed job sizes the limiting workload and response time distribution can be expressed via the unique solution of a simple set of ordinary differential equations. Numerical and analytical results that compare response time of the classic power-of-d-choices algorithm and the LL(d) policy are also presented and the accuracy of the limiting response time distribution for finite systems is illustrated using simulations.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Tim  Hellemans", "Benny Van Houdt"], "year": 2018, "n_citations": 27}
{"id": 3539455, "s2_id": "942e47bd4f7252382aa57ff684a287c4dd2e50ef", "title": "Multiple Antenna Cyclostationary Spectrum Sensing Based on the Cyclic Correlation Significance Test", "abstract": "In this paper, we propose and analyze a spectrum sensing method based on cyclostationarity specifically targeted for receivers with multiple antennas. This detection method is used for determining the presence or absence of primary users in cognitive radio networks based on the eigenvalues of the cyclic covariance matrix of received signals. In particular, the cyclic correlation significance test is used to detect a specific signal-of-interest by exploiting knowledge of its cyclic frequencies. Analytical asymptotic expressions for the probability of detection and probability of false-alarm under both spatially uncorrelated and spatially correlated noise are derived and verified by simulations. The detection performance in a Rayleigh flat-fading environment is found and verified through simulations. One of the advantages of the proposed method is that the detection threshold is shown to be independent of both the number of samples and the noise covariance, effectively eliminating the dependence on accurate noise estimation. The proposed method is also shown to provide higher detection probability and better robustness to noise uncertainty than existing multiple antenna cyclostationary-based spectrum sensing algorithms under both AWGN as well as a quasi-static Rayleigh fading channel.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Paulo  Urriza", "Eric  Rebeiz", "Danijela  Cabric"], "year": 2013, "n_citations": 60}
{"id": 3542712, "s2_id": "05e7b82ab47ea3f678a1671130855e0f8bfcf494", "title": "SRPT for Multiserver Systems", "abstract": "The Shortest Remaining Processing Time (SRPT) scheduling policy and its variants have been extensively studied in both theoretical and practical settings. While beautiful results are known for single-server SRPT, much less is known for multiserver SRPT. In particular, stochastic analysis of the M/G/k under SRPT is entirely open. Intuition suggests that multiserver SRPT should be optimal or near-optimal for minimizing mean response time. However, the only known analysis of multiserver SRPT is in the worst-case adversarial setting, where SRPT can be far from optimal. In this paper, we give the first stochastic analysis bounding mean response time of the M/G/k under SRPT. Using our response time bound, we show that multiserver SRPT has asymptotically optimal mean response time in the heavy-traffic limit. The key to our bounds is a strategic combination of stochastic and worst-case techniques. Beyond SRPT, we prove similar response time bounds and optimality results for several other multiserver scheduling policies. This article is an introduction to our longer paper, [1].", "venue": "PERV", "authors": ["Isaac  Grosof", "Ziv  Scully", "Mor  Harchol-Balter"], "year": 2019, "n_citations": 0}
{"id": 3546690, "s2_id": "bb5d2a23a59ff232748a75dbe470cc0f675efebe", "title": "ptp++: A Precision Time Protocol Simulation Model for OMNeT++ / INET", "abstract": "Precise time synchronization is expected to play a key role in emerging distributed and real-time applications such as the smart grid and Internet of Things (IoT) based applications. The Precision Time Protocol (PTP) is currently viewed as one of the main synchronization solutions over a packet-switched network, which supports microsecond synchronization accuracy. In this paper, we present a PTP simulation model for OMNeT++ INET, which allows to investigate the synchronization accuracy under different network configurations and conditions. To show some illustrative simulation results using the developed module, we investigate on the network load fluctuations and their impacts on the PTP performance by considering a network with class- based quality-of-service (QoS) support. The simulation results show that the network load significantly affects the network delay symmetry, and investigate a new technique called class probing to improve the PTP accuracy and mitigate the load fluctuation effects. I. INTRODUCTION", "venue": "ArXiv", "authors": ["Martin  L\u00e9vesque", "David  Tipper"], "year": 2015, "n_citations": 10}
{"id": 3555422, "s2_id": "17d0a8cd161f6228ed304f92c7f1082b3b24a1b0", "title": "Accurate Throughput Prediction of Basic Blocks on Recent Intel Microarchitectures", "abstract": "Tools to predict the throughput of basic blocks on a specific microarchitecture are useful to optimize software performance and to build optimizing compilers. In recent work, several such tools have been proposed. However, the accuracy of their predictions has been shown to be relatively low. In this paper, we identify the most important factors for these inaccuracies. To a significant degree these inaccuracies are due to elements and parameters of the pipelines of recent CPUs that are not taken into account by previous tools. A primary reason for this is that the necessary details are often undocumented. In this paper, we build more precise models of relevant components by reverse engineering using microbenchmarks. Based on these models, we develop a simulator for predicting the throughput of basic blocks. In addition to predicting the throughput, our simulator also provides insights into how the code is executed. Our tool supports all Intel Core microarchitecture generations released in the last decade. We evaluate it on an improved version of the BHive benchmark suite. On many recent microarchitectures, its predictions are more accurate than the predictions of state-of-the-art tools by more than an order of magnitude.", "venue": "ArXiv", "authors": ["Andreas  Abel", "Jan  Reineke"], "year": 2021, "n_citations": 1}
{"id": 3555661, "s2_id": "f6f806951b94d8fcef3f1739b4e53a7bf409bfbb", "title": "OpenCL Performance Prediction using Architecture-Independent Features", "abstract": "OpenCL is an attractive programming model for heterogeneous high-performance computing systems, with wide support from hardware vendors and significant performance portability. To support efficient scheduling on HPC systems it is necessary to perform accurate performance predictions for OpenCL workloads on varied compute devices, which is challenging due to diverse computation, communication and memory access characteristics which result in varying performance between devices. The Architecture Independent Workload Characterization (AIWC) tool can be used to characterize OpenCL kernels according to a set of architecture-independent features. This work presents a methodology where AIWC features are used to form a model capable of predicting accelerator execution times. We used this methodology to predict execution times for a set of 37 computational kernels running on 15 different devices representing a broad range of CPU, GPU and MIC architectures. The predictions are highly accurate, differing from the measured experimental run-times by an average of only 1.2%, and correspond to actual execution time mispredictions of 9 ps to 1 sec according to problem size. A previously unencountered code can be instrumented once and the AIWC metrics embedded in the kernel, to allow performance prediction across the full range of modelled devices. The results suggest that this methodology supports correct selection of the most appropriate device for a previously unen- countered code, which is highly relevant to the HPC scheduling setting.", "venue": "2018 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Beau  Johnston", "Gregory  Falzon", "Josh  Milthorpe"], "year": 2018, "n_citations": 7}
{"id": 3556131, "s2_id": "42023c4d8d678499af2214f42f41a2ea0124abd5", "title": "Rate Reduction for State-labelled Markov Chains with Upper Time-bounded CSL Requirements", "abstract": "This paper presents algorithms for identifying and reducing a dedicated set of controllable transition rates of a state-labelled continuous-time Markov chain model. The purpose of the reduction is to make states to satisfy a given requirement, specified as a CSL upper time-bounded Until formula. We distinguish two different cases, depending on the type of probability bound. A natural partitioning of the state space allows us to develop possible solutions, leading to simple algorithms for both cases.", "venue": "Cassting/SynCoP", "authors": ["Bharath Siva Kumar Tati", "Markus  Siegle"], "year": 2016, "n_citations": 6}
{"id": 3559651, "s2_id": "08116eaad2876b6747c79458822c25f8828e2ea7", "title": "Labor-right Protecting Dispatch of Meal Delivery Platforms", "abstract": "The boom in the meal delivery industry brings growing concern about the labor rights of riders. Current dispatch policies of meal-delivery platforms focus mainly on satisfying consumers or minimizing the number of riders for cost savings. There are few discussions on improving the working conditions of riders by algorithm design. The lack of concerns on labor rights in mechanism and dispatch design has resulted in a very large time waste for riders and their risky driving. In this research, we propose a queuing-model-based framework to discuss optimal dispatch policy with the goal of labor rights protection. We apply our framework to develop an algorithm minimizing the waiting time of food delivery riders with guaranteed user experience. Our framework also allows us to manifest the value of restaurants\u2019 data about their offlineorder numbers on improving the benefits of riders.", "venue": "ArXiv", "authors": ["Wentao  Weng", "Yang  Yu"], "year": 2021, "n_citations": 0}
{"id": 3561945, "s2_id": "9fde884f635c2c6a8d8586a027e5e637bfad78ba", "title": "Accelerating reduction and scan using tensor core units", "abstract": "Driven by deep learning, there has been a surge of specialized processors for matrix multiplication, referred to as Tensor Core Units (TCUs). These TCUs are capable of performing matrix multiplications on small matrices (usually 4 \u00d7 4 or 16 \u00d7 16) to accelerate HPC and deep learning workloads. Although TCUs are prevalent and promise increase in performance and/or energy efficiency, they suffer from over specialization as only matrix multiplication on small matrices is supported. In this paper we express both reduction and scan in terms of matrix multiplication operations and map them onto TCUs. To our knowledge, this paper is the first to try to broaden the class of algorithms expressible as TCU operations and is the first to show benefits of this mapping in terms of: program simplicity, efficiency, and performance. We implemented the reduction and scan algorithms using NVIDIA's V100 TCUs and achieved 89% -- 98% of peak memory copy bandwidth. Our results are orders of magnitude faster (up to 100 \u00d7 for reduction and 3 \u00d7 for scan) than state-of-the-art methods for small segment sizes (common in HPC and deep learning applications). Our implementation achieves this speedup while decreasing the power consumption by up to 22% for reduction and 16% for scan.", "venue": "ICS", "authors": ["Abdul  Dakkak", "Cheng  Li", "Isaac  Gelado", "Jinjun  Xiong", "Wen-mei  Hwu"], "year": 2019, "n_citations": 29}
{"id": 3561991, "s2_id": "09067aaf301732a05978b51dd00e3582f187358c", "title": "Effect of Interleaved FEC Code on Wavelet Based MC-CDMA System with Alamouti STBC in Different Modulation Schemes", "abstract": "In this paper, the impact of Forward Error Correction (FEC) code namely Trellis code with interleaver on the performance of wavelet based MC-CDMA wireless communication system with the implementation of Alamouti antenna diversity scheme has been investigated in terms of Bit Error Rate (BER) as a function of Signal-to-Noise Ratio (SNR) per bit. Simulation of the system under proposed study has been done in M-ary modulation schemes (MPSK, MQAM and DPSK) over AWGN and Rayleigh fading channel incorporating Walsh Hadamard code as orthogonal spreading code to discriminate the message signal for individual user. It is observed via computer simulation that the performance of the interleaved coded based proposed system outperforms than that of the uncoded system in all modulation schemes over Rayleigh fading channel.", "venue": "ArXiv", "authors": ["Rifat Ara Shams", "M. Hasnat Kabir", "Shaikh Enayet Ullah"], "year": 2012, "n_citations": 3}
{"id": 3562802, "s2_id": "e7b18909cb1913aba19423f9335cd24ba933cbca", "title": "A performance evaluation of container technologies on Internet of Things devices", "abstract": "The use of virtualization technologies in different contexts - such as Cloud Environments, Internet of Things (IoT), Software Defined Networking (SDN) - has rapidly increased during the last years. Among these technologies, container-based solutions own characteristics for deploying distributed and lightweight applications. This paper presents a performance evaluation of container technologies on constrained devices, in this case, on Raspberry Pi. The study shows that, overall, the overhead added by containers is negligible.", "venue": "2016 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)", "authors": ["Roberto  Morabito"], "year": 2016, "n_citations": 71}
{"id": 3563020, "s2_id": "d92eab734922f1b6a2b4de2055114f77162edfa7", "title": "Ranking Online Social Users by Their Influence", "abstract": "We introduce an original mathematical model to analyze the diffusion of posts within a generic online social platform. The main novelty is that each user is not simply considered as a node on the social graph, but is further equipped with his/her own Wall and Newsfeed, and has his/her own individual self-posting and re-posting activity. As a main result using our developed model, we derive in closed form the probabilities that posts originating from a given user are found on the Wall and Newsfeed of any other. These are the solution of a linear system of equations, which can be resolved iteratively. In fact, our model is very flexible with respect to the modeling assumptions. Using the probabilities derived from the solution, we define a new measure of per-user influence over the entire network, the <inline-formula> <tex-math notation=\"LaTeX\">$\\Psi $ </tex-math></inline-formula>-score, which combines the user position on the graph with user (re-)posting activity. In the homogeneous case where all users have the same activity rates, it is shown that a variant of the <inline-formula> <tex-math notation=\"LaTeX\">$\\Psi $ </tex-math></inline-formula>-score is equal to PageRank. Furthermore, we compare the new model and its <inline-formula> <tex-math notation=\"LaTeX\">$\\Psi $ </tex-math></inline-formula>-score against the empirical influence measured from very large data traces (Twitter, Weibo). The results illustrate that these new tools can accurately rank influencers with asymmetric (re-)posting activity for such real world applications.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Anastasios  Giovanidis", "Bruno  Baynat", "Cl\u00e9mence  Magnien", "Antoine  Vendeville"], "year": 2021, "n_citations": 0}
{"id": 3568506, "s2_id": "518240b385e01eff065755f7d7153d0746d71b94", "title": "Closed-form waiting time approximations for polling systems", "abstract": "A typical polling system consists of a number of queues, attended by a single server in a fixed order. The vast majority of papers on polling systems focus on Poisson arrivals, whereas very few results are available for general arrivals. The current study is the first one presenting simple closed-form approximations for the mean waiting times in polling systems with renewal arrival processes, performing well for all workloads. The approximations are constructed using heavy traffic limits and newly developed light traffic limits. The closed-form approximations may prove to be extremely useful for system design and optimisation in application areas as diverse as telecommunications, maintenance, manufacturing and transportation.", "venue": "Perform. Evaluation", "authors": ["Marko A. A. Boon", "Erik M. M. Winands", "Ivo J. B. F. Adan", "A. C. C. van Wijk"], "year": 2011, "n_citations": 35}
{"id": 3571713, "s2_id": "79a2b08795098485d1e08d844e04f25ae601b6e0", "title": "Optimizing age-of-information in a multi-class queueing system", "abstract": "We consider the age-of-information in a multi-class M/G/1 queueing system, where each class generates packets containing status information. Age of information is a relatively new metric that measures the amount of time that elapsed between status updates, thus accounting for both the queueing delay and the delay between packet generation. This gives rise to a tradeoff between frequency of status updates, and queueing delay. In this paper, we study this tradeoff in a system with heterogenous users modeled as a multi-class M/G/1 queue. To this end, we derive the exact peak age-of-Information (PAoI) profile of the system, which measures the \u201cfreshness\u201d of the status information. We then seek to optimize the age of information, by formulating the problem using quasiconvex optimization, and obtain structural properties of the optimal solution.", "venue": "2015 IEEE International Symposium on Information Theory (ISIT)", "authors": ["Longbo  Huang", "Eytan  Modiano"], "year": 2015, "n_citations": 227}
{"id": 3572342, "s2_id": "c3bd2662d0e940aa5c0967e03db20c7309d1d38d", "title": "PiBooster: A Light-Weight Approach to Performance Improvements in Page Table Management for Paravirtual Virtual-Machines", "abstract": "In paravirtualization, the page table management components of the guest operating systems are properly patched for the security guarantees of the hypervisor. However, none of them pay enough attentions to the performance improvements, which results in two noticeable performance issues. First, such security patches exacerbate the problem that the execution paths of the guest page table (de)allocations become extremely long, which would consequently increase the latencies of process creations and exits. Second, the patches introduce many additional IOTLB flushes, leading to extra IOTLB misses, and the misses would have negative impacts on I/O performance of all peripheral devices. In this paper, we propose PiBooster, a novel lightweight approach for improving the performance in page table management. First, PiBooster shortens the execution paths of the page table (de)allocations by the PiBooster cache, which maintains dedicated buffers for serving page table (de)allocations. Second, PiBooster eliminates the additional IOTLB misses with a fine-grained validation scheme, which performs page table and DMA validations separately, instead of doing both together. We implement a prototype on Xen with Linux as the guest kernel. We do small modifications on Xen (166 SLoC) and Linux kernel (350 SLoC). We evaluate the I/O performance in both micro and macro ways. The micro experiment results indicate that PiBooster is able to completely eliminate the additional IOTLB flushes in the workload-stable environments, and effectively reduces (de)allocation time of the page table by 47% on average. The macro benchmarks show that the latencies of the process creations and exits are expectedly reduced by 16% on average. Moreover, the SPECINT,lmbench and netperf results indicate that PiBooster has no negative performance impacts on CPU computation, network I/O, and disk I/O.", "venue": "ArXiv", "authors": ["Zhi  Zhang", "Yueqiang  Cheng"], "year": 2019, "n_citations": 0}
{"id": 3573233, "s2_id": "8cfc0ba44bffe3c0599893af31a29d1265fb48ba", "title": "Characterising radio telescope software with the Workload Characterisation Framework", "abstract": "We present a modular framework, the Workload Characterisation Framework (WCF), that is developed to reproducibly obtain, store and compare key characteristics of radio astronomy processing software. As a demonstration, we discuss the experiences using the framework to characterise a LOFAR calibration and imaging pipeline.", "venue": "ArXiv", "authors": ["Y. G. Grange", "R.  Lakhoo", "Matthias  Petschow", "C.  Wu", "Bram  Veenboer", "I.  Emsley", "T. J. Dijkema", "A. P. Mechev", "G.  Mariani"], "year": 2016, "n_citations": 0}
{"id": 3573275, "s2_id": "0a7b3626ffa45a4cfdc753d8dc95ed2944cdc84f", "title": "Run Time Approximation of Non-blocking Service Rates for Streaming Systems", "abstract": "Stream processing is a compute paradigm that promises safe and efficient parallelism. Its realization requires optimization of multiple parameters such as kernel placement and communications. Most techniques to optimize streaming systems use queueing network models or network flow models, which often require estimates of the execution rate of each compute kernel. This is known as the non-blocking \"service rate\" of the kernel within the queueing literature. Current approaches to divining service rates are static. To maintain a tuned application during execution (while online) with non-static workloads, dynamic instrumentation of service rate is highly desirable. Our approach enables online service rate monitoring for streaming applications under most conditions, obviating the need to rely on steady state predictions for what are likely non-steady state phenomena. This work describes an algorithm to approximate non-blocking service rate, its implementation in the open source RaftLib framework, and validates the methodology using streaming applications on multi-core hardware.", "venue": "2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems", "authors": ["Jonathan C. Beard", "Roger D. Chamberlain"], "year": 2015, "n_citations": 5}
{"id": 3573604, "s2_id": "c8c43afaf7ea3e492c6d6ef7e5bd08e45df9f3f3", "title": "Extended Differential Aggregations in Process Algebra for Performance and Biology", "abstract": "We study aggregations for ordinary differential equations induced by fluid semantics for Markovian process algebra which can capture the dynamics of performance models and chemical reaction networks. Whilst previous work has required perfect symmetry for exact aggregation, we present approximate fluid lumpability, which makes nearby processes perfectly symmetric after a perturbation of their parameters. We prove that small perturbations yield nearby differential trajectories. Numerically, we show that many heterogeneous processes can be aggregated with negligible errors.", "venue": "QAPL", "authors": ["Max  Tschaikowski", "Mirco  Tribastone"], "year": 2014, "n_citations": 6}
{"id": 3574312, "s2_id": "490a1181f61f19fc3bec95e9e16a326f544ff796", "title": "Cloud-scale VM-deflation for Running Interactive Applications On Transient Servers", "abstract": "Transient computing has become popular in public cloud environments for running delay-insensitive batch and data processing applications at low cost. Since transient cloud servers can be revoked at any time by the cloud provider, they are considered unsuitable for running interactive application such as web services. In this paper, we present VM deflation as an alternative mechanism to server preemption for reclaiming resources from transient cloud servers under resource pressure. Using real traces from top-tier cloud providers, we show the feasibility of using VM deflation as a resource reclamation mechanism for interactive applications in public clouds. We show how current hypervisor mechanisms can be used to implement VM deflation and present cluster deflation policies for resource management of transient and on-demand cloud VMs. Experimental evaluation of our deflation system on a Linux cluster shows that microservice-based applications can be deflated by up to 50% with negligible performance overhead. Our cluster-level deflation policies allow overcommitment levels as high as 50%, with less than a 1% decrease in application throughput, and can enable cloud platforms to increase revenue by 30%.", "venue": "HPDC", "authors": ["Alexander  Fuerst", "Ahmed  Ali-Eldin", "Prashant  Shenoy", "Prateek  Sharma"], "year": 2020, "n_citations": 2}
{"id": 3579018, "s2_id": "8599604553442bf143cc5e2c2de198e4d6be2241", "title": "Performance of RPL in Healthcare Wireless Sensor Network", "abstract": "The new advances of the Internet of Things (IoT) technology can be utilized to promote service delivery in several real-life applications such as healthcare systems. The Routing Protocol for Low Power and Loss Network (RPL) is a routing protocol designed to serve as a proper routing protocol for packets in Wireless Sensor Networks (WSN). Among the most prominent issues exist in the RPL protocol are packet loss within the WSN and sensors power consumption especially in healthcare WSNs. Multiple Objective Functions (OF) in RPL intended to find the routes from source nodes to a destination node. This paper presents an evaluation to discover which OF is more efficient for a WSN in a healthcare scenario where the Packet Delivery Ratio (PDR) of WSN and the sensors' power consumption are prominent concerns. Expected transmission Count (ETX) and Objective Function Zero (OF0) of RPL were examined in various network densities and network topologies such as the grid and random topology. The simulation outcomes revealed that the OF0 is more efficient regarding the PDR and power consumption compared to the ETX in random", "venue": "ArXiv", "authors": ["Bassam  Al-Shargabi", "Mohammed  Aleswid"], "year": 2020, "n_citations": 2}
{"id": 3588153, "s2_id": "575b555beae0d658d7bbced4361299230e66c657", "title": "Automatically harnessing sparse acceleration", "abstract": "Sparse linear algebra is central to many scientific programs, yet compilers fail to optimize it well. High-performance libraries are available, but adoption costs are significant. Moreover, libraries tie programs into vendor-specific software and hardware ecosystems, creating non-portable code. In this paper, we develop a new approach based on our specification Language for implementers of Linear Algebra Computations (LiLAC). Rather than requiring the application developer to (re)write every program for a given library, the burden is shifted to a one-off description by the library implementer. The LiLAC-enabled compiler uses this to insert appropriate library routines without source code changes. LiLAC provides automatic data marshaling, maintaining state between calls and minimizing data transfers. Appropriate places for library insertion are detected in compiler intermediate representation, independent of source languages. We evaluated on large-scale scientific applications written in FORTRAN; standard C/C++ and FORTRAN benchmarks; and C++ graph analytics kernels. Across heterogeneous platforms, applications and data sets we show speedups of 1.1\u00d7to over 10\u00d7without user intervention.", "venue": "CC", "authors": ["Philip  Ginsbach", "Bruce  Collie", "Michael F. P. O'Boyle"], "year": 2020, "n_citations": 3}
{"id": 3589059, "s2_id": "de4c8b6385d5a7b262b39641003c8ffe562a004f", "title": "Joint channel and queue aware scheduling for wireless links with multiple fading states", "abstract": "In this work, we address the delay optimal scheduling problem for wireless transmission with fixed modulation over multi-state fading channels. We propose a stochastic scheduling policy which schedules the source to transmit with probability jointly based on the buffer and channel states, with an average power constraint at the transmitter. Our objective is to derive the optimal transmission probabilities such that the average queueing delay is minimized subject to the power constraint. Using Markov chain modeling, we formulate a power-constrained delay minimization problem, and then transform it into a Linear Programming one. By analyzing its property, we can derive the optimal threshold-based scheduling policy together with the corresponding transmission probabilities. Our theoretical analysis is corroborated by simulation results.", "venue": "2015 IEEE/CIC International Conference on Communications in China (ICCC)", "authors": ["Juan  Liu", "Wei  Chen", "Khaled Ben Letaief"], "year": 2015, "n_citations": 4}
{"id": 3595231, "s2_id": "6666b37424161119dc93e350aa952ebe7a465316", "title": "Rank and run-time aware compression of NLP Applications", "abstract": "Sequence model based NLP applications canbe large. Yet, many applications that benefit from them run on small devices with very limited compute and storage capabilities, while still having run-time constraints.As a result, there is a need for a compression technique that can achieve significant compression without negatively impacting inference run-time and task accuracy. This paper proposes a new compression technique called Hybrid Matrix Factorization (HMF) that achieves this dual objective. HMF improves low-rank matrix factorization (LMF) techniques by doubling the rank of the matrix using an intelligent hybrid-structure leading to better accuracy than LMF. Further, by preserving dense matrices, it leads to faster inference run-timethan pruning or structure matrix based compression technique. We evaluate the impact of this technique on 5 NLP benchmarks across multiple tasks (Translation, Intent Detection,Language Modeling) and show that for similar accuracy values and compression factors, HMF can achieve more than 2.32x faster inference run-time than pruning and 16.77% better accuracy than LMF.", "venue": "SUSTAINLP", "authors": ["Urmish  Thakker", "Jesse  Beu", "Dibakar  Gope", "Ganesh  Dasika", "Matthew  Mattina"], "year": 2020, "n_citations": 3}
{"id": 3596804, "s2_id": "e5921136d6accdc362369fa82614a9745a6797f2", "title": "FastDeepIoT: Towards Understanding and Optimizing Neural Network Execution Time on Mobile and Embedded Devices", "abstract": "Deep neural networks show great potential as solutions to many sensing application problems, but their excessive resource demand slows down execution time, pausing a serious impediment to deployment on low-end devices. To address this challenge, recent literature focused on compressing neural network size to improve performance. We show that changing neural network size does not proportionally affect performance attributes of interest, such as execution time. Rather, extreme run-time nonlinearities exist over the network configuration space. Hence, we propose a novel framework, called FastDeepIoT, that uncovers the non-linear relation between neural network structure and execution time, then exploits that understanding to find network configurations that significantly improve the trade-off between execution time and accuracy on mobile and embedded devices. FastDeepIoT makes two key contributions. First, FastDeepIoT automatically learns an accurate and highly interpretable execution time model for deep neural networks on the target device. This is done without prior knowledge of either the hardware specifications or the detailed implementation of the used deep learning library. Second, FastDeepIoT informs a compression algorithm how to minimize execution time on the profiled device without impacting accuracy. We evaluate FastDeepIoT using three different sensing-related tasks on two mobile devices: Nexus 5 and Galaxy Nexus. FastDeepIoT further reduces the neural network execution time by 48% to 78% and energy consumption by 37% to 69% compared with the state-of-the-art compression algorithms.", "venue": "SenSys", "authors": ["Shuochao  Yao", "Yiran  Zhao", "Huajie  Shao", "Shengzhong  Liu", "Dongxin  Liu", "Lu  Su", "Tarek F. Abdelzaher"], "year": 2018, "n_citations": 57}
{"id": 3598540, "s2_id": "c6d7aefeccf2ec0d7ea160487d792f1529bc1017", "title": "A performance analysis of HICCUPS\u2014a steganographic system for WLAN", "abstract": "The paper presents an analysis of performance features of the HICCUPS (HIdden Communication system for CorrUPted networkS) including the efficiency and the cost of the system in WLANs (Wireless Local Area Networks). The analysis relies on the original CSMA/CA (Carrier Sense Multiple Access with Collision Avoidance) 802.11 Markov chain-based model and proves that the HICCUPS is the efficient steganographic method with the reasonable cost.", "venue": "2009 International Conference on Multimedia Information Networking and Security", "authors": ["Krzysztof  Szczypiorski"], "year": 2009, "n_citations": 21}
{"id": 3599510, "s2_id": "dfb84625bf50282d576574d462c23d67f027fed7", "title": "Self-* Overload Control for Distributed Web Systems", "abstract": "Unexpected increases in demand and most of all flash crowds are considered the bane of every Web application as they may cause intolerable delays or even service unavailability. Proper quality of service policies must guarantee rapid reactivity and responsiveness even in such critical situations. Previous solutions fail to meet common performance requirements when the system has to face sudden and unpredictable surges of traffic. Indeed they often rely on a proper setting of key parameters which requires laborious manual tuning, preventing a fast adaptation of the control policies. We contribute an original self-overload control (SOC) policy. This allows the system to self-configure a dynamic constraint on the rate of admitted sessions in order to respect service level agreements and maximize the resource utilization at the same time. Our policy does not require any prior information on the incoming traffic or manual configuration of key parameters. We ran extensive simulations under a wide range of operating conditions, showing that SOC rapidly adapts to time varying traffic and self-optimizes the resource utilization. It admits as many new sessions as possible in observance of the agreements, even under intense workload variations. We compared our algorithm to previously proposed approaches highlighting a more stable behavior and a better performance.", "venue": "2008 16th Interntional Workshop on Quality of Service", "authors": ["Novella  Bartolini", "Giancarlo  Bongiovanni", "Simone  Silvestri"], "year": 2008, "n_citations": 9}
{"id": 3600871, "s2_id": "2e6e21db8032eedf3c88b4e0bf781efcade47d5b", "title": "GPU Acceleration of 3D Agent-Based Biological Simulations", "abstract": "Researchers in biology are faced with the tough challenge of developing high-performance computer simulations of their increasingly complex agent-based models. BioDynaMo is an open-source agent-based simulation platform that aims to alleviate researchers from the intricacies that go into the development of high-performance computing. Through a high-level interface, researchers can implement their models on top of BioDynaMo's multi-threaded core execution engine to rapidly develop simulations that effectively utilize parallel computing hardware. In biological agent-based modeling, the type of operations that are typically the most compute-intensive are those that involve agents interacting with their local neighborhood. In this work, we investigate the currently implemented method of handling neighborhood interactions of cellular agents in BioDynaMo, and ways to improve the performance to enable large-scale and complex simulations. We propose to replace the kd-tree implementation to find and iterate over the neighborhood of each agent with a uniform grid method that allows us to take advantage of the massively parallel architecture of graphics processing units (GPUs). We implement the uniform grid method in both CUDA and OpenCL to address GPUs from all major vendors and evaluate several techniques to further improve the performance. Furthermore, we analyze the performance of our implementations for models with a varying density of neighboring agents. As a result, the performance of the mechanical interactions method improved by up to two orders of magnitude in comparison to the multithreaded baseline version. The implementations are open-source and publicly available on Github.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Ahmad  Hesam", "Lukas  Breitwieser", "Fons  Rademakers", "Zaid  Al-Ars"], "year": 2021, "n_citations": 0}
{"id": 3602442, "s2_id": "79c9190de8a8199c28f1490122b9b10dd2330eb4", "title": "Scalability of the plasma physics code GEM", "abstract": "We discuss a detailed weak scaling analysis of GEM, a 3D MPI-parallelised gyrofluid code used in theoretical plasma physics at the Max Planck Institute of Plasma Physics, IPP at Garching b. M\\\"unchen, Germany. Within a PRACE Preparatory Access Project various versions of the code have been analysed on the HPC systems SuperMUC at LRZ and JUQUEEN at J\\\"ulich Supercomputing Centre (JSC) to improve the parallel scalability of the application. The diagnostic tool Scalasca has been used to filter out suboptimal routines. The code uses the electromagnetic gyrofluid model which is a superset of magnetohydrodynamic and drift-Alfv\\'en microturbulance and also includes several relevant kinetic processes. GEM can be used with different geometries depending on the targeted use case, and has been proven to show good scalability when the computational domain is distributed amongst two dimensions. Such a distribution allows grids with sufficient size to describe small scale tokamak devices. In order to enable simulation of very large tokamaks (such as the next generation nuclear fusion device ITER in Cadarache, France) the third dimension has been parallelised and weak scaling has been achieved for significantly larger grids.", "venue": "ArXiv", "authors": ["Bruce D. Scott", "Volker  Weinberg", "Olivier  Hoenen", "Anumap  Karmakar", "Luis  Fazendeiro"], "year": 2013, "n_citations": 1}
{"id": 3606112, "s2_id": "9cee745e931a31329c597fed74812bf7771e5378", "title": "Next-Generation Information Technology Systems for Fast Detectors in Electron Microscop", "abstract": "The Gatan K2 IS direct electron detector (Gatan Inc., 2018), which was introduced in 2014, marked a watershed moment in the development of cameras for transmission electron microscopy (TEM) (Pan & Czarnik, 2016). Its pixel frequency, i.e. the number of data points (pixels) recorded per second, was two orders of magnitude higher than the fastest cameras available only five years before. Starting from 2009, the data rate of TEM cameras has outpaced the development of network, mass storage and memory bandwidth by almost two orders of magnitude. Consequently, solutions based on personal computers (PCs) that were adequate until then are no longer able to handle the resulting data rates. Instead, tailored high-performance setups are necessary. Similar developments have occurred for advanced X-ray sources such as the European XFEL, requiring special information technology (IT) systems for data handling (Sauter, Hattne, Grosse-Kunstleve, & Echols, 2013) (Fangohr, et al., 2018). Information and detector technology are currently under rapid development and involve disruptive technological innovations. This chapter briefly reviews the technological developments of the past 20 years, presents a snapshot of the current situation at the beginning of 2019 with many practical considerations, and looks forward to future developments.", "venue": "ArXiv", "authors": ["Dieter  Weber", "Alexander  Clausen", "Rafal E. Dunin-Borkowski"], "year": 2020, "n_citations": 4}
{"id": 3606164, "s2_id": "393f94e6870e292c97ca2f8a417b8bcb8ae29bbc", "title": "Scheduling with regular performance measures and optional job rejection on a single machine", "abstract": "Abstract We address single machine problems with optional job rejection, and focus on minimizing regular performance measures, ie, functions that are non-decreasing in the jobs, completion times, subject to the constraint that the total rejection cost cannot exceed a predefined upper bound. Our contribution is twofold. First, we consider two problems that, to the best of our knowledge, were not addressed in scheduling theory \u2013 total (unweighted) tardiness with a common due date and total weighted tardiness with a common due date. For these problems, we show that they are NP-hard and present pseudo-polynomial-time dynamic programming (DP) solution algorithms. Second, we revisit three problems: makespan with release-dates, total completion time, and total weighted completion time, and present enhanced DP solution algorithms. To all studied problems, we provide extensive numerical studies, verifying their efficiency, subsequently demonstrating both theoretical and practical enhancement.", "venue": "J. Oper. Res. Soc.", "authors": ["Baruch  Mor", "Dana  Shapira"], "year": 2020, "n_citations": 9}
{"id": 3606683, "s2_id": "81c53e8237a87499a2342164f9251097ce08241b", "title": "Bit-Parallel Vector Composability for Neural Acceleration", "abstract": "Conventional neural accelerators rely on isolated self-sufficient functional units that perform an atomic operation while communicating the results through an operand delivery-aggregation logic. Each single unit processes all the bits of their operands atomically and produce all the bits of the results in isolation. This paper explores a different design style, where each unit is only responsible for a slice of the bit-level operations to interleave and combine the benefits of bit-level parallelism with the abundant data-level parallelism in deep neural networks. A dynamic collection of these units cooperate at runtime to generate bits of the results, collectively. Such cooperation requires extracting new grouping between the bits, which is only possible if the operands and operations are vectorizable. The abundance of Data-Level Parallelism and mostly repeated execution patterns, provides a unique opportunity to define and leverage this new dimension of Bit-Parallel Vector Composability. This design intersperses bit parallelism within data-level parallelism and dynamically interweaves the two together. As such, the building block of our neural accelerator is a Composable Vector Unit that is a collection of Narrower-Bitwidth Vector Engines, which are dynamically composed or decomposed at the bit granularity. Using six diverse CNN and LSTM deep networks, we evaluate this design style across four design points: with and without algorithmic bitwidth heterogeneity and with and without availability of a high-bandwidth off-chip memory. Across these four design points, Bit-Parallel Vector Composability brings (1.4\u00d7 to 3.5\u00d7) speedup and (1.1\u00d7 to 2.7\u00d7) energy reduction. We also comprehensively compare our design style to the Nvidia's RTX 2080 TI GPU, which also supports INT-4 execution. The benefits range between 28.0\u00d7 and 33.7\u00d7 improvement in Performance-per-Watt.", "venue": "2020 57th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Soroush  Ghodrati", "Hardik  Sharma", "Cliff  Young", "Nam Sung Kim", "Hadi  Esmaeilzadeh"], "year": 2020, "n_citations": 5}
{"id": 3607524, "s2_id": "e0503c05bf0a6e228bae262db4fe84234d6521f1", "title": "Pigouvian Tolls and Welfare Optimality with Parallel Servers and Heterogeneous Customers", "abstract": "Congestion externalities are a well-known phenomenon in transportation and communication networks, healthcare etc. Optimization by self-interested agents in such settings typically results in equilibria which are sub-optimal for social welfare. Pigouvian taxes or tolls, which impose a user charge equal to the negative externality caused by the marginal user to other users, are a mechanism for combating this problem. In this paper, we study a non-atomic congestion game in which heterogeneous agents choose amongst a finite set of heterogeneous servers. The delay at a server is an increasing function of its load. Agents differ in their sensitivity to delay. We show that, while selfish optimisation by agents is sub-optimal for social welfare, imposing admission charges at the servers equal to the Pigouvian tax causes the user equilibrium to maximize social welfare. In addition, we characterize the structure of welfare optimal and of equilibrium allocations.", "venue": "Journal of the Indian Institute of Science", "authors": ["Tejas  Bodas", "Ayalvadi  Ganesh", "D.  Manjunath"], "year": 2021, "n_citations": 0}
{"id": 3608344, "s2_id": "e1c8d89acd9f83e81e6fe14307e1433ed72bdcab", "title": "Transient Performance Analysis of Zero-Attracting LMS", "abstract": "Zero-attracting least-mean-square (ZA-LMS) algorithm has been widely used for online sparse system identification. It combines the LMS framework and l1-norm regularization to promote sparsity, and relies on subgradient iterations. Despite the significant interest in ZA-LMS, few works analyzed its transient behavior. The main difficulty lies in the nonlinearity of the update rule. In this study, a detailed analysis in the mean and mean-square sense is carried out in order to examine the behavior of the algorithm. Simulation results illustrate the accuracy of the model and highlight its performance through comparisons with an existing model.", "venue": "IEEE Signal Processing Letters", "authors": ["Jie  Chen", "C\u00e9dric  Richard", "Yingying  Song", "David  Brie"], "year": 2016, "n_citations": 26}
{"id": 3612790, "s2_id": "c5537775d297865cce8844d8c69f1fd4067d3031", "title": "Performance Evaluation of Mesh based Multicast Reactive Routing Protocol under Black Hole Attack", "abstract": "A mobile ad-hoc network is an autonomous system of mobile nodes connected by wireless links in which nodes cooperate by forwarding packets for each other thereby enabling communication beyond direct wireless transmission range. The wireless and dynamic nature of ad-hoc networks makes them vulnerable to attacks especially in routing protocols. Providing security in mobile ad-hoc networks has been a major issue over the recent years. One of the prominent mesh base reactive multicast routing protocols used in ad-hoc networks is On Demand Multicast Routing protocol (ODMRP). The security of ODMRP is compromised by a primary routing attack called black hole attack. In this attack a malicious node advertises itself as having the shortest path to the node whose packets it wants to intercept. This paper discusses the impact of black hole attack on ODMRP under various scenarios. The performance is evaluated using metrics such as packet delivery ratio and end to end delay for various numbers of senders and receivers via simulation. Simulations are carried out using network simulator ns-2. The results enable us to propose solutions to counter the effect of black hole attack.", "venue": "ArXiv", "authors": ["E. A. Mary Anita", "V.  Vasudevan"], "year": 2009, "n_citations": 7}
{"id": 3613203, "s2_id": "707a440c3da5e04d96c673536a6b2e0c954a833e", "title": "Measurement and Prediction of Centrical/Peripheral Network Properties based on Regression Analysis - A Parametric Foundation for Performance Self-Management in WSNs", "abstract": "Predicting performance-related behavior of the underlying network structure becomes more and more indispensable in terms of the aspired application outcome quality. However, the reliable forecast of QoS metrics like packet transfer delay in wireless network systems is still a challenging task. Even though existing approaches are technically capable of determining such network properties under certain assumptions, they mostly abstract away from primal aspects that inherently have an essential impact on temporal network performance dynamics. Also, they usually require auxiliary resources to be implemented and deployed along with the actual network components. In the course of developing a lightweight measurement-based alternative for the self-inspection and prediction of volatile performance characteristics in environments of any kind, we selectively investigate the duration of message delivery and packet loss rate against various parameters peculiar to common radio network technologies like Wireless Sensor Networks (WSNs). Our hands-on experiments reveal the relations between the oftentimes underestimated medium access delay and a variety of main influencing factors including packet size, backoff period, and number of neighbor nodes contending for the communication medium. A closed formulation of selected weighted drivers facilitates the average-case prediction of inter-node packet transfer delays for arbitrary configurations of given network parameters even on resource-scarce WSN devices. We validate our prediction method against basic multi-hop networking scenarios. Yield field test results proof the basic feasibility and high precision of our approach to network property estimation in virtue of self-governed local measurements and regression-based calculations paving the way for a prospective self-management of network properties based upon autonomous distributed coordination.", "venue": "ArXiv", "authors": ["Adam  Bachorek", "Bagavathiannan  Palanisamy", "Jens B. Schmitt"], "year": 2013, "n_citations": 1}
{"id": 3614440, "s2_id": "bddc95111add014446ca4b6d7704a63fcb52af6d", "title": "Methodology for assessing system performance loss within a proactive maintenance framework", "abstract": "Maintenance plays now a critical role in manufacturing for achieving important cost savings and competitive advantage while preserving product conditions. It suggests moving from conventional maintenance practices to predictive strategy. Indeed the maintenance action has to be done at the right time based on the system performance and component Remaining Useful Life (RUL) assessed by a prognostic process. In that way, this paper proposes a methodology in order to evaluate the performance loss of the system according to the degradation of component and the deviations of system input flows. This methodology is supported by the neuro-fuzzy tool ANFIS (Adaptive Neuro-Fuzzy Inference Systems) that allows to integrate knowledge from two different sources: expertise and real data. The feasibility and added value of such methodology is then highlighted through an application case extracted from the TELMA platform used for education and research.", "venue": "ArXiv", "authors": ["Pierre  Cocheteux", "Alexandre  Voisin", "Eric  Levrat", "Beno\u00eet  Iung"], "year": 2009, "n_citations": 6}
{"id": 3618550, "s2_id": "95ab755e1e9b88fc0f6d4fd4c655237449bd1f75", "title": "A Technical Report On Grid Benchmarking using SEE V.O", "abstract": "Grids include heterogeneous resources, which are based on different hardware and software architectures or components. In correspondence with this diversity of the infrastructure, the execution time of any single job, as well as the total grid performance can both be affected substantially, which can be demonstrated by measurements. Running a simple benchmarking suite can show this heterogeneity and give us results about the differences over the grid sites.", "venue": "ArXiv", "authors": ["John  Kouvakis", "Fotis  Georgatos"], "year": 2007, "n_citations": 1}
{"id": 3621009, "s2_id": "62ad2b99c62dd1979f8efa664727dd385a07be85", "title": "Zoom: SSD-based Vector Search for Optimizing Accuracy, Latency and Memory", "abstract": "With the advancement of machine learning and deep learning, vector search becomes instrumental to many information retrieval systems, to search and find best matches to user queries based on their semantic similarities.These online services require the search architecture to be both effective with high accuracy and efficient with low latency and memory footprint, which existing work fails to offer. We develop, Zoom, a new vector search solution that collaboratively optimizes accuracy, latency and memory based on a multiview approach. (1) A \"preview\" step generates a small set of good candidates, leveraging compressed vectors in memory for reduced footprint and fast lookup. (2) A \"fullview\" step on SSDs reranks those candidates with their full-length vector, striking high accuracy. Our evaluation shows that, Zoom achieves an order of magnitude improvements on efficiency while attaining equal or higher accuracy, comparing with the state-of-the-art.", "venue": "ArXiv", "authors": ["Minjia  Zhang", "Yuxiong  He"], "year": 2018, "n_citations": 3}
{"id": 3623222, "s2_id": "4bc646626ffe734d8d20e800827ebaa1bc5b068b", "title": "Impact of network delays on Hyperledger Fabric", "abstract": "Blockchain has become one of the most attractive technologies for applications, with a large range of deployments such as production, economy, or banking. Under the hood, Blockchain technology is a type of distributed database that supports untrusted parties. In this paper we focus Hyperledger Fabric, the first blockchain in the market tailored for a private environment, allowing businesses to create a permissioned network. Hyperledger Fabric implements a PBFT consensus in order to maintain a non forking blockchain at the application level. We deployed this framework over an area network between France and Germany in order to evaluate its performance when potentially large network delays are observed. Overall we found that when network delay increases significantly (i.e. up to 3.5 seconds at network layer between two clouds), we observed that the blocks added to our blockchain had up to 134 seconds offset after 100th block from one cloud to another. Thus by delaying block propagation, we demonstrated that Hyperledger Fabric does not provide sufficient consistency guaranties to be deployed in critical environments. Our work, is the fist to evidence the negative impact of network delays on a PBFT-based blockchain.", "venue": "IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)", "authors": ["Thanh Son Lam Nguyen", "Guillaume  Jourjon", "Maria Gradinariu Potop-Butucaru", "Kim  Thai"], "year": 2019, "n_citations": 19}
{"id": 3630409, "s2_id": "24a8d77ba0f7ee7f86744b17553798006f7b19fe", "title": "Fast prototyping of an SDR WLAN 802.11b receiver for an indoor positioning system", "abstract": "Indoor positioning systems (IPS) are emerging technologies due to an increasing popularity and demand in location based service (LBS). Because traditional positioning systems such as GPS are limited to outdoor applications, many IPS have been proposed in literature. WLAN-based IPS are the most promising due to its proven accuracy and infrastructure deployment. Several WLAN-based IPS have been proposed in the past, from which the best results have been shown by so-called fingerprint-based systems. This paper proposes an indoor positioning system which extends traditional WLAN fingerprinting by using received signal strength (RSS) measurements along with channel estimates as an effort to improve classification accuracy for scenarios with a low number of Access Points (APs). The channel estimates aim to characterize complex indoor environments making it a unique signature for fingerprinting-based IPS and therefore improving pattern recognition in radio-maps. Since commercial WLAN cards offer limited measurement information, software-defined radio (SDR) as an emerging trend for fast prototyping and research integration is chosen as the best cost-effective option to extract channel estimates. Therefore, this paper first proposes an 802.11b WLAN SDR beacon receiver capable of measuring RSS and channel estimates. The SDR is designed using LabVIEW (LV) environment and leverages several inherent platform acceleration features that achieve real-time capturing. The receiver achieves a fast-rate measurement capture of 9 packets per second per AP. The classification of the propose IPS uses a support vector machine (SVM) for offline training and online navigation. Several tests are conducted in a cluttered indoor environment with a single AP in 802.11b legacy mode. Finally, navigation accuracy results are discussed.", "venue": "Proceedings of the 31st International Technical Meeting of The Satellite Division of the Institute of Navigation (ION GNSS+ 2018)", "authors": ["Erick  Schmidt", "David  Akopian"], "year": 2018, "n_citations": 2}
{"id": 3633624, "s2_id": "bfe4f746d7d38109370e8c4159553bf3dcc94513", "title": "Lazy State Determination: More concurrency for contending linearizable transactions", "abstract": "The concurrency control algorithms in transactional systems limits concurrency to provide strong semantics, which leads to poor performance under high contention. As a consequence, many transactional systems eschew strong semantics to achieve acceptable performance. We show that by leveraging semantic information associated with the transactional programs to increase concurrency, it is possible to significantly improve performance while maintaining linearizability. To this end, we introduce the lazy state determination API to easily expose the semantics of application transactions to the database, and propose new optimistic and pessimistic concurrency control algorithms that leverage this information to safely increase concurrency in the presence of contention. Our evaluation shows that our approach can achieve up to 5x more throughput with 1.5c less latency than standard techniques in the popular TPC-C benchmark.", "venue": "ArXiv", "authors": ["Tiago M. Vale", "Jo\u00e3o  Leit\u00e3o", "Nuno M. Pregui\u00e7a", "Rodrigo  Rodrigues", "Ricardo J. Dias", "Jo\u00e3o  Louren\u00e7o"], "year": 2020, "n_citations": 0}
{"id": 3634748, "s2_id": "c9855385e3f5398a97c5417851fc9efbbe9acd05", "title": "Time Varying Channel Tracking With Spatial and Temporal BEM for Massive MIMO Systems", "abstract": "In this paper, we design a channel tracking method for massive multiple-input multiple-output systems under both time-varying and spatial-varying circumstances. By exploiting the characteristics of massive antenna array, a spatial-temporal basis expansion model is proposed to reduce the effective dimension of uplink/downlink channel, which decomposes channel state information into time-varying spatial information and gain information. We first model the user\u2019s movement as the one-order unknown Markov process, whose parameters are blindly obtained by expectation and maximization learning. Then, the uplink time-varying spatial information can also be blindly tracked by unscented Kalman filter and Taylor series expansion of the steering vector, while the rest of uplink channel gain information can be trained by only a few pilot symbols. Due to physical angle reciprocity, the spatial information of the downlink channel can be immediately computed from the uplink counterpart, which greatly reduces the complexity of downlink channel tracking. Various numerical results are provided to demonstrate the effectiveness of the proposed method.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Jianwei  Zhao", "Hongxiang  Xie", "Feifei  Gao", "Weimin  Jia", "Shi  Jin", "Hai  Lin"], "year": 2018, "n_citations": 28}
{"id": 3636238, "s2_id": "acb02b7cbe3aaf863aea3c140c96b33751872db7", "title": "Towards a modern CMake workflow", "abstract": "Modern CMake offers the features to manage versatile and complex projects with ease. With respect to OMNeT++ projects, a workflow relying on CMake enables projects to combine discrete event simulation and production code in a common development environment. Such a combination means less maintenance effort and thus potentially more sustainable and long-living software. This paper highlights the significant improvements since the first attempt of using CMake in OMNeT++ projects. In particular, a state-of-the-art integration of OMNeT++ in Visual Studio Code including support for debugging and multi-platform compilation is presented. Last but not least, an exemplary use case demonstrates the powerful mix of production and simulation code in a common software architecture supported by the OMNeT++ CMake package.", "venue": "ArXiv", "authors": ["Heinz-Peter  Liechtenecker", "Raphael  Riebl"], "year": 2021, "n_citations": 0}
{"id": 3639717, "s2_id": "64ee70b75343754a5d63fe986669c8a714a81500", "title": "A Collaborative Filtering Approach for the Automatic Tuning of Compiler Optimisations", "abstract": "Selecting the right compiler optimisations has a severe impact on programs' performance. Still, the available optimisations keep increasing, and their effect depends on the specific program, making the task human intractable. Researchers proposed several techniques to search in the space of compiler optimisations. Some approaches focus on finding better search algorithms, while others try to speed up the search by leveraging previously collected knowledge. The possibility to effectively reuse previous compilation results inspired us toward the investigation of techniques derived from the Recommender Systems field. The proposed approach exploits previously collected knowledge and improves its characterisation over time. Differently from current state-of-the-art solutions, our approach is not based on performance counters but relies on Reaction Matching, an algorithm able to characterise programs looking at how they react to different optimisation sets. The proposed approach has been validated using two widely used benchmark suites, cBench and PolyBench, including 54 different programs. Our solution, on average, extracted 90% of the available performance improvement 10 iterations before current state-of-the-art solutions,which corresponds to 40% fewer compilations and performance tests to perform.", "venue": "LCTES", "authors": ["Stefano  Cereda", "Gianluca  Palermo", "Paolo  Cremonesi", "Stefano  Doni"], "year": 2020, "n_citations": 4}
{"id": 3639980, "s2_id": "97709858442f3f23ca3208688772cf23f4ae25aa", "title": "Performance Modeling and Evaluation for Information-Driven Networks", "abstract": "Information-driven networks include a large category of networking systems, where network nodes are aware of information delivered and thus can not only forward data packets but may also perform information processing. In many situations, the quality of service (QoS) in information-driven networks is provisioned with the redundancy in information. Traditional performance models generally adopt evaluation measures suitable for packet-oriented service guarantee, such as packet delay, throughput, and packet loss rate. These performance measures, however, do not align well with the actual need of information-driven networks. New performance measures and models for information-driven networks, despite their importance, have been mainly blank, largely because information processing is clearly application dependent and cannot be easily captured within a generic framework. To fill the vacancy, we present a new performance evaluation framework particularly tailored for information-driven networks, based on the recent development of stochastic network calculus. We analyze the QoS with respect to information delivery and study the scheduling problem with the new performance metrics. Our analytical framework can be used to calculate the network capacity in information delivery and in the meantime to help transmission scheduling for a large body of systems where QoS is stochastically guaranteed with the redundancy in information.", "venue": "ArXiv", "authors": ["Kui  Wu", "Yuming  Jiang", "Guoqiang  Hu"], "year": 2008, "n_citations": 1}
{"id": 3640768, "s2_id": "19470b058b9b7a7bf86516a0e542c28c888e88ba", "title": "Throughput Limits of IEEE 802.11 and IEEE 802.15.3", "abstract": "IEEE 802.11 and IEEE 802.15.3 are wireless standards originally designed for wireless local area network (WLAN) and wireless personal area network (WPAN). This paper studies MAC throughput analysis of both standards. We present a comparative analysis of both standards in terms of MAC throughput and bandwidth efficiency. Numerical results show that the performance of IEEE 802.15.3 transcends IEEE 802.11 in all cases.", "venue": "2008 4th International Conference on Wireless Communications, Networking and Mobile Computing", "authors": ["Sana  Ullah", "Yingji  Zhong", "S. M. Riazul Islam", "Ahasanun  Nessa", "Kyung Sup Kwak"], "year": 2008, "n_citations": 1}
{"id": 3643001, "s2_id": "2494c56aff4bfa6da01ebec656930778c3644348", "title": "Performance Bounds for Multiclass FIFO in Communication Networks: A Deterministic Case", "abstract": "Multiclass FIFO is used in communication networks such as in input-queueing routers/switches and in wireless networks. For the concern of providing service guarantees in such networks, it is crucial to have analytical results, e.g. bounds, on the performance of multi-class FIFO. Surprisingly, there are few such results in the literature. This paper is devoted to filling the gap. Specifically, a single hop deterministic case is studied, for which, delay and backlog bounds are derived, in addition to guaranteed rate and service curve characterizations that may be exploited to extend the analysis to network cases.", "venue": "ArXiv", "authors": ["Yuming  Jiang"], "year": 2013, "n_citations": 2}
{"id": 3643742, "s2_id": "27dd79b06ccb76dc544ee76dab00738decfe7235", "title": "Evaluation of Pilot Jobs for Apache Spark Applications on HPC Clusters", "abstract": "Big Data has become prominent throughout many scientific fields, and as a result, scientific communities have sought out Big Data frameworks to accelerate the processing of their increasingly data-intensive pipelines. However, while scientific communities typically rely on High-Performance Computing (HPC) clusters for the parallelization of their pipelines, many popular Big Data frameworks such as Hadoop and Apache Spark were primarily designed to be executed on dedicated commodity infrastructures. This paper evaluates the benefits of pilot jobs over traditional batch submission for Apache Spark on HPC clusters. Surprisingly, our results show that the speed-up provided by pilot jobs over batch scheduling is moderate to non-existent (0.98 on average) despite the presence of long queuing times. In addition, pilot jobs provide an extra layer of scheduling that complicates debugging and deployment. We conclude that traditional batch scheduling should remain the default strategy to deploy Apache Spark applications on HPC clusters.", "venue": "2019 15th International Conference on eScience (eScience)", "authors": ["Val\u00e9rie  Hayot-Sasson", "Tristan  Glatard"], "year": 2019, "n_citations": 0}
{"id": 3652055, "s2_id": "91cc5192a2c389a6b71b5adb6643c4eb14302f1f", "title": "FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference", "abstract": "Deep learning models typically use single-precision (FP32) floating point data types for representing activations and weights, but a slew of recent research work has shown that computations with reduced-precision data types (FP16, 16-bit integers, 8-bit integers or even 4or 2-bit integers) are enough to achieve same accuracy as FP32 and are much more efficient. Therefore, we designed fbgemm, a high-performance kernel library, from ground up to perform high-performance quantized inference on current generation CPUs. fbgemm achieves efficiency by fusing common quantization operations with a high-performance gemm implementation and by shapeand size-specific kernel code generation at runtime. The library has been deployed at Facebook, where it delivers greater than 2\u00d7 performance gains with respect to our current production baseline.", "venue": "ArXiv", "authors": ["Daya  Khudia", "Jianyu  Huang", "Protonu  Basu", "Summer  Deng", "Haixin  Liu", "Jongsoo  Park", "Mikhail  Smelyanskiy"], "year": 2021, "n_citations": 8}
{"id": 3652135, "s2_id": "dcdd4e71209c896921e4e5e7a24a5116507a3647", "title": "Accelerating Deep Learning Inference via Learned Caches", "abstract": "Deep Neural Networks (DNNs) are witnessing increased adoption in multiple domains owing to their high accuracy in solving real-world problems. However, this high accuracy has been achieved by building deeper networks, posing a fundamental challenge to the low latency inference desired by user-facing applications. Current low latency solutions trade-off on accuracy or fail to exploit the inherent temporal locality in prediction serving workloads. We observe that caching hidden layer outputs of the DNN can introduce a form of late-binding where inference requests only consume the amount of computation needed. This enables a mechanism for achieving low latencies, coupled with an ability to exploit temporal locality. However, traditional caching approaches incur high memory overheads and lookup latencies, leading us to design learned caches caches that consist of simple ML models that are continuously updated. We present the design of GATI, an end-to-end prediction serving system that incorporates learned caches for low-latency DNN inference. Results show that GATI can reduce inference latency by up to 7.69\u00d7 on realistic workloads.", "venue": "ArXiv", "authors": ["Arjun  Balasubramanian", "Adarsh  Kumar", "Yuhan  Liu", "Han  Cao", "Shivaram  Venkataraman", "Aditya  Akella"], "year": 2021, "n_citations": 3}
{"id": 3653649, "s2_id": "21e2d938b8cc4fa7d48f002690453a80f06790ce", "title": "Importing Relationships into a Running Graph Database Using Parallel Processing", "abstract": "Importing relationships into a running graph database using multiple threads running concurrently is a difficult task, as multiple threads cannot write information to the same node at the same time. Here we present an algorithm in which relationships are sorted into bins, then imported such that no two threads ever access the same node concurrently. When this algorithm was implemented as a procedure to run on the Neo4j graph database, it reduced the time to import relationships by up to 69% when 32 threads were used.", "venue": "ArXiv", "authors": ["Joshua  Porter", "Aleks  Ontman"], "year": 2020, "n_citations": 0}
{"id": 3653695, "s2_id": "b8cabac7bfad71ed8daabfd6f298ddd8972dff9b", "title": "Optimal Transaction Queue Waiting in Blockchain Mining", "abstract": "Blockchain systems are being used in a wide range of application domains. They can support trusted transactions in time critical applications. In this paper, we study how miners should pick up transactions from a transaction pool so as to minimize the average waiting time per transaction. We derive an expression for the average transaction waiting time of the proposed mining scheme and determine the optimum decision rule. Numerical results show that the average waiting time per transaction can be reduced by about 10% compared to the traditional no-wait scheme in which miners immediately start the next mining round using all transactions waiting in the pool.", "venue": "ArXiv", "authors": ["Gholamreza  Ramezan", "Cyril  Leung", "Chunyan  Miao"], "year": 2020, "n_citations": 0}
{"id": 3654606, "s2_id": "7234c8c8ea752e9d4bbd2384903a6af49f18c22d", "title": "Triple State QuickSort, A replacement for the C/C++ library qsort", "abstract": "An industrial grade Quicksort function along with its new algorithm is presented. Compared to 4 other well known implementations of Quicksort, the new algorithm reduces both the number of comparisons and swaps in most cases while staying close to the best of the 4 in worst cases. We trade space for performance, at the price of n/2 temporary extra spaces in the worst case. Run time tests reveal an overall improvement of at least 15.8% compared to the overall best of the other 4 functions. Furthermore, our function scores a 32.7% run time improvement against Yaroslavskiy's new Dual Pivot Quicksort. Our function is pointer based, which is meant as a replacement for the C/C++ library qsort(). But we also provide an array based function of the same algorithm for easy porting to different programming languages.", "venue": "ArXiv", "authors": ["Ammar  Muqaddas"], "year": 2015, "n_citations": 0}
{"id": 3657395, "s2_id": "c8e6b782fb515e464408cc79ae91a6d6f79ccc10", "title": "Effect of Spatial Interference Correlation on the Performance of Maximum Ratio Combining", "abstract": "While the performance of maximum ratio combining (MRC) is well understood for a single isolated link, the same is not true in the presence of interference, which is typically correlated across antennas due to the common locations of interferers. For tractability, prior work focuses on the two extreme cases where the interference power across antennas is either assumed to be fully correlated or fully uncorrelated. In this paper, we address this shortcoming and characterize the performance of MRC in the presence of spatially-correlated interference across antennas. Modeling the interference field as a Poisson point process, we derive the exact distribution of the signal-to-interference ratio (SIR) for the case of two receive antennas, and upper and lower bounds for the general case. Using these results, we study the diversity behavior of MRC and characterize the critical density of simultaneous transmissions for a given outage constraint. The exact SIR distribution is also useful in benchmarking simpler correlation models. We show that the full-correlation assumption is considerably pessimistic (up to 30% higher outage probability for typical values) and the no-correlation assumption is significantly optimistic compared to the true performance.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Ralph  Tanbourgi", "Harpreet S. Dhillon", "Jeffrey G. Andrews", "Friedrich  Jondral"], "year": 2014, "n_citations": 76}
{"id": 3657720, "s2_id": "f0f41d9457e6d2cb9c40088da68180029ddc06ae", "title": "Sustainable Throughput of Wireless LANs with Multipacket Reception Capability under Bounded Delay-Moment Requirements", "abstract": "With the rapid proliferation of broadband wireless services, it is of paramount importance to understand how fast data can be sent through a wireless local area network (WLAN). Thanks to a large body of research following the seminal work of Bianchi, WLAN throughput under saturated traffic condition has been well understood. By contrast, prior investigations on throughput performance under unsaturated traffic condition was largely based on phenomenological observations, which lead to a common misconception that WLAN can support a traffic load as high as saturation throughput, if not higher, under nonsaturation condition. In this paper, we show through rigorous analysis that this misconception may result in unacceptable quality of service: mean packet delay and delay jitter may approach infinity even when the traffic load is far below the saturation throughput. Hence, saturation throughput is not a sound measure of WLAN capacity under nonsaturation condition. To bridge the gap, we define safe-bounded-mean-delay (SBMD) throughput and safe-bounded-delay-jitter (SBDJ) throughput that reflect the actual network capacity users can enjoy when they require finite mean delay and delay jitter, respectively. Our earlier work proved that in a WLAN with multi-packet reception (MPR) capability, saturation throughput scales superlinearly with the MPR capability of the network. This paper extends the investigation to the nonsaturation case and shows that superlinear scaling also holds for SBMD and SBDJ throughputs. Our results here complete the demonstration of MPR as a powerful capacity-enhancement technique for WLAN under both saturation and nonsaturation conditions.", "venue": "IEEE Transactions on Mobile Computing", "authors": ["Ying Jun Zhang", "Soung Chang Liew", "Da Rui Chen"], "year": 2010, "n_citations": 24}
{"id": 3660762, "s2_id": "f8c930f62fb8daf5777a5070b65af227fd70b6cc", "title": "A Tutorial of the Mobile Multimedia Wireless Sensor Network OMNeT++ Framework", "abstract": "In this work, we will give a detailed tutorial instruction about how to use the Mobile Multi-Media Wireless Sensor Networks (M3WSN) simulation framework. The M3WSN framework has been published as a scientific paper in the 6th International Workshop on OMNeT++ (2013) [1]. M3WSN framework enables the multimedia transmission of real video se- \nquence. Therefore, a set of multimedia algorithms, protocols, and services can be evaluated by using QoE metrics. Moreover, key video-related information, such as frame types, GoP length and intra-frame dependency can be used for creating new assessment and optimization solutions. To support mobility, M3WSN utilizes different mobility traces to enable the understanding of how the network behaves under mobile situations. This tutorial will cover how to install and configure the M3WSN framework, setting and running the experiments, creating mobility and video traces, \nand how to evaluate the performance of different protocols. The tutorial will be given in an environment of Ubuntu 12.04 LTS and OMNeT++ 4.2.", "venue": "ArXiv", "authors": ["Zhongliang  Zhao", "Denis do Ros\u00e1rio", "Torsten  Braun", "Eduardo  Cerqueira"], "year": 2015, "n_citations": 0}
{"id": 3660897, "s2_id": "fd7a0a67835c53c65dc5f310bfa8e64f827ed7e7", "title": "An analysis framework for hardware and software implementations with applications from cryptography", "abstract": "Abstract With the richness of present-day hardware architectures, tightening the synergy between hardware and software has attracted a great attention. The interest in unified approaches paved the way for newborn frameworks that target hardware and software co-design. This paper confirms that a unified statistical framework can successfully classify algorithms based on a combination of the heterogeneous characteristics of their hardware and software implementations. The proposed framework produces customizable indicators for any hybridization of processing systems and can be contextualized for any area of application. The framework is used to develop the Lightness Indicator System (LIS) as a case-study that targets a set of cryptographic algorithms that are known in the literature to be tiny and light. The LIS targets state-of-the-art multi-core processors and high-end Field Programmable Gate Arrays (FPGAs). The presented work includes a generic benchmark model that aids the clear presentation of the framework and extensive performance analysis and evaluation.", "venue": "Comput. Electr. Eng.", "authors": ["Issam W. Damaj", "Safaa J. Kasbah"], "year": 2018, "n_citations": 16}
{"id": 3667226, "s2_id": "90e4cc4e42c1b5d45751f7fad33142909e496289", "title": "Star sampling with and without replacement", "abstract": "Star sampling (SS) is a random sampling procedure on a graph wherein each sample consists of a randomly selected vertex (the star center) and its one-hop neighbors (the star endpoints). We consider the use of star sampling to find any member of an arbitrary target set of vertices in a graph, where the figure of merit (cost) is either the expected number of samples (unit cost) or the expected number of star centers plus star endpoints (linear cost) until a vertex in the target set is encountered, either as a star center or as a star point. We analyze this performance measure on three related star sampling paradigms: SS with replacement (SSR), SS without center replacement (SSC), and SS without star replacement (SSS). We derive exact and approximate expressions for the expected unit and linear costs of SSR, SSC, and SSS on Erdos-Renyi (ER) graphs. Our results show there is i) little difference in unit cost, but ii) significant difference in linear cost, across the three paradigms. Although our results are derived for ER graphs, experiments on \"real-world\" graphs suggest our performance expressions are reasonably accurate for non-ER graphs.", "venue": "ArXiv", "authors": ["Jonathan  Stokes", "Steven  Weber"], "year": 2019, "n_citations": 2}
{"id": 3668543, "s2_id": "2455152a09fd93b084fe95df3b63598d8ca46df7", "title": "HALO 1.0: A Hardware-agnostic Accelerator Orchestration Framework for Enabling Hardware-agnostic Programming with True Performance Portability for Heterogeneous HPC", "abstract": "Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper, we present HALO 1.0, an open-ended extensible multi-agent software framework, that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application codes across heterogeneous accelerator resources. The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows the same hardware-agnostic application codes of the HPC kernels, without any change, to run across all the computing devices with a consistently maximum performance portability score of 1.0, which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score.", "venue": "ArXiv", "authors": ["Michael  Riera", "Erfan Bank Tavakoli", "Masudul Hassan Quraishi", "Fengbo  Ren"], "year": 2020, "n_citations": 1}
{"id": 3670815, "s2_id": "faf613d3e3ecbf78d7730a6956dec7f31a545fbd", "title": "Profiling and improving the duty-cycling performance of Linux-based IoT devices", "abstract": "Minimizing the energy consumption of Linux-based devices is an essential step towards their wide deployment in various IoT scenarios. Energy saving methods such as duty-cycling aim to address this constraint by limiting the amount of time the device is powered on. In this work we study and improve the amount of time a Linux-based IoT device is powered on to accomplish its tasks. We analyze the processes of system boot up and shutdown on two platforms, the Raspberry Pi 3 and Raspberry Pi Zero Wireless, and enhance duty-cycling performance by identifying and disabling time-consuming or unnecessary units initialized in the userspace. We also study whether SD card speed and SD card capacity utilization affect boot up duration and energy consumption. In addition, we propose Pallex, a parallel execution framework built on top of the systemd init system to run a user application concurrently with userspace initialization. We validate the performance impact of Pallex when applied to various IoT application scenarios: (1) capturing an image, (2) capturing and encrypting an image, (3) capturing and classifying an image using the k-nearest neighbor algorithm, and (4) capturing images and sending them to a cloud server. Our results show that system lifetime is increased by 18.3%, 16.8%, 13.9% and 30.2%, for these application scenarios, respectively.", "venue": "J. Ambient Intell. Humaniz. Comput.", "authors": ["Immanuel  Amirtharaj", "Tai  Groot", "Behnam  Dezfouli"], "year": 2020, "n_citations": 8}
{"id": 3673530, "s2_id": "5af6abb941421607e46b51284c00563940bf12ed", "title": "Scalable Comparative Visualization of Ensembles of Call Graphs", "abstract": "Optimizing the performance of large-scale parallel codes is critical for efficient utilization of computing resources. Code developers often explore various execution parameters, such as hardware configurations, system software choices, and application parameters, and are interested in detecting and understanding bottlenecks in different executions. They often collect hierarchical performance profiles represented as call graphs, which combine performance metrics with their execution contexts. The crucial task of exploring multiple call graphs together is tedious and challenging because of the many structural differences in the execution contexts and significant variability in the collected performance metrics (e.g., execution runtime). In this paper, we present EnsembleCallFlow to support the exploration of ensembles of call graphs using new types of visualizations, analysis, graph operations, and features. We introduce ensemble-Sankey, a new visual design that combines the strengths of resource-flow (Sankey) and box-plot visualization techniques. Whereas the resource-flow visualization can easily and intuitively describe the graphical nature of the call graph, the box plots overlaid on the nodes of Sankey convey the performance variability within the ensemble. Our interactive visual interface provides linked views to help explore ensembles of call graphs, e.g., by facilitating the analysis of structural differences, and identifying similar or distinct call graphs. We demonstrate the effectiveness and usefulness of our design through case studies on large-scale parallel codes.", "venue": "IEEE transactions on visualization and computer graphics", "authors": ["Suraj P. Kesavan", "Harsh  Bhatia", "Abhinav  Bhatele", "Todd  Gamblin", "Peer-Timo  Bremer", "Kwan-Liu  Ma"], "year": 2021, "n_citations": 0}
{"id": 3673922, "s2_id": "b1b3cd5e6a081b3cef9360cbece7a746a5ecabea", "title": "Parameter and State Estimation in Queues and Related Stochastic Models: A Bibliography", "abstract": "This is an annotated bibliography on estimation and inference results for queues and related stochastic models. The purpose of this document is to collect and categorise works in the field, allowing for researchers and practitioners to explore the various types of results that exist. This bibliography attempts to include all known works that satisfy both of these requirements: -Works that deal with queueing models. -Works that contain contributions related to the methodology of parameter estimation, state estimation, hypothesis testing, confidence interval and/or actual datasets of application areas. Our attempt is to make this bibliography exhaustive, yet there are possibly some papers that we have missed. As it is updated continuously, additions and comments are welcomed. The sections below categorise the works based on several categories. A single paper may appear in several categories simultaneously. The final section lists all works in chronological order along with short descriptions of the contributions. This bibliography is maintained at this http URL and may be cited as such. We welcome additions and corrections.", "venue": "ArXiv", "authors": ["Azam  Asanjarani", "Yoni  Nazarathy", "Philip K. Pollett"], "year": 2017, "n_citations": 13}
{"id": 3674037, "s2_id": "f3de07ffb658de1695444701f8f40537b71ac988", "title": "Mapping Large Scale Research Metadata to Linked Data: A Performance Comparison of HBase, CSV and XML", "abstract": "OpenAIRE, the Open Access Infrastructure for Research in Europe, comprises a database of all EC FP7 and H2020 funded research projects, including metadata of their results (publications and datasets). These data are stored in an HBase NoSQL database, post-processed, and exposed as HTML for human consumption, and as XML through a web service interface. As an intermediate format to facilitate statistical computations, CSV is generated internally. To interlink the OpenAIRE data with related data on the Web, we aim at exporting them as Linked Open Data (LOD). The LOD export is required to integrate into the overall data processing workflow, where derived data are regenerated from the base data every day. We thus faced the challenge of identifying the best-performing conversion approach. We evaluated the performances of creating LOD by a MapReduce job on top of HBase, by mapping the intermediate CSV files, and by mapping the XML output.", "venue": "MTSR", "authors": ["Sahar  Vahdati", "Farah  Karim", "Jyun-Yao  Huang", "Christoph  Lange"], "year": 2015, "n_citations": 15}
{"id": 3680600, "s2_id": "79eb89d5348b05b4725fb7eb27f45f1c4748e5c5", "title": "Theoretical Analysis and Evaluation of NoCs with Weighted Round-Robin Arbitration", "abstract": "Fast and accurate performance analysis techniques are essential in early design space exploration and pre-silicon evaluations, including software eco-system development. In particular, on-chip communication continues to play an increasingly important role as the many-core processors scale up. This paper presents the first performance analysis technique that targets networks-on-chip (NoCs) that employ weighted round-robin (WRR) arbitration. Besides fairness, WRR arbitration provides flexibility in allocating bandwidth proportionally to the importance of the traffic classes, unlike basic round-robin and priority-based arbitration. The proposed approach first estimates the effective service time of the packets in the queue due to WRR arbitration. Then, it uses the effective service time to compute the average waiting time of the packets. Next, we incorporate a decomposition technique to extend the analytical model to handle NoC of any size. The proposed approach achieves less than 5% error while executing real applications and 10% error under challenging synthetic traffic with different burstiness levels.", "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Sumit K. Mandal", "Jie  Tong", "Raid  Ayoub", "Michael  Kishinevsky", "Ahmed  Abousamra", "\u00dcmit Y. Ogras"], "year": 2021, "n_citations": 0}
{"id": 3681104, "s2_id": "2b236f782df3827d7f44d2c29aeac5ea051a2574", "title": "Evaluating the SiteStory Transactional Web Archive With the ApacheBench Tool", "abstract": "Conventional Web archives are created by periodically crawling a Web site and archiving the responses from the Web server. Although easy to implement and commonly deployed, this form of archiving typically misses updates and may not be suitable for all preservation scenarios, for example a site that is required (perhaps for records compliance) to keep a copy of all pages it has served. In contrast, transactional archives work in conjunction with a Web server to record all content that has been served. Los Alamos National Laboratory has developed SiteStory, an open-source transactional archive written in Java that runs on Apache Web servers, provides a Memento compatible access interface, and WARC file export features. We used Apache\u2019s ApacheBench utility on a pre-release version of SiteStory to measure response time and content delivery time in different environments. The performance tests were designed to determine the feasibility of SiteStory as a production-level solution for high fidelity automatic Web archiving. We found that SiteStory does not significantly affect content server performance when it is performing transactional archiving. Content server performance slows from 0.076 seconds to 0.086 seconds per Web page access when the content server is under load, and from 0.15 seconds to 0.21 seconds when the resource has many embedded and changing resources.", "venue": "TPDL", "authors": ["Justin F. Brunelle", "Michael L. Nelson"], "year": 2013, "n_citations": 14}
{"id": 3685486, "s2_id": "ecb350c02fe12ab90764fb0f5578ead9eff1c0cd", "title": "The Gittins Policy in the M/G/1 Queue", "abstract": "The Gittins policy is a highly general scheduling policy that minimizes a wide variety of mean holding cost metrics in the M/G/1 queue. Perhaps most famously, Gittins minimizes mean response time in the M/G/1 when jobs\u2019 service times are unknown to the scheduler. Gittins also minimizes weighted versions of mean response time. For example, the well-known \"c\u03bc rule\", which minimizes class-weighted mean response time in the multiclass M/M/1, is a special case of Gittins.However, despite the extensive literature on Gittins in the M/G/1, it contains no fully general proof of Gittins\u2019s optimality. This is because Gittins was originally developed for the multiarmed bandit problem. Translating arguments from the multiarmed bandit to the M/G/1 is technically demanding, so it has only been done rigorously in some special cases. The extent of Gittins\u2019s optimality in the M/G/1 is thus not entirely clear.In this work we provide the first fully general proof of Gittins\u2019s optimality in the M/G/1. The optimality result we obtain is even more general than was previously known. For example, we show that Gittins minimizes mean slowdown in the M/G/1 with unknown or partially known service times, and we show that Gittins\u2019s optimality holds under batch arrivals. Our proof uses a novel approach that works directly with the M/G/1, avoiding the difficulties of translating from the multi-armed bandit problem.", "venue": "2021 19th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt)", "authors": ["Ziv  Scully", "Mor  Harchol-Balter"], "year": 2021, "n_citations": 1}
{"id": 3687719, "s2_id": "c4b01676340aeeb1433100da42eabcadc4760a7c", "title": "GB-PANDAS:: Throughput and heavy-traffic optimality analysis for affinity scheduling", "abstract": "Dynamic affinity scheduling has been an open problem for nearly three decades. The problem is to dynamically schedule multi-type tasks to multi-skilled servers such that the resulting queueing system is both stable in the capacity region (throughput optimality) and the mean delay of tasks is minimized at high loads near the boundary of the capacity region (heavy-traffic optimality). As for applications, dataintensive analytics like MapReduce, Hadoop, and Dryad fit into this setting, where the set of servers is heterogeneous for different task types, so the pair of task type and server determines the processing rate of the task. The load balancing algorithm used in such frameworks is an example of affinity scheduling which is desired to be both robust and delay optimal at high loads when hot-spots occur. Fluid model planning, the MaxWeight algorithm, and the generalized c?-rule are among the first algorithms proposed for affinity scheduling that have theoretical guarantees on being optimal in different senses, which will be discussed in the related work section. All these algorithms are not practical for use in data center applications because of their non-realistic assumptions. The join-the-shortest-queue-MaxWeight (JSQMaxWeight), JSQ-Priority, and weighted-workload algorithms are examples of load balancing policies for systems with two and three levels of data locality with a rack structure. In this work, we propose the Generalized-Balanced-Pandas algorithm (GB-PANDAS) for a system with multiple levels of data locality and prove its throughput optimality. We prove this result under an arbitrary distribution for service times, whereas most previous theoretical work assumes geometric distribution for service times. The extensive simulation results show that the GB-PANDAS algorithm alleviates the mean delay and has a better performance than the JSQMaxWeight algorithm by up to twofold at high loads. We believe that the GB-PANDAS algorithm is heavy-traffic optimal in a larger region than JSQ-MaxWeight, which is an interesting problem for future work.", "venue": "PERV", "authors": ["Ali  Yekkehkhany", "Avesta  Hojjati", "Mohammad Hassan Hajiesmaili"], "year": 2018, "n_citations": 26}
{"id": 3687964, "s2_id": "55002b795985e78654e264dd2ee405229c9e8468", "title": "A Multi-Server Scheduling Framework for Resource Allocation in Wireless Multi-Carrier Networks", "abstract": "Multiuser resource allocation has recently been recognized as an effective methodology for enhancing the power and spectrum efficiency in OFDM (orthogonal frequency division multiplexing) systems. It is, however, not directly applicable to current packet-switched networks because most existing packet- scheduling schemes are based on a single-server model and do not serve multiple users at the same time. In this paper, we propose a cross-layer resource allocation algorithm based on a novel multi-server scheduling framework to achieve overall high system power efficiency in packet-switched OFDM networks. Our contribution is four fold: (i) we propose and analyze a MPGPS (multi-server packetized general processor sharing) service discipline that serves multiple users at the same time and facilitates multiuser resource allocation; (ii) we present a MPGPS-based joint MAC-PHY resource allocation scheme that incorporates packet scheduling, subcarrier allocation, and power allocation in an integrated framework; (iii) by investigating the fundamental tradeoff between multiuser-diversity and queueing performance, we present an A-MPGPS (adaptive MPGPS) service discipline that strikes balance between power efficiency and queueing performance; and (iv) we extend MPGPS to an O-MPGPS (opportunistic MPGPS) service discipline to further enhance the resource utilization efficiency.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Ying Jun Zhang"], "year": 2007, "n_citations": 6}
{"id": 3689063, "s2_id": "2a99e5047980ee8f1c0d376d89743d4b98e9796f", "title": "Benchmarking Inference Performance of Deep Learning Models on Analog Devices", "abstract": "Analog hardware implemented deep learning models are promising for computation and energy constrained systems such as edge computing devices. However, the analog nature of the device and the many associated noise sources will cause changes to the value of the weights in the trained deep learning models deployed on such devices. In this study, systematic evaluation of the inference performance of trained popular deep learning models for image classification deployed on analog devices has been carried out, where additive white Gaussian noise has been added to the weights of the trained models during inference. It is observed that deeper models and models with more redundancy in design, such as VGG, are more robust to the noise in general. Also, it is observed that the performance is affected by the design philosophy of the model, the detailed structure of the model, the exact machine learning task, as well as the datasets.", "venue": "2021 International Joint Conference on Neural Networks (IJCNN)", "authors": ["Omobayode  Fagbohungbe", "Lijun  Qian"], "year": 2021, "n_citations": 3}
{"id": 3690338, "s2_id": "bd97e937149c59d3fbeace3fbef9546d18ec37f3", "title": "A Temporal Approach to Stochastic Network Calculus", "abstract": "Stochastic network calculus is a newly developed theory for stochastic service guarantee analysis of computer networks. In the current stochastic network calculus literature, its fundamental models are based on the cumulative amount of traffic or cumulative amount of service. However, there are network scenarios where direct application of such models is difficult. This paper presents a temporal approach to stochastic network calculus. The key idea is to develop models and derive results from the time perspective. Particularly, we define traffic models and service models based on the cumulative packet inter-arrival time and the cumulative packet service time, respectively. Relations among these models as well as with the existing models in the literature are established. In addition, we prove the basic properties of the proposed models, such as delay bound and backlog bound, output characterization, concatenation property and superposition property. These results form a temporal stochastic network calculus and compliment the existing results.", "venue": "ArXiv", "authors": ["Jing  Xie", "Yuming  Jiang", "Min  Xie"], "year": 2011, "n_citations": 5}
{"id": 3692658, "s2_id": "1199cc3401ad638c377ddbbae248d425a9b87946", "title": "Branch prediction related Optimizations for Multithreaded Processors", "abstract": "Major chip manufacturers have all introduced Multithreaded processors. These processors are used for running a variety of workloads. Efficient resource utilization is an important design aspect in such processors. Depending on the workload, mis-speculated execution can severely impact resource utilization and power utilization. In general, compared to a uniprocessor, a multithreaded processor may have better tolerance towards mis-speculation. However there can still be phases where even a multi-threaded processor performance may get impacted by branch induced mis-speculation. In this paper I propose monitoring the branch predictor behavior of various hardware threads running on the multi-threaded processor and use that information as a feedback to the thread arbiter/picker which schedules the next thread to fetch instructions from. If I find that a particular thread is going through a phase where it is consistently mis-predicting its branches and its average branch misprediction stall is above a specific threshold then I temporarily reduce the priority for picking that thread. I do a qualitative comparison of various solutions to the problem of resource inefficiency caused due to mis-speculated branches in multithreaded processors. This work can be extended by doing a quantitative evaluation.", "venue": "ArXiv", "authors": ["Murthy  Durbhakula"], "year": 2019, "n_citations": 0}
{"id": 3698412, "s2_id": "acce9525087764dd3dc8ac0e916800fe8be26b17", "title": "Optimal Heavy-Traffic Queue Length Scaling in an Incompletely Saturated Switch", "abstract": "We consider an input queued switch operating under the MaxWeight scheduling algorithm. This system is interesting to study because it is a model for Internet routers and data center networks. Recently, it was shown that the MaxWeight algorithm has optimal heavy-traffic queue length scaling when all ports are uniformly saturated. Here we consider the case where a fraction of the ports are saturated and others are not (which we call the incompletely saturated case), and also the case where the rates at which the ports are saturated can be different. We use a recently developed drift technique to show that the heavy-traffic queue length under the MaxWeight scheduling algorithm has optimal scaling with respect to the switch size even in these cases.", "venue": "SIGMETRICS 2016", "authors": ["Siva Theja Maguluri", "Sai Kiran Burle", "R.  Srikant"], "year": 2016, "n_citations": 23}
{"id": 3699307, "s2_id": "849fe4dfbd45bfa4cbd47a84c95fda8ab14bd93d", "title": "Scaling Video Analytics on Constrained Edge Nodes", "abstract": "As video camera deployments continue to grow, the need to process large volumes of real-time data strains wide area network infrastructure. When per-camera bandwidth is limited, it is infeasible for applications such as traffic monitoring and pedestrian tracking to offload high-quality video streams to a datacenter. This paper presents FilterForward, a new edge-to-cloud system that enables datacenter-based applications to process content from thousands of cameras by installing lightweight edge filters that backhaul only relevant video frames. FilterForward introduces fast and expressive per-application microclassifiers that share computation to simultaneously detect dozens of events on computationally constrained edge nodes. Only matching events are transmitted to the cloud. Evaluation on two real-world camera feed datasets shows that FilterForward reduces bandwidth use by an order of magnitude while improving computational efficiency and event detection accuracy for challenging video content.", "venue": "MLSys", "authors": ["Christopher  Canel", "Thomas  Kim", "Giulio  Zhou", "Conglong  Li", "Hyeontaek  Lim", "David G. Andersen", "Michael  Kaminsky", "Subramanya R. Dulloor"], "year": 2019, "n_citations": 50}
{"id": 3702285, "s2_id": "1c46f698433752a79ca7de8ab52a88ff6ecede37", "title": "Monte Carlo execution time estimation for Privacy-preserving Distributed Function Evaluation protocols", "abstract": "Recent developments in Machine Learning and Deep Learning depend heavily on cloud computing and specialized hardware, such as GPUs and TPUs. This forces those using those models to trust private data to cloud servers. Such scenario has prompted a large interest on Homomorphic Cryptography and Secure Multi-party Computation protocols that allow the use of cloud computing power in a privacy-preserving manner. When comparing the efficiency of such protocols, most works in literature resort to complexity analysis that gives asymptotic higher-bounding limits of computational cost when input size tends to infinite. These limits may be very different from the actual cost or execution time, when performing such computations over small, or average-sized datasets. We argue that Monte Carlomethods can render better computational cost and time estimates, fostering better design and implementation decisions for complex systems, such as Privacy-Preserving Machine Learning Frameworks. Keywords\u2014Monte Carlo Methods, Cost Analysis, Homomorphic Cryptography, Secret Sharing", "venue": "ArXiv", "authors": ["Stefano M P C Souza", "Daniel G Silva"], "year": 2021, "n_citations": 0}
{"id": 3704859, "s2_id": "09322f463d964334d1e63bb30d10d27dccba11e8", "title": "Cross-Entropy Optimisation of Importance Sampling Parameters for Statistical Model Checking", "abstract": "Statistical model checking avoids the exponential growth of states associated with probabilistic model checking by estimating probabilities from multiple executions of a system and by giving results within confidence bounds. Rare properties are often important but pose a particular challenge for simulation-based approaches, hence a key objective for statistical model checking (SMC) is to reduce the number and length of simulations necessary to produce a result with a given level of confidence. Importance sampling can achieves this, however to maintain the advantages of SMC it is necessary to find good importance sampling distributions without considering the entire state space. \n \nHere we present a simple algorithm that uses the notion of cross-entropy to find an optimal importance sampling distribution. In contrast to previous work, our algorithm uses a naturally defined low dimensional vector of parameters to specify this distribution and thus avoids the intractable explicit representation of a transition matrix. We show that our parametrisation leads to a unique optimum and can produce many orders of magnitude improvement in simulation efficiency. We demonstrate the efficacy of our methodology by applying it to models from reliability engineering and biochemistry.", "venue": "CAV", "authors": ["Cyrille  J\u00e9gourel", "Axel  Legay", "Sean  Sedwards"], "year": 2012, "n_citations": 53}
{"id": 3705985, "s2_id": "6b29b762c28ee9839b7466c551be3dc2364426fe", "title": "Effect of cell residence time variance on the performance of an advanced paging algorithm", "abstract": "The use of advanced sequential paging algorithms has been suggested as a means to reduce the signaling cost in future mobile cellular networks. In a proposed algorithm (Koukoutsidis and Theologou, 2003), the system can use the additional information of the last interaction cell combined with a mobility model to predict the short-term location probabilities at the time of an incoming call arrival. The short-term location probabilities reduce the uncertainty in mobile user position and thus greatly improve the search. In this paper, an analytical model is derived that allows for a general distribution of cell residence times. By considering a Gamma distribution, we study the effect of the variance of cell residence times and derive useful results on the performance of the algorithm.", "venue": "ArXiv", "authors": ["Ioannis Z. Koukoutsidis", "Petros I. Papaioannou", "Michael E. Theologou"], "year": 2009, "n_citations": 1}
{"id": 3710624, "s2_id": "63db9875b8cd0994f6bfdfc68172e579682adc96", "title": "Blockchains vs. Distributed Databases: Dichotomy and Fusion", "abstract": "Blockchain has come a long way - a system that was initially proposed specifically for cryptocurrencies is now being adapted and adopted as a general-purpose transactional system. As blockchain evolves into another data management system, the natural question is how it compares against distributed database systems. Existing works on this comparison focus on high-level properties, such as security and throughput. They stop short of showing how the underlying design choices contribute to the overall differences. Our work fills this important gap. We perform a twin study of blockchains and distributed database systems as two types of transactional systems. We propose a taxonomy that illustrates the dichotomy across four dimensions, namely replication, concurrency, storage, and sharding. Within each dimension, we discuss how the design choices are driven by two goals: security for blockchains, and performance for distributed databases. We conduct an extensive and in-depth performance analysis of two blockchains, namely Quorum and Hyperledger Fabric, and three distributed databases, namely CockroachDB, TiDB, and etcd. Our analysis exposes the impact of different design choices on the overall performance. Concisely, our work provides a principled framework for analyzing the emerging trend of blockchain-database fusion.", "venue": "SIGMOD Conference", "authors": ["Pingcheng  Ruan", "Tien Tuan Anh Dinh", "Dumitrel  Loghin", "Meihui  Zhang", "Gang  Chen", "Qian  Lin", "Beng Chin Ooi"], "year": 2021, "n_citations": 5}
{"id": 3711532, "s2_id": "5af0af80f15def450f3798c47c0ac7c72789fa91", "title": "End-to-End Delay in Two Hop Relay MANETs with Limited Buffer", "abstract": "Despite lots of literature has been dedicated to researching the delay performance in two-hop relay (2HR) mobile ad hoc networks (MANETs), however, they usually assume the buffer size of each node is infinite, so these studies are not applicable to and thus may not reflect the real delay performance of a practical MANET with limited buffer. To address this issue, in this paper we explore the packet end-to-end delay in a 2HR MANET, where each node is equipped with a bounded and shared relay-buffer for storing and forwarding packets of all other flows. The transmission range of each node can be adjusted and a group-based scheduling scheme is adopted to avoid interference between simultaneous transmissions, meanwhile a handshake mechanism is added to the 2HR routing algorithm to avoid packet loss. With the help of Markov Chain Theory and Queuing Theory, we develop a new framework to fully characterize the packet delivery processes, and obtain the relay-buffer blocking probability (RBP) under any given exogenous packet input rate. Based on the RBP, we can compute the packet queuing delay in its source node and delivery delay respectively, and further derive the end-to-end delay in such a MANET with limited buffer.", "venue": "2014 Second International Symposium on Computing and Networking", "authors": ["Jia  Liu", "Yang  Xu", "Xiaohong  Jiang"], "year": 2014, "n_citations": 3}
{"id": 3714045, "s2_id": "aed565cdfa2ce68fcfbf0ea0a59770fa82f3e060", "title": "Sharp utilization thresholds for some realtime scheduling problems", "abstract": "Scheduling policies for real-time systems exhibit threshold behavior that is related to the utilization of the task set they schedule, and in some cases this threshold is sharp. A task set is considered schedulable if it can be scheduled to meet all associated deadlines. A schedulability test for a chosen policy is a test of feasibility: given a task set, can all deadlines be met? For the rate monotonic scheduling policy, we show that periodic workload with utilization less than a threshold URM can be scheduled almost surely and that all workload with utilization greater than URM is almost surely not schedulable. We study such sharp threshold behavior in the context of processor scheduling using static task priorities, not only for periodic real-time tasks but for aperiodic real-time tasks as well. The notion of a utilization threshold provides a simple schedulability test for most real-time applications. These results improve our understanding of scheduling policies and provide an interesting characterization of the typical behavior of policies. The threshold is sharp (small deviations around the threshold cause schedulability, as a property, to appear or disappear) for most policies; this is a happy consequence that can be used to address the limitations of existing utilization-based tests for schedulability. We demonstrate the use of such an approach for balancing power consumption with the need to meet deadlines in web servers.", "venue": "PERV", "authors": ["Sathish  Gopalakrishnan"], "year": 2012, "n_citations": 7}
{"id": 3724693, "s2_id": "062755d9cd811d3cd1fd30a3f2f452b8b017d362", "title": "Function-as-a-Service Benchmarking Framework", "abstract": "Cloud Service Providers deliver their products in form of \u201das-a-Service\u201d, which are typically categorized by the level of abstraction. This approach hides the implementation details and shows only functionality to the user. However, the problem is that it is hard to measure the performance of Cloud services, because they behave like black boxes. Especially with Function-as-a-Service it is even more difficult because it completely hides server and infrastructure management from users by design. Cloud Service Prodivers usually restrict the maximum size of code, memory and runtime of Cloud Functions. Nevertheless, users need clarification if more ressources are needed to deliver services in high quality. In this regard, we present the architectural design of a new Function-as-a-Service benchmarking tool, which allows users to evaluate the performance of Cloud Functions. Furthermore, the capabilities of the framework are tested on an isolated platform with a specific workload. The results show that users are able to get insights into Function-as-a-Service environments. This, in turn, allows users to identify factors which may slow down or speed up the performance of Cloud Functions.", "venue": "CLOSER", "authors": ["Roland  Pellegrini", "Igor  Ivkic", "Markus  Tauber"], "year": 2019, "n_citations": 3}
{"id": 3724754, "s2_id": "beb1a5bfded4ab81f8d0f584b2b4294208fc23b6", "title": "BAMSim Simulator", "abstract": "Resource allocation is an essential design aspect for current systems and bandwidth allocation is an essential design aspect in multi-protocol label switched and OpenFlow/SDN network infrastructures. The bandwidth allocation models (BAMs) are an alternative to allocate and share bandwidth among network users. BAMs have an extensive number of parameters that need to be defined and tuned to achieve an expected network performance. This paper presents the BAMSim simulator to support the network manager decision process in choosing a set of BAM configuration parameters for network design or during network operation.", "venue": "ArXiv", "authors": ["Rafael F. Reale", "Walter P. neto", "Joberto S. B. Martins"], "year": 2021, "n_citations": 0}
{"id": 3725851, "s2_id": "8e3a1086ac2f2eae0ca883cc26d3fb5b98bf85f8", "title": "Measuring NUMA effects with the STREAM benchmark", "abstract": "Modern high-end machines feature multiple processor packages, each of which contains multiple independent cores and integrated memory controllers connected directly to dedicated physical RAM. These packages are connected via a shared bus, creating a system with a heterogeneous memory hierarchy. Since this shared bus has less bandwidth than the sum of the links to memory, aggregate memory bandwidth is higher when parallel threads all access memory local to their processor package than when they access memory attached to a remote package. \nBut, the impact of this heterogeneous memory architecture is not easily understood from vendor benchmarks. Even where these measurements are available, they provide only best-case memory throughput. This work presents a series of modifications to the well-known STREAM benchmark to measure the effects of NUMA on both a 48-core AMD Opteron machine and a 32-core Intel Xeon machine.", "venue": "ArXiv", "authors": ["Lars  Bergstrom"], "year": 2011, "n_citations": 25}
{"id": 3726295, "s2_id": "a66a77bbd306dde93b2dd63bd937c33d687e9f1a", "title": "Non-linear estimation is easy", "abstract": "Non-linear state estimation and some related topics like parametric estimation, fault diagnosis and perturbation attenuation are tackled here via a new methodology in numerical differentiation. The corresponding basic system theoretic definitions and properties are presented within the framework of differential algebra, which permits to handle system variables and their derivatives of any order. Several academic examples and their computer simulations, with online estimations, illustrate our viewpoint.", "venue": "Int. J. Model. Identif. Control.", "authors": ["Michel  Fliess", "C\u00e9dric  Join", "Hebertt  Sira-Ram\u00edrez"], "year": 2008, "n_citations": 326}
{"id": 3730175, "s2_id": "77d69735025cc8911249affd4d0968e28bbf2d08", "title": "Easy-to-Use On-the-Fly Binary Program Acceleration on Many-Cores", "abstract": "This paper introduces Binary Acceleration At Runtime (BAAR), an easy-to-use on-the-fly binary acceleration mechanism which aims to tackle the problem of enabling existent software to automatically utilize accelerators at runtime. BAAR is based on the LLVM Compiler Infrastructure and has a client-server architecture. The client runs the program to be accelerated in an environment which allows program analysis and profiling. Program parts which are identified as suitable for the available accelerator are exported and sent to the server. The server optimizes these program parts for the accelerator and provides RPC execution for the client. The client transforms its program to utilize accelerated execution on the server for offloaded program parts. \nWe evaluate our work with a proof-of-concept implementation of BAAR that uses an Intel Xeon Phi 5110P as the acceleration target and performs automatic offloading, parallelization and vectorization of suitable program parts. The practicality of BAAR for real-world examples is shown based on a study of stencil codes. Our results show a speedup of up to 4x without any developer-provided hints and 5.77x with hints over the same code compiled with the Intel Compiler at optimization level O2 and running on an Intel Xeon E5-2670 machine. Based on our insights gained during implementation and evaluation we outline future directions of research, e.g., offloading more fine-granular program parts than functions, a more sophisticated communication mechanism or introducing on-stack-replacement.", "venue": "ArXiv", "authors": ["Marvin  Damschen", "Christian  Plessl"], "year": 2014, "n_citations": 7}
{"id": 3734128, "s2_id": "e99ecaeb3464eb0b7e91dee4932e5cbad5537ccd", "title": "Energy Efficiency: The New Holy Grail of Data Management Systems Research", "abstract": "Energy costs are quickly rising in large-scale data centers and are soon projected to overtake the cost of hardware. As a result, data center operators have recently started turning into using more energy-friendly hardware. Despite the growing body of research in power management techniques, there has been little work to date on energy efficiency from a data management software perspective. In this paper, we argue that hardware-only approaches are only part of the solution, and that data management software will be key in optimizing for energy efficiency. We discuss the problems arising from growing energy use in data centers and the trends that point to an increasing set of opportunities for software-level optimizations. Using two simple experiments, we illustrate the potential of such optimizations, and, motivated by these examples, we discuss general approaches for reducing energy waste. Lastly, we point out existing places within database systems that are promising for energy-efficiency optimizations and urge the data management systems community to shift focus from performance-oriented research to energy-efficient computing.", "venue": "CIDR", "authors": ["Stavros  Harizopoulos", "Mehul A. Shah", "Justin  Meza", "Parthasarathy  Ranganathan"], "year": 2009, "n_citations": 136}
{"id": 3734473, "s2_id": "81dc0a445a1373c2a11ef894f51d4750e16699f1", "title": "Analysis of Optimization Techniques to Improve User Response Time of Web Applications and Their Implementation for MOODLE", "abstract": "Analysis of six optimization techniques grouped under three categories (hardware, back-end, and front-end) is done to study the reduction in average user response time for Modular Object Oriented Dynamic Learning Environment (Moodle), a Learning Management System which is scripted in PHP5, runs on Apache web server and utilizes MySQL database software. Before the implementation of these techniques, performance analysis of Moodle is performed for varying number of concurrent users. The results obtained for each optimization technique are then reported in a tabular format. The maximum reduction in end user response time was achieved for hardware optimization which requires Moodle server and database to be installed on solid state disk.", "venue": "IAIT", "authors": ["Priyanka  Manchanda"], "year": 2013, "n_citations": 1}
{"id": 3737253, "s2_id": "99944aaa6d0d5438963fff03d8e73232035e9afa", "title": "A Domain Specific Approach to Heterogeneous Computing: From Availability to Accessibility", "abstract": "We advocate a domain specific software development methodology for heterogeneous computing platforms such as Multicore CPUs, GPUs and FPGAs. We argue that three specific benefits are realised from adopting such an approach: portable, efficient implementations across heterogeneous platforms; domain specific metrics of quality that characterise platforms in a form software developers will understand; automatic, optimal partitioning across the available computing resources. These three benefits allow a development methodology for software developers where they describe their computational problems in a single, easy to understand form, and after a modeling procedure on the available resources, select how they would like to trade between various domain specific metrics. Our work on the Forward Financial Framework ($F^3$) demonstrates this methodology in practise. We are able to execute a range of computational finance option pricing tasks efficiently upon a wide range of CPU, GPU and FPGA computing platforms. We can also create accurate financial domain metric models of walltime latency and statistical confidence. Furthermore, we believe that we can support automatic, optimal partitioning using this execution and modelling capability.", "venue": "ArXiv", "authors": ["Gordon  Inggs", "David B. Thomas", "Wayne  Luk"], "year": 2014, "n_citations": 4}
{"id": 3740376, "s2_id": "151d7ee43018c185f9340a729f8de819c8f0dea5", "title": "Autotuning PolyBench Benchmarks with LLVM Clang/Polly Loop Optimization Pragmas Using Bayesian Optimization", "abstract": "An autotuning is an approach that explores a search space of possible implementations/configurations of a kernel or an application by selecting and evaluating a subset of implementations/configurations on a target platform and/or use models to identify a high performance implementation/configuration. In this paper, we develop an autotuning framework that leverages Bayesian optimization to explore the parameter space search. We select six of the most complex benchmarks from the application domains of the PolyBench benchmarks (syr2k, 3mm, heat-3d, lu, covariance, and Floyd-Warshall) and apply the newly developed LLVM Clang/Polly loop optimization pragmas to the benchmarks to optimize them. We then use the autotuning framework to optimize the pragma parameters to improve their performance. The experimental results show that our autotuning approach outperforms the other compiling methods to provide the smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and covariance with two large datasets in 200 code evaluations for effectively searching the parameter spaces with up to 170,368 different configurations. We compare four different supervised learning methods within Bayesian optimization and evaluate their effectiveness. We find that the Floyd-Warshall benchmark did not benefit from autotuning because Polly uses heuristics to optimize the benchmark to make it run much slower. To cope with this issue, we provide some compiler option solutions to improve the performance.", "venue": "2020 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)", "authors": ["Xingfu  Wu", "Michael  Kruse", "Prasanna  Balaprakash", "Hal  Finkel", "Paul  Hovland", "Valerie  Taylor", "Mary  Hall"], "year": 2020, "n_citations": 5}
{"id": 3745216, "s2_id": "b028806aea1fd812912ad414907b485e19e55a33", "title": "On Resource Pooling and Separation for LRU Caching", "abstract": "Caching systems using the Least Recently Used (LRU) principle have now become ubiquitous. A fundamental question for these systems is whether the cache space should be pooled together or divided to serve multiple flows of data item requests in order to minimize the miss probabilities. In this paper, we show that there is no straight yes or no answer to this question, and depends on complex combinations of critical factors, including, e.g., request rates, overlapped data items across different request flows, data item popularities and their sizes. To this end, we characterize the performance of multiple flows of data item requests under resource pooling and separation when the cache size is large. Analytically we show that it is asymptotically optimal to jointly serve multiple flows if their data item sizes and popularity distributions are similar, and their arrival rates do not differ significantly; the self-organizing property of LRU caching automatically optimizes the resource allocation among them asymptotically. Otherwise, separating these flows could be better, e.g., when data sizes vary significantly. We also quantify critical points beyond which resource pooling is better than separation for each of the flows when the overlapped data items exceed certain levels. These results provide new insights on the performance of caching systems.", "venue": "Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems", "authors": ["Jian  Tan", "Guocong  Quan", "Kaiyi  Ji", "Ness  Shroff"], "year": 2018, "n_citations": 2}
{"id": 3747302, "s2_id": "8db7feba5f0220f899a8cbd32581403072053063", "title": "AWRP: Adaptive Weight Ranking Policy for Improving Cache Performance", "abstract": "Due to the huge difference in performance between the computer memory and processor , the virtual memory management plays a vital role in system performance .A Cache memory is the fast memory which is used to compensate the speed difference between the memory and processor. This paper gives an adaptive replacement policy over the traditional policy which has low overhead, better performance and is easy to implement. Simulations show that our algorithm performs better than Least-Recently-Used (LRU), First-In-First-Out (FIFO) and Clock with Adaptive Replacement (CAR).", "venue": "ArXiv", "authors": ["Debabala  Swain", "Bijay  Paikaray", "Debabrata  Swain"], "year": 2011, "n_citations": 17}
{"id": 3750028, "s2_id": "6a161ed0106c300fe61a12d82c254958e734081c", "title": "Window-Based Greedy Contention Management for Transactional Memory", "abstract": "We consider greedy contention managers for transactional memory for M \u00d7 N execution windows of transactions with M threads and N transactions per thread. Assuming that each transaction has duration \u03c4 and conflicts with at most C other transactions inside the window, a trivial greedy contention manager can schedule them within \u03c4CN time. In this paper, we explore the theoretical performance boundaries of this approach from the worst-case perspective. Particularly, we present and analyze two new randomized greedy contention management algorithms. The first algorithm Offline-Greedy produces a schedule of length O(\u03c4 \u010b (C + N log(MN))) with high probability, and gives competitive ratio O(log(MN)) for C \u2264 N log(MN). The offline algorithm depends on knowing the conflict graph which evolves while the execution of the transactions progresses. The second algorithm Online-Greedy produces a schedule of length O(\u03c4 \u010b (C log(MN) + N log2(MN))), with high probability, which is only a O(log(NM)) factor worse, but does not require knowledge of the conflict graph. Both of the algorithms exhibit competitive ratio very close to O(s), where s is the number of shared resources. Our algorithms provide new tradeoffs for greedy transaction scheduling that parameterize window sizes and transaction conflicts within the window.", "venue": "DISC", "authors": ["Gokarna  Sharma", "Brett  Estrade", "Costas  Busch"], "year": 2010, "n_citations": 23}
{"id": 3750925, "s2_id": "ff740149571305b02d2b6662b588d59df260037d", "title": "Improving rewards in overloaded real-time systems", "abstract": "Competitive analysis of online algorithms has commonly been applied to understand the behaviour of real-time systems during overload conditions. While competitive analysis provides insight into the behaviour of certain algorithms, it is hard to make inferences about the performance of those algorithms in practice. Other approaches to dealing with overload resort to heuristics that seem to perform well but are hard to prove as being good. Further, most work on handling overload in real-time systems does not consider using information regarding the distribution of arrival rates of jobs and execution times to make scheduling decisions. We present an scheduling policy (obtained through stochastic approximation, and using information about the workload) to handle overload in real-time systems and improve the revenue earned when each successful job completion results in revenue accrual. We prove that the policy we outline does lead to increased revenue when compared to a class of scheduling policies that make static resource allocations to different service classes. We also use empirical evidence to underscore the fact that this policy performs better than a variety of other scheduling policies. The ideas presented can be applied to several soft real-time systems, specifically systems with multiple service classes.", "venue": "ArXiv", "authors": ["Sathish  Gopalakrishnan"], "year": 2018, "n_citations": 0}
{"id": 3755195, "s2_id": "3b31bc03cbe2a5c1fdba52aca0cec32f6b6ab07e", "title": "Asymptotic Performance Evaluation of Battery Swapping and Charging Station for Electric Vehicles", "abstract": "A battery swapping and charging station (BSCS) is an energy refueling station, where i) electric vehicles (EVs) with depleted batteries (DBs) can swap their DBs for fully-charged ones, and ii) the swapped DBs are then charged until they are fully-charged. Successful deployment of a BSCS system necessitates a careful planning of swapping- and charging-related infrastructures, and thus a comprehensive performance evaluation of the BSCS is becoming crucial. This paper studies such a performance evaluation problem with a novel mixed queueing network (MQN) model and validates this model with extensive numerical simulation. We adopt the EVs' blocking probability as our quality-of-service measure and focus on studying the impact of the key parameters of the BSCS (e.g., the numbers of parking spaces, swapping islands, chargers, and batteries) on the blocking probability. We prove a necessary and sufficient condition for showing the ergodicity of the MQN when the number of batteries approaches infinity, and further prove that the blocking probability has two different types of asymptotic behaviors. Meanwhile, for each type of asymptotic behavior, we analytically derive the asymptotic lower bound of the blocking probability.", "venue": "Perform. Evaluation", "authors": ["Xiaoqi  Tan", "Bo  Sun", "Yuan  Wu", "Danny H. K. Tsang"], "year": 2018, "n_citations": 22}
{"id": 3755883, "s2_id": "69c75ef243535ef56e197a499378b4012c332f9f", "title": "WiseMove: A Framework for Safe Deep Reinforcement Learning for Autonomous Driving", "abstract": "Machine learning can provide efficient solutions to the complex problems encountered in autonomous driving, but ensuring their safety remains a challenge. A number of authors have attempted to address this issue, but there are few publicly-available tools to adequately explore the trade-offs between functionality, scalability, and safety. \nWe thus present WiseMove, a software framework to investigate safe deep reinforcement learning in the context of motion planning for autonomous driving. WiseMove adopts a modular learning architecture that suits our current research questions and can be adapted to new technologies and new questions. We present the details of WiseMove, demonstrate its use on a common traffic scenario, and describe how we use it in our ongoing safe learning research.", "venue": "QEST", "authors": ["Jaeyoung  Lee", "Aravind  Balakrishnan", "Ashish  Gaurav", "Krzysztof  Czarnecki", "Sean  Sedwards"], "year": 2019, "n_citations": 7}
{"id": 3762626, "s2_id": "b939361cdefc5ddf5ead94de8857fd4fb1496101", "title": "SOBA: Session optimal MDP-based network friendly recommendations", "abstract": "Caching content over CDNs or at the network edge has been solidified as a means to improve network cost and offer better streaming experience to users. Furthermore, nudging the users towards low-cost content has recently gained momentum as a strategy to boost network performance. We focus on the problem of optimal policy design for Network Friendly Recommendations (NFR). We depart from recent modeling attempts, and propose a Markov Decision Process (MDP) formulation. MDPs offer a unified framework that can model a user with random session length. As it turns out, many state-of-the-art approaches can be cast as subcases of our MDP formulation. Moreover, the approach offers flexibility to model users who are reactive to the quality of the received recommendations. In terms of performance, for users consuming an arbitrary number of contents in sequence, we show theoretically and using extensive validation over real traces that the MDP approach outperforms myopic algorithms both in session cost as well as in offered recommendation quality. Finally, even compared to optimal state-of-art algorithms targeting specific subcases, our MDP framework is significantly more efficient, speeding the execution time by a factor of 10, and enjoying better scaling with the content catalog and recommendation batch sizes.", "venue": "IEEE INFOCOM 2021 - IEEE Conference on Computer Communications", "authors": ["Theodoros  Giannakas", "Anastasios  Giovanidis", "Thrasyvoulos  Spyropoulos"], "year": 2021, "n_citations": 1}
{"id": 3764081, "s2_id": "f3adc9c4c61e654a39772261386852039c339cc9", "title": "Protecting Real-Time GPU Kernels on Integrated CPU-GPU SoC Platforms", "abstract": "Integrated CPU-GPU architecture provides excellent acceleration capabilities for data parallel applications on embedded platforms while meeting the size, weight and power (SWaP) requirements. However, sharing of main memory between CPU applications and GPU kernels can severely affect the execution of GPU kernels and diminish the performance gain provided by GPU. For example, in the NVIDIA Tegra K1 platform which has the integrated CPU-GPU architecture, we noticed that in the worst case scenario, the GPU kernels can suffer as much as 4X slowdown in the presence of co-running memory intensive CPU applications compared to their solo execution. In this paper, we propose a software mechanism, which we call BWLOCK++, to protect the performance of GPU kernels from co-scheduled memory intensive CPU applications.", "venue": "ECRTS", "authors": ["Waqar  Ali", "Heechul  Yun"], "year": 2018, "n_citations": 5}
{"id": 3766019, "s2_id": "5a3bca715298c2e4a7845d9d84ec88d23bcf756a", "title": "Wireless energy and information transfer in networks with hybrid ARQ", "abstract": "In this paper, we consider a class of wireless powered communication devices using hybrid automatic repeat request (HARQ) protocol to ensure reliable communications. In particular, we analyze the trade-off between accumulating mutual information and harvesting RF energy at the receiver of a point-to-point link over a time-varying independent and identically distributed (i.i.d.) channel. The transmitter is assumed to have a constant energy source while the receiver relies, solely, on the RF energy harvested from the received signal. At each time slot, the incoming RF signal is split between information accumulation and energy accumulation with the objective of minimizing the expected number of re-transmissions. A major finding of this work is that the optimal policy minimizing the expected number of re-transmissions utilizes the incoming RF signal to either exclusively harvest energy or to accumulate mutual information. This finding enables achieving an optimal solution in feasible time by converting a two dimensional uncountable state Markov decision process (MDP) with continuous action space into a countable state MDP with binary decision space.", "venue": "2018 IEEE Wireless Communications and Networking Conference (WCNC)", "authors": ["Mehdi Salehi Heydar Abad", "\u00d6zg\u00fcr  Er\u00e7etin", "Tamer A. ElBatt", "Mohammed  Nafie"], "year": 2018, "n_citations": 0}
{"id": 3768750, "s2_id": "2cfdce996a6206df8a686e47f25d5ffae391f27f", "title": "Primary user traffic classification in dynamic spectrum access networks", "abstract": "We propose a primary user (PU) traffic distribution classifier for dynamic spectrum access networks based on multi-hypothesis sequential probability ratio test (MSPRT). In specific, we propose two classifiers: (i) an estimate-then-classify classifier, and (ii) a modified MSPRT classifier based on the average likelihood function considering partial knowledge of the PU traffic parameters. Using the sequential algorithm, we show that our proposed classifiers can achieve higher classification performance compared to the traditional maximum likelihood classifier using constant number of samples.", "venue": "GLOBECOM", "authors": ["Chun-Hao  Liu", "Eric  Rebeiz", "Przemyslaw  Pawelczak", "Danijela  Cabric"], "year": 2013, "n_citations": 3}
{"id": 3769580, "s2_id": "722521543f911356ecc45e26fed1a48b12e8c35b", "title": "n-Qubit Operations on Sphere and Queueing Scaling Limits for Programmable Quantum Computer", "abstract": "We study n-qubit operation rules on (n+1)-sphere with the target to help developing a (photon or other technique) based programmable quantum computer. In the meanwhile, we derive the scaling limits (called reflecting Gaussian random fields on a (n+1)-sphere) for n-qubit quantum computer based queueing systems under two different heavy traffic regimes. The queueing systems are with multiple classes of users and batch quantum random walks over the (n + 1)-sphere as arrival inputs. In the first regime, the qubit number n is fixed and the scaling is in terms of both time and space. Under this regime, performance modeling during deriving the scaling limit in terms of balancing the arrival and service rates under first-in first-out and work conserving service policy is conducted. In the second regime, besides the time and space scaling parameters, the qubit number n itself is also considered as a varying scaling parameter with the additional aim to find a suitable number of qubits for the design of a quantum computer. This regime is in contrast to the well-known Halfin-Whitt regime.", "venue": "ArXiv", "authors": ["Wanyang  Dai"], "year": 2021, "n_citations": 0}
{"id": 3773356, "s2_id": "5be55453515815f8ed51d268f424f9cf1b3ae29d", "title": "A New Theoretical Framework of Pyramid Markov Processes for Blockchain Selfish Mining", "abstract": "In this paper, we provide a new theoretical framework of pyramid Markov processes to solve some open and fundamental problems of blockchain selfish mining. To this end, we first describe a more general blockchain selfish mining with both a two-block leading competitive criterion and a new economic incentive, and establish a pyramid Markov process to express the dynamic behavior of the selfish mining from both consensus protocol and economic incentive. Then we show that the pyramid Markov process is stable and so is the blockchain, and its stationary probability vector is matrix-geometric with an explicitly representable rate matrix. Furthermore, we use the stationary probability vector to be able to analyze the waste of computational resource due to generating a lot of orphan (or stale) blocks. Nextly, we set up a pyramid Markov reward process to investigate the long-run average profits of the honest and dishonest mining pools, respectively. Specifically, we show that the long-run average profits are multivariate linear such that we can measure the improvement of mining efficiency of the dishonest mining pool comparing to the honest mining pool. As a by-product, we build three approximative Markov processes when the system states are described as the block-number difference of two forked block branches. Also, by using their special cases with non network latency, we can further provide some useful interpretation for both the Markov chain (Figure 1) and the revenue analysis ((1) to (3)) of the seminal work by Eyal and Sirer (2014). Finally, we use some numerical examples to verify the correctness and computability of our theoretical results. We hope that the methodology and results developed in this paper shed light on the blockchain selfish mining such that a series of promising research can be produced potentially.", "venue": "Journal of Systems Science and Systems Engineering", "authors": ["Quan-Lin  Li", "Yan-Xia  Chang", "Xiaole  Wu", "Guoqing  Zhang"], "year": 2021, "n_citations": 6}
{"id": 3777702, "s2_id": "9bdabb3ebf5fc051f8cf43634ca497e53dcb797b", "title": "Deployment and configuration of MEC apps with Simu5G", "abstract": "Multi-access Edge Computing (MEC) is expected to act as the enabler for the integration of 5G (and future 6G) communication technologies with cloud-computing-based capabilities at the edge of the network. This will enable low-latency and context-aware applications for users of such mobile networks. In this paper we describe the implementation of a MEC model for the Simu5G simulator and illustrate how to configure the environment to evaluate MEC applications in both simulation and real-time emulation modes.", "venue": "ArXiv", "authors": ["Alessandro  Noferi", "Giovanni  Nardini", "Giovanni  Stea", "Antonio  Virdis"], "year": 2021, "n_citations": 0}
{"id": 3777779, "s2_id": "da4fa2b89093d52a80b47d65efe9346a897c1fd0", "title": "Mapping Matters: Application Process Mapping on 3-D Processor Topologies", "abstract": "Applications' performance is influenced by the mapping of processes to computing nodes, the frequency and volume of exchanges among processing elements, the network capacity, and the routing protocol. A poor mapping of application processes degrades performance and wastes resources. Process mapping is frequently ignored as an explicit optimization step since the system typically offers a default mapping, users may lack awareness of their applications' communication behavior, and the opportunities for improving performance through mapping are often unclear. This work studies the impact of application process mapping on several processor topologies. We propose a workflow that renders mapping as an explicit optimization step for parallel applications. We apply the workflow to a set of four applications, twelve mapping algorithms, and three direct network topologies. We assess the mappings' quality in terms of volume, frequency, and distance of exchanges using metrics such as dilation (measured in hop$\\cdot$Byte). With a parallel trace-based simulator, we predict the applications' execution on the three topologies using the twelve mappings. We evaluate the impact of process mapping on the applications' simulated performance in terms of execution and communication times and identify the mappings that achieve the highest performance in both cases. To ensure the correctness of the simulations, we compare the pre- and post-simulation results. This work emphasizes the importance of process mapping as an explicit optimization step and offers a solution for parallel applications to exploit the full potential of the allocated resources on a given system.", "venue": "ArXiv", "authors": ["Jonas H. M\u00fcller Kornd\u00f6rfer", "Mario  Bielert", "La\u00e9rcio Lima Pilla", "Florina M. Ciorba"], "year": 2020, "n_citations": 0}
{"id": 3778053, "s2_id": "be8c62802da2b94ed2dcc3dd0ca18a09df83c00b", "title": "Approximations and Bounds for (n, k) Fork-Join Queues: A Linear Transformation Approach", "abstract": "(n, k) fork-join queues are prevalent in popular distributed systems, erasure coding based cloud storages, and modern network protocols like multipath routing, estimating the sojourn time of such queues is thus critical for the performance measurement and resource plan of computer clusters. However, the estimating keeps to be a well-known open challenge for years, and only rough bounds for a limited range of load factors have been given. This paper developed a closed-form linear transformation technique for jointly-identical random variables: An order statistic can be represented by a linear combination of maxima. This brand-new technique is then used to transform the sojourn time of non-purging (n, k) fork-join queues into a linear combination of the sojourn times of basic (k, k), (k+1, k+1),..., (n, n) fork-join queues. Consequently, existing approximations for basic fork-join queues can be bridged to the approximations for non-purging (n, k) fork-join queues. The uncovered approximations are then used to improve the upper bounds for purging (n, k) fork-join queues. Simulation experiments show that this linear transformation approach is practiced well for moderate n and relatively large k.", "venue": "2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)", "authors": ["Huajin  Wang", "Jianhui  Li", "Zhihong  Shen", "Yuanchun  Zhou"], "year": 2018, "n_citations": 8}
{"id": 3781214, "s2_id": "80f1049da1e67d0970a93e4450213e19271b1b28", "title": "Energy efficient video fusion with heterogeneous CPU-FPGA devices", "abstract": "This paper presents a complete video fusion system with hardware acceleration and investigates the energy trade-offs between computing in the CPU or the FPGA device. The video fusion application is based on the Dual-Tree Complex Wavelet Transforms (DT-CWT). Video fusion combines information from different spectral bands into a single representation and advanced algorithms based on wavelet transforms are compute and energy intensive. In this work the transforms are mapped to a hardware accelerator using high-level synthesis tools for the FPGA and also vectorized code for the single instruction multiple data (SIMD) engine available in the CPU. The accelerated system reduces computation time and energy by a factor of 2. Moreover, the results show a key finding that the FPGA is not always the best choice for acceleration, and the SIMD engine should be selected when the wavelet decomposition reduces the frame size below a certain threshold. This dependency on workload size means that an adaptive system that intelligently selects between the SIMD engine and the FPGA achieves the most energy and performance efficiency point.", "venue": "2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Peng  Sun", "Alin  Achim", "Ian  Hasler", "Paul  Hill", "Jose  Nunez-Yanez"], "year": 2016, "n_citations": 2}
{"id": 3782354, "s2_id": "71f4e11cd4a1a77333f50d325606dca08e56a710", "title": "Effect of Meltdown and Spectre Patches on the Performance of HPC Applications", "abstract": "In this work we examine how the updates addressing Meltdown and Spectre vulnerabilities impact the performance of HPC applications. To study this we use the application kernel module of XDMoD to test the performance before and after the application of the vulnerability patches. We tested the performance difference for multiple application and benchmarks including: NWChem, NAMD, HPCC, IOR, MDTest and IMB. The results show that although some specific functions can have execution times decreased by as much as 74%, the majority of individual metrics indicates little to no decrease in performance. The real-world applications show a 2-3% decrease in performance for single node jobs and a 5-11% decrease for parallel multi node jobs.", "venue": "ArXiv", "authors": ["Nikolay  Simakov", "Martins D. Innus", "Matthew D. Jones", "Joseph P. White", "Steven M. Gallo", "Robert L. DeLeon", "Thomas R. Furlani"], "year": 2018, "n_citations": 16}
{"id": 3785462, "s2_id": "5709e46f3e84959ad89f7bd5aa1500463e671b62", "title": "Accelerating quantum many-body configuration interaction with directives", "abstract": "Many-Fermion Dynamics\u2014nuclear, or MFDn, is a configuration interaction (CI) code for nuclear structure calculations. It is a platform-independent Fortran 90 code using a hybrid MPI+X programming model. For CPU platforms the application has a robust and optimized OpenMP implementation for shared memory parallelism. As part of the NESAP application readiness program for NERSC\u2019s latest Perlmutter system, MFDn has been updated to take advantage of accelerators. The current mainline GPU port is based on OpenACC. In this work we describe some of the key challenges of creating an efficient GPU implementation. Additionally, we compare the support of OpenMP and OpenACC on AMD and NVIDIA GPUs.", "venue": "ArXiv", "authors": ["Brandon  Cook", "Patrick J. Fasano", "Pieter  Maris", "Chao  Yang", "Dossay  Oryspayev"], "year": 2021, "n_citations": 0}
{"id": 3785654, "s2_id": "914ac30c6db3db258d01b892d34da598ff75c60d", "title": "Pass-and-swap queues", "abstract": "Order-independent (OI) queues, introduced by Berezner et al. (Queueing Syst 19(4):345\u2013359, 1995), expanded the family of multi-class queues that are known to have a product-form stationary distribution by allowing for intricate class-dependent service rates. This paper further broadens this family by introducing pass-and-swap (P&S) queues, an extension of OI queues where, upon a service completion, the customer that completes service is not necessarily the one that leaves the system. More precisely, we supplement the OI queue model with an undirected graph on the customer classes, which we call a swapping graph, such that there is an edge between two classes if customers of these classes can be swapped with one another. When a customer completes service, it passes over customers in the remainder of the queue until it finds a customer it can swap positions with, that is, a customer whose class is a neighbor in the graph. In its turn, the customer that is ejected from its position takes the position of the next customer it can be swapped with, and so on. This is repeated until a customer can no longer find another customer to be swapped with; this customer is the one that leaves the queue. After proving that P&S queues have a product-form stationary distribution, we derive a necessary and sufficient stability condition for (open networks of) P&S queues that also applies to OI queues. We then study irreducibility properties of closed networks of P&S queues and derive the corresponding product-form stationary distribution. Lastly, we demonstrate that closed networks of P&S queues can be applied to describe the dynamics of new and existing load-distribution and scheduling protocols in clusters of machines in which jobs have assignment constraints.", "venue": "Queueing Syst. Theory Appl.", "authors": ["C'eline  Comte", "Jan-Pieter  Dorsman"], "year": 2021, "n_citations": 3}
{"id": 3791746, "s2_id": "b6072f8e889c756aace5797dd4e384376d2e5c4b", "title": "Performance comparison of IEEE 802.11g and IEEE 802.11n in the presence of interference from 802.15.4 networks", "abstract": "In this paper we compare the packet error rate (PER) and maximum throughput of IEEE 802.11n and IEEE 802.11g under interference from IEEE 802.15.4 by using MATLAB to simulate the IEEE PHY for 802.11n and 802.11g networks.", "venue": "ArXiv", "authors": ["Syed Haani Masood"], "year": 2013, "n_citations": 3}
{"id": 3802589, "s2_id": "1f1e36183125242ecc1eb6d6b386fb90d03e8ba7", "title": "Fast Packed String Matching for Short Patterns", "abstract": "Searching for all occurrences of a pattern in a text is a fundamental problem in computer science with applications in many other fields, like natural language processing, information retrieval and computational biology. In the last two decades a general trend has appeared trying to exploit the power of the word RAM model to speed-up the performances of classical string matching algorithms. In this model an algorithm operates on words of length w, grouping blocks of characters, and arithmetic and logic operations on the words take one unit of time. \n \nIn this paper we use specialized word-size packed string matching instructions, based on the Intel streaming SIMD extensions (SSE) technology, to design very fast string matching algorithms in the case of short patterns. From our experimental results it turns out that, despite their quadratic worst case time complexity, the new presented algorithms become the clear winners on the average for short patterns, when compared against the most Effective algorithms known in literature.", "venue": "ALENEX", "authors": ["Simone  Faro", "M. Oguzhan K\u00fclekci"], "year": 2013, "n_citations": 25}
{"id": 3804210, "s2_id": "1a86d0d0aa6946b2d711bbcfa7f9b8e607b07c7b", "title": "Availability Evaluation of Multi-Tenant Service Function Chaining Infrastructures by Multidimensional Universal Generating Function", "abstract": "The Network Function Virtualization (NFV) paradigm has been devised as an enabler of next generation network infrastructures by speeding up the provisioning and the composition of novel network services. The latter are implemented via a chain of virtualized network functions, a process known as Service Function Chaining. In this paper, we evaluate the availability of multi-tenant SFC infrastructures, where every network function is modeled as a multi-state system and is shared among different and independent tenants. To this aim, we propose a Universal Generating Function (UGF) approach, suitably extended to handle performance vectors, that we call Multidimensional UGF. This novel methodology is validated in a realistic multi-tenant telecommunication network scenario, where the service chain is composed by the network elements of an IP Multimedia Subsystem implemented via NFV. A steady-state availability evaluation of such an exemplary system is presented and a redundancy optimization problem is solved, so providing the SFC infrastructure which minimizes deployment cost while respecting a given availability requirement.", "venue": "IEEE Transactions on Services Computing", "authors": ["Mario  Di Mauro", "Maurizio  Longo", "Fabio  Postiglione"], "year": 2021, "n_citations": 15}
{"id": 3804945, "s2_id": "ee8a8f8c130967b7bbd238f68a4e4523fd09a48b", "title": "To Compress or Not To Compress: Processing vs Transmission Tradeoffs for Energy Constrained Sensor Networking", "abstract": "In the past few years, lossy compression has been widely applied in the field of wireless sensor networks (WSN), where energy efficiency is a crucial concern due to the constrained nature of the transmission devices. Often, the common thinking among researchers and implementers is that compression is always a good choice, because the major source of energy consumption in a sensor node comes from the transmission of the data. Lossy compression is deemed a viable solution as the imperfect reconstruction of the signal is often acceptable in WSN, subject to some application dependent maximum error tolerance. Nevertheless, this is seldom supported by quantitative evidence. In this paper, we thoroughly review a number of lossy compression methods from the literature, and analyze their performance in terms of compression efficiency, computational complexity and energy consumption. We consider two different scenarios, namely, wireless and underwater communications, and show that signal compression may or may not help in the reduction of the overall energy consumption, depending on factors such as the compression algorithm, the signal statistics and the hardware characteristics, i.e., micro-controller and transmission technology. The lesson that we have learned, is that signal compression may in fact provide some energy savings. However, its usage should be carefully evaluated, as in quite a few cases processing and transmission costs are of the same order of magnitude, whereas, in some other cases, the former may even dominate the latter. In this paper, we show quantitative comparisons to assess these tradeoffs in the above mentioned scenarios (i.e., wireless versus underwater). In addition, we consider recently proposed and lightweight algorithms such as Lightweight Temporal Compression (LTC) as well as more sophisticated FFT- or DCT-based schemes and show that the former are the best option in wireless settings, whereas the latter solutions are preferable for underwater networks. Finally, we provide formulas, obtained through numerical fittings, to gauge the computational complexity, the overall energy consumption and the signal representation accuracy of the best performing algorithms as a function of the most relevant system parameters.", "venue": "ArXiv", "authors": ["Davide  Zordan", "Borja  Mart\u00ednez", "Ignasi  Vilajosana", "Michele  Rossi"], "year": 2012, "n_citations": 17}
{"id": 3807648, "s2_id": "dab1a1f05a73aa62b5a03a1b31c07b72b44c3038", "title": "Batch Size Influence on Performance of Graphic and Tensor Processing Units during Training and Inference Phases", "abstract": "The impact of the maximally possible batch size (for the better runtime) on performance of graphic processing units (GPU) and tensor processing units (TPU) during training and inference phases is investigated. The numerous runs of the selected deep neural network (DNN) were performed on the standard MNIST and Fashion-MNIST datasets. The significant speedup was obtained even for extremely low-scale usage of Google TPUv2 units (8 cores only) in comparison to the quite powerful GPU NVIDIA Tesla K80 card with the speedup up to 10x for training stage (without taking into account the overheads) and speedup up to 2x for prediction stage (with and without taking into account overheads). The precise speedup values depend on the utilization level of TPUv2 units and increase with the increase of the data volume under processing, but for the datasets used in this work (MNIST and Fashion-MNIST with images of sizes 28x28) the speedup was observed for batch sizes >512 images for training phase and >40 000 images for prediction phase. It should be noted that these results were obtained without detriment to the prediction accuracy and loss that were equal for both GPU and TPU runs up to the 3rd significant digit for MNIST dataset, and up to the 2nd significant digit for Fashion-MNIST dataset.", "venue": "Advances in Computer Science for Engineering and Education II", "authors": ["Yuriy  Kochura", "Yuri  Gordienko", "Vlad  Taran", "Nikita  Gordienko", "Alexandr  Rokovyi", "Oleg  Alienin", "Sergii  Stirenko"], "year": 2019, "n_citations": 13}
{"id": 3809337, "s2_id": "b5e57aae10a633e8d5abddc9aac494dcd5409ceb", "title": "Some parametrized dynamic priority policies for 2-class M/G/1 queues: completeness and applications", "abstract": "Completeness of a dynamic priority scheduling scheme is of fundamental importance for the optimal control of queues in areas as diverse as computer communications, communication networks, supply chains and manufacturing systems. Our first main contribution is to identify the mean waiting time completeness as a unifying aspect for four different dynamic priority scheduling schemes by proving their completeness and equivalence in 2-class M/G/1 queue. These dynamic priority schemes are earliest due date based, head of line priority jump, relative priority, and probabilistic priority. \nIn our second main contribution, we characterize the optimal scheduling policies for the case studies in different domains by exploiting the completeness of above dynamic priority schemes. The major theme of second main contribution is resource allocation/optimal control in revenue management problems for contemporary systems such as cloud computing, high-performance computing, etc., where congestion is inherent. Using completeness and theoretically tractable nature of relative priority policy, we study the impact of approximation in a fairly generic data network utility framework. We introduce the notion of min-max fairness in multi-class queues and show that a simple global FCFS policy is min-max fair. Next, we re-derive the celebrated $c/\\rho$ rule for 2-class M/G/1 queues by an elegant argument and also simplify a complex joint pricing and scheduling problem for a wider class of scheduling policies.", "venue": "ArXiv", "authors": ["Manu K. Gupta", "Nandyala  Hemachandra", "Jayendran  Venkateswaran"], "year": 2018, "n_citations": 1}
{"id": 3809601, "s2_id": "da5fc507d545a38d62a4b6f0f2303fbe1908f130", "title": "On Access Control in Cabin-Based Transport Systems", "abstract": "We analyze a boarding solution for a transport system in which the number of passengers allowed to enter a transport cabin is automatically controlled. Expressions characterizing the stochastic properties of the passenger queue length, waiting time, and cabin capacity are derived using queuing theory for a transport line with deterministic arrivals of cabins and Poisson arrivals of passengers. The expected cabin capacity and stability threshold for each station are derived for a general passenger arrival distribution. The results show that a significant reduction of the waiting time at a given station is only possible at the cost of making the stability of one of the preceding stations worse than that of the given station. Studies with real passenger arrivals are necessary to draw firm conclusions.", "venue": "IEEE Transactions on Intelligent Transportation Systems", "authors": ["Pasquale  Grippa", "Udo  Schilcher", "Christian  Bettstetter"], "year": 2019, "n_citations": 3}
{"id": 3811462, "s2_id": "71f7f610e8a4368ae27777d8ac11fd0dd88dcc02", "title": "A Performance Study of GA and LSH in Multiprocessor Job Scheduling", "abstract": "Multiprocessor task scheduling is an important and computationally difficult problem. This paper proposes a comparison study of genetic algorithm and list scheduling algorithm. Both algorithms are naturally parallelizable but have heavy data dependencies. Based on experimental results, this paper presents a detailed analysis of the scalability, advantages and disadvantages of each algorithm. Multiprocessors have emerged as a powerful computing means for running real-time applications, especially where a uni-processor system would not be sufficient enough to execute all the tasks. The high performance and reliability of multiprocessors have made them a powerful computing resource. Such computing environment requires an efficient algorithm to determine when and on which processor a given task should execute. In multiprocessor systems, an efficient scheduling of a parallel program onto the processors that minimizes the entire execution time is vital for achieving a high performance. This scheduling problem is known to be NPHard. In multiprocessor scheduling problem, a given program is to be scheduled in a given multiprocessor system such that the program\u2019s execution time is minimized. The last job must be completed as early as possible. Genetic algorithm (GA) is one of the widely used techniques for constrained optimization. Genetic algorithms are basically search algorithms based on the mechanics of natural selection and natural genetics. List scheduling techniques assign a priority to each task to be scheduled then sort the list of tasks in decreasing priority. As processors become available, the highest priority task in the task list is assigned to be processed and removed from the list. If more than one task has the same priority, selection from among the candidate tasks is typically random. This paper compares Genetic algorithm (GA) with List Scheduling heuristic (LSH) to solve scheduling problem of multiprocessors.", "venue": "ArXiv", "authors": ["S. R. Vijayalakshmi", "G.  Padmavathi"], "year": 2010, "n_citations": 12}
{"id": 3812222, "s2_id": "cc1e864c07668994d67efe8ce5723df3193cc8b5", "title": "Optimal power cost management using stored energy in data centers", "abstract": "Since the electricity bill of a data center constitutes a significant portion of its overall operational costs, reducing this has become important. We investigate cost reduction opportunities that arise by the use of uninterrupted power supply (UPS) units as energy storage devices. This represents a deviation from the usual use of these devices as mere transitional fail-over mechanisms between utility and captive sources such as diesel generators. We consider the problem of opportunistically using these devices to reduce the time average electric utility bill in a data center. Using the technique of Lyapunov optimization, we develop an online control algorithm that can optimally exploit these devices to minimize the time average cost. This algorithm operates without any knowledge of the statistics of the workload or electricity cost processes, making it attractive in the presence of workload and pricing uncertainties. An interesting feature of our algorithm is that its deviation from optimality reduces as the storage capacity is increased. Our work opens up a new area in data center power management.", "venue": "PERV", "authors": ["Rahul  Urgaonkar", "Bhuvan  Urgaonkar", "Michael J. Neely", "Anand  Sivasubramaniam"], "year": 2011, "n_citations": 345}
{"id": 3821891, "s2_id": "5a14b9884411f0b533d9c32434b97ba7adffcca9", "title": "Worst-case Bounds and Optimized Cache on Mth Request Cache Insertion Policies under Elastic Conditions", "abstract": "This 2-page extended abstract provides an overview of the key results presented in more detail in our full length paper, with the same title, to appear in Performance Evaluation [2].", "venue": "PERV", "authors": ["Niklas  Carlsson", "Derek L. Eager"], "year": 2019, "n_citations": 1}
{"id": 3822016, "s2_id": "5afd1233e5d3087ee4c51d4df4a70dd593a26fe0", "title": "Significance of parallel computing on the performance of Digital Image Correlation algorithms in MATLAB", "abstract": "Digital Image Correlation (DIC) is a powerful tool used to evaluate displacements and deformations in a non-intrusive manner. By comparing two images, one of the undeformed reference state of a specimen and another of the deformed target state, the relative displacement between those two states is determined. DIC is well known and often used for post-processing analysis of in-plane displacements and deformation of specimen. Increasing the analysis speed to enable real-time DIC analysis will be beneficial and extend the field of use of this technique. Here we tested several combinations of the most common DIC methods in combination with different parallelization approaches in MATLAB and evaluated their performance to determine whether real-time analysis is possible with these methods. To reflect improvements in computing technology different hardware settings were also analysed. We found that implementation problems can reduce the efficiency of a theoretically superior algorithm such that it becomes practically slower than a sub-optimal algorithm. The Newton-Raphson algorithm in combination with a modified Particle Swarm algorithm in parallel image computation was found to be most effective. This is contrary to theory, suggesting that the inverse-compositional Gauss-Newton algorithm is superior. As expected, the Brute Force Search algorithm is the least effective method. We also found that the correct choice of parallelization tasks is crucial to achieve improvements in computing speed. A poorly chosen parallelisation approach with high parallel overhead leads to inferior performance. Finally, irrespective of the computing mode the correct choice of combinations of integer-pixel and sub-pixel search algorithms is decisive for an efficient analysis. Using currently available hardware real-time analysis at high framerates remains an aspiration.", "venue": "ArXiv", "authors": ["Andreas  Thoma", "Sridhar  Ravi"], "year": 2019, "n_citations": 0}
{"id": 3825956, "s2_id": "c72681bab54688b6d18da5d5c53ce2e0a4b47ff8", "title": "Performance Evaluation of the Quorum Blockchain Platform", "abstract": "Quorum is a permissioned blockchain platform built from the Ethereum codebase with adaptations to make it a permissioned consortium platform. It is one of the key contenders in the permissioned ledger space. Quorum supports confidentiality and privacy of smart contracts and transactions, and crash and Byzantine fault tolerant consensus algorithms. In this paper, we characterize the performance features of Quorum. We study the throughput and latency characteristics of Quorum with different workloads and consensus algorithms that it supports. Through a suite of micro-benchmarks, we explore how certain transaction and smart contract parameters can affect transaction latencies.", "venue": "ArXiv", "authors": ["Arati  Baliga", "I  Subhod", "Pandurang  Kamat", "Siddhartha  Chatterjee"], "year": 2018, "n_citations": 57}
{"id": 3826189, "s2_id": "5849c4fb9cf5832b05cafd0db62f1b7b05f8584a", "title": "Tromino: Demand and DRF Aware Multi-Tenant Queue Manager for Apache Mesos Cluster", "abstract": "Apache Mesos, a two-level resource scheduler, provides resource sharing across multiple users in a multi-tenant clustered environment. Computational resources (i.e., CPU, memory, disk, etc.) are distributed according to the Dominant Resource Fairness (DRF) policy. Mesos frameworks (users) receive resources based on their current usage and are responsible for scheduling their tasks within the allocation. We have observed that multiple frameworks can cause fairness imbalance in a multi-user environment. For example, a greedy framework consuming more than its fair share of resources can deny resource fairness to others. The user with the least Dominant Share is considered first by the DRF module to get its resource allocation. However, the default DRF implementation, in Apache Mesos' Master allocation module, does not consider the overall resource demands of the tasks in the queue for each user/framework. This lack of awareness can lead to poor performance as users without any pending task may receive more resource offers, and users with a queue of pending tasks can starve due to their high dominant shares. In a multi-tenant environment, the characteristics of frameworks and workloads must be understood by cluster managers to be able to define fairness based on not only resource share but also resource demand and queue wait time. We have developed a policy driven queue manager, Tromino, for an Apache Mesos cluster where tasks for individual frameworks can be scheduled based on each framework's overall resource demands and current resource consumption. Dominant Share and demand awareness of Tromino and scheduling based on these attributes can reduce (1) the impact of unfairness due to a framework specific configuration, and (2) unfair waiting time due to higher resource demand in a pending task queue. In the best case, Tromino can significantly reduce the average waiting time of a framework by using the proposed Demand-DRF aware policy.", "venue": "2018 IEEE/ACM 11th International Conference on Utility and Cloud Computing (UCC)", "authors": ["Pankaj  Saha", "Angel  Beltre", "Madhusudhan  Govindaraju"], "year": 2018, "n_citations": 4}
{"id": 3826270, "s2_id": "c06d8b387b00ddc248fb09e8d2971d81ec41a5d5", "title": "Performance Optimization and Parallelization of a Parabolic Equation Solver in Computational Ocean Acoustics on Modern Many-core Computer", "abstract": "As one of open-source codes widely used in computational ocean acoustics, FOR3D can provide a very good estimate for underwater acoustic propagation. In this paper, we propose a performance optimization and parallelization to speed up the running of FOR3D. We utilized a variety of methods to enhance the entire performance, such as using a multi-threaded programming model to exploit the potential capability of the many-core node of high-performance computing (HPC) system, tuning compile options, using efficient tuned mathematical library and utilizing vectorization optimization instruction. In addition, we extended the application from single-frequency calculation to multi-frequency calculation successfully by using OpenMP+MPI hybrid programming techniques on the mainstream HPC platform. A detailed performance evaluation was performed and the results showed that the proposed parallelization obtained good accelerated effect of 25.77 \u00d7 when testing a typical three-dimensional medium-sized case on Tianhe-2 supercomputer. It also showed that the tuned parallel version has a weak-scalability. The speed of calculation of underwater sound field can be greatly improved by the strategy mentioned in this paper. The method used in this paper is not only applicable to other similar computing models in computational ocean acoustics but also a guideline of performance enhancement for scientific and engineering application running on modern many-core-computing platform.", "venue": "ArXiv", "authors": ["Min  Xu", "Yong-Yian  Wang", "Anthony T. Chronopoulos", "Hao  Yue"], "year": 2017, "n_citations": 1}
{"id": 3826626, "s2_id": "53094b48287db57f22eb34c3cba1f590c80a12b7", "title": "Towards Fine-Grained Billing For Cloud Networking", "abstract": "We revisit multi-tenant network virtualization in data centers, and make the case for tenant-specific virtual switches. In particular, tenant-specific virtual switches allow cloud providers to extend fine-grained billing (known, e.g., from serverless architectures) to the network, accounting not only for IO, but also CPU or energy. We sketch an architecture and present economical motivation and recent technological enablers. We also find that virtual switches today do not offer sufficient multi-tenancy and can introduce artificial performance bottlenecks, e.g., in load balancers. We conclude by discussing additional use cases for tentant-specific switches.", "venue": "ArXiv", "authors": ["Kashyap  Thimmaraju", "Stefan  Schmid"], "year": 2020, "n_citations": 1}
{"id": 3827250, "s2_id": "d722c13be9299c81f0d75722749d6440d26e779b", "title": "Analyzing the Performance of Active Queue Management Algorithms", "abstract": "Congestion is an important issue which researchers focus on in the Transmission Control Protocol (TCP) network environment. To keep the stability of the whole network, congestion control algorithms have been extensively studied. Queue management method employed by the routers is one of the important issues in the congestion control study. Active queue management (AQM) has been proposed as a router-based mechanism for early detection of congestion inside the network. In this paper we analyzed several active queue management algorithms with respect to their abilities of maintaining high resource utilization, identifying and restricting disproportionate bandwidth usage, and their deployment complexity. We compare the performance of FRED, BLUE, SFB, and CHOKe based on simulation results, using RED and Drop Tail as the evaluation baseline. The characteristics of different algorithms are also discussed and compared. Simulation is done by using Network Simulator(NS2) and the graphs are drawn using X- graph.", "venue": "ArXiv", "authors": ["G. F. Ali Ahammed", "Reshma  Banu"], "year": 2010, "n_citations": 46}
{"id": 3831731, "s2_id": "b13c62ba036a77a9307d0eea3955e9782cf004a8", "title": "Report: Performance comparison between C2075 and P100 GPU cards using cosmological correlation functions", "abstract": "In this report, some cosmological correlation functions are used to evaluate the differential performance between C2075 and P100 GPU cards. In the past, the correlation functions used in this work have been widely studied and exploited on some previous GPU architectures. The analysis of the performance indicates that a speedup in the range from 13 to 15 is achieved without any additional optimization process for the P100 card.", "venue": "ArXiv", "authors": ["Miguel  C\u00e1rdenas-Montes", "Iv\u00e1n  M\u00e9ndez-Jim\u00e9nez", "Juan Jos\u00e9 Rodr\u00edguez-V\u00e1zquez", "Jos\u00e9 Mar\u00eda Hern\u00e1ndez Calama"], "year": 2017, "n_citations": 2}
{"id": 3832520, "s2_id": "e409d3bda8c6ecf1c85472a333948dc156c61205", "title": "Scheduling Parallel-Task Jobs Subject to Packing and Placement Constraints", "abstract": "Motivated by modern parallel computing applications, we consider the problem of scheduling parallel-task jobs with heterogeneous resource requirements in a cluster of machines. Each job consists of a set of tasks that can be processed in parallel, however, the job is considered completed only when all its tasks finish their processing, which we refer to as \"synchronization\" constraint. Further, assignment of tasks to machines is subject to \"placement\" constraints, i.e., each task can be processed only on a subset of machines, and processing times can also be machine dependent. Once a task is scheduled on a machine, it requires a certain amount of resource from that machine for the duration of its processing. A machine can process (\"pack\") multiple tasks at the same time, however the cumulative resource requirement of the tasks should not exceed the machine's capacity. \nOur objective is to minimize the weighted average of the jobs' completion times. The problem, subject to synchronization, packing and placement constraints, is NP-hard, and prior theoretical results only concern much simpler models. For the case that migration of tasks among the placement-feasible machines is allowed, we propose a preemptive algorithm with an approximation ratio of $(6+\\epsilon)$. In the special case that only one machine can process each task, we design an algorithm with improved approximation ratio of $4$. Finally, in the case that migrations (and preemptions) are not allowed, we design an algorithm with an approximation ratio of $24$. Our algorithms use a combination of linear program relaxation and greedy packing techniques. We present extensive simulation results, using a real traffic trace, that demonstrate that our algorithms yield significant gains over the prior approaches.", "venue": "ArXiv", "authors": ["Mehrnoosh  Shafiee", "Javad  Ghaderi"], "year": 2020, "n_citations": 0}
{"id": 3836705, "s2_id": "99f46e941be340f8aa64b5977e70546284c5e073", "title": "Performance Measurements Within Asynchronous Task-Based Runtime Systems: A Double White Dwarf Merger as an Application", "abstract": "Analyzing performance within asynchronous many-task-based runtime systems is challenging because millions of tasks are launched concurrently. Especially for long-term runs, the amount of data collected becomes overwhelming. We study HPX and its performance-counter framework and autonomic performance environment for Exascale to collect performance data and energy consumption. We added HPX application-specific performance counters to the Octo-Tiger full 3-D adaptive multigrid code astrophysics application. This enables the combined visualization of physical and performance data to highlight bottlenecks with respect to different solvers. We examine the overhead introduced by these measurements, which is around 1%, with respect to the overall application runtime. We perform a resolution study for four different levels of refinement and analyze the application's performance with respect to adaptive grid refinement. The measurements\u2019 overheads are small, enabling the combined use of performance data and physical properties with the goal of improving the code's performance. All runs were obtained on NERSC's Cori, Louisiana Optical Network Infrastructure's QueenBee2, and Indiana University's Big Red 3.", "venue": "Computing in Science & Engineering", "authors": ["Patrick  Diehl", "Dominic  Marcello", "Parsa  Armini", "Hartmut  Kaiser", "Sagiv  Shiber", "Geoffrey C. Clayton", "Juhan  Frank", "Gregor  Daiss", "Dirk  Pfluger", "David  Eder", "Alice  Koniges", "Kevin  Huck"], "year": 2021, "n_citations": 2}
{"id": 3839920, "s2_id": "038029c39fa32747fbd33e4ad767f02e6e3595af", "title": "HARQ in Full-Duplex Relay-Assisted Transmissions for URLLC", "abstract": "The Release 16 completion unlocks the road to an exciting phase pertain to the sixth generation (6G) era. Meanwhile, to sustain far-reaching applications with unprecedented challenges in terms of latency and reliability, much interest is already getting intensified toward physical layer specifications of 6G. In support of this vision, this work exhibits the forward-looking perception of full-duplex (FD) cooperative relaying in support of upcoming generations and adopts as a mean concern the critical contribution of hybrid automatic repeat request (HARQ) mechanism to ultra-reliable and low-latency communication (URLLC). Indeed, the HARQ roundtrip time (RTT) is known to include basic physical delays that may cause the HARQ abandonment for the 1 ms latency use case of URLLC. Taking up these challenges, this article proposes a hybrid FD amplify-and-forward (AF)-selective decode-and-forward (SDF) relay-based system for URLLC. Over this build system, two HARQ procedures within which the HARQ RTT is shortened, are suggested to face latency and reliability issues, namely, the proposed and the enhanced HARQ procedures. We develop then an analytical framework of this relay based HARQ system within its different procedures. Finally, using Monte-Carlo simulations, we confirm the theoretical results and compare the proposed relay-assisted HARQ procedures to the source-to-destination (S2D) HARQ-based system where no relay assists the communication between the source and the destination.", "venue": "IEEE Open Journal of the Communications Society", "authors": ["Fatima Ezzahra Airod", "Houda  Chafnaji", "Halim  Yanikomeroglu"], "year": 2021, "n_citations": 0}
{"id": 3840658, "s2_id": "39bdfae53a5325806ab75b13fb21fd77e07c75b2", "title": "Reengineering multi tiered enterprise business applications for performance enhancement and reciprocal or rectangular hyperbolic relation of variation of data transportation time with row pre-fetch size of relational database drivers", "abstract": "In a traditional multitier applications performance bottlenecks can be in user interactions level or network latency or data access or business logic level. The solutions as changes or tuning parameters can be applied at architect, design, framework or algorithm or at coding level. This paper highlights an inquisitive, experimental, top down, tear apart, drill down and analytical approach across two aspects one across end to end process flow and data flow on those specific use cases or scenarios requiring performance improvement and another across layers of abstraction like architecture, framework, design, logic and coding. Re engineering for performance gain requires identifying hot spots on both aspects viz which architectural, design decision or which processing or data flow stage is having performance issue. Once identified one can further drill down and identify root cause and also can find solution as a change. To help the application owner in decision making process, the analysis outcome should have tuning parameters, relationship between them, optimum values, tradeoffs on each changes, effort, risks, cost and benefits for incorporating each change. Following the above mentioned approach on a system in production with large enterprise we could drill down to a rectangular hyperbolic or reciprocal relation between elapsed time to transport all records retrieved from a query and the number of records being pre fetched (pre fetch size) and cached in the data base client application by the database driver in each trip. Because of the reciprocal nature , we could observe that when the pre fetch size is low drastic reduction in elapsed time could be obtained even for a small increase in pre fetch size, whereas when the pre fetch size is high the gain in performance is not so significantly high even for larger increase in pre fetch size.", "venue": "ArXiv", "authors": ["Sridhar  Sowmiyanarayanan"], "year": 2012, "n_citations": 0}
{"id": 3857235, "s2_id": "73e8fc4292ecbd271db9c4229c1cc9736b053760", "title": "ProGraML: Graph-based Deep Learning for Program Optimization and Analysis", "abstract": "The increasing complexity of computing systems places a tremendous burden on optimizing compilers, requiring ever more accurate and aggressive optimizations. Machine learning offers significant benefits for constructing optimization heuristics but there remains a gap between what state-of-the-art methods achieve and the performance of an optimal heuristic. Closing this gap requires improvements in two key areas: a representation that accurately captures the semantics of programs, and a model architecture with sufficient expressiveness to reason about this representation. \nWe introduce ProGraML - Program Graphs for Machine Learning - a novel graph-based program representation using a low level, language agnostic, and portable format; and machine learning models capable of performing complex downstream tasks over these graphs. The ProGraML representation is a directed attributed multigraph that captures control, data, and call relations, and summarizes instruction and operand types and ordering. Message Passing Neural Networks propagate information through this structured representation, enabling whole-program or per-vertex classification tasks. \nProGraML provides a general-purpose program representation that equips learnable models to perform the types of program analysis that are fundamental to optimization. To this end, we evaluate the performance of our approach first on a suite of traditional compiler analysis tasks: control flow reachability, dominator trees, data dependencies, variable liveness, and common subexpression detection. On a benchmark dataset of 250k LLVM-IR files covering six source programming languages, ProGraML achieves an average 94.0 F1 score, significantly outperforming the state-of-the-art approaches. We then apply our approach to two high-level tasks - heterogeneous device mapping and program classification - setting new state-of-the-art performance in both.", "venue": "ArXiv", "authors": ["Chris  Cummins", "Zacharias V. Fisches", "Tal  Ben-Nun", "Torsten  Hoefler", "Hugh  Leather"], "year": 2020, "n_citations": 26}
{"id": 3858237, "s2_id": "26ea34ef3805ff84bbe8f7f6ef91624d272ecbc2", "title": "GAPP: A Fast Profiler for Detecting Serialization Bottlenecks in Parallel Linux Applications", "abstract": "We present a parallel profiling tool, GAPP, that identifies serialization bottlenecks in parallel Linux applications arising from load imbalance or contention for shared resources . It works by tracing kernel context switch events using kernel probes managed by the extended Berkeley Packet Filter (eBPF) framework. The overhead is thus extremely low (an average 4% runtime overhead for the applications explored), the tool requires no program instrumentation and works for a variety of serialization bottlenecks. We evaluate GAPP using the Parsec3.0 benchmark suite and two large open-source projects: MySQL and Nektar++ (a spectral/hp element framework). We show that GAPP is able to reveal a wide range of bottleneck-related performance issues, for example arising from synchronization primitives, busy-wait loops, memory operations, thread imbalance and resource contention.", "venue": "ICPE", "authors": ["Reena  Nair", "Tony  Field"], "year": 2020, "n_citations": 0}
{"id": 3858423, "s2_id": "db2b7c6d58c55f4646989f3a062b674ca59af150", "title": "Efficient Tensor Kernel methods for sparse regression", "abstract": "Recently, classical kernel methods have been extended by the introduction of suitable tensor kernels so to promote sparsity in the solution of the underlying regression problem. Indeed, they solve an lp-norm regularization problem, with p=m/(m-1) and m even integer, which happens to be close to a lasso problem. However, a major drawback of the method is that storing tensors requires a considerable amount of memory, ultimately limiting its applicability. In this work we address this problem by proposing two advances. First, we directly reduce the memory requirement, by intriducing a new and more efficient layout for storing the data. Second, we use a Nystrom-type subsampling approach, which allows for a training phase with a smaller number of data points, so to reduce the computational cost. Experiments, both on synthetic and read datasets, show the effectiveness of the proposed improvements. Finally, we take case of implementing the cose in C++ so to further speed-up the computation.", "venue": "ArXiv", "authors": ["Feliks  Hibraj", "Marcello  Pelillo", "Saverio  Salzo", "Massimiliano  Pontil"], "year": 2020, "n_citations": 0}
{"id": 3861537, "s2_id": "b42a3c95a6b1c9e8fd0f9b2e07acd575e0585e41", "title": "DRESS: Dynamic RESource-Reservation Scheme for Congested Data-Intensive Computing Platforms", "abstract": "In the past few years, we have envisioned an increasing number of businesses start driving by big data analytics, such as Amazon recommendations and Google Advertisements. At the back-end side, the businesses are powered by big data processing platforms to quickly extract information and make decisions. Running on top of a computing cluster, those platforms utilize scheduling algorithms to allocate resources. An efficient scheduler is crucial to the system performance due to limited resources, e.g. CPU and Memory, and a large number of user demands. However, besides requests from clients and current status of the system, it has limited knowledge about execution length of the running jobs, and incoming jobs' resource demands, which make assigning resources a challenging task. If most of the resources are occupied by a long-running job, other jobs will have to keep waiting until it releases them. This paper presents a new scheduling strategy, named DRESS that particularly aims to optimize the allocation among jobs with various demands. Specifically, it classifies the jobs into two categories based on their requests, reserves a portion of resources for each of category, and dynamically adjusts the reserved ratio by monitoring the pending requests and estimating release patterns of running jobs. The results demonstrate DRESS significantly reduces the completion time for one category, up to 76.1% in our experiments, and in the meanwhile, maintains a stable overall system performance.", "venue": "2018 IEEE 11th International Conference on Cloud Computing (CLOUD)", "authors": ["Ying  Mao", "Victoria  Green", "Jiayin  Wang", "Haoyi  Xiong", "Zhishan  Guo"], "year": 2018, "n_citations": 10}
{"id": 3872754, "s2_id": "6e551b88eb0fc6a5a4d20ea148eabc890ff4e6cf", "title": "Distributed Server Allocation for Content Delivery Networks", "abstract": "We propose a dynamic formulation of file-sharing networks in terms of an average cost Markov decision process with constraints. By analyzing a Whittle-like relaxation thereof, we propose an index policy in the spirit of Whittle and compare it by simulations with other natural heuristics.", "venue": "ArXiv", "authors": ["Sarath  Pattathil", "Vivek S. Borkar", "Gaurav S. Kasbekar"], "year": 2017, "n_citations": 2}
{"id": 3874218, "s2_id": "faf634f0ee0022fdcfa63c4e8163ca48d5c7234a", "title": "On Stability and Sojourn Time of Peer-to-Peer Queuing Systems", "abstract": "Recent development of peer-to-peer (P2P) services systems introduces a new type of queue systems that receive little attention before, where both job and server arrive and depart randomly. Current study on these models focuses on the stability condition, under exponential workload assumption. This paper extends existing result in two aspects. In the first part of the paper we relax the exponential workload assumption, and study the stability of systems with general workload distribution. The second part of the paper focuses on the job sojourn time. An upper bound and a lower bound for job sojourn time are investigated. We evaluate tightness of the bounds by numerical analysis.", "venue": "ArXiv", "authors": ["Taoyu  Li", "Minghua  Chen", "Tony  Lee", "Xing  Li"], "year": 2016, "n_citations": 1}
{"id": 3878564, "s2_id": "34d56b5345e278a3cac0a2ae4111fe4315c6579a", "title": "Multi-Tier Caching Analysis in CDN-Based Over-the-Top Video Streaming Systems", "abstract": "Internet video traffic has been rapidly increasing and is further expected to increase with the emerging 5G applications, such as higher definition videos, the IoT, and augmented/virtual reality applications. As end users consume video in massive amounts and in an increasing number of ways, the content distribution network (CDN) should be efficiently managed to improve the system efficiency. The streaming service can include multiple caching tiers, at the distributed servers and the edge routers, and efficient content management at these locations affects the quality of experience (QoE) of the end users. In this paper, we propose a model for video streaming systems, typically composed of a centralized origin server, several CDN sites, and edge-caches located closer to the end user. We comprehensively consider different systems design factors, including the limited caching space at the CDN sites, allocation of CDN for a video request, choice of different ports (or paths) from the CDN and the central storage, bandwidth allocation, the edge-cache capacity, and the caching policy. We focus on minimizing a performance metric, stall duration tail probability (SDTP), and present a novel and efficient algorithm accounting for the multiple design flexibilities. The theoretical bounds with respect to the SDTP metric are also analyzed and presented. The implementation of a virtualized cloud system managed by Openstack demonstrates that the proposed algorithms can significantly improve the SDTP metric compared with the baseline strategies.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Abubakr O. Al-Abbasi", "Vaneet  Aggarwal", "Moo-Ryong  Ra"], "year": 2019, "n_citations": 18}
{"id": 3879377, "s2_id": "ed7a095bef7226443ade7594b821b8c09e66bc34", "title": "Benchmarking Blunders and Things That Go Bump in the Night", "abstract": "Benchmarking; by which I mean any computer system that is driven by a controlled workload, is the ultimate in performance testing and simulation. Aside from being a form of institutionalized cheating, it also offer countless opportunities for systematic mistakes in the way the workloads are applied and the resulting measurements interpreted. Right test, wrong conclusion is a ubiquitous mistake that happens because test engineers tend to treat data as divine. Such reverence is not only misplaced, it's also a sure ticket to production hell when the application finally goes live. I demonstrate how such mistakes can be avoided by means of two war stories that are real WOPRs. (a) How to resolve benchmark flaws over the psychic hotline and (b) How benchmarks can go flat with too much Java juice. In each case I present simple performance models and show how they can be applied to correctly assess benchmark data.", "venue": "ArXiv", "authors": ["Neil J. Gunther"], "year": 2004, "n_citations": 5}
{"id": 3879446, "s2_id": "44caae72d923a19ca8e767283aa818349cc228d4", "title": "An inversion formula with hypergeometric polynomials and application to singular integral operators", "abstract": "Given parameters $x \\notin \\mathbb{R}^- \\cup \\{1\\}$ and $\\nu$, $\\mathrm{Re}(\\nu) < 0$, and the space $\\mathscr{H}_0$ of entire functions in $\\mathbb{C}$ vanishing at $0$, we consider the family of operators $\\mathfrak{L} = c_0 \\cdot \\delta \\circ \\mathfrak{M}$ with constant $c_0 = \\nu(1-\\nu)x/(1-x)$, $\\delta = z \\, \\mathrm{d}/\\mathrm{d}z$ and integral operator $\\mathfrak{M}$ defined by $$ \\mathfrak{M}f(z) = \\int_0^1 e^{- \\frac{z}{x}t^{-\\nu}(1-(1-x)t)} \\, f \\left ( \\frac{z}{x} \\, t^{-\\nu}(1-t) \\right ) \\, \\frac{\\mathrm{d}t}{t}, \\qquad z \\in \\mathbb{C}, $$ for all $f \\in \\mathscr{H}_0$. Inverting $\\mathfrak{L}$ or $\\mathfrak{M}$ proves equivalent to solve a singular Volterra equation of the first kind. \nThe inversion of operator $\\mathfrak{L}$ on $\\mathscr{H}_0$ leads us to derive a new class of linear inversion formulas $T = A(x,\\nu) \\cdot S \\Leftrightarrow S = B(x,\\nu) \\cdot T$ between sequences $S = (S_n)_{n \\in \\mathbb{N}^*}$ and $T = (T_n)_{n \\in \\mathbb{N}^*}$, where the infinite lower-triangular matrix $A(x,\\nu)$ and its inverse $B(x,\\nu)$ involve Hypergeometric polynomials $F(\\cdot)$, namely $$ \n\\left\\{ \n\\begin{array}{ll} \nA_{n,k}(x,\\nu) = \\displaystyle (-1)^k\\binom{n}{k}F(k-n,-n\\nu;-n;x), \nB_{n,k}(x,\\nu) = \\displaystyle (-1)^k\\binom{n}{k}F(k-n,k\\nu;k;x) \n\\end{array} \\right. $$ for $1 \\leqslant k \\leqslant n$. Functional relations between the ordinary (resp. exponential) generating functions of the related sequences $S$ and $T$ are also given. These relations finally enable us to derive the integral representation $$ \\mathfrak{L}^{-1}f(z) = \\frac{1-x}{2i\\pi x} \\, e^{z} \\int_{(0+)}^1 \\frac{e^{-xtz}}{t(1-t)} \\, f \\left ( xz \\, (-t)^{\\nu}(1-t)^{1-\\nu} \\right ) \\, \\mathrm{d}t, \\quad z \\in \\mathbb{C}, $$ for the inverse $\\mathfrak{L}^{-1}$ of operator $\\mathfrak{L}$ on $\\mathscr{H}_0$, where the integration contour encircles the point 0.", "venue": "ArXiv", "authors": ["R.  Nasri", "A.  Simonian", "F.  Guillemin"], "year": 2019, "n_citations": 1}
{"id": 3887209, "s2_id": "0bbb0ec07036f9d822ff0c5403bb08c85c238de7", "title": "NumaPerf: Predictive and Full NUMA Profiling", "abstract": "Parallel applications are extremely challenging to achieve the optimal performance on the NUMA architecture, which necessitates the assistance of profiling tools. However, existing NUMA-profiling tools share some similar shortcomings, such as portability, effectiveness, and helpfulness issues. This paper proposes a novel profiling tool\u2013NumaPerf\u2013that overcomes these issues. NumaPerf aims to identify potential performance issues for any NUMA architecture, instead of only on the current hardware. To achieve this, NumaPerf focuses on memory sharing patterns between threads, instead of real remote accesses. NumaPerf further detects potential thread migrations and load imbalance issues that could significantly affect the performance but are omitted by existing profilers. NumaPerf also separates cache coherence issues that may require different fix strategies. Based on our extensive evaluation, NumaPerf is able to identify more performance issues than any existing tool, while fixing these bugs leads to up to 5.94\u00d7 performance speedup.", "venue": "ArXiv", "authors": ["Xin  Zhao", "Jin  Zhou", "Hui  Guan", "Wei  Wang", "Xu  Liu", "Tongping  Liu"], "year": 2021, "n_citations": 1}
{"id": 3891301, "s2_id": "121e31b8ec40801e2b297d88a89c2d96f5f39a57", "title": "A Technical Report On Grid Benchmarking using ATLAS V.O", "abstract": "Grids include heterogeneous resources, which are based on different hardware and software architectures or components. In correspondence with this diversity of the infrastructure, the execution time of any single job, as well as the total grid performance can both be affected substantially, which can be demonstrated by measurements. Running a simple benchmarking suite can show this heterogeneity and give us results about the differences over the grid sites.", "venue": "ArXiv", "authors": ["John  Kouvakis", "Fotis  Georgatos"], "year": 2007, "n_citations": 1}
{"id": 3891957, "s2_id": "1feba6dc2bbe0e4b27980ae7ce2cd3df2e3a99bf", "title": "Differential Performance Debugging with Discriminant Regression Trees", "abstract": "Differential performance debugging is a technique to find performance problems. It applies in situations where the performance of a program is (unexpectedly) different for different classes of inputs. The task is to explain the differences in asymptotic performance among various input classes in terms of program internals. We propose a data-driven technique based on discriminant regression tree (DRT) learning problem where the goal is to discriminate among different classes of inputs. We propose a new algorithm for DRT learning that first clusters the data into functional clusters, capturing different asymptotic performance classes, and then invokes off-the-shelf decision tree learning algorithms to explain these clusters. We focus on linear functional clusters and adapt classical clustering algorithms (K-means and spectral) to produce them. For the K-means algorithm, we generalize the notion of the cluster centroid from a point to a linear function. We adapt spectral clustering by defining a novel kernel function to capture the notion of linear similarity between two data points. We evaluate our approach on benchmarks consisting of Java programs where we are interested in debugging performance. We show that our algorithm significantly outperforms other well-known regression tree learning algorithms in terms of running time and accuracy of classification.", "venue": "AAAI", "authors": ["Saeid  Tizpaz-Niari", "Pavol  Cern\u00fd", "Bor-Yuh Evan Chang", "Ashutosh  Trivedi"], "year": 2018, "n_citations": 7}
{"id": 3896489, "s2_id": "65f2247c7faaec634f9ed9c03868c2a282c9f990", "title": "Latency and Throughput Optimization in Modern Networks: A Comprehensive Survey", "abstract": "Modern applications are highly sensitive to communication delays and throughput. This paper surveys major attempts on reducing latency and increasing the throughput. These methods are surveyed on different networks and surroundings such as wired networks, wireless networks, application layer transport control, Remote Direct Memory Access, and machine learning based transport control.", "venue": "ArXiv", "authors": ["Amir  Mirzaeinia", "Mehdi  Mirzaeinia", "Abdelmounaam  Rezgui"], "year": 2020, "n_citations": 1}
{"id": 3899190, "s2_id": "498bd63852b5b81072c5c807b170c30f454676e4", "title": "Toward an End-to-End Auto-tuning Framework in HPC PowerStack", "abstract": "Efficiently utilizing procured power and optimizing performance of scientific applications under power and energy constraints are challenging. The HPC PowerStack defines a software stack to manage power and energy of high-performance computing systems and standardizes the interfaces between different components of the stack. This survey paper presents the findings of a working group focused on the end-to-end tuning of the PowerStack. First, we provide a background on the PowerStack layer-specific tuning efforts in terms of their high-level objectives, the constraints and optimization goals, layer-specific telemetry, and control parameters, and we list the existing software solutions that address those challenges. Second, we propose the PowerStack end-to-end auto-tuning framework, identify the opportunities in co-tuning different layers in the PowerStack, and present specific use cases and solutions. Third, we discuss the research opportunities and challenges for collective auto-tuning of two or more management layers (or domains) in the PowerStack. This paper takes the first steps in identifying and aggregating the important R&D challenges in streamlining the optimization efforts across the layers of the PowerStack.", "venue": "2020 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Xingfu  Wu", "Aniruddha  Marathe", "Siddhartha  Jana", "Ondrej  Vysocky", "Jophin  John", "Andrea  Bartolini", "Lubomir  Riha", "Michael  Gerndt", "Valerie  Taylor", "Sridutt  Bhalachandra"], "year": 2020, "n_citations": 2}
{"id": 3907597, "s2_id": "786008a5dff12d30dc5021521f6915b21839aa8d", "title": "Scheduling Algorithms for Age of Information Differentiation with Random Arrivals", "abstract": "We study age-agnostic scheduling in a non-preemptive status update system with two sources sending time-stamped information packets at random instances to a common monitor through a single server. The server is equipped with a waiting room holding the freshest packet from each source called \"single-buffer per-source queueing\". The server is assumed to be work-conserving and when the waiting room has two waiting packets (one from each source), a probabilistic scheduling policy is applied so as to provide Age of Information (AoI) differentiation for the two sources of interest. Assuming Poisson packet arrivals and exponentially distributed service times, the exact distributions of AoI and also Peak AoI (PAoI) for each source are first obtained. Subsequently, this analytical tool is used to numerically obtain the optimum probabilistic scheduling policy so as to minimize the weighted average AoI/PAoI by means of which differentiation can be achieved between the two sources. In addition, a pair of heuristic age-agnostic schedulers are proposed on the basis of heavy-traffic analysis and comparatively evaluated in a wide variety of scenarios, and guidelines are provided for scheduling and AoI differentiation in status update systems with two sources.", "venue": "ArXiv", "authors": ["Nail  Akar", "Ezhan  Karasan"], "year": 2021, "n_citations": 0}
{"id": 3907997, "s2_id": "fda4caf92cb258d3413c3ff46e1bc9469e71ea74", "title": "Performance Characteristics of the BlueField-2 SmartNIC", "abstract": "High-performance computing (HPC) researchers have long envisioned scenarios where application workflows could be improved through the use of programmable processing elements embedded in the network fabric. Recently, vendors have introduced programmable Smart Network Interface Cards (SmartNICs) that enable computations to be offloaded to the edge of the network. There is great interest in both the HPC and high-performance data analytics (HPDA) communities in understanding the roles these devices may play in the data paths of upcoming systems. This paper focuses on characterizing both the networking and computing aspects of NVIDIA\u2019s new BlueField-2 SmartNIC when used in a 100Gb/s Ethernet environment. For the networking evaluation we conducted multiple transfer experiments between processors located at the host, the SmartNIC, and a remote host. These tests illuminate how much effort is required to saturate the network and help estimate the processing headroom available on the SmartNIC during transfers. For the computing evaluation we used the stress-ng benchmark to compare the BlueField-2 to other servers and place realistic bounds on the types of offload operations that are appropriate for the hardware. Our findings from this work indicate that while the BlueField-2 provides a flexible means of processing data at the network\u2019s edge, great care must be taken to not overwhelm the hardware. While the host can easily saturate the network link, the SmartNIC\u2019s embedded processors may not have enough computing resources to sustain more than half the expected bandwidth when using kernel-space packet processing. From a computational perspective, encryption operations, memory operations under contention, and on-card IPC operations on the SmartNIC perform significantly better than the general-purpose servers used for comparisons in our experiments. Therefore, applications that mainly focus on these operations may be good candidates for offloading to the SmartNIC.", "venue": "ArXiv", "authors": ["Jianshen  Liu", "Carlos  Maltzahn", "Craig  Ulmer", "Matthew Leon Curry"], "year": 2021, "n_citations": 1}
{"id": 3910337, "s2_id": "22edfac8ed7562f5be465c5e24ae0e1aa8fb1d99", "title": "Performance comparison between iSCSI and other hardware and software solutions", "abstract": "We report on our investigations on some technologies that can be used to build disk servers and networks of disk servers using commodity hardware and software solutions. It focuses on the performance that can be achieved by these systems and gives measured figures for different configurations. \nIt is divided into two parts : iSCSI and other technologies and hardware and software RAID solutions. \nThe first part studies different technologies that can be used by clients to access disk servers using a gigabit ethernet network. It covers block access technologies (iSCSI, hyperSCSI, ENBD). Experimental figures are given for different numbers of clients and servers. \nThe second part compares a system based on 3ware hardware RAID controllers, a system using linux software RAID and IDE cards and a system mixing both hardware RAID and software RAID. Performance measurements for reading and writing are given for different RAID levels.", "venue": "ArXiv", "authors": ["Mathias  Gug"], "year": 2003, "n_citations": 0}
{"id": 3912970, "s2_id": "bb26bc84a1e9cebb514675e2bd897d8b0a11d008", "title": "Inferring Catchment in Internet Routing", "abstract": "BGP is the de-facto Internet routing protocol for exchanging prefix reachability information between Autonomous Systems (AS). It is a dynamic, distributed, path-vector protocol that enables rich expressions of network policies (typically treated as secrets). In this regime, where complexity is interwoven with information hiding, answering questions such as \"what is the expected catchment of the anycast sites of a content provider on the AS-level, if new sites are deployed?\", or \"how will load-balancing behave if an ISP changes its routing policy for a prefix?\", is a hard challenge. In this work, we present a formal model and methodology that takes into account policy-based routing and topological properties of the Internet graph, to predict the routing behavior of networks. We design algorithms that provide new capabilities for informative route inference (e.g., isolating the effect of randomness that is present in prior simulation-based approaches). We analyze the properties of these inference algorithms, and evaluate them using publicly available routing datasets and real-world experiments. The proposed framework can be useful in a number of applications: measurements, traffic engineering, network planning, Internet routing models, etc. As a use case, we study the problem of selecting a set of measurement vantage points to maximize route inference. Our methodology is general and can capture standard valley-free routing, as well as more complex topological and routing setups appearing in practice.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Pavlos  Sermpezis", "Vasileios  Kotronis"], "year": 2019, "n_citations": 3}
{"id": 3913702, "s2_id": "c17fb22157b791142d17b7ccfa1d7320e2a0bb62", "title": "Performance and energy consumption of HPC workloads on a cluster based on Arm ThunderX2 CPU", "abstract": "Abstract In this paper, we analyze the performance and energy consumption of an Arm-based high-performance computing (HPC) system developed within the European project Mont-Blanc\u00a03. This system, called Dibona, has been integrated by ATOS/Bull, and it is powered by the latest Marvell\u2019s CPU, ThunderX2. This CPU is the same one that powers the Astra supercomputer, the first Arm-based supercomputer entering the Top500 in November 2018. We study from micro-benchmarks up to large production codes. We include an interdisciplinary evaluation of three scientific applications (a finite-element fluid dynamics code, a smoothed particle hydrodynamics code, and a lattice Boltzmann code) and the Graph 500 benchmark, focusing on parallel and energy efficiency as well as studying their scalability up to thousands of Armv8 cores. For comparison, we run the same tests on state-of-the-art x86 nodes included in Dibona and the Tier-0 supercomputer MareNostrum4. Our experiments show that the ThunderX2 has a 25% lower performance on average, mainly due to its small vector unit yet somewhat compensated by its 30% wider links between the CPU and the main memory. We found that the software ecosystem of the Armv8 architecture is comparable to the one available for Intel. Our results also show that ThunderX2 delivers similar or better energy-to-solution and scalability, proving that Arm-based chips are legitimate contenders in the market of next-generation HPC systems.", "venue": "Future Gener. Comput. Syst.", "authors": ["Filippo  Mantovani", "Marta  Garcia-Gasulla", "Jos'e  Gracia", "Esteban  Stafford", "Fabio  Banchelli", "Marc  Josep-Fabrego", "Joel  Criado-Ledesma", "Mathias  Nachtmann"], "year": 2020, "n_citations": 6}
{"id": 3915161, "s2_id": "6d8de44b3a9662b1f7f5fecd5322a99d529a489e", "title": "Efficient GPU implementation of randomized SVD and its applications", "abstract": "Matrix decompositions are ubiquitous in machine learning, including applications in dimensionality reduction, data compression and deep learning algorithms. Typical solutions for matrix decompositions have polynomial complexity which significantly increases their computational cost and time. In this work, we leverage efficient processing operations that can be run in parallel on modern Graphical Processing Units (GPUs), predominant computing architecture used e.g. in deep learning, to reduce the computational burden of computing matrix decompositions. More specifically, we reformulate the randomized decomposition problem to incorporate fast matrix multiplication operations (BLAS-3) as building blocks. We show that this formulation, combined with fast random number generators, allows to fully exploit the potential of parallel processing implemented in GPUs. Our extensive evaluation confirms the superiority of this approach over the competing methods and we release the results of this research as a part of the official CUDA implementation1. Keyword: Matrix decompositions, randomized SVD, eigenvalues, CUDA, GPU", "venue": "ArXiv", "authors": ["Lukasz  Struski", "Pawel  Morkisz", "Przemyslaw  Spurek", "Samuel Rodriguez Bernabeu", "Tomasz  Trzci'nski"], "year": 2021, "n_citations": 0}
{"id": 3920883, "s2_id": "014d6cfcf63e9d8b0349bc7518b110627489722b", "title": "Artery-C: An OMNeT++ Based Discrete Event Simulation Framework for Cellular V2X", "abstract": "Introduced with LTE Release 14, Cellular V2X enables device-to-device communication to support road safety and traffic efficiency applications. We present Artery-C, a simulation framework for the performance evaluation of Cellular V2X protocols and V2X applications. Our simulator relies on the simulation framework SimuLTE and substantially extends it by implementing control and user planes, a dedicated sidelink interface with specific focus on dynamic mode switching and some advanced features of 5G mobile networks, such as variable numerologies. Artery-C integrates seamlessly into the simulation framework Artery, which enables the simulation of standardized V2X messages at the facilities layer as well as the coupling to the mobility simulator SUMO. In order to demonstrate the capabilities of Artery-C, we evaluate V2X-based platooning as a representative use case and present results for mode 3, mode 4 and mode switching in a highway scenario.", "venue": "MSWiM", "authors": ["Anupama  Hegde", "Andreas  Festag"], "year": 2020, "n_citations": 7}
{"id": 3922677, "s2_id": "604d46b0684ff4540d024cf320271fef72c2a3cf", "title": "Vertical partitioning of relational OLTP databases using integer programming", "abstract": "A way to optimize performance of relational row store databases is to reduce the row widths by vertically partitioning tables into table fractions in order to minimize the number of irrelevant columns/attributes read by each transaction. This paper considers vertical partitioning algorithms for relational row-store OLTP databases with an H-store-like architecture, meaning that we would like to maximize the number of single-sited transactions. We present a model for the vertical partitioning problem that, given a schema together with a vertical partitioning and a workload, estimates the costs (bytes read/written by storage layer access methods and bytes transferred between sites) of evaluating the workload on the given partitioning. The cost model allows for arbitrarily prioritizing load balancing of sites vs. total cost minimization. We show that finding a minimum-cost vertical partitioning in this model is NP-hard and therefore the problem should obviously not be solved manually by a human DBA. We present two algorithms returning solutions in which single-sitedness of read queries is preserved while allowing column replication (which may allow a drastically reduced cost compared to disjoint partitioning). The first algorithm is a quadratic integer program that finds optimal minimum-cost solutions with respect to the model, and the second algorithm is a more scalable heuristic based on simulated annealing. Experiments show that the algorithms can reduce the cost of the model objective by 37% when applied to the TPC-C benchmark and the heuristic is shown to obtain solutions with costs close to the ones found using the quadratic program.", "venue": "2010 IEEE 26th International Conference on Data Engineering Workshops (ICDEW 2010)", "authors": ["Rasmus Resen Amossen"], "year": 2010, "n_citations": 19}
{"id": 3923042, "s2_id": "0bf466bfb4e53a98e7afc038b0ebe9a33404e2c9", "title": "A Security Cost Modelling Framework for Cyber-Physical Systems", "abstract": "Cyber-Physical Systems (CPS) are formed through interconnected components capable of computation, communication, sensing and changing the physical world. The development of these systems poses a significant challenge since they have to be designed in away to ensure cyber-security without impacting their performance. This article presents the Security Cost Modelling Framework (SCMF) and shows supported by an experimental study how it can be used to measure, normalise and aggregate the overall performance of a CPS. Unlike previous studies, our approach uses different metrics to measure the overall performance of a CPS and provides a methodology for normalising the measurement results of different units to a common Cost Unit. Moreover, we show how the Security Costs can be extracted from the overall performance measurements which allows to quantify the overhead imposed by performing security-related tasks. Furthermore, we describe the architecture of our experimental testbed and demonstrate the applicability of SCMF in an experimental study. Our results show that measuring the overall performance and extracting the security costs using SCMF can serve as basis to redesign interactions to achieve the same overall goal at less costs.", "venue": "ArXiv", "authors": ["Igor  Ivkic", "Patrizia  Sailer", "Antonios  Gouglidis", "Andreas  Mauthe", "Markus  Tauber"], "year": 2021, "n_citations": 0}
{"id": 3923473, "s2_id": "ab7f7727498f9d98d7b53d1f1cb65b3f64da38fa", "title": "A Comparison of Random Task Graph Generation Methods for Scheduling Problems", "abstract": "How to generate instances with relevant properties and without bias remains an open problem of critical importance for a fair comparison of heuristics. In the context of scheduling with precedence constraints, the instance consists of a task graph that determines a partial order on task executions. To avoid selecting instances among a set populated mainly with trivial ones, we rely on properties that quantify the characteristics specific to difficult instances. Among numerous identified such properties, the mass measures how much a task graph can be decomposed into smaller ones. This property, together with an in-depth analysis of existing random task graph generation methods, establishes the sub-exponential generic time complexity of the studied problem. Empirical observations on the impact of existing generation methods on scheduling heuristics concludes our study.", "venue": "Euro-Par", "authors": ["Louis-Claude  Canon", "Mohamad El Sayah", "Pierre-Cyrille  H\u00e9am"], "year": 2019, "n_citations": 6}
{"id": 3924242, "s2_id": "788e01dde98e003b73eb45c6e7e8b5746c4f6795", "title": "Propagation and Decay of Injected One-Off Delays on Clusters: A Case Study", "abstract": "Analytic, first-principles performance modeling of distributed-memory applications is difficult due to a wide spectrum of random disturbances caused by the application and the system. These disturbances (commonly called \u201cnoise\u201d) run contrary to the assumptions about regularity that one usually employs when constructing simple analytic models. Despite numerous efforts to quantify, categorize, and reduce such effects, a comprehensive quantitative understanding of their performance impact is not available, especially for long, one-off delays of execution periods that have global consequences for the parallel application. In this work, we investigate various traces collected from synthetic benchmarks that mimic real applications on simulated and real message-passing systems in order to pin-point the mechanisms behind delay propagation. We analyze the dependence of the propagation speed of \u201cidle waves,\u201d i.e., propagating phases of inactivity, emanating from injected delays with respect to the execution and communication properties of the application, study how such delays decay under increased noise levels, and how they interact with each other. We also show how fine-grained noise can make a system immune against the adverse effects of propagating idle waves. Our results contribute to a better understanding of the collective phenomena that manifest themselves in distributed-memory parallel applications.", "venue": "2019 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Ayesha  Afzal", "Georg  Hager", "Gerhard  Wellein"], "year": 2019, "n_citations": 6}
{"id": 3926581, "s2_id": "b11ebe4f00a18e37e8673e55a26b85a55be1ade1", "title": "Balanced fair resource sharing in computer clusters", "abstract": "We represent a computer cluster as a multi-server queue with some arbitrary graph of compatibilities between jobs and servers. Each server processes its jobs sequentially in FCFS order. The service rate of a job at any given time is the sum of the service rates of all servers processing this job. We show that the corresponding queue is quasi-reversible and use this property to design a scheduling algorithm achieving balanced fair sharing of the computing resources.", "venue": "Perform. Evaluation", "authors": ["Thomas  Bonald", "C\u00e9line  Comte"], "year": 2017, "n_citations": 30}
{"id": 3932194, "s2_id": "4219a49271985ebd7f875f3a763cb5429b368636", "title": "ESPBench: The Enterprise Stream Processing Benchmark", "abstract": "Growing data volumes and velocities in fields such as Industry 4.0 or the Internet of Things have led to the increased popularity of data stream processing systems. Enterprises can leverage these developments by enriching their core business data and analyses with up-to-date streaming data. Comparing streaming architectures for these complex use cases is challenging, as existing benchmarks do not cover them. ESPBench is a new enterprise stream processing benchmark that fills this gap. We present its architecture, the benchmarking process, and the query workload. We employ ESPBench on three state-of-the-art stream processing systems, Apache Spark, Apache Flink, and Hazelcast Jet, using provided query implementations developed with Apache Beam. Our results highlight the need for the provided ESPBench toolkit that supports benchmark execution, as it enables query result validation and objective latency measures.", "venue": "ICPE", "authors": ["Guenter  Hesse", "Christoph  Matthies", "Michael  Perscheid", "Matthias  Uflacker", "Hasso  Plattner"], "year": 2021, "n_citations": 1}
{"id": 3935186, "s2_id": "45cb90ed63927982fa7a744429d08ef9ea3a0e6c", "title": "Efficient loading of reduced data ensembles produced at ORNL SNS/HFIR neutron time-of-flight facilities", "abstract": "We present algorithmic improvements to the loading operations of certain reduced data ensembles produced from neutron scattering experiments at Oak Ridge National Laboratory (ORNL) facilities. Ensembles from multiple measurements are required to cover a wide range of the phase space of a sample material of interest. They are stored using the standard NeXus schema on individual HDF5 files. This makes it a scalability challenge, as the number of experiments stored increases in a single ensemble file. The present work follows up on our previous efforts on data management algorithms, to address identified input output (I/O) bottlenecks in Mantid, an open-source data analysis framework used across several neutron science facilities around the world. We reuse an in-memory binary-tree metadata index that resembles data access patterns, to provide a scalable search and extraction mechanism. In addition, several memory operations are refactored and optimized for the current common use cases, ranging most frequently from 10 to 180, and up to 360 separate measurement configurations. Results from this work show consistent speed ups in wall-clock time on the Mantid LoadMD routine, ranging from 19% to 23% on average, on ORNL production computing systems. The latter depends on the complexity of the targeted instrument-specific data and the system I/O and compute variability for the shared computational resources available to users of ORNL\u2019s Spallation Neutron Source (SNS) and the High Flux Isotope Reactor (HFIR) instruments. Nevertheless, we continue to highlight the need for more research to address reduction challenges as experimental data volumes, user time and processing costs increase.", "venue": "ArXiv", "authors": ["William F Godoy", "Andrei T Savici", "Steven E Hahn", "Peter F Peterson"], "year": 2021, "n_citations": 0}
{"id": 3936259, "s2_id": "ceb471b9e94a6bc48281cbdcace6cd3515696a5b", "title": "Extracting clean performance models from tainted programs", "abstract": "Performance models are well-known instruments to understand the scaling behavior of parallel applications. They express how performance changes as key execution parameters, such as the number of processes or the size of the input problem, vary. Besides reasoning about program behavior, such models can also be automatically derived from performance data. This is called empirical performance modeling. While this sounds simple at the first glance, this approach faces several serious interrelated challenges, including expensive performance measurements, inaccuracies inflicted by noisy benchmark data, and overall complex experiment design, starting with the selection of the right parameters. The more parameters one considers, the more experiments are needed and the stronger the impact of noise. In this paper, we show how taint analysis, a technique borrowed from the domain of computer security, can substantially improve the modeling process, lowering its cost, improving model quality, and help validate performance models and experimental setups.", "venue": "PPoPP", "authors": ["Marcin  Copik", "Alexandru  Calotoiu", "Tobias  Grosser", "Nicolas  Wicki", "Felix  Wolf", "Torsten  Hoefler"], "year": 2021, "n_citations": 2}
{"id": 3939339, "s2_id": "c2f5cea2c9cd30be2879fff70e0e0b625605f453", "title": "A Roofline Visualization Framework", "abstract": "The Roofline Model and its derivatives provide an intuitive representation of the best achievable performance on a given architecture. The Roofline Toolkit project is a collaboration among researchers at Argonne National Laboratory, Lawrence Berkeley National Laboratory, and the University of Oregon and consists of three main parts: hardware characterization, software characterization, and data manipulation and visualization interface. These components address the different aspects of performance data acquisition and manipulation required for performance analysis, modeling and optimization of codes on existing and emerging architectures. In this paper we introduce an initial implementation of the third component, a system for visualizing roofline charts and managing roofline performance analysis data. We discuss the implementation and rationale for the integration of the roofline visualization system into the Eclipse IDE. An overview of our continuing efforts and goals in the development of this project is provided.", "venue": "ArXiv", "authors": ["Wyatt  Spear", "Boyana  Norris"], "year": 2015, "n_citations": 1}
{"id": 3944785, "s2_id": "2dfac640bd453a95a1ec46255abc6bcfabbfe160", "title": "Tools for modelling and simulating the Smart Grid", "abstract": "The Smart Grid (SG) is a Cyber-Physical System (CPS) considered a critical infrastructure divided into cyber (software) and physical (hardware) counterparts that complement each other. It is responsible for timely power provision wrapped by Information and Communication Technologies (ICT) for handling bi-directional energy flows in electric power grids. Enacting control and performance over the massive infrastructure of the SG requires convenient analysis methods. Modelling and simulation (M&S) is a performance evaluation technique used to study virtually any system by testing designs and artificially creating 'what-if' scenarios for system reasoning and advanced analysis. M&S preserves stressing the actual physical infrastructure and systems in production by addressing the problem in a computational perspective. Present work compiles a non-exhaustive list of tools for M&S of interest when tackling SG capabilities. Our contribution is to delineate available options for modellers when modelling power systems in combination with ICT. We also show the auxiliary tools and details of most relevant solutions pointing out major features and combinations over the years.", "venue": "ArXiv", "authors": ["Ricardo M. Czekster"], "year": 2020, "n_citations": 1}
{"id": 3950610, "s2_id": "d196c5ad3299db5bbec0c335b10fd9582f106718", "title": "Optimising AI Training Deployments using Graph Compilers and Containers", "abstract": "Artificial Intelligence (AI) applications based on Deep Neural Networks (DNN) or Deep Learning (DL) have become popular due to their success in solving problems like image analysis and speech recognition. Training a DNN is computationally intensive and High Performance Computing (HPC) has been a key driver in AI growth. Virtualisation and container technology have led to the convergence of cloud and HPC infrastructure. These infrastructures with diverse hardware increase the complexity of deploying and optimising AI training workloads. AI training deployments in HPC or cloud can be optimised with target-specific libraries, graph compilers, and by improving data movement or IO. Graph compilers aim to optimise the execution of a DNN graph by generating an optimised code for a target hardware/backend. As part of SODALITE (a Horizon 2020 project), MODAK tool is developed to optimise application deployment in software defined infrastructures. Using input from the data scientist and performance modelling, MODAK maps optimal application parameters to a target infrastructure and builds an optimised container. In this paper, we introduce MODAK and review container technologies and graph compilers for AI. We illustrate optimisation of AI training deployments using graph compilers and Singularity containers. Evaluation using MNIST-CNN and ResNet50 training workloads shows that custom built optimised containers outperform the official images from DockerHub. We also found that the performance of graph compilers depends on the target hardware and the complexity of the neural network.", "venue": "2020 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Nina  Mujkanovic", "Karthee  Sivalingam", "Alfio  Lazzaro"], "year": 2020, "n_citations": 0}
{"id": 3951560, "s2_id": "8318178460c79f6a55f3b182cd663186185cddd9", "title": "Proceedings of the 3rd OMNeT++ Community Summit, Brno University of Technology - Czech Republic, September 15-16, 2016", "abstract": "These are the Proceedings of the 3rd OMNeT++ Community Summit, which was held at the University of Technology in Brno - Czech Republic - on September 15-16, 2016.", "venue": "ArXiv", "authors": ["Anna  F\u00f6rster", "Vladim\u00edr  Vesel\u00fd", "Antonio  Virdis", "Michael  Kirsche"], "year": 2016, "n_citations": 0}
{"id": 3953792, "s2_id": "a1792fad30f39bf9be8672cd33f61361db19e374", "title": "Accelerating Sparse Approximate Matrix Multiplication on GPUs", "abstract": "Although the matrix multiplication plays a vital role in computational linear algebra, there are few efficient solutions for matrix multiplication of the near-sparse matrices. The Sparse Approximate Matrix Multiply (SpAMM) is one of the algorithms to fill the performance gap neglected by traditional optimizations for dense/sparse matrix multiplication. However, existing SpAMM algorithms fail to exploit the performance potential of GPUs for acceleration. In this paper, we present cuSpAMM, the first parallel SpAMM algorithm optimized for multiple GPUs. Several performance optimizations have been proposed, including algorithm re-design to adapt to the thread parallelism, blocking strategies for memory access optimization, and the acceleration with the tensor core. In addition, we scale cuSpAMM to run on multiple GPUs with an effective load balance scheme. We evaluate cuSpAMM on both synthesized and real-world datasets on multiple GPUs. The experiment results show that cuSpAMM achieves significant performance speedup compared to vendor optimized cuBLAS and cuSPARSE libraries.", "venue": "ArXiv", "authors": ["Xiaoyan  Liu", "Yi  Liu", "Ming  Dun", "Bohong  Yin", "Hailong  Yang", "Zhongzhi  Luan", "Depei  Qian"], "year": 2021, "n_citations": 0}
{"id": 3957035, "s2_id": "6ee2159987bdcd1bb4ec653c46d32dc055986d86", "title": "A Robust Queueing Network Analyzer Based on Indices of Dispersion", "abstract": "We develop a robust queueing network analyzer algorithm to approximate the steady-state performance of a single-class open queueing network of single-server queues with Markovian routing. The algorithm allows non-renewal external arrival processes, general service-time distributions and customer feedback. We focus on the customer flows, defined as the continuous-time processes counting customers flowing into or out of the network, or flowing from one queue to another. Each flow is partially characterized by its rate and a continuous function that measures the stochastic variability over time. This function is a scaled version of the variance-time curve, called the index of dispersion for counts (IDC). The required IDC functions for the flows can be calculated from the model primitives, estimated from data or approximated by solving a set of linear equations. A robust queueing technique is used to generate approximations of the mean steady-state performance at each queue from the IDC of the total arrival flow and the service specification at that queue. The algorithm effectiveness is supported by extensive simulation studies and heavy-traffic limits.", "venue": "ArXiv", "authors": ["Ward  Whitt", "Wei  You"], "year": 2020, "n_citations": 6}
{"id": 3960271, "s2_id": "60f64d1bca39480536a7379dffa0d5f42003fa65", "title": "GPA: A GPU Performance Advisor Based on Instruction Sampling", "abstract": "Developing efficient GPU kernels can be difficult because of the complexity of GPU architectures and programming models. Existing performance tools only provide coarse-grained tuning advice at the kernel level, if any. In this paper, we describe GPA, a performance advisor for NVIDIA GPUs that suggests potential code optimizations at a hierarchy of levels, including individual lines, loops, and functions. To relieve users of the burden of interpreting performance counters and analyzing bottlenecks, GPA uses data flow analysis to approximately attribute measured instruction stalls to their root causes and uses information about a program's structure and the GPU to match inefficiency patterns with optimization strategies. To quantify the potential benefits of each optimization strategy, we developed PC sampling-based performance models to estimate its speedup. Our experiments with benchmarks and applications show that GPA provides insightful reports to guide performance optimization. Using GPA, we obtained speedups on a Volta V100 GPU ranging from 1.01 x to 3.58 \u00d7, with a geometric mean of 1.22 x.", "venue": "2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)", "authors": ["Keren  Zhou", "Xiaozhu  Meng", "Ryuichi  Sai", "John  Mellor-Crummey"], "year": 2021, "n_citations": 0}
{"id": 3961191, "s2_id": "d1fb58c07788ab8e2db198d357c5f708a94baadf", "title": "Disaggregated and optically interconnected memory: when will it be cost effective?", "abstract": "The \"Disaggregated Server\" concept has been proposed for datacenters where the same type server resources are aggregated in their respective pools, for example a compute pool, memory pool, network pool, and a storage pool. Each server is constructed dynamically by allocating the right amount of resources from these pools according to the workload's requirements. Modularity, higher packaging and cooling efficiencies, and higher resource utilization are among the suggested benefits. With the emergence of very large datacenters, \"clouds\" containing tens of thousands of servers, datacenter efficiency has become an important topic. Few computer chip and systems vendors are working on and making frequent announcements on silicon photonics and disaggregated memory systems. \nIn this paper we study the trade-off between cost and performance of building a disaggregated memory system where DRAM modules in the datacenter are pooled, for example in memory-only chassis and racks. The compute pool and the memory pool are interconnected by an optical interconnect to overcome the distance and bandwidth issues of electrical fabrics. We construct a simple cost model that includes the cost of latency, cost of bandwidth and the savings expected from a disaggregated memory system. We then identify the level at which a disaggregated memory system becomes cost competitive with a traditional direct attached memory system. \nOur analysis shows that a rack-scale disaggregated memory system will have a non-trivial performance penalty, and at the datacenter scale the penalty is impractically high, and the optical interconnect costs are at least a factor of 10 more expensive than where they should be when compared to the traditional direct attached memory systems.", "venue": "ArXiv", "authors": ["B\u00fclent  Abali", "Richard J. Eickemeyer", "Hubertus  Franke", "Chung-Sheng  Li", "Marc  Taubenblatt"], "year": 2015, "n_citations": 29}
{"id": 3961985, "s2_id": "1e70307bfcbbff55a1e8681c19917fdd9b840bc2", "title": "Capacity of Large-Scale CSMA Wireless Networks", "abstract": "In the literature, asymptotic studies of multihop wireless network capacity often consider only centralized and deterministic time-division multiple-access (TDMA) coordination schemes. There have been fewer studies of the asymptotic capacity of large-scale wireless networks based on carrier-sensing multiple access (CSMA), which schedules transmissions in a distributed and random manner. With the rapid and widespread adoption of CSMA technology, a critical question is whether CSMA networks can be as scalable as TDMA networks. To answer this question and explore the capacity of CSMA networks, we first formulate the models of CSMA protocols to take into account the unique CSMA characteristics not captured by existing interference models in the literature. These CSMA models determine the feasible states, and consequently the capacity of CSMA networks. We then study the throughput efficiency of CSMA scheduling as compared to TDMA. Finally, we tune the CSMA parameters so as to maximize the throughput to the optimal order. As a result, we show that CSMA can achieve throughput as \u03a9([1/\u221a(n)]), the same order as optimal centralized TDMA, on uniform random networks. Our CSMA scheme makes use of an efficient backbone-peripheral routing scheme and a careful design of dual carrier-sensing and dual channel scheme. We also address implementation issues of our CSMA scheme.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Chi-Kin  Chau", "Minghua  Chen", "Soung Chang Liew"], "year": 2011, "n_citations": 60}
{"id": 3963361, "s2_id": "893b61aea47810cc697e11833e688b71cde6e545", "title": "Link adaptation for wireless video communication systems", "abstract": "Modern wireless communication standards are mainly designed to operate as data-centric networks. As a result, their link adaptation is designed to maximize the error-free data throughput. However, when transmitting video, error-free data throughput is not the only pivotal metric. Issues such as latency and jitter must also be considered. Moreover, the limited power of mobile devices is also a challenge in wireless video communication. The aim of this project is, therefore, to develop link adaptation solutions for wireless video communication systems with the objectives of minimized power consumption, high video quality and continuous playback.", "venue": "2013 IEEE 20th International Conference on Electronics, Circuits, and Systems (ICECS)", "authors": ["Husameldin  Mukhtar"], "year": 2013, "n_citations": 1}
{"id": 3964243, "s2_id": "2ebe6aeb8f3b66ffbf2925efc41ee7738f78cdd3", "title": "On the benchmarking of partitioned real-time systems", "abstract": "Avionic software is the subject of critical real time, determinism and safety constraints. Software designers face several challenges, one of them being the estimation of worst-case execution time (WCET) of applications, that dictates the execution time of the system. A pessimistic WCET estimation can lead to low execution performances of the system, while an over-optimistic estimation can lead to deadline misses, breaking one the basic constraints of critical real-time systems (RTS). Partitioned systems are one special category of real time systems, employed by the avionic community to deploy avionic software. The ARINC-653 standard is one common avionic standard that employs the concept of partitions. This standard defines partitioned architectures where one partition should never directly interfere with another one. Assessing WCET of general purpose RTSs is achievable by the usage of one of the many published benchmark or WCET estimation frameworks. Contrarily, partitioned RTSs are special cases, in which common benchmark tools may not capture all the metrics. In this document, we present SFPBench, a generic benchmark framework for the assessment of performance metrics on partitioned RTSs. The general organization of the framework and its applications are illustrated, as well as an use-case, employing SFPBench on an industrial partitioned operating system (OS) executing on a Commercial Off-The-shelf (COTS) processor.", "venue": "ArXiv", "authors": ["Felipe Gohring de Magalhaes", "Alexy Torres Aurora Dugo", "Jean-Baptiste  Lefoul", "Gabriela  Nicolescu"], "year": 2020, "n_citations": 0}
{"id": 3965729, "s2_id": "bc81827f8ee6e7f600453ca7640f0850c8d5afdb", "title": "Resource Allocation in One-dimensional Distributed Service Networks with Applications", "abstract": "Abstract We consider assignment policies that allocate resources to users, where both resources and users are located on a one-dimensional line [ 0 , \u221e ) . First, we consider unidirectional assignment policies that allocate resources only to users located to their left. We propose the Move to Right (MTR) policy, which scans from left to right assigning nearest rightmost available resource to a user, and contrast it to the Unidirectional Gale\u2013Shapley (UGS) matching policy. While both policies among all unidirectional policies, minimize the expected distance traveled by a request (request distance), MTR is fairer. Moreover, we show that when user and resource locations are modeled by statistical point processes, and resources are allowed to satisfy more than one user, the spatial system under unidirectional policies can be mapped into bulk service queueing systems, thus allowing the application of many queueing theory results that yield closed form expressions. As we consider a case where different resources can satisfy different numbers of users, we also generate new results for bulk service queues. We also consider bidirectional policies where there are no directional restrictions on resource allocation and develop an algorithm for computing the optimal assignment which is more efficient than known algorithms in the literature when there are more resources than users. Numerical evaluation of performance of unidirectional and bidirectional allocation schemes yields design guidelines beneficial for resource placement. Finally, we present a heuristic algorithm, which applies the optimal dynamic programming scheme for one-dimensional inputs to obtain approximate solutions to the optimal assignment problem for the two-dimensional scenario and empirically yields request distances within a constant factor of the optimal solution.", "venue": "Perform. Evaluation", "authors": ["Nitish  Panigrahy", "Prithwish  Basu", "Philippe  Nain", "Donald F. Towsley", "Ananthram  Swami", "Kevin S. Chan", "Kin K. Leung"], "year": 2020, "n_citations": 2}
{"id": 3968335, "s2_id": "be88a24e9b6e26f10d4d84183c8f05ba33eb60c0", "title": "A Stitch in Time Saves Nine - SPARQL querying of Property Graphs using Gremlin Traversals", "abstract": "Knowledge graphs have become popular over the past years and frequently rely on the Resource Description Framework (RDF) or Property Graphs (PG) as underlying data models. However, the query languages for these two data models -- SPARQL for RDF and Gremlin for property graph traversal -- are lacking interoperability. We present Gremlinator, a novel SPARQL to Gremlin translator. Gremlinator translates SPARQL queries to Gremlin traversals for executing graph pattern matching queries over graph databases. This allows to access and query a wide variety of Graph Data Management Systems (DMS) using the W3C standardized SPARQL query language and avoid the learning curve of a new Graph Query Language. Gremlin is a system-agnostic traversal language covering both OLTP graph database or OLAP graph processors, thus making it a desirable choice for supporting interoperability wrt. querying Graph DMSs. We present a comprehensive empirical evaluation of Gremlinator and demonstrate its validity and applicability by executing SPARQL queries on top of the leading graph stores Neo4J, Sparksee, and Apache TinkerGraph and compare the performance with the RDF stores Virtuoso, 4Store and JenaTDB. Our evaluation demonstrates the substantial performance gain obtained by the Gremlin counterparts of the SPARQL queries, especially for star-shaped and complex queries.", "venue": "ArXiv", "authors": ["Harsh  Thakkar", "Dharmen  Punjani", "Yashwant  Keswani", "Jens  Lehmann", "S\u00f6ren  Auer"], "year": 2018, "n_citations": 18}
{"id": 3975946, "s2_id": "69d5e2b2ad46101b74ffff65afa4e722ef92e25c", "title": "FPGA implementation of the procedures for video quality assessment", "abstract": "Video resolutions used in variety of media are constantly rising. While manufacturers struggle to perfect their screens it is also important to ensure high quality of displayed image. Overall quality can be measured using Mean Opinion Score (MOS). Video quality can be affected by miscellaneous artifacts, appearing at every stage of video creation and transmission. In this paper, we present a solution to calculate four distinct video quality metrics that can be applied to a real time video quality assessment system. Our assessment module is capable of processing 8K resolution in real time set at the level of 30 frames per second. Throughput of 2.19 GB/s surpasses performance of pure software solutions. To concentrate on architectural optimization, the module was created using high level language.", "venue": "Comput. Sci.", "authors": ["Maciej  Wielgosz", "Michal  Karwatowski", "Marcin  Pietron", "Kazimierz  Wiatr"], "year": 2018, "n_citations": 0}
{"id": 3977667, "s2_id": "cbab055052c6098b21c210e945ca17df143273da", "title": "JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services", "abstract": "Deep learning models are being deployed in many mobile intelligent applications. End-side services, such as intelligent personal assistants, autonomous cars, and smart home services often employ either simple local models on the mobile or complex remote models on the cloud. However, recent studies have shown that partitioning the DNN computations between the mobile and cloud can increase the latency and energy efficiencies. In this paper, we propose an efficient, adaptive, and practical engine, JointDNN, for collaborative computation between a mobile device and cloud for DNNs in both inference and training phase. JointDNN not only provides an energy and performance efficient method of querying DNNs for the mobile side but also benefits the cloud server by reducing the amount of its workload and communications compared to the cloud-only approach. Given the DNN architecture, we investigate the efficiency of processing some layers on the mobile device and some layers on the cloud server. We provide optimization formulations at layer granularity for forward- and backward-propagations in DNNs, which can adapt to mobile battery limitations and cloud server load constraints and quality of service. JointDNN achieves up to 18 and 32 times reductions on the latency and mobile energy consumption of querying DNNs compared to the status-quo approaches, respectively.", "venue": "IEEE Transactions on Mobile Computing", "authors": ["Amir Erfan Eshratifar", "Mohammad Saeed Abrishami", "Massoud  Pedram"], "year": 2021, "n_citations": 80}
{"id": 3978173, "s2_id": "deec2ff6d5704706ec8a7041eb1e0c74fd83c405", "title": "The generalized join the shortest orbit queue system: Stability, exact tail asymptotics and stationary approximations", "abstract": "We introduce the generalized join the shortest queue model with retrials and two infinite capacity orbit queues. Three independent Poisson streams of jobs, namely a smart, and two dedicated streams, flow into a single server system, which can hold at most one job. Arriving jobs that find the server occupied are routed to the orbits as follows: Blocked jobs from the smart stream are routed to the shortest orbit queue, and in case of a tie, they choose an orbit randomly. Blocked jobs from the dedicated streams are routed directly to their orbits. Orbiting jobs retry to connect with the server at different retrial rates, i.e., heterogeneous orbit queues. Applications of such a system are found in the modelling of wireless cooperative networks. We are interested in the asymptotic behaviour of the stationary distribution of this model, provided that the system is stable. More precisely, we investigate the conditions under which the tail asymptotic of the minimum orbit queue length is exactly geometric. Moreover, we apply a heuristic asymptotic approach to obtain approximations of the steady-state joint orbit queue-length distribution. Useful numerical examples are presented and shown that the results obtained through the asymptotic analysis and the heuristic approach agreed.", "venue": "ArXiv", "authors": ["Ioannis  Dimitriou"], "year": 2021, "n_citations": 0}
{"id": 3978327, "s2_id": "cabbd4f953474b5c6d476456725ec8d2936ed4df", "title": "Business Process Measures", "abstract": "The paper proposes a new methodology for defining business process measures and their computation. The approach is based on metamodeling according to MOF. Especially, a metamodel providing precise definitions of typical process measures for UML activity diagram-like notation is proposed, including precise definitions how measures should be aggregated for composite process elements. The proposed approach allows defining values in a natural way, and measurement of data, which are of interest to business, without deep investigation into specific technical solutions. This provides new possibilities for business process measurement, decreasing the gap between technical solutions and asset management methodologies.", "venue": "ArXiv", "authors": ["Valdis  Vitolins"], "year": 2004, "n_citations": 21}
{"id": 3978345, "s2_id": "ef640a5f2f31235cb161aa5b8c69b9d36df750c3", "title": "SERENADE: A Parallel Randomized Algorithm Suite for Crossbar Scheduling in Input-Queued Switches", "abstract": "Most of today's high-speed switches and routers adopt an input-queued crossbar switch architecture. Such a switch needs to compute a matching (crossbar schedule) between the input ports and output ports during each switching cycle (time slot). A key research challenge in designing large (in number of input/output ports $N$) input-queued crossbar switches is to develop crossbar scheduling algorithms that can compute \"high quality\" matchings - i.e. those that result in high switch throughput (ideally $100\\%$) and low queueing delays for packets - at line rates. SERENA is arguably the best algorithm in that regard: It outputs excellent matching decisions that result in $100\\%$ switch throughput and near-optimal queueing delays. However, since SERENA is a centralized algorithm with $O(N)$ computational complexity, it cannot support switches that both are large (in terms of $N$) and have a very high line rate per port. In this work, we propose SERENADE (SERENA, the Distributed Edition), a parallel algorithm suite that emulates SERENA in only $O(\\log N)$ %(distributed) iterations between input ports and output ports, and hence has a time complexity of only $O(\\log N)$ per port. Through extensive simulations, we show that all three variants in the SERENADE suite can, either provably or empirically, achieve 100\\% throughput, and that they have similar delay performances as SERENA under heavy traffic loads.", "venue": "ArXiv", "authors": ["Long  Gong", "Liang  Liu", "Sen  Yang", "Jun  Xu", "Yi  Xie", "Xinbing  Wang"], "year": 2017, "n_citations": 2}
{"id": 3979610, "s2_id": "368918d8a5d93e0aa6de3456cdd1a7d8cee2afc3", "title": "DJXPerf: Identifying Memory Inefficiencies via Object-centric Profiling for Java", "abstract": "Java is the \u201cgo-to\u201d programming language choice for developing scalable enterprise cloud applications. In such systems, even a few percent CPU time savings can offer a significant competitive advantage and cost saving. Although performance tools abound in Java, those that focus on the data locality in the memory hierarchy are rare. In this paper, we present DJXPerf, a lightweight, object-centric memory profiler for Java, which associates memory-hierarchy performancemetrics (e.g., cache/TLBmisses) with Java objects.DJXPerf uses statistical sampling of hardware performancemonitoring counters to attribute metrics to not only source code locations but also Java objects. DJXPerf presents Java object allocation contexts combined with their usage contexts and presents them ordered by the poor locality behaviors. DJXPerf\u2019s performance measurement, object attribution, and presentation techniques guide optimizing object allocation, layout, and access patterns. DJXPerf incurs only \u223c8% runtime overhead and \u223c5% memory overhead on average, requiring no modifications to hardware, OS, Java virtual machine, or application source code, which makes it attractive to use in production. Guided by DJXPerf, we study and optimize a number of Java and Scala programs, including well-known benchmarks and real-world applications, and demonstrate significant speedups. CCS Concepts \u2022 Software and its engineering \u2192 Compilers; General programming languages.", "venue": "ArXiv", "authors": ["Bolun  Li", "Pengfei  Su", "Milind  Chabbi", "Shuyin  Jiao", "Xu  Liu"], "year": 2021, "n_citations": 0}
{"id": 3979711, "s2_id": "53b94ec65b96c75a94412f9d9f86ecfdec16007e", "title": "Latency Analysis of an Aerial Video Tracking System Using Fiacre and Tina", "abstract": "We describe our experience with modeling a video tracking system used to detect and follow moving targets from an airplane. We provide a formal model that takes into account the real-time properties of the system and use it to compute the worst and best-case end to end latency. We also compute a lower bound on the delay between the loss of two frames. Our approach is based on the model-checking tool Tina, that provides state-space generation and model-checking algorithms for an extension of Time Petri Nets with data and priorities. We propose several models divided in two main categories: first Time Petri Net models, which are used to study the behavior of the system in the most basic way; then models based on the Fiacre specification language, where we take benefit of richer data structures to directly model the buffering of video information and the use of an unbounded number of frame identifiers.", "venue": "ArXiv", "authors": ["Silvano  Dal-Zilio", "Bernard  Berthomieu", "Didier Le Botlan"], "year": 2015, "n_citations": 3}
{"id": 3979951, "s2_id": "65a59943ba3b128e20297b13fc46afd8191b8fa5", "title": "Infinite Unlimited Churn", "abstract": "We study unlimited infinite churn in peer-to-peer overlay networks. Under this churn, arbitrary many peers may concurrently request to join or leave the overlay network; moreover these requests may never stop coming. We prove that unlimited adversarial churn, where processes may just exit the overlay network, is unsolvable. We focus on cooperative churn where exiting processes participate in the churn handling algorithm. We define the problem of unlimited infinite churn in this setting. We distinguish the fair version of the problem, where each request is eventually satisfied, from the unfair version that just guarantees progress. We focus on local solutions to the problem, and prove that a local solution to the Fair Infinite Unlimited Churn is impossible. We then present and prove correct an algorithm UIUC that solves the Unfair Infinite Unlimited Churn Problem for a linearized peer-to-peer overlay network. We extend this solution to skip lists and skip graphs.", "venue": "ArXiv", "authors": ["Dianne  Foreback", "Mikhail  Nesterenko", "S\u00e9bastien  Tixeuil"], "year": 2016, "n_citations": 3}
{"id": 3980069, "s2_id": "165736ed3c7c78b56705ff31b6b3414dcfdd356c", "title": "SonicChain: A Wait-free, Pseudo-Static Approach Toward Concurrency in Blockchains", "abstract": "Blockchains have a two-sided reputation: they are praised for disrupting some of our institutions through innovative technology for good, yet notorious for being slow and expensive to use. In this work, we tackle this issue with concurrency, yet we aim to take a radically different approach by valuing simplicity. We embrace the simplicity through two steps: first, we formulate a simple runtime mechanism to deal with conflicts called concurrency delegation. This method is much simpler and has less overhead, particularly in scenarios where conflicting transactions are relatively rare. Moreover, to further reduce the number of conflicting transactions, we propose using static annotations attached to each transaction, provided by the programmer. These annotations are pseudo-static: they are static with respect to the lifetime of the transaction, and therefore are free to use information such as the origin and parameters of the transaction. We propose a distributor component that can use the output of this pseudostatic annotations and use them to effectively distribute transactions between threads in the least-conflicting way. We name the outcome of a system combining concurrency delegation and pseudo-static annotations as SonicChain. We evaluate SonicChain for both validation and authoring tasks against a common workload in blockchains, namely, balance transfers, and observe that it performs expectedly well while introducing very little overhead and additional complexity to the system.", "venue": "ArXiv", "authors": ["Kian  Paimani"], "year": 2021, "n_citations": 0}
{"id": 3980442, "s2_id": "7f337085e51d3f2f617d1b8dee14c87d86ecee36", "title": "How to Emulate Web Traffic Using Standard Load Testing Tools", "abstract": "Conventional load-testing tools are based on a fifty-year old time-share computer paradigm where a finite number of users submit requests and respond in a synchronized fashion. Conversely, modern web traffic is essentially asynchronous and driven by an unknown number of users. This difference presents a conundrum for testing the performance of modern web applications. Even when the difference is recognized, performance engineers often introduce modifications to their test scripts based on folklore or hearsay published in various Internet fora, much of which can lead to wrong results. We present a coherent methodology, based on two fundamental principles, for emulating web traffic using a standard load-test environment.", "venue": "ArXiv", "authors": ["James F. Brady", "Neil J. Gunther"], "year": 2016, "n_citations": 3}
{"id": 3983969, "s2_id": "ab69f49fedb6936ce04b2e9d1f161772b2f24b7d", "title": "Architecture-aware optimization of an HEVC decoder on asymmetric multicore processors", "abstract": "Abstract Low-power asymmetric multicore processors (AMPs) have attracted considerable attention due to their appealing performance/power ratio for energy-constrained environments. However, these processors pose a significant programming challenge due to the integration of cores with different performance capabilities, asking for an asymmetry-aware scheduling solution that carefully distributes the workload. The recent HEVC standard, which offers several high-level parallelization strategies, is an important application that can benefit from an implementation tailored for the low-power AMPs present in many current mobile or handheld devices. In this scenario, we present an architecture-aware implementation of an HEVC decoder that embeds a criticality-aware scheduling strategy tuned for a Samsung Exynos 5422 System-on-Chip furnished with an ARM big.LITTLE AMP. The performance and energy efficiency of our solution are further enhanced by exploiting the NEON vector engine available in the ARM big.LITTLE architecture. Our experimental results expose a 1080p real-time HEVC decoding at 24\u00a0frames/s and a reduction of energy consumption over 20\u00a0%.", "venue": "Journal of Real-Time Image Processing", "authors": ["Rafael  Rodr\u00edguez-S\u00e1nchez", "Enrique S. Quintana-Ort\u00ed"], "year": 2016, "n_citations": 5}
{"id": 3984782, "s2_id": "d04cec47704442162b6dd0f61f584367de4ed015", "title": "A Multiobjective Optimization Framework for Routing in Wireless Ad Hoc Networks", "abstract": "Wireless ad hoc networks are seldom characterized by one single performance metric, yet the current literature lacks a flexible framework to assist in characterizing the design tradeoffs in such networks. The aim of this paper is not to propose another routing strategy. Instead, we address this problem by proposing a new modeling framework for routing in ad hoc networks, which will result in a better understanding of network behavior and performance when multiple criteria are relevant. Our approach is to take a holistic view of the network that captures the cross-interactions among interference management techniques implemented at various layers of the protocol stack. The resulting framework is a complex multiobjective optimization problem that can be solved through existing multiobjective search techniques. In this contribution, we present the Pareto optimal sets for an example sensor network when delay, robustness and energy are considered.", "venue": "ArXiv", "authors": ["Katia  Jaffr\u00e8s-Runser", "Cristina  Comaniciu", "Jean-Marie  Gorce"], "year": 2009, "n_citations": 9}
{"id": 3985627, "s2_id": "c0667567589971013266df80d0ca07d218414614", "title": "Simplex Queues for Hot-Data Download", "abstract": "In distributed systems, reliable data storage is accomplished through redundancy, which has traditionally been achieved by simple replication of data across multiple nodes [6]. A special class of erasure codes, known as locally repairable codes (LRCs) [7], has started to replace replication in practice [8], as a more storage-efficient way to provide a desired reliability. It has recently been recognized, that storage redundancy can also provide fast access of stored data (see e.g. [5,9,10] and references therein). Most of these papers consider download scenarios of all jointly encoded pieces of data, and very few [11,12,14] are concerned with download of only some, possibly hot, pieces of data that are jointly encoded with those of less interest. So far, only low traffic regime has been partially addressed. In this paper, we are concerned with hot data download from systems implementing a special class of locally repairable codes, known as LRCs with availability [13,15]. We consider simplex codes, a particular subclass of LRCs with availability, because 1) they are in a certain sense optimal [2] and 2) they are minimally different from replication.", "venue": "SIGMETRICS 2017", "authors": ["Mehmet Fatih Aktas", "Elie  Najm", "Emina  Soljanin"], "year": 2017, "n_citations": 13}
{"id": 3990790, "s2_id": "22fa3add047b80b8966aa650a321eb94ccbed16d", "title": "An End-to-End Probabilistic Network Calculus with Moment Generating Functions", "abstract": "Network calculus is a min-plus system theory for performance evaluation of queuing networks. Its elegance steins from intuitive convolution formulas for concatenation of deterministic servers. Recent research dispenses with the worst-case assumptions of network calculus to develop a probabilistic equivalent that benefits from statistical multiplexing. Significant achievements have been made, owing for example to the theory of effective bandwidths; however, the outstanding scalability set up by concatenation of deterministic servers has not been shown. This paper establishes a concise, probabilistic network calculus with moment generating functions. The presented work features closed-form, end-to-end, probabilistic performance bounds that achieve the objective of scaling linearly in the number of servers in series. The consistent application of moment generating functions put forth in this paper utilizes independence beyond the scope of current statistical multiplexing of flows. A relevant additional gain is demonstrated for tandem servers with independent cross-traffic", "venue": "200614th IEEE International Workshop on Quality of Service", "authors": ["Markus  Fidler"], "year": 2006, "n_citations": 198}
{"id": 3992476, "s2_id": "07c213c37d0d39189045d85b3781e81fed9fbe1d", "title": "Evaluating load balancing policies for performance and energy-efficiency", "abstract": "Nowadays, more and more increasingly hard computations are performed in challenging fields like weather forecasting, oil and gas exploration, and cryptanalysis. Many of such computations can be implemented using a computer cluster with a large number of servers. Incoming computation requests are then, via a so-called load balancing policy, distributed over the servers to ensure optimal performance. Additionally, being able to switch-off some servers during low period of workload, gives potential to reduced energy consumption. Therefore, load balancing forms, albeit indirectly, a trade-off between performance and energy consumption. In this paper, we introduce a syntax for load-balancing policies to dynamically select a server for each request based on relevant criteria, including the number of jobs queued in servers, power states of servers, and transition delays between power states of servers. To evaluate many policies, we implement two load balancers in: (i) iDSL, a language and tool-chain for evaluating service-oriented systems, and (ii) a simulation framework in AnyLogic. Both implementations are successfully validated by comparison of the results.", "venue": "QAPL", "authors": ["Freek van den Berg", "Bj\u00f6rn F. Postema", "Boudewijn R. Haverkort"], "year": 2016, "n_citations": 2}
{"id": 3994414, "s2_id": "4a430ac890ff5c20d875d0eb8e0a397fd3a47659", "title": "DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression Framework", "abstract": "As one of most fascinating machine learning techniques, deep neural network (DNN) has demonstrated excellent performance in various intelligent tasks such as image classification. DNN achieves such performance, to a large extent, by performing expensive training over huge volumes of training data. To reduce the data storage and transfer overhead in smart resource-limited Internet-of-Thing (IoT) systems, effective data compression is a \u201cmust-have\u201d feature before transferring real-time produced dataset for training or classification. While there have been many well-known image compression approaches (such as JPEG), we for the first time find that a human-visual based image compression approach such as JPEG compression is not an optimized solution for DNN systems, especially with high compression ratios. To this end, we develop an image compression framework tailored for DNN applications, named \u201cDeepN-JPEG\u201d, to embrace the nature of deep cascaded information process mechanism of DNN architecture. Extensive experiments, based on \u201cImageNet\u201d dataset with various state-of-the-art DNNs, show that \u201cDeepN-JPEG\u201d can achieve $\\sim 3.5$\u00d7 higher compression rate over the popular JPEG solution while maintaining the same accuracy level for image recognition, demonstrating its great potential of storage and power efficiency in DNN-based smart IoT system design.", "venue": "2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)", "authors": ["Zihao  Liu", "Tao  Liu", "Wujie  Wen", "Lei  Jiang", "Jie  Xu", "Yanzhi  Wang", "Gang  Quan"], "year": 2018, "n_citations": 51}
{"id": 3996736, "s2_id": "72c85f71eb4284a9367dd00df7e12bc3b2f362e7", "title": "A Fluid Limit for an Overloaded X Model via a Stochastic Averaging Principle", "abstract": "We prove a many-server heavy-traffic fluid limit for an overloaded Markovian queueing system having two customer classes and two service pools, known in the call-center literature as the X model. The system uses the fixed-queue-ratio-with-thresholds FQR-T control, which we proposed in a recent paper as a way for one service system to help another in face of an unexpected overload. Under FQR-T, customers are served by their own service pool until a threshold is exceeded. Then, one-way sharing is activated with customers from one class allowed to be served in both pools. After the control is activated, it aims to keep the two queues at a prespecified fixed ratio. For large systems that fixed ratio is achieved approximately. For the fluid limit, or FWLLN functional weak law of large numbers, we consider a sequence of properly scaled X models in overload operating under FQR-T. Our proof of the FWLLN follows the compactness approach, i.e., we show that the sequence of scaled processes is tight and then show that all converging subsequences have the specified limit. The characterization step is complicated because the queue-difference processes, which determine the customer-server assignments, need to be considered without spatial scaling. Asymptotically, these queue-difference processes operate on a faster time scale than the fluid-scaled processes. In the limit, because of a separation of time scales, the driving processes converge to a time-dependent steady state or local average of a time-varying fast-time-scale process FTSP. This averaging principle allows us to replace the driving processes with the long-run average behavior of the FTSP.", "venue": "Math. Oper. Res.", "authors": ["Ohad  Perry", "Ward  Whitt"], "year": 2013, "n_citations": 21}
{"id": 3997112, "s2_id": "a3725e12c1a1590ea80669ad495c83ce4e98db1c", "title": "LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU", "abstract": "A lot of deep learning applications are desired to be run on mobile devices. Both accuracy and inference time are meaningful for a lot of them. While the number of FLOPs is usually used as a proxy for neural network latency, it may not be the best choice. In order to obtain a better approximation of latency, the research community uses lookup tables of all possible layers for the calculation of the inference on a mobile CPU. It requires only a small number of experiments. Unfortunately, on a mobile GPU, this method is not applicable in a straightforward way and shows low precision. In this work, we consider latency approximation on a mobile GPU as a data- and hardware-specific problem. Our main goal is to construct a convenient Latency Estimation Tool for Investigation (LETI) of neural network inference and building robust and accurate latency prediction models for each specific task. To achieve this goal, we make tools that provide a convenient way to conduct massive experiments on different target devices focusing on a mobile GPU. After evaluation of the dataset, one can train the regression model on experimental data and use it for future latency prediction and analysis. We experimentally demonstrate the applicability of such an approach on a subset of the popular NAS-Benchmark 101 dataset for two different mobile GPU.", "venue": "Computers", "authors": ["Evgeny  Ponomarev", "Sergey  Matveev", "Ivan  Oseledets"], "year": 2021, "n_citations": 1}
{"id": 4013443, "s2_id": "ee640906d5820c76efafbb4053566d238a470212", "title": "Optimizing System Quality of Service through Rejuvenation for Long-Running Applications with Real-Time Constraints", "abstract": "Reliability, longevity, availability, and deadline guarantees are the four most important metrics to measure the QoS of long-running safety-critical real-time applications. Software aging is one of the major factors that impact the safety of long-running real-time applications as the degraded performance and increased failure rate caused by software aging can lead to deadline missing and catastrophic consequences. Software rejuvenation is one of the most commonly used approaches to handle issues caused by software aging. In this paper, we study the optimal time when software rejuvenation shall take place so that the system's reliability, longevity, and availability are maximized, and application delays caused by software rejuvenation is minimized. In particular, we formally analyze the relationships between software rejuvenation frequency and system reliability, longevity, and availability. Based on the theoretic analysis, we develop approaches to maximizing system reliability, longevity, and availability, and use simulation to evaluate the developed approaches. In addition, we design the MIN-DELAY semi-priority-driven scheduling algorithm to minimize application delays caused by rejuvenation processes. The simulation experiments show that the developed semi-priority-driven scheduling algorithm reduces application delays by 9.01% and 14.24% over the earliest deadline first (EDF) and least release time (LRT) scheduling algorithms, respectively.", "venue": "ArXiv", "authors": ["Chunhui  Guo", "Hao  Wu", "Xiayu  Hua", "Shangping  Ren", "Jerzy  Nogiec"], "year": 2018, "n_citations": 0}
{"id": 4017242, "s2_id": "7ff20f5553f3d99d336cc722ddb541d6463ad572", "title": "Doubly Exponential Solution for Randomized Load Balancing Models with General Service Times", "abstract": "In this paper, we provide a novel and simple approach to study the supermarket model with general service times. This approach is based on the supplementary variable method used in analyzing stochastic models extensively. We organize an infinite-size system of integral-differential equations by means of the density dependent jump Markov process, and obtain a close-form solution: doubly exponential structure, for the fixed point satisfying the system of nonlinear equations, which is always a key in the study of supermarket models. The fixed point is decomposited into two groups of information under a product form: the arrival information and the service information. based on this, we indicate two important observations: the fixed point for the supermarket model is different from the tail of stationary queue length distribution for the ordinary M/G/1 queue, and the doubly exponential solution to the fixed point can extensively exist even if the service time distribution is heavy-tailed. Furthermore, we analyze the exponential convergence of the current location of the supermarket model to its fixed point, and study the Lipschitz condition in the Kurtz Theorem under general service times. Based on these analysis, one can gain a new understanding how workload probing can help in load balancing jobs with general service times such as heavy-tailed service.", "venue": "ArXiv", "authors": ["Quan-Lin  Li"], "year": 2010, "n_citations": 3}
{"id": 4017940, "s2_id": "98b1d15fff08aab55f668fb9238fb2e5cdd5a643", "title": "Technical Report about Tiramisu: a Three-Layered Abstraction for Hiding Hardware Complexity from DSL Compilers", "abstract": "High-performance DSL developers work hard to take advantage of modern hardware. The DSL compilers have to build their own complex middle-ends before they can target a common back-end such as LLVM, which only handles single instruction streams with SIMD instructions. We introduce Tiramisu, a common middle-end that can generate efficient code for modern processors and accelerators such as multicores, GPUs, FPGAs and distributed clusters. Tiramisu introduces a novel three-level IR that separates the algorithm, how that algorithm is executed, and where intermediate data are stored. This separation simplifies optimization and makes targeting multiple hardware architectures from the same algorithm easier. As a result, DSL compilers can be made considerably less complex with no loss of performance while immediately targeting multiple hardware or hardware combinations such as distributed nodes with both CPUs and GPUs. We evaluated Tiramisu by creating a new middle-end for the Halide and Julia compilers. We show that Tiramisu extends Halide and Julia with many new capabilities including the ability to: express new algorithms (such as recurrent filters and non-rectangular iteration spaces), perform new complex loop nest transformations (such as wavefront parallelization, loop shifting and loop fusion) and generate efficient code for more architectures (such as combinations of distributed clusters, multicores, GPUs and FPGAs). Finally, we demonstrate that Tiramisu can generate very efficient code that matches the highly optimized Intel MKL gemm (generalized matrix multiplication) implementation, we also show speedups reaching 4X in Halide and 16X in Julia due to optimizations enabled by Tiramisu.", "venue": "ArXiv", "authors": ["Riyadh  Baghdadi", "Jessica  Ray", "Malek Ben Romdhane", "Emanuele Del Sozzo", "Patricia  Suriana", "Shoaib  Kamil", "Saman P. Amarasinghe"], "year": 2018, "n_citations": 0}
{"id": 4018155, "s2_id": "2301e2dff083018238961650f9e058aaf846959f", "title": "Confidence Interval Estimators for MOS Values", "abstract": "For the quantification of QoE, subjects often provide individual rating scores on certain rating scales which are then aggregated into Mean Opinion Scores (MOS). From the observed sample data, the expected value is to be estimated. While the sample average only provides a point estimator, confidence intervals (CI) are an interval estimate which contains the desired expected value with a given confidence level. In subjective studies, the number of subjects performing the test is typically small, especially in lab environments. The used rating scales are bounded and often discrete like the 5-point ACR rating scale. Therefore, we review statistical approaches in the literature for their applicability in the QoE domain for MOS interval estimation (instead of having only a point estimator, which is the MOS). We provide a conservative estimator based on the SOS hypothesis and binomial distributions and compare its performance (CI width, outlier ratio of CI violating the rating scale bounds) and coverage probability with well known CI estimators. We show that the provided CI estimator works very well in practice for MOS interval estimators, while the commonly used studentized CIs suffer from a positive outlier ratio, i.e., CIs beyond the bounds of the rating scale. As an alternative, bootstrapping, i.e., random sampling of the subjective ratings with replacement, is an efficient CI estimator leading to typically smaller CIs, but lower coverage than the proposed estimator.", "venue": "ArXiv", "authors": ["Tobias  Ho\u00dffeld", "Poul E. Heegaard", "Mart\u00edn  Varela", "Lea  Skorin-Kapov"], "year": 2018, "n_citations": 7}
{"id": 4018775, "s2_id": "cf412fa0f1f639b863ce5aa316d9cba6f6a6cb0c", "title": "Federated Learning over Next-Generation Ethernet Passive Optical Networks", "abstract": "Federated Learning (FL) is a distributed machine learning (ML) type of processing that preserves the privacy of user data, sharing only the parameters of ML models with a common server. The processing of FL requires specific latency and bandwidth demands that need to be fulfilled by the operation of the communication network. This paper introduces a Dynamic Wavelength and Bandwidth Allocation algorithm for Quality of Service (QoS) provisioning for FL traffic over 50 Gb/s Ethernet Passive Optical Networks. The proposed algorithm prioritizes FL traffic and reduces the delay of FL and delay-critical applications supported on the same infrastructure. Keywords\u2014Dynamic Wavelength and Bandwidth Allocation, Federated Learning, Ethernet Passive Optical Networks, Quality of Service, Multi-services, Future Access Networks.", "venue": "ArXiv", "authors": ["Oscar J. Ciceri", "Carlos A. Astudillo", "Zuqing  Zhu", "Nelson L. S. da Fonseca"], "year": 2021, "n_citations": 0}
{"id": 4018832, "s2_id": "4864df61a0981a4fd33fc0ff0003537fb3b63542", "title": "Speeding up Generalized PSR Parsers by Memoization Techniques", "abstract": "Predictive shift-reduce (PSR) parsing for hyperedge replacement (HR) grammars is very efficient, but restricted to a subclass of unambiguous HR grammars. To overcome this restriction, we have recently extended PSR parsing to generalized PSR (GPSR) parsing along the lines of Tomita-style generalized LR parsing. Unfortunately, GPSR parsers turned out to be too inefficient without manual tuning. This paper proposes to use memoization techniques to speed up GPSR parsers without any need of manual tuning, and which has been realized within the graph parser distiller Grappa. We present running time measurements for some example languages; they show a significant speed up by some orders of magnitude when parsing valid graphs. But memoization techniques do not help when parsing invalid graphs or if all parses of an ambiguous input graph shall be determined.", "venue": "GCM@STAF", "authors": ["Mark  Minas"], "year": 2019, "n_citations": 2}
{"id": 4021592, "s2_id": "d82eda36917b99832850efb0fe39000a70002c16", "title": "A Generalized Loss Network Model with Overflow for Capacity Planning of a Perinatal Network", "abstract": "We develop a generalized loss network framework for capacity planning of a perinatal network in the UK. Decomposing the network by hospitals, each unit is analyzed with a GI/G/c/0 overflow loss network model. A two-moment approximation is performed to obtain the steady state solution of the GI/G/c/0 loss systems, and expressions for rejection probability and overflow probability have been derived. Using the model framework, the number of required cots can be estimated based on the rejection probability at each level of care of the neonatal units in a network. The generalization ensures that the model can be applied to any perinatal network for renewal arrival and discharge processes.", "venue": "ArXiv", "authors": ["Md.  Asaduzzaman", "Thierry J. Chaussalet"], "year": 2011, "n_citations": 2}
{"id": 4022193, "s2_id": "c92123e7cd34fb764bdcfe45986386a9b3636333", "title": "Smart at what cost?: characterising mobile deep neural networks in the wild", "abstract": "With smartphones' omnipresence in people's pockets, Machine Learning (ML) on mobile is gaining traction as devices become more powerful. With applications ranging from visual filters to voice assistants, intelligence on mobile comes in many forms and facets. However, Deep Neural Network (DNN) inference remains a compute intensive workload, with devices struggling to support intelligence at the cost of responsiveness. On the one hand, there is significant research on reducing model runtime requirements and supporting deployment on embedded devices. On the other hand, the strive to maximise the accuracy of a task is supported by deeper and wider neural networks, making mobile deployment of state-of-the-art DNNs a moving target. In this paper, we perform the first holistic study of DNN usage in the wild in an attempt to track deployed models and match how these run on widely deployed devices. To this end, we analyse over 16k of the most popular apps in the Google Play Store to characterise their DNN usage and performance across devices of different capabilities, both across tiers and generations. Simultaneously, we measure the models' energy footprint, as a core cost dimension of any mobile deployment. To streamline the process, we have developed gaugeNN, a tool that automates the deployment, measurement and analysis of DNNs on devices, with support for different frameworks and platforms. Results from our experience study paint the landscape of deep learning deployments on smartphones and indicate their popularity across app developers. Furthermore, our study shows the gap between bespoke techniques and real-world deployments and the need for optimised deployment of deep learning models in a highly dynamic and heterogeneous ecosystem.", "venue": "Internet Measurement Conference", "authors": ["Mario  Almeida", "Stefanos  Laskaridis", "Abhinav  Mehrotra", "Lukasz  Dudziak", "Ilias  Leontiadis", "Nicholas D. Lane"], "year": 2021, "n_citations": 0}
{"id": 4025105, "s2_id": "bb823533b33eec037b997ec5f3d7343cc807dd7e", "title": "Balancing work and size with bounded buffers", "abstract": "We consider the fundamental problem of managing a bounded size queue buffer where traffic consists of packets of varying size, each packet requires several rounds of processing before it can be transmitted out, and the goal is to maximize the throughput, i.e., total size of successfully transmitted packets. Our work addresses the tension between two conflicting algorithmic approaches: favoring packets with fewer processing requirements as opposed to packets of larger size. We present a novel model for studying such systems and study the performance of online algorithms that aim to maximize throughput.", "venue": "2014 Sixth International Conference on Communication Systems and Networks (COMSNETS)", "authors": ["Kirill  Kogan", "Alejandro  L\u00f3pez-Ortiz", "Sergey I. Nikolenko", "Gabriel  Scalosub", "Michael  Segal"], "year": 2014, "n_citations": 18}
{"id": 4025858, "s2_id": "af99fd4e948438eace39c228411944b600017f65", "title": "Community detection in node-attributed social networks: a survey", "abstract": "Community detection is a fundamental problem in social network analysis consisting in unsupervised dividing social actors (nodes in a social graph) with certain social connections (edges in a social graph) into densely knitted and highly related groups with each group well separated from the others. Classical approaches for community detection usually deal only with network structure and ignore features of its nodes (called node attributes), although many real-world social networks provide additional actors' information such as interests. It is believed that the attributes may clarify and enrich the knowledge about the actors and give sense to the communities. This belief has motivated the progress in developing community detection methods that use both the structure and the attributes of network (i.e. deal with a node-attributed graph) to yield more informative and qualitative results. \nDuring the last decade many such methods based on different ideas have appeared. Although there exist partial overviews of them, a recent survey is a necessity as the growing number of the methods may cause repetitions in methodology and uncertainty in practice. \nIn this paper we aim at describing and clarifying the overall situation in the field of community detection in node-attributed social networks. Namely, we perform an exhaustive search of known methods and propose a classification of them based on when and how structure and attributes are fused. We not only give a description of each class but also provide general technical ideas behind each method in the class. Furthermore, we pay attention to available information which methods outperform others and which datasets and quality measures are used for their evaluation. Basing on the information collected, we make conclusions on the current state of the field and disclose several problems that seem important to be resolved in future.", "venue": "Comput. Sci. Rev.", "authors": ["Petr  Chunaev"], "year": 2020, "n_citations": 46}
{"id": 4027084, "s2_id": "4a6e37e00a75d17cea0c8559b9d2f5f5985add75", "title": "Failure Analysis of Hadoop Schedulers using an Integration of Model Checking and Simulation", "abstract": "The Hadoop scheduler is a centerpiece of Hadoop, the leading processing framework for data-intensive applications in the cloud. Given the impact of failures on the performance of applications running on Hadoop, testing and verifying the performance of the Hadoop scheduler is critical. Existing approaches such as performance simulation and analytical modeling are inadequate because they are not able to ascertain a complete verification of a Hadoop scheduler. This is due to the wide range of constraints and aspects involved in Hadoop. In this paper, we propose a novel methodology that integrates and combines simulation and model checking techniques to perform a formal verification of Hadoop schedulers, focusing on the following properties: schedulability, fairness and resources-deadlock freeness. We use the CSP language to formally describe a Hadoop scheduler, and the PAT model checker to verify its properties. Next, we use the proposed formal model to analyze the scheduler of OpenCloud, a Hadoop-based cluster that simulates the Hadoop load, in order to illustrate the usability and benefits of our work. Results show that our proposed methodology can help identify several tasks failures (up to 78%) early on, i.e., before the tasks are executed on the cluster.", "venue": "SCSS", "authors": ["Mbarka  Soualhia", "Foutse  Khomh", "Sofiene  Tahar"], "year": 2021, "n_citations": 0}
{"id": 4033789, "s2_id": "322c219db71432d773fa710eca20cf39f74d9ab4", "title": "A General, Tractable and Accurate Model for a Cascade of Caches", "abstract": "Performance evaluation of caching systems is an old and widely investigated research topic. The research community is once again actively working on this topic because the Internet is evolving towards new transfer modes, which envisage to cache both contents and instructions within the network. In particular, there is interest in characterizing multi-cache systems, in which requests not satisfied by a cache are forwarded to other caches. \nIn this field, this paper contributes as follows. First, we devise a simple but accurate approximate analysis for caches fed by general \"renewal\" traffic patterns. Second, we characterize and model the traffic statistics for the output (miss) stream. Third, we show in the simple example case of tandem caches how the resulting output stream model can be conveniently exploited to analyze the performance of subsequent cache stages. The main novelty of our work stems in the ability to handle traffic patterns beyond the traditional independent reference model, thus permitting simple assessment of cascade of caches as well as improved understanding of the phenomena involved in cache hierarchies.", "venue": "ArXiv", "authors": ["Giuseppe  Bianchi", "Nicola  Blefari-Melazzi", "Alberto  Caponi", "Andrea  Detti"], "year": 2013, "n_citations": 2}
{"id": 4034143, "s2_id": "d1ba9c0dd6d31a0392868f4fd87a6ae82257232b", "title": "Serving the Grid: an Experimental Study of Server Clusters as Real-Time Demand Response Resources", "abstract": "Demand response is a crucial technology to allow large-scale penetration of intermittent renewable energy sources in the electric grid. This paper is based on the thesis that datacenters represent especially attractive candidates for providing flexible, real-time demand response services to the grid; they are capable of finely-controllable power consumption, fast power ramp-rates, and large dynamic range. This paper makes two main contributions: (a) it provides detailed experimental evidence justifying this thesis, and (b) it presents a comparative investigation of three candidate software interfaces for power control within the servers. All of these results are based on a series of experiments involving real-time power measurements on a lab-scale server cluster. This cluster was specially instrumented for accurate and fast power measurements on a time-scale of 100 ms or less. Our results provide preliminary evidence for the feasibility of large scale demand response using datacenters, and motivates future work on exploiting this capability.", "venue": "ArXiv", "authors": ["Josiah  McClurg", "Raghuraman  Mudumbai"], "year": 2016, "n_citations": 0}
{"id": 4037285, "s2_id": "fc21a3fd3e3fac006f300ad6bc37d3426c482282", "title": "Randomized Work Stealing versus Sharing in Large-scale Systems with Non-exponential Job Sizes", "abstract": "Work sharing and work stealing are two scheduling paradigms to redistribute work when performing distributed computations. In work sharing, processors attempt to migrate pending jobs to other processors in the hope of reducing response times. In work stealing, on the other hand, underutilized processors attempt to steal jobs from other processors. Both paradigms generate a certain communication overhead and the question addressed in this paper is which of the two reduces the response time the most given that they use the same amount of communication overhead. \nPrior work presented explicit bounds, for large scale systems, on when randomized work sharing outperforms randomized work stealing in case of Poisson arrivals and exponential job durations and indicated that work sharing is best when the load is below $\\phi -1 \\approx 0.6180$, with $\\phi$ being the golden ratio. \nIn this paper we revisit this problem and study the impact of the job size distribution using a mean field model. We present an efficient method to determine the boundary between the regions where sharing or stealing is best for a given job size distribution, as well as bounds that apply to any (phase-type) job size distribution. The main insight is that work stealing benefits significantly from having more variable job sizes and work sharing may become inferior to work stealing for loads as small as $1/2 + \\epsilon$ for any $\\epsilon > 0$.", "venue": "IEEE/ACM Trans. Netw.", "authors": ["Benny Van Houdt"], "year": 2019, "n_citations": 4}
{"id": 4039827, "s2_id": "5c267299dc98d04dd6bc9702088b3ce78201c5dc", "title": "GraphChallenge.org Triangle Counting Performance", "abstract": "The rise of graph analytic systems has created a need for new ways to measure and compare the capabilities of graph processing systems. The MIT/Amazon/IEEE Graph Challenge has been developed to provide a well-defined community venue for stimulating research and highlighting innovations in graph analysis software, hardware, algorithms, and systems. GraphChallenge.org provides a wide range of pre-parsed graph data sets, graph generators, mathematically defined graph algorithms, example serial implementations in a variety of languages, and specific metrics for measuring performance. The triangle counting component of GraphChallenge.org tests the performance of graph processing systems to count all the triangles in a graph and exercises key graph operations found in many graph algorithms. In 2017, 2018, and 2019 many triangle counting submissions were received from a wide range of authors and organizations. This paper presents a performance analysis of the best performers of these submissions. These submissions show that their state-of-the-art triangle counting execution time, $T_{tri}$, is a strong function of the number of edges in the graph, $N_{e}$, which improved significantly from 2017 ($T_{tri} \\approx (N_{e}/10^{8})4/3)$ to 2018 ($T_{tri}\\approx N_{e}/10^{9})$ and remained comparable from 2018 to 2019. Graph Challenge provides a clear picture of current graph analysis systems and underscores the need for new innovations to achieve high performance on very large graphs.", "venue": "2020 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Siddharth  Samsi", "Jeremy  Kepner", "Vijay  Gadepally", "Michael  Hurley", "Michael  Jones", "Edward  Kao", "Sanjeev  Mohindra", "Albert  Reuther", "Steven  Smith", "William  Song", "Diane  Staheli", "Paul  Monticciolo"], "year": 2020, "n_citations": 5}
{"id": 4040887, "s2_id": "33a7ad21d3dcf194d76f750c325f34ff2d133c67", "title": "LITMUS: An Open Extensible Framework for Benchmarking RDF Data Management Solutions", "abstract": "Developments in the context of Open, Big, and Linked Data have led to an enormous growth of structured data on the Web. To keep up with the pace of efficient consumption and management of the data at this rate, many data Management solutions have been developed for specific tasks and applications. We present LITMUS, a framework for benchmarking data management solutions. LITMUS goes beyond classical storage benchmarking frameworks by allowing for analysing the performance of frameworks across query languages. In this position paper we present the conceptual architecture of LITMUS as well as the considerations that led to this architecture.", "venue": "ArXiv", "authors": ["Harsh  Thakkar", "Mohnish  Dubey", "Gezim  Sejdiu", "Axel-Cyrille Ngonga Ngomo", "Jeremy  Debattista", "Christoph  Lange", "Jens  Lehmann", "S\u00f6ren  Auer", "Maria-Esther  Vidal"], "year": 2016, "n_citations": 2}
{"id": 4042484, "s2_id": "5d4c586739230f78332466219bf2db7703191048", "title": "The Effect of Data Marshalling on Computation Offloading Decisions", "abstract": "We conducted an extensive set of experiments with an offloading testbed to understand the impact that data marshalling techniques have on computation offloading decisions. We find that the popular JSON format to marshall data between client and server comes at a significant computational expense compared to a minimalistic raw data transfer. The computational time is significant in that it affects computation offloading decisions in a variety of conditions. We outline some of these conditions.", "venue": "ArXiv", "authors": ["Julio A. Reyes-Munoz", "Michael P. McGarry"], "year": 2018, "n_citations": 0}
{"id": 4044794, "s2_id": "484b5b30b363f70fc3592c0965878bdd7e64a9ea", "title": "Scalable distributed-memory external sorting", "abstract": "We engineer algorithms for sorting huge data sets on massively parallel machines. The algorithms are based on the multiway merging paradigm. We first outline an algorithm whose I/O requirement is close to a lower bound. Thus, in contrast to naive implementations of multiway merging and all other approaches known to us, the algorithm works with just two passes over the data even for the largest conceivable inputs. A second algorithm reduces communication overhead and uses more conventional specifications of the result at the cost of slightly increased I/O requirements. An implementation wins the well known sorting benchmark in several categories and by a large margin over its competitors.", "venue": "2010 IEEE 26th International Conference on Data Engineering (ICDE 2010)", "authors": ["Mirko  Rahn", "Peter  Sanders", "Johannes  Singler"], "year": 2010, "n_citations": 26}
{"id": 4047634, "s2_id": "12c3ef0f4f3cc02f87d0923c5f29bbd89dc2e068", "title": "Optimized Dynamic Cache Instantiation and Accurate LRU Approximations under Time-varying Request Volume", "abstract": "Content-delivery applications can achieve scalability and reduce wide-area network traffic using geographically distributed caches. However, each deployed cache has an associated cost, and under timevarying request rates (e.g., a daily cycle) there may be long periods when the request rate from the local region is not high enough to justify this cost. Cloud computing offers a solution to problems of this kind, by supporting dynamic allocation and release of resources. In this paper, we analyze the potential benefits from dynamically instantiating caches using resources from cloud service providers. We develop novel analytic caching models that accommodate time-varying request rates, transient behavior as a cache fills following instantiation, and selective cache insertion policies. Within the context of a simple cost model, we then develop bounds and compare policies with optimized parameter selections to obtain insights into key cost/performance tradeoffs. We find that dynamic cache instantiation can provide substantial cost reductions, that potential reductions strongly dependent on the object popularity skew, and that selective cache insertion can be even more beneficial in this context than with conventional edge caches. Finally, our contributions also include accurate and easy-to-compute approximations that are shown applicable to LRU caches under time-varying workloads.", "venue": "IEEE Transactions on Cloud Computing", "authors": ["Niklas  Carlsson", "Derek L. Eager"], "year": 2021, "n_citations": 1}
{"id": 4049557, "s2_id": "ceec542ca278009bdb83e116ce58c0b772114bf4", "title": "Achieving Zero Asymptotic Queueing Delay for Parallel Jobs", "abstract": "Zero queueing delay is highly desirable in large-scale computing systems. Existing work has shown that it can be asymptotically achieved by using the celebrated Power-of-d-choices (pod) policy with a probe overhead $d = \u00f8mega\u0142eft(\\frac\u0142og N 1-\u0142ambda \\right)$, and it is impossible when $d = O\u0142eft(\\frac1 1-\u0142ambda \\right)$, where N is the number of servers and $\u0142ambda$ is the load of the system. However, these results are based on the model where each job is an indivisible unit, which does not capture the parallel structure of jobs in today's predominant parallel computing paradigm. This paper thus considers a model where each job consists of a batch of parallel tasks. Under this model, we propose a new notion of zero (asymptotic) queueing delay that requires the job delay under a policy to approach the job delay given by the max of its tasks' service times, i.e., the job delay assuming its tasks entered service right upon arrival. This notion quantifies the effect of queueing on a job level for jobs consisting of multiple tasks, and thus deviates from the conventional zero queueing delay for single-task jobs in the literature. We show that zero queueing delay for parallel jobs can be achieved using the batch-filling policy (a variant of the celebrated \\pod\\ policy) with a probe overhead $d = \u00f8mega\u0142eft(\\frac1 (1-\u0142ambda)\u0142og k \\right)$ in the sub-Halfin-Whitt heavy-traffic regime, where k is the number of tasks in each job and k properly scales with N (the number of servers). This result demonstrates that for parallel jobs, zero queueing delay can be achieved with a smaller probe overhead. We also establish an impossibility result: we show that zero queueing delay cannot be achieved if $d = \\exp\u0142eft(o\u0142eft(\\frac\u0142og N \u0142og k \\right) \\right)$. Simulation results are provided to demonstrate the consistency between numerical results and theoretical results under reasonable settings, and to investigate gaps in the theoretical analysis.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Wentao  Weng", "Weina  Wang"], "year": 2020, "n_citations": 7}
{"id": 4051084, "s2_id": "1c09084e2bac63d5de2c8cbec8d36776083e5674", "title": "Distributed dynamic load balancing for task parallel programming", "abstract": "In this paper, we derive and investigate approaches to dynamically load balance a distributed task parallel application software. The load balancing strategy is based on task migration. Busy processes export parts of their ready task queue to idle processes. Idle--busy pairs of processes find each other through a random search process that succeeds within a few steps with high probability. We evaluate the load balancing approach for a block Cholesky factorization implementation and observe a reduction in execution time on the order of 5\\% in the selected test cases.", "venue": "ArXiv", "authors": ["Afshin  Zafari", "Elisabeth  Larsson"], "year": 2018, "n_citations": 1}
{"id": 4053465, "s2_id": "b478f8d51a98528b61b5e4260f1e543021979392", "title": "A model-driven approach for a new generation of adaptive libraries", "abstract": "Efficient high-performance libraries often expose multiple tunable parameters to provide highly optimized routines. These can range from simple loop unroll factors or vector sizes all the way to algorithmic changes, given that some implementations can be more suitable for certain devices by exploiting hardware characteristics such as local memories and vector units. Traditionally, such parameters and algorithmic choices are tuned and then hard-coded for a specific architecture and for certain characteristics of the inputs. However, emerging applications are often data-driven, thus traditional approaches are not effective across the wide range of inputs and architectures used in practice. In this paper, we present a new adaptive framework for data-driven applications which uses a predictive model to select the optimal algorithmic parameters by training with synthetic and real datasets. We demonstrate the effectiveness of a BLAS library and specifically on its matrix multiplication routine. We present experimental results for two GPU architectures and show significant performance gains of up to 3x (on a high-end NVIDIA Pascal GPU) and 2.5x (on an embedded ARM Mali GPU) when compared to a traditionally optimized library.", "venue": "ArXiv", "authors": ["Marco  Cianfriglia", "Flavio  Vella", "Cedric  Nugteren", "Anton  Lokhmotov", "Grigori  Fursin"], "year": 2018, "n_citations": 5}
{"id": 4053603, "s2_id": "06e196cd6122bddf0cd9d4de41a9e1b219423741", "title": "Visualizing the robustness of query execution", "abstract": "In database query processing, actual run-time conditions (e.g., actual selectivities and actual available memory) very often differ from compile-time expectations of run-time conditions (e.g., estimated predicate selectivities and anticipated memory availability). Robustness of query processing can be defined as the ability to handle unexpected conditions. Robustness of query execution, specifically, can be defined as the ability to process a specific plan efficiently in an unexpected condition. We focus on query execution (run-time), ignoring query optimization (compile-time), in order to complement existing research and to explore untapped potential for improved robustness in database query processing. \nOne of our initial steps has been to devise diagrams or maps that show how well plans perform in the face of varying run-time conditions and how gracefully a system's query architecture, operators, and their implementation degrade in the face of adverse conditions. In this paper, we show several kinds of diagrams with data from three real systems and report on what we have learned both about these visualization techniques and about the three database systems", "venue": "CIDR", "authors": ["Goetz  Graefe", "Harumi A. Kuno", "Janet L. Wiener"], "year": 2009, "n_citations": 15}
{"id": 4055351, "s2_id": "7f94fe52f63a70a5c72383e05756c68478dc7da5", "title": "User Performance in Small Cells Networks with Inter-Cell Mobility", "abstract": "We analyze the impact of intra-cell mobility on user performance in dense networks such as that enabled by LTE-A and 5G. To this end, we consider a homogeneous network of small cells and first show how to reduce the evaluation of user performance to the case of a single representative cell. We then propose simple analytical models that capture mobility through the distribution of the residual sojourn time of mobile users in the cell. An approximate model, based on Quasi-Stationary (QS) assumptions, is developed in order to speed up computation in the Markovian framework. We use these models to derive the average throughput of both mobile and static users, along with the probability of handover for mobile users. Numerical evaluation and simulation results are provided to assess the accuracy of the proposed models. We show, in particular, that both classes of users benefit from a throughput gain induced by the \"opportunistic\" displacement of mobile users among cells.", "venue": "ArXiv", "authors": ["Philippe  Olivier", "Alain  Simonian"], "year": 2016, "n_citations": 2}
{"id": 4056049, "s2_id": "73de3f286907d6c084a46ce35f30da5628178bd0", "title": "QuickCast: Fast and Efficient Inter-Datacenter Transfers Using Forwarding Tree Cohorts", "abstract": "Several organizations have built multiple datacenters connected via dedicated wide area networks over which large inter-datacenter transfers take place. Since many such transfers move the same data from one source to multiple destinations, using multicast forwarding trees can reduce bandwidth needs and improve completion times. However, using a single forwarding tree per transfer can lead to poor performance as the slowest receiver dictates the completion time for all receivers. Using multiple forwarding trees per transfer alleviates this concern-the average receiver could finish early; however, if done naively, bandwidth usage would also increase and it is apriori unclear how best to partition receivers, how to construct the multiple trees and how to determine the rate and schedule of flows on these trees. This paper presents QuickCast, a first solution to these problems. Using simulations on real-world network topologies, we see that QuickCast can speed up the average receiver's completion time by as much as $10\\times$ while only using $1.04\\times$ more bandwidth; further, the completion time for all receivers also improves by as much as $1.57 >$ faster at high loads. Thereby, while some implementation challenges remain, we advocate using a cohort of forwarding trees.", "venue": "IEEE INFOCOM 2018 - IEEE Conference on Computer Communications", "authors": ["Mohammad  Noormohammadpour", "Cauligi S. Raghavendra", "Srikanth  Kandula", "Sriram  Rao"], "year": 2018, "n_citations": 29}
{"id": 4057722, "s2_id": "5be83e638fb0bb16d1e3bb51c2f5a5e814d813dc", "title": "Weld: Rethinking the Interface Between Data-Intensive Applications", "abstract": "Data analytics applications combine multiple functions from different libraries and frameworks. Even when each function is optimized in isolation, the performance of the combined application can be an order of magnitude below hardware limits due to extensive data movement across these functions. To address this problem, we propose Weld, a new interface between data-intensive libraries that can optimize across disjoint libraries and functions. Weld exposes a lazily-evaluated API where diverse functions can submit their computations in a simple but general intermediate representation that captures their data-parallel structure. It then optimizes data movement across these functions and emits efficient code for diverse hardware. Weld can be integrated into existing frameworks such as Spark, TensorFlow, Pandas and NumPy without changing their user-facing APIs. We demonstrate that Weld can speed up applications using these frameworks by up to 29x.", "venue": "ArXiv", "authors": ["Shoumik  Palkar", "James J. Thomas", "Deepak  Narayanan", "Anil  Shanbhag", "Rahul  Palamuttam", "Holger  Pirk", "Malte  Schwarzkopf", "Saman P. Amarasinghe", "Samuel  Madden", "Matei  Zaharia"], "year": 2017, "n_citations": 17}
{"id": 4063097, "s2_id": "2071ad3fa9919d1065aedfc21fa115be810621bd", "title": "Performance Analysis of Wavelet Based MC-CDMA System with Implementation of Various Antenna Diversity Schemes", "abstract": "The impact of using wavelet based technique on the performance of a MC-CDMA wireless communication system has been investigated. The system under proposed study incorporates Walsh Hadamard codes to discriminate the message signal for individual user. A computer program written in Mathlab source code is developed and this simulation study is made with implementation of various antenna diversity schemes and fading (Rayleigh and Rician) channel. Computer simulation results demonstrate that the proposed wavelet based MC-CDMA system outperforms in Alamouti (two transmit antenna and one receive antenna) under AWGN and Rician channel.", "venue": "ArXiv", "authors": ["Md. Matiqul Islam", "M. Hasnat Kabir", "Shaikh Enayet Ullah"], "year": 2012, "n_citations": 11}
{"id": 4072006, "s2_id": "0eb5e3cb9ca7e0b4dcf5fda17e1d9101f21c336f", "title": "When Does the Gittins Policy Have Asymptotically Optimal Response Time Tail?", "abstract": "We consider scheduling in the M/G/1 queue with unknown job sizes. It is known that the Gittins policy minimizes mean response time in this setting. However, the behavior of the tail of response time under Gittins is poorly understood, even in the large-response-time limit. Characterizing Gittins\u2019s asymptotic tail behavior is important because if Gittins has optimal tail asymptotics, then it simultaneously provides optimal mean response time and good tail performance. In this work, we give the first comprehensive account of Gittins\u2019s asymptotic tail behavior. For heavy-tailed job sizes, we find that Gittins always has asymptotically optimal tail. The story for light-tailed job sizes is less clear-cut: Gittins\u2019s tail can be optimal, pessimal, or in between. To remedy this, we show that a modification of Gittins avoids pessimal tail behavior while achieving near-optimal mean response time.", "venue": "ArXiv", "authors": ["Ziv  Scully", "Lucas van Kreveld"], "year": 2021, "n_citations": 0}
{"id": 4072973, "s2_id": "81330f5c6241a16ab1371502d2ce02ffeb4f2be6", "title": "On the Influence of Initial Qubit Placement During NISQ Circuit Compilation", "abstract": "Noisy Intermediate-Scale Quantum (NISQ) machines are not fault-tolerant, operate few qubits (currently, less than hundred), but are capable of executing interesting computations. Above the quantum supremacy threshold (approx. 60 qubits), NISQ machines are expected to be more powerful than existing classical computers. One of the most stringent problems is that computations (expressed as quantum circuits) have to be adapted (compiled) to the NISQ hardware, because the hardware does not support arbitrary interactions between the qubits. This procedure introduces additional gates (e.g. SWAP gates) into the circuits while leaving the implemented computations unchanged. Each additional gate increases the failure rate of the adapted (compiled) circuits, because the hardware and the circuits are not fault-tolerant. It is reasonable to expect that the placement influences the number of additionally introduced gates. Therefore, a combinatorial problem arises: how are circuit qubits allocated (placed) initially to the hardware qubits? The novelty of this work relies on the methodology used to investigate the influence of the initial placement. To this end, we introduce a novel heuristic and cost model to estimate the number of gates necessary to adapt a circuit to a given NISQ architecture. We implement the heuristic (source code available on github) and benchmark it using a standard compiler (e.g. from IBM Qiskit) treated as a black box. Preliminary results indicate that cost reductions of up to 10% can be achieved for practical circuit instances on realistic NISQ architectures only by placing qubits differently than default (trivial placement).", "venue": "QTOP@NetSys", "authors": ["Alexandru  Paler"], "year": 2017, "n_citations": 25}
{"id": 4073374, "s2_id": "c7be1cd830fb1dcee0cb0e3ab848f15808509a53", "title": "Introducing a Performance Model for Bandwidth-Limited Loop Kernels", "abstract": "We present a diagnostic performance model for bandwidth-limited loop kernels which is founded on the analysis of modern cache based microarchitectures. This model allows an accurate performance prediction and evaluation for existing instruction codes. It provides an in-depth understanding of how performance for different memory hierarchy levels is made up. The performance of raw memory load, store and copy operations and a stream vector triad are analyzed and benchmarked on three modern x86-type quad-core architectures in order to demonstrate the capabilities of the model.", "venue": "PPAM", "authors": ["Jan  Treibig", "Georg  Hager"], "year": 2009, "n_citations": 70}
{"id": 4076585, "s2_id": "cfc674e02b2d78e412845d6544b5a0963a1daf9b", "title": "Delay comparison of delivery and coding policies in data clusters", "abstract": "A key function of cloud infrastructure is to store and deliver diverse files, e.g., scientific datasets, social network information, videos, etc. In such systems, for the purpose of fast and reliable delivery, files are divided into chunks, replicated or erasure-coded, and disseminated across servers. It is neither known in general how delays scale with the size of a request nor how delays compare under different policies for coding, data dissemination, and delivery. Motivated by these questions, we develop and explore a set of evolution equations as a unified model which captures the above features. These equations allow for both efficient simulation and mathematical analysis of several delivery policies under general statistical assumptions. In particular, we quantify in what sense a workload aware delivery policy performs better than a workload agnostic policy. Under a dynamic or stochastic setting, the sample path comparison of these policies does not hold in general. The comparison is shown to hold under the weaker increasing convex stochastic ordering, still stronger than the comparison of averages. This result further allows us to obtain insightful computable performance bounds. For example, we show that in a system where files are divided into chunks of equal size, replicated or erasure-coded, and disseminated across servers at random, the job delays increase sub-logarithmically in the request size for small and medium-sized files but linearly for large files.", "venue": "2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "authors": ["Virag  Shah", "Anne  Bouillard", "Fran\u00e7ois  Baccelli"], "year": 2017, "n_citations": 10}
{"id": 4077570, "s2_id": "5f32a7eadc62e037ac93d375476d5dac621e03c4", "title": "Network Capability in Localizing Node Failures via End-to-End Path Measurements", "abstract": "We investigate the capability of localizing node failures in communication networks from binary states (normal/failed) of end-to-end paths. Given a set of nodes of interest, uniquely localizing failures within this set requires that different observable path states associate with different node failure events. However, this condition is difficult to test on large networks due to the need to enumerate all possible node failures. Our first contribution is a set of sufficient/necessary conditions for identifying a bounded number of failures within an arbitrary node set that can be tested in polynomial time. In addition to network topology and locations of monitors, our conditions also incorporate constraints imposed by the probing mechanism used. We consider three probing mechanisms that differ according to whether measurement paths are: (i) arbitrarily controllable; (ii) controllable but cycle-free; or (iii) uncontrollable (determined by the default routing protocol). Our second contribution is to quantify the capability of failure localization through: 1) the maximum number of failures (anywhere in the network) such that failures within a given node set can be uniquely localized and 2) the largest node set within which failures can be uniquely localized under a given bound on the total number of failures. Both measures in 1) and 2) can be converted into the functions of a per-node property, which can be computed efficiently based on the above sufficient/necessary conditions. We demonstrate how measures 1) and 2) proposed for quantifying failure localization capability can be used to evaluate the impact of various parameters, including topology, number of monitors, and probing mechanisms.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Liang  Ma", "Ting  He", "Ananthram  Swami", "Don  Towsley", "Kin K. Leung"], "year": 2017, "n_citations": 31}
{"id": 4078847, "s2_id": "6e395e81e1171ab6319c930b332d2722e294b1c6", "title": "Performance evaluation of the random replacement policy for networks of caches", "abstract": "The overall performance of content distribution networks as well as recently proposed information-centric networks rely on both memory and bandwidth capacities. The hit ratio is the key performance indicator which captures the bandwidth/memory tradeoff for a given global performance. This paper focuses on the estimation of the hit ratio in a network of caches that employ the Random replacement policy (RND). Assuming that requests are independent and identically distributed, general expressions of miss probabilities for a single RND cache are provided as well as exact results for specific popularity distributions (such results also hold for the FIFO replacement policy). Moreover, for any Zipf popularity distribution with exponent @a>1, we obtain asymptotic equivalents for the miss probability in the case of large cache size. We extend the analysis to networks of RND caches, when the topology is either a line or a homogeneous tree. In that case, approximations for miss probabilities across the network are derived by neglecting time correlations between miss events at any node; the obtained results are compared to the same network using the Least-Recently-Used discipline, already addressed in the literature. We further analyze the case of a mixed tandem cache network where the two nodes employ either Random or Least-Recently-Used policies. In all scenarios, asymptotic formulas and approximations are extensively compared to simulation results and shown to be very accurate. Finally, our results enable us to propose recommendations for cache replacement disciplines in a network dedicated to content distribution.", "venue": "Perform. Evaluation", "authors": ["Massimo  Gallo", "Bruno  Kauffmann", "Luca  Muscariello", "Alain  Simonian", "Christian  Tanguy"], "year": 2014, "n_citations": 12}
{"id": 4083154, "s2_id": "4a5f93dbba1b84a21377196d2a878ccfa60a512f", "title": "AutoTiering: Automatic data placement manager in multi-tier all-flash datacenter", "abstract": "In the year of 2017, the capital expenditure of Flash-based Solid State Drivers (SSDs) keeps declining and the storage capacity of SSDs keeps increasing. As a result, the \u201cselling point\u201d of traditional spinning Hard Disk Drives (HDDs) as a backend storage \u2014 low cost and large capacity \u2014 is no longer unique, and eventually they will be replaced by low-end SSDs which have large capacity but perform orders of magnitude better than HDDs. Thus, it is widely believed that all-flash multi-tier storage systems will be adopted in the enterprise datacenters in the near future. However, existing caching or tiering solutions for SSD-HDD hybrid storage systems are not suitable for all-flash storage systems. This is because that all-flash storage systems do not have a large speed difference (e.g., 10x) among each tier. Instead, different specialties (such as high performance, high capacity, etc.) of each tier should be taken into consideration. Motivated by this, we develop an automatic data placement manager called \u201cAutoTiering\u201d to handle virtual machine disk files (VMDK) allocation and migration in an all-flash multitier datacenter to best utilize the storage resource, optimize the performance, and reduce the migration overhead. AutoTiering is based on an optimization framework, whose core technique is to predict VM's performance change on different tiers with different specialties without conducting real migration. As far as we know, AutoTiering is the first optimization solution designed for all-flash multi-tier datacenters. We implement AutoTiering on VMware ESXi [1], and experimental results show that it can significantly improve the I/O performance compared to existing solutions.", "venue": "2017 IEEE 36th International Performance Computing and Communications Conference (IPCCC)", "authors": ["Zhengyu  Yang", "Morteza  Hoseinzadeh", "Allen  Andrews", "Clay  Mayers", "David Thomas Evans", "Rory Thomas Bolt", "Janki  Bhimani", "Ningfang  Mi", "Steven  Swanson"], "year": 2017, "n_citations": 35}
{"id": 4084568, "s2_id": "346c16742e48234b458f704316ac252a6f3414b3", "title": "SimAS: A simulation\u2010assisted approach for the scheduling algorithm selection under perturbations", "abstract": "Many scientific applications consist of large and computationally intensive loops. Dynamic loop self\u2010scheduling (DLS) techniques are used to parallelize and to balance the load of such applications during execution. Load imbalance arises from variations in the loop iteration (or tasks) execution times, caused by problem, algorithmic, or systemic characteristics. Variations in systemic characteristics are referred to as perturbations. Our hypothesis is that no single DLS technique can achieve the absolute best performance under various perturbations on heterogeneous high\u2010performance computing (HPC) systems. Therefore, the selection of the most efficient DLS technique is critical to achieve the best application performance. The goal of this work is to solve the algorithm selection problem for the scheduling of computationally intensive applications under perturbations. Existing work only considers perturbations caused by variations in the delivered computational speed of the HPC systems. However, perturbations in available network bandwidth or latency are inevitable on production HPC systems. A simulation\u2010assisted scheduling algorithm selection (SimAS) approach is introduced herein as a novel control\u2010theoretic\u2010inspired approach to select DLS techniques dynamically that improve the performance of applications executing on heterogeneous HPC systems under perturbations. The present work examines the performance of seven applications on a heterogeneous HPC system under all the above system perturbations. SimAS is evaluated using native and simulative experiments. The performance results confirm the original hypothesis that motivates this work. The experimental evaluation shows that the SimAS\u2010based DLS selection identifies the most efficient technique and improves application performance in most cases.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Ali  Mohammed", "Florina M. Ciorba"], "year": 2020, "n_citations": 2}
{"id": 4084632, "s2_id": "be28968e1f97f2fe212002f07aeb0d10f61fd0de", "title": "Transmission of Voice Signal: BER Performance Analysis of Different FEC Schemes Based OFDM System over Various Channels", "abstract": "In this paper, we investigate the impact of Forward Error Correction (FEC) codes namely Cyclic Redundancy Code and Convolution Code on the performance of OFDM wireless communication system for speech signal transmission over both AWGN and fading (Rayleigh and Rician) channels in term of Bit Error Probability. The simulation has been done in conjunction with QPSK digital modulation and compared with uncoded resultstal modulation. In the fading channels, it is found via computer simulation that the performance of the Convolution interleaved based OFDM systems outperform than that of CRC interleaved OFDM system as well as uncoded OFDM channels.", "venue": "ArXiv", "authors": ["Md. Golam Rashed", "M. Hasnat Kabir", "Md. Selim Reza", "Md. Matiqul Islam", "Rifat Ara Shams", "Saleh  Masum", "Shaikh Enayet Ullah"], "year": 2012, "n_citations": 18}
{"id": 4085819, "s2_id": "ba1072651523f4b2cddb99b935aabcf0611e7b64", "title": "Mobile Energy Requirements of the Upcoming NIST Post-Quantum Cryptography Standards", "abstract": "Standardization of Post-Quantum Cryptography (PQC) was started by NIST in 2016 and has proceeded to its second elimination round. The upcoming standards are intended to replace (or supplement) current RSA and Elliptic Curve Cryptography (ECC) on all targets, including lightweight, embedded, and mobile systems. We present an energy requirement analysis based on extensive measurements of PQC candidate algorithms on a Cortex M4 - based reference platform. We relate computational (energy) costs of PQC algorithms to their data transmission costs which are expected to increase with new types of public keys and ciphertext messages. The energy, bandwidth, and latency needs of PQC algorithms span several orders of magnitude, which is substantial enough to impact battery life, user experience, and application protocol design. We propose metrics and guidelines for PQC algorithm usage in IoT and mobile systems based on our findings. Our evidence supports the view that fast structured-lattice PQC schemes are the preferred choice for cloud-connected mobile devices in most use cases, even when per-bit data transmission energy cost is relatively high.", "venue": "2020 8th IEEE International Conference on Mobile Cloud Computing, Services, and Engineering (MobileCloud)", "authors": ["Markku-Juhani O. Saarinen"], "year": 2020, "n_citations": 8}
{"id": 4098221, "s2_id": "912618aec056cf4691f52906773fa2eb0d2502fe", "title": "Synthesis of Optimal Resilient Control Strategies", "abstract": "Repair mechanisms are important within resilient systems to maintain the system in an operational state after an error occurred. Usually, constraints on the repair mechanisms are imposed, e.g., concerning the time or resources required (such as energy consumption or other kinds of costs). For systems modeled by Markov decision processes (MDPs), we introduce the concept of resilient schedulers, which represent control strategies guaranteeing that these constraints are always met within some given probability. Assigning rewards to the operational states of the system, we then aim towards resilient schedulers which maximize the long-run average reward, i.e., the expected mean payoff. We present a pseudo-polynomial algorithm that decides whether a resilient scheduler exists and if so, yields an optimal resilient scheduler. We show also that already the decision problem asking whether there exists a resilient scheduler is PSPACE-hard.", "venue": "ATVA", "authors": ["Christel  Baier", "Clemens  Dubslaff", "L\u2019ubo\u0161  Koren\u010diak", "Anton\u00edn  Ku\u010dera", "Vojt\u011bch  \u0158eh\u00e1k"], "year": 2017, "n_citations": 4}
{"id": 4099850, "s2_id": "f4b9b0b7b14dfefa89a7fe489a1b435162a8cb63", "title": "Performance Analysis of an Interference-Limited RIS-Aided Network", "abstract": "In this work, the performance of reconfigurable intelligent surface (RIS)-aided communication systems corrupted by the co-channel interference (CCI) at the destination is investigated. Assuming Rayleigh fading and equal-power CCI, we present the analysis for the outage probability (OP), average bit error rate (BER), and ergodic capacity. In addition, an asymptotic outage analysis is carried in order to obtain further insights. Our analysis shows that the number of reflecting elements as well as the number of interferers have a great impact on the overall system performance.", "venue": "ArXiv", "authors": ["Liang  Yang", "Yin  Yang", "Daniel Benevides da Costa", "Imene  Trigui"], "year": 2020, "n_citations": 3}
{"id": 4105857, "s2_id": "d315f789f612fa0122746fe195c2ac7dad3ea578", "title": "Building an Information System for a Distributed Testbed", "abstract": "This paper describes an information system designed to support the large volume of monitoring information generated by a distributed testbed. This monitoring information is produced by several subsystems and consists of status and performance data that needs to be federated, distributed, and stored in a timely and easy to use manner. Our approach differs from existing approaches because it federates and distributes information at a low architectural level via messaging; a natural match to many of the producers and consumers of information. In addition, a database is easily layered atop the messaging layer for consumers that want to query and search the information. Finally, a common language to represent information in all layers of the information system makes it significantly easier for users to consume information. Performance data shows that this approach meets the significant needs of FutureGrid and would meet the needs of other infrastructures.", "venue": "XSEDE '14", "authors": ["Warren  Smith", "Shava  Smallen"], "year": 2014, "n_citations": 2}
{"id": 4108362, "s2_id": "e1c8f117412ef3f0a78f28c126083f696a6ad599", "title": "High-Performance Parallel Graph Coloring with Strong Guarantees on Work, Depth, and Quality", "abstract": "We develop the first parallel graph coloring heuristics with strong theoretical guarantees on work and depth and coloring quality. The key idea is to design a relaxation of the vertex degeneracy order, a well-known graph theory concept, and to color vertices in the order dictated by this relaxation. This introduces a tunable amount of parallelism into the degeneracy ordering that is otherwise hard to parallelize. This simple idea enables significant benefits in several key aspects of graph coloring. For example, one of our algorithms ensures polylogarithmic depth and a bound on the number of used colors that is superior to all other parallelizable schemes, while maintaining work-efficiency. In addition to provable guarantees, the developed algorithms have competitive run-times for several real-world graphs, while almost always providing superior coloring quality. Our degeneracy ordering relaxation is of separate interest for algorithms outside the context of coloring.", "venue": "SC", "authors": ["Maciej  Besta", "Armon  Carigiet", "Zur  Vonarburg-Shmaria", "Kacper  Janda", "Lukas  Gianinazzi", "Torsten  Hoefler"], "year": 2020, "n_citations": 11}
{"id": 4108446, "s2_id": "7e654c21978d214018f8abd189b25167ee9310df", "title": "High-Level FPGA Accelerator Design for Structured-Mesh-Based Explicit Numerical Solvers", "abstract": "This paper presents a workflow for synthesizing near-optimal FPGA implementations of structured-mesh based stencil applications for explicit solvers. It leverages key characteristics of the application class and its computation-communication pattern and the architectural capabilities of the FPGA to accelerate solvers for high-performance computing applications. Key new features of the workflow are (1) the unification of standard state-of-the-art techniques with a number of high-gain optimizations such as batching and spatial blocking/tiling, motivated by increasing throughput for real-world workloads and (2) the development and use of a predictive analytical model to explore the design space, and obtain resource and performance estimates. Three representative applications are implemented using the design workflow on a Xilinx Alveo U280 FPGA, demonstrating near-optimal performance and over 85% predictive model accuracy. These are compared with equivalent highly-optimized implementations of the same applications on modern HPC-grade GPUs (Nvidia V100), analyzing time to solution, bandwidth, and energy consumption. Performance results indicate comparable runtimes with the V100 GPU, with over 2\u00d7 energy savings for the largest non-trivial application on the FPGA. Our investigation shows the challenges of achieving high performance on current generation FPGAs compared to traditional architectures. We discuss determinants for a given stencil code to be amenable to FPGA implementation, providing insights into the feasibility and profitability of a design and its resulting performance.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Kamalakkannan  Kamalavasan", "Gihan R. Mudalige", "I. Z. Reguly", "Suhaib A. Fahmy"], "year": 2021, "n_citations": 1}
{"id": 4110419, "s2_id": "4962ad0cfc719ea1a2f2a95f6dccbd6dd38239ec", "title": "Practical Bounds on Optimal Caching with Variable Object Sizes", "abstract": "Many recent caching systems aim to improve miss ratios, but there is no good sense among practitioners of how much further miss ratios can be improved. In other words, should the systems community continue working on this problem? Currently, there is no principled answer to this question. In practice, object sizes often vary by several orders of magnitude, where computing the optimal miss ratio (OPT) is known to be NP-hard. The few known results on caching with variable object sizes provide very weak bounds and are impractical to compute on traces of realistic length. We propose a new method to compute upper and lower bounds on OPT. Our key insight is to represent caching as a min-cost flow problem, hence we call our method the flow-based offline optimal (FOO). We prove that, under simple independence assumptions, FOO's bounds become tight as the number of objects goes to infinity. Indeed, FOO's error over 10M requests of production CDN and storage traces is negligible: at most 0.3%. FOO thus reveals, for the first time, the limits of caching with variable object sizes. While FOO is very accurate, it is computationally impractical on traces with hundreds of millions of requests. We therefore extend FOO to obtain more efficient bounds on OPT, which we call practical flow-based offline optimal (PFOO). We evaluate PFOO on several full production traces and use it to compare OPT to prior online policies. This analysis shows that current caching systems are in fact still far from optimal, suffering 11--43% more cache misses than OPT, whereas the best prior offline bounds suggest that there is essentially no room for improvement.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Daniel S. Berger", "Nathan  Beckmann", "Mor  Harchol-Balter"], "year": 2018, "n_citations": 34}
{"id": 4110832, "s2_id": "ccc65463198ee0a0db9b303a3dc903c762dbccaa", "title": "Adaptive Selection of Deep Learning Models on Embedded Systems", "abstract": "The recent ground-breaking advances in deep learning networks ( DNNs ) make them attractive for embedded systems. However, it can take a long time for DNNs to make an inference on resource-limited embedded devices. Offloading the computation into the cloud is often infeasible due to privacy concerns, high latency, or the lack of connectivity. As such, there is a critical need to find a way to effectively execute the DNN models locally on the devices. This paper presents an adaptive scheme to determine which DNN model to use for a given input, by considering the desired accuracy and inference time. Our approach employs machine learning to develop a predictive model to quickly select a pre-trained DNN to use for a given input and the optimization constraint. We achieve this by first training off-line a predictive model, and then use the learnt model to select a DNN model to use for new, unseen inputs. We apply our approach to the image classification task and evaluate it on a Jetson TX2 embedded deep learning platform using the ImageNet ILSVRC 2012 validation dataset. We consider a range of influential DNN models. Experimental results show that our approach achieves a 7.52% improvement in inference accuracy, and a 1.8x reduction in inference time over the most-capable single DNN model.", "venue": "ArXiv", "authors": ["Ben  Taylor", "Vicent Sanz Marco", "Willy  Wolff", "Yehia  El-khatib", "Zheng  Wang"], "year": 2018, "n_citations": 6}
{"id": 4112345, "s2_id": "cfdba5550ec5b3080d3fa4fd11124ac835fe7879", "title": "Network cache design under stationary requests: Exact analysis and Poisson approximation", "abstract": "Abstract The design of caching algorithms to maximize hit probability has been extensively studied. In this paper, we associate each content with a utility, which is a function of either the corresponding content hit rate. We formulate a cache optimization problem to maximize the sum of utilities over all contents under stationary and ergodic request processes. This problem is non-convex in general but we reformulate it as a convex optimization problem when the inter-request time (irt) distribution has a non-increasing hazard rate function. We provide explicit optimal solutions for some irt distributions. We formulate a reverse engineering based dual implementation of LRU under stationary arrivals. We also propose online algorithms that can be implemented using limited information and use a discrete time Lyapunov technique (DTLT) to correctly characterize their stability. Informed by these results, we further propose lightweight Poisson approximate online algorithms that are accurate and efficient in achieving optimal hit rates. We compare the solutions of the hit-rate based (HRB) and hit-probability based (HPB) problems. We find that online algorithms that solve HRB are more robust than online HPB algorithms.", "venue": "Comput. Networks", "authors": ["Nitish  Panigrahy", "Jian  Li", "Donald F. Towsley", "Christopher V. Hollot"], "year": 2020, "n_citations": 2}
{"id": 4116813, "s2_id": "b4631708f3c48b246d0ca38990da401369da5486", "title": "Defining Big Data Analytics Benchmarks for Next Generation Supercomputers", "abstract": "The design and construction of high performance computing (HPC) systems relies on exhaustive performance analysis and benchmarking. Traditionally this activity has been geared exclusively towards simulation scientists, who, unsurprisingly, have been the primary customers of HPC for decades. However, there is a large and growing volume of data science work that requires these large scale resources, and as such the calls for inclusion and investments in data for HPC have been increasing. So when designing a next generation HPC platform, it is necessary to have HPC-amenable big data analytics benchmarks. In this paper, we propose a set of big data analytics benchmarks and sample codes designed for testing the capabilities of current and next generation supercomputers.", "venue": "ArXiv", "authors": ["Drew  Schmidt", "Junqi  Yin", "Michael A. Matheson", "Bronson  Messer", "Mallikarjun  Shankar"], "year": 2018, "n_citations": 2}
{"id": 4120425, "s2_id": "8bd67c49ab13dbd92c6421937913d2ca3e96c1b0", "title": "Performance of a multiple-access DCSK-CC system over Nakagami-m fading channels", "abstract": "In this paper1, we propose a novel cooperative scheme to enhance the performance of multiple-access (MA) differential-chaos-shift-keying (DCSK) systems. We provide the bit-error-rate (BER) performance and throughput analyses for the new system with a decode-and-forward (DF) protocol over Nakagami-m fading channels. Our simulated results not only show that this system significantly improves the BER performance as compared to the existing DCSK non-cooperative (DCSK-NC) system and the multiple-input multiple-output DCSK (MIMO-DCSK) system, but also verify the theoretical analyses. Furthermore, we show that the throughput of this system approximately equals that of the DCSK-NC system, both of which have prominent improvements over the MIMO-DCSK system. We thus believe that the proposed system can be a good framework for chaos-modulation-based wireless communications.", "venue": "2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)", "authors": ["Yi  Fang", "Lin  Wang", "Guanrong  Chen"], "year": 2013, "n_citations": 16}
{"id": 4126924, "s2_id": "e3664827df3e3350bf6196ef5c47dac87a283aca", "title": "Quantized Neural Network Inference with Precision Batching", "abstract": "We present PrecisionBatching, a quantized inference algorithm for speeding up neural network execution on traditional hardware platforms at low bitwidths without the need for retraining or recalibration. PrecisionBatching decomposes a neural network into individual bitlayers and accumulates them using fast 1-bit operations while maintaining activations in full precision. PrecisionBatching not only facilitates quantized inference at low bitwidths (< 8 bits) without the need for retraining/recalibration, but also 1) enables traditional hardware platforms the ability to realize inference speedups at a finer granularity of quantization (e.g: 1-16 bit execution) and 2) allows accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers to accumulate as a tunable parameter. Across a variety of applications (MNIST, language modeling, natural language inference) and neural network architectures (fully connected, RNN, LSTM), PrecisionBatching yields end-to-end speedups of over 8x on a GPU within a < 1% error margin of the full precision baseline, outperforming traditional 8-bit quantized inference by over 1.5x-2x at the same error tolerance.", "venue": "ArXiv", "authors": ["Maximilian  Lam", "Zachary  Yedidia", "Colby  Banbury", "Vijay Janapa Reddi"], "year": 2020, "n_citations": 1}
{"id": 4130524, "s2_id": "f9df45770028a591d2c6dfa8b09d7582769c99d8", "title": "Optimal Asynchronous Dynamic Policies in Energy-Efficient Data Centers", "abstract": "In this paper, we use a Markov decision process to find optimal asynchronous policy of an energy-efficient data center with two groups of heterogeneous servers, a finite buffer, and a fast setup process at sleep state. Servers in Group 1 always work. Servers in Group 2 may either work or sleep, and a fast setup process occurs when server's states are changed from sleep to work. In such a data center, an asynchronous dynamic policy is designed as two sub-policies: The setup policy and the sleep policy, which determine the switch rule between the work and sleep states for the servers in Group 2. To analyze the optimal asynchronous dynamic policy, we apply the Markov decision process to establish a policy-based Poisson equation, which provides expression for the unique solution of the performance potential by means of the RG-factorization. Based on this, we can characterize the monotonicity and optimality of the long-run average profit of the data center with respect to the asynchronous dynamic policy under different service prices. Furthermore, we prove that the bang-bang control is always optimal for this optimization problem, and supports a threshold-type dynamic control in the energy-efficient data center. We hope that the methodology and results derived in this paper can shed light to the study of more general energy-efficient data centers.", "venue": "ArXiv", "authors": ["Jing-Yu  Ma", "Quan-Lin  Li", "Li  Xia"], "year": 2019, "n_citations": 3}
{"id": 4131096, "s2_id": "d293197f7f1d18be335e7c706498693f4aad68ac", "title": "PAM: When Overloaded, Push Your Neighbor Aside!", "abstract": "Recently SmartNICs are widely used to accelerate service chains in NFV. However, when the SmartNIC is overloaded, casually migrating vNFs away from SmartNIC to CPU may lead to additional packet transmissions between SmartNIC and CPU. To address this problem, we present PAM, push aside migration to effectively alleviate the hot spot on SmartNIC with no performance overhead. Our key novelty is to push vNFs on the border of SmartNIC and CPU aside to release resources for the bottleneck vNF. Evaluation shows that PAM could efficiently alleviate the hot spot on SmartNIC and generate a service chain with much lower latency compared with the naive solution.", "venue": "SIGCOMM Posters and Demos", "authors": ["Zili  Meng", "Jun  Bi", "Chen  Sun", "Shuhe  Wang", "Minhu  Wang", "Hongxin  Hu"], "year": 2018, "n_citations": 3}
{"id": 4131139, "s2_id": "3dc822204575cc5a50a96996c607b73457aaa016", "title": "ReCA: An Efficient Reconfigurable Cache Architecture for Storage Systems with Online Workload Characterization", "abstract": "In recent years, Solid-State Drives (SSDs) have gained tremendous attention in computing and storage systems due to significant performance improvement over Hard Disk Drives (HDDs). The cost per capacity of SSDs, however, prevents them from entirely replacing HDDs in such systems. One approach to effectively take advantage of SSDs is to use them as a caching layer to store performance critical data blocks in order to reduce the number of accesses to HDD-based disk subsystem. Due to characteristics of Flash-based SSDs such as limited write endurance and long latency on write operations, employing caching algorithms at the Operating System (OS) level necessitates to take such characteristics into consideration. Previous OS-level caching techniques are optimized towards only one type of application, which affects both generality and applicability. In addition, they are not adaptive when the workload pattern changes over time. This paper presents an efficient Reconfigurable Cache Architecture (ReCA) for storage systems using a comprehensive workload characterization to find an optimal cache configuration for I/O intensive applications. For this purpose, we first investigate various types of I/O workloads and classify them into five major classes. Based on this characterization, an optimal cache configuration is presented for each class of workloads. Then, using the main features of each class, we continuously monitor the characteristics of an application during system runtime and the cache organization is reconfigured if the application changes from one class to another class of workloads. The cache reconfiguration is done online and workload classes can be extended to emerging I/O workloads in order to maintain its efficiency with the characteristics of I/O requests. Experimental results obtained by implementing ReCA in a 4U rackmount server with SATA 6Gb/s disk interfaces running Linux 3.17.0 show that the proposed architecture improves performance and lifetime up to 24 and 33 percent, respectively.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Reza  Salkhordeh", "Shahriar  Ebrahimi", "Hossein  Asadi"], "year": 2018, "n_citations": 21}
{"id": 4132876, "s2_id": "1d2c8d3d022ef82e2015e96b2d0a7726c9f2ecd2", "title": "Customized Monte Carlo Tree Search for LLVM/Polly's Composable Loop Optimization Transformations", "abstract": "Polly is the LLVM project's polyhedral loop optimizer. Recent user-directed loop transformation pragmas were proposed based on LLVM/Clang and Polly. The search space exposed by the transformation pragmas is a tree, wherein each node represents a specific combination of loop transformations that can be applied to the code resulting from the parent node's loop transformations. To find the best combination of these loop transformations, we have developed a search algorithm based on Monte Carlo tree search (MCTS). The algorithm consists of two phases: exploring loop transformations at different depths of the tree to identify promising regions in the tree search space and exploiting those regions by performing a local search. Moreover, a restart mechanism is used to avoid the MCTS getting trapped in a local solution. The best and worst solutions are transferred from the previous phases of the restarts to leverage the search history. We compare our approach with breadth-first, beam, global greedy, and random search methods using PolyBench benchmarks and ECP proxy applications. Experimental results show that our MCTS algorithm finds pragma combinations with a speedup of 2.3x over Polly's heuristic optimizations on average.", "venue": "2021 International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)", "authors": ["Jaehoon  Koo", "Prasanna  Balaprakash", "Michael  Kruse", "Xingfu  Wu", "Paul  Hovland", "Mary  Hall"], "year": 2021, "n_citations": 0}
{"id": 4134262, "s2_id": "a46cbe2fa639d4b9c359d987f0ede77d3dc1b718", "title": "Using hardware performance counters to speed up autotuning convergence on GPUs", "abstract": "Nowadays, GPU accelerators are commonly used to speed up general-purpose computing tasks on a variety of hardware. However, due to the diversity of GPU architectures and processed data, optimization of codes for a particular type of hardware and specific data characteristics can be extremely challenging. The autotuning of performance-relevant sourcecode parameters allows for automatic optimization of applications and keeps their performance portable. Although the autotuning process typically results in code speed-up, searching the tuning space can bring unacceptable overhead if (i) the tuning space is vast and full of poorly-performing implementations, or (ii) the autotuning process has to be repeated frequently because of changes in processed data or migration to different hardware. In this paper, we introduce a novel method for searching generic tuning spaces. The tuning spaces can contain tuning parameters changing any user-defined property of the source code. The method takes advantage of collecting hardware performance counters (also known as profiling counters) during empirical tuning. Those counters are used to navigate the searching process towards faster implementations. The method requires the tuning space to be sampled on any GPU. It builds a problem-specific model, which can be used during autotuning on various, even previously unseen inputs or GPUs. Using a set of five benchmarks, we experimentally demonstrate that our method can speed up autotuning when an application needs to be ported to different hardware or when it needs to process data with different characteristics. We also compared our method to state of the art and show that our method is superior in terms of the number of searching steps and typically outperforms other searches in terms of convergence time.", "venue": "J. Parallel Distributed Comput.", "authors": ["Jiri  Filipovic", "Jana  Hozzov\u00e1", "Amin  Nezarat", "Jaroslav  Olha", "Filip  Petrovic"], "year": 2022, "n_citations": 1}
{"id": 4134392, "s2_id": "f5ac6c7a105c325a4f62600e42b49626e88cffd9", "title": "EdgeBench: A Workflow-based Benchmark for Edge Computing", "abstract": "Edge computing has been developed to utilize multiple tiers of resources for privacy, cost and Quality of Service (QoS) reasons. Edge workloads have the characteristics of data-driven and latency-sensitive. Because of this, edge systems have developed to be both heterogeneous and distributed. The unique characteristics of edge workloads and edge systems have motivated EdgeBench, a workflow-based benchmark aims to provide the ability to explore the full design space of edge workloads and edge systems. EdgeBench is both customizable and representative. It allows users to customize the workflow logic of edge workloads, the data storage backends, and the distribution of the individual workflow stages to different computing tiers. To illustrate the usability of EdgeBench, we also implements two representative edge workflows, a video analytics workflow and an IoT hub workflow that represents two distinct but common edge workloads. Both workflows are evaluated using the workflow-level and function-level metrics reported by EdgeBench to illustrate both the performance bottlenecks of the edge systems and the edge workloads.", "venue": "ArXiv", "authors": ["Qirui  Yang", "Runyu  Jin", "Nabil  Gandhi", "Xiongzi  Ge", "Hoda Aghaei Khouzani", "Ming  Zhao"], "year": 2020, "n_citations": 1}
{"id": 4138704, "s2_id": "d2f6899dbdacbb590b3c4de6160aef957e67f088", "title": "Parameter Sensitivity Analysis of the SparTen High Performance Sparse Tensor Decomposition Software: Extended Analysis", "abstract": "Tensor decomposition models play an increasingly important role in modern data science applications. One problem of particular interest is fitting a low-rank Canonical Polyadic (CP) tensor decomposition model when the tensor has sparse structure and the tensor elements are nonnegative count data. SparTen is a high-performance C++ library which computes a low-rank decomposition using different solvers: a first-order quasi-Newton or a second-order damped Newton method, along with the appropriate choice of runtime parameters. Since default parameters in SparTen are tuned to experimental results in prior published work on a single real-world dataset conducted using MATLAB implementations of these methods, it remains unclear if the parameter defaults in SparTen are appropriate for general tensor data. Furthermore, it is unknown how sensitive algorithm convergence is to changes in the input parameter values. This report addresses these unresolved issues with large-scale experimentation on three benchmark tensor data sets. Experiments were conducted on several different CPU architectures and replicated with many initial states to establish generalized profiles of algorithm convergence behavior.", "venue": "ArXiv", "authors": ["Jeremy M. Myers", "Daniel M. Dunlavy", "Keita  Teranishi", "D. S. Hollman"], "year": 2020, "n_citations": 0}
{"id": 4140165, "s2_id": "32c9958e859a3804836e642d123cb09019cd2c34", "title": "On the Secrecy Rate of Spatial Modulation-Based Indoor Visible Light Communications", "abstract": "In this paper, we investigate the physical-layer security for a spatial modulation (SM)-based indoor visible light communication (VLC) system, which includes multiple transmitters, a legitimate receiver, and a passive eavesdropper (Eve). At the transmitters, the SM scheme is employed, i.e., only one transmitter is active at each time instant. To choose the active transmitter, a uniform selection (US) scheme is utilized. Two scenarios are considered: one is with non-negativity and average optical intensity constraints and the other is with non-negativity, average optical intensity, and peak optical intensity constraints. Then, lower and upper bounds on the secrecy rate are derived for these two scenarios. Besides, the asymptotic behaviors for the derived secrecy rate bounds at high signal-to-noise ratio (SNR) are analyzed. To further improve the secrecy performance, a channel adaptive selection (CAS) scheme and a greedy selection (GS) scheme are proposed to select the active transmitter. Numerical results show that the lower and upper bounds of the secrecy rate are tight. At high SNR, small asymptotic performance gaps exist between the derived lower and the upper bounds. Moreover, the proposed GS scheme has the best performance, followed by the CAS scheme and the US scheme.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Jin-Yuan  Wang", "Hong  Ge", "Min  Lin", "Jun-Bo  Wang", "Jianxin  Dai", "Mohamed-Slim  Alouini"], "year": 2019, "n_citations": 17}
{"id": 4141731, "s2_id": "18d20199627793cdb587909dcfc53fba9bd0784c", "title": "Performance Analysis of a Heterogeneous Traffic Scheduler using Large Deviation Principle", "abstract": "In this paper, we study the stability of light traffic achieved by a scheduling algorithm which is suitable for heterogeneous traffic networks. Since analyzing a scheduling algorithm is intractable using the conventional mathematical tool, our goal is to minimize the largest queue-overflow probability achieved by the algorithm. In the large deviation setting, this problem is equivalent to maximizing the asymptotic decay rate of the largest queue-overflow probability. We first derive an upper bound on the decay rate of the queue overflow probability as the queue overflow threshold approaches infinity. Then, we study several structural properties of the minimum-cost-path to overflow of the queue with the largest length, which is basically equivalent to the decay rate of the largest queue-overflow probability. Given these properties, we prove that the queue with the largest length follows a sample path with linear increment. For certain parameter value, the scheduling algorithm is asymptotically optimal in reducing the largest queue length. Through numerical results, we have shown the large deviation properties of the queue length typically used in practice while varying one parameter of the algorithm.", "venue": "ArXiv", "authors": ["Rukhsana  Ruby", "Victor C. M. Leung"], "year": 2015, "n_citations": 0}
{"id": 4147960, "s2_id": "4c3f26d367d88e3888d245598a38d3d27be4365d", "title": "Towards the Tradeoff Between Service Performance and Information Freshness", "abstract": "The last decade has witnessed an unprecedented growth in the demand for data-driven real-time services. These services are fueled by emerging applications that require rapidly injecting data streams and computing updated analytics results in real-time (or near-real-time). In many of such applications, the computing resources are often shared for processing both updates from information sources and queries from end users. This requires joint scheduling of updates and queries because the service provider needs to make a critical decision upon receiving a user query: either it responds immediately with currently available but possibly stale information, or it first processes new updates and then responds with fresher information. Hence, the tradeoff between service performance (e.g., response time) and information freshness naturally arises in this context. To that end, we propose a simple single-server two-queue model that captures the coupled scheduling of updates and queries and aim to design scheduling policies that can properly address the important tradeoff between performance and freshness. Specifically, we consider the response time as a performance metric and the Age of Information (AoI) as a freshness metric. After demonstrating the limitations of the simplest First-Come-First-Served (FCFS) policy, we propose two threshold-based policies: the Query-k policy that prioritizes queries and the Update-k policy that prioritizes updates. Then, we rigorously analyze both the response time and the Peak AoI (PAoI) of the threshold-based policies. Further, we propose the Joint-(M, N) policy, which allows flexibly prioritizing updates or queries through choosing different values of two thresholds M and N. Finally, we conduct simulations to evaluate the response time and the PAoI of the proposed policies. The results show that our proposed threshold-based policies can effectively control the balance between performance and freshness.", "venue": "ICC 2019 - 2019 IEEE International Conference on Communications (ICC)", "authors": ["Zhongdong  Liu", "Bo  Ji"], "year": 2019, "n_citations": 6}
{"id": 4150567, "s2_id": "d0ef199ecdf0c2da3abcfa362f29906eb096d1e2", "title": "AUGURY: A Time Series Based Application for the Analysis and Forecasting of System and Network Performance Metrics", "abstract": "This paper presents AUGURY, an application for the analysis of monitoring data from computers, servers or cloud infrastructures. The analysis is based on the extraction of patterns and trends from historical data, using elements of time-series analysis. The purpose of AUGURY is to aid a server administrator by forecasting the behaviour and resource usage of specific applications and in presenting a status report in a concise manner. AUGURY provides tools for identifying network traffic congestion and peak usage times, and for making memory usage projections. The application data processing specialises in two tasks: the parametrisation of the memory usage of individual applications and the extraction of the seasonal component from network traffic data. AUGURY uses a different underlying assumption for each of these two tasks. With respect to the memory usage, a limited number of single-valued parameters are assumed to be sufficient to parameterize any application being hosted on the server. Regarding the network traffic data, long-term patterns, such as hourly or daily exist and are being induced by work-time schedules and automatised administrative jobs. In this paper, the implementation of each of the two tasks is presented, tested using locally-generated data, and applied to data from weather forecasting applications hosted on a web server. This data is used to demonstrate the insight that AUGURY can add to the monitoring of server and cloud infrastructures.", "venue": "2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)", "authors": ["Nicolas  Gutierrez", "Manuela  Wiesinger-Widi"], "year": 2016, "n_citations": 2}
{"id": 4151337, "s2_id": "21f52b461a597f257da921ebbc1d5a387c4078c7", "title": "Power and Energy-efficiency Roofline Model for GPUs", "abstract": "Energy consumption has been a great deal of concern in recent years and developers need to take energy-efficiency into account when they design algorithms. Their design needs to be energy-efficient and low-power while it tries to achieve attainable performance provided by underlying hardware. However, different optimization techniques have different effects on power and energy-efficiency and a visual model would assist in the selection process. \nIn this paper, we extended the roofline model and provided a visual representation of optimization strategies for power consumption. Our model is composed of various ceilings regarding each strategy we included in our models. One roofline model for computational performance and one for memory performance is introduced. We assembled our models based on some optimization strategies for two widespread GPUs from NVIDIA: Geforce GTX 970 and Tesla K80.", "venue": "ArXiv", "authors": ["Millad  Ghane", "Jeffrey M. Larkin", "Larry  Shi", "Sunita  Chandrasekaran", "Margaret S. Cheung"], "year": 2018, "n_citations": 4}
{"id": 4151450, "s2_id": "c19e8fb462213b4df3701d7c9c41b8022305ffbc", "title": "Highly Parallel Sparse Matrix-Matrix Multiplication", "abstract": "Generalized sparse matrix-matrix multiplication is a key primitive for many high performance graph algorithms as well as some linear solvers such as multigrid. We present the first parallel algorithms that achieve increasing speed ups for an unbounded number of processors. Our algorithms are based on two-dimensional block distribution of sparse matrices where serial sections use a novel hypersparse kernel for scalability. We give a state-of-the-art MPI implementation of one of our algorithms. Our experiments show scaling up to thousands of processors on a variety of test scenarios.", "venue": "ArXiv", "authors": ["Aydin  Bulu\u00e7", "John R. Gilbert"], "year": 2010, "n_citations": 29}
{"id": 4151847, "s2_id": "8744f47a6801ceffb18c2bf973f1622fd587b1ec", "title": "Optimal Content Replication and Request Matching in Large Caching Systems", "abstract": "We consider models of content delivery networks in which the servers are constrained by two main resources: memory and bandwidth. In such systems, the throughput crucially depends on how contents are replicated across servers and how the requests of specific contents are matched to servers storing those contents. In this paper, we first formulate the problem of computing the optimal replication policy which if combined with the optimal matching policy maximizes the throughput of the caching system in the stationary regime. It is shown that computing the optimal replication policy for a given system is an NP-hard problem. A greedy replication scheme is proposed and it is shown that the scheme provides a constant factor approximation guarantee. We then propose a simple randomized matching scheme which avoids the problem of interruption in service of the ongoing requests due to re-assignment or repacking of the existing requests in the optimal matching policy. The dynamics of the caching system is analyzed under the combination of proposed replication and matching schemes. We study a limiting regime, where the number of servers and the arrival rates of the contents are scaled proportionally, and show that the proposed policies achieve asymptotic optimality. Extensive simulation results are presented to evaluate the performance of different policies and study the behavior of the caching system under different service time distributions of the requests.", "venue": "IEEE INFOCOM 2018 - IEEE Conference on Computer Communications", "authors": ["Arpan  Mukhopadhyay", "Nidhi  Hegde", "Marc  Lelarge"], "year": 2018, "n_citations": 3}
{"id": 4155397, "s2_id": "2ae7c9a89c7f6c105b68343dda9a0159eb2f118d", "title": "RepFlow: Minimizing flow completion times with replicated flows in data centers", "abstract": "Short TCP flows that are critical for many interactive applications in data centers are plagued by long flows and head-of-line blocking in switches. Hash-based load balancing schemes such as ECMP aggravate the matter and result in long-tailed flow completion times (FCT). Previous work on reducing FCT usually requires custom switch hardware and/or protocol changes. We propose RepFlow, a simple yet practically effective approach that replicates each short flow to reduce the completion times, without any change to switches or host kernels. With ECMP the original and replicated flows traverse distinct paths with different congestion levels, thereby reducing the probability of having long queueing delay. We develop a simple analytical model to demonstrate the potential improvement. Further, we conduct NS-3 simulations and Mininet implementation and show that RepFlow provides 50%-70% speedup in both mean and 99-th percentile FCT for all loads, and offers near-optimal FCT when used with DCTCP.", "venue": "IEEE INFOCOM 2014 - IEEE Conference on Computer Communications", "authors": ["Hong  Xu", "Baochun  Li"], "year": 2014, "n_citations": 88}
{"id": 4159174, "s2_id": "91e0e497383914e4a54d19e1771bb6975bc2d70d", "title": "Persistent Spread Measurement for Big Network Data Based on Register Intersection", "abstract": "Persistent spread measurement is to count the number of distinct elements that persist in each network flow for predefined time periods. It has many practical applications, including detecting long-term stealthy network activities in the background of normal-user activities, such as stealthy DDoS attack, stealthy network scan, or faked network trend, which cannot be detected by traditional flow cardinality measurement. With big network data, one challenge is to measure the persistent spreads of a massive number of flows without incurring too much memory overhead as such measurement may be performed at the line speed by network processors with fast but small on-chip memory. We propose a highly compact Virtual Intersection HyperLogLog (VI-HLL) architecture for this purpose. It achieves far better memory efficiency than the best prior work of V-Bitmap, and in the meantime drastically extends the measurement range. Theoretical analysis and extensive experiments demonstrate that VI-HLL provides good measurement accuracy even in very tight memory space of less than 1 bit per flow.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["You  Zhou", "Yian  Zhou", "Min  Chen", "Shigang  Chen"], "year": 2017, "n_citations": 10}
{"id": 4159835, "s2_id": "044d43997ffd2286171734e99dccb8d97ab27c62", "title": "Experimental Performances Analysis of Load Balancing Algorithms in IEEE 802.11", "abstract": "In IEEE 802.11, load balancing algorithms (LBA) consider only the associated stations to balance th e load of the available access points (APs). However, althoug h the APs are balanced, it causes a bad situation if the AP h as a lower signal length (SNR) less than the neighbor APs. So, balance the load and associate one mobile station to an acc ess point without care about the signal to noise ratio (SNR) of the AP cause possibly an unforeseen QoS; such as the bit r ate, the end to end delay, the packet loss, \u2026 In this way, w e study an improvement load balancing algorithm with SNR integration at the selection policy.", "venue": "ArXiv", "authors": ["Salah  Hamdi", "Adel  Soudani", "Rached  Tourki"], "year": 2009, "n_citations": 3}
{"id": 4160164, "s2_id": "89cc6e1df11b374c3de49cb953d7d4c0b9ecb7b3", "title": "Leveraging Architectural Support of Three Page Sizes with Trident", "abstract": "Large pages are commonly deployed to reduce address translation overheads for big-memory workloads. Modern x86-64 processors from Intel and AMD support two large page sizes -- 1GB and 2MB. However, previous works on large pages have primarily focused on 2MB pages, partly due to lack of substantial evidence on the profitability of 1GB pages to real-world applications. We argue that in fact, inadequate system software support is responsible for a decade of underutilized hardware support for 1GB pages. \nThrough extensive experimentation on a real system, we demonstrate that 1GB pages can improve performance over 2MB pages, and when used in tandem with 2MB pages for an important set of applications; the support for the latter is crucial but missing in current systems. Our design and implementation of \\trident{} in Linux fully exploit hardware supported large pages by dynamically and transparently allocating 1GB, 2MB, and 4KB pages as deemed suitable. \\trident{} speeds up eight memory-intensive applications by {$18\\%$}, on average, over Linux's use of 2MB pages. We also propose \\tridentpv{}, an extension to \\trident{} that effectively virtualizes 1GB pages via copy-less promotion and compaction in the guest OS. Overall, this paper shows that even GB-sized pages have considerable practical significance with adequate software enablement, in turn motivating architects to continue investing/innovating in large pages.", "venue": "ArXiv", "authors": ["Venkat Sri Sai Ram", "Ashish  Panwar", "Arkaprava  Basu"], "year": 2020, "n_citations": 0}
{"id": 4160495, "s2_id": "d4a95c5cd65f5d953aceea06c62522703d4bdb28", "title": "heSRPT: Parallel Scheduling to Minimize Mean Slowdown", "abstract": "Abstract Modern data centers serve workloads which are capable of exploiting parallelism. When a job parallelizes across multiple servers it will complete more quickly, but jobs receive diminishing returns from being allocated additional servers. Because allocating multiple servers to a single job is inefficient, it is unclear how best to allocate a fixed number of servers between many parallelizable jobs. This paper provides the first optimal allocation policy for minimizing the mean slowdown of parallelizable jobs of known size when all jobs are present at time 0. Our policy provides a simple closed form formula for the optimal allocations at every moment in time. Minimizing mean slowdown usually requires favoring short jobs over long ones (as in the SRPT policy). However, because parallelizable jobs have sublinear speedup functions, system efficiency is also an issue. System efficiency is maximized by giving equal allocations to all jobs and thus competes with the goal of prioritizing small jobs. Our optimal policy, high-efficiency SRPT (heSRPT), balances these competing goals. heSRPT completes jobs according to their size order, but maintains overall system efficiency by allocating some servers to each job at every moment in time. Our results generalize to also provide the optimal allocation policy with respect to mean flow time. Finally, we consider the online case where jobs arrive to the system over time. While optimizing mean slowdown in the online setting is even more difficult, we find that heSRPT provides an excellent heuristic policy for the online setting. In fact, our simulations show that heSRPT significantly outperforms state-of-the-art allocation policies for parallelizable jobs.", "venue": "Perform. Evaluation", "authors": ["Benjamin  Berg", "Rein  Vesilo", "Mor  Harchol-Balter"], "year": 2020, "n_citations": 1}
{"id": 4162546, "s2_id": "317145b910b2450ce6d1a6d74454b9e5092e6fd6", "title": "Age of information without service preemption", "abstract": "When designing a message transmission system, from the point of view of making sure that the information transmitted is as fresh as possible, two rules of thumb seem reasonable: use small buffers and adopt a last-in-first-out policy. In this paper, we measure freshness of information using the \u201cage of information\u201d performance measure. Considering it as a stochastic process operating in a stationary regime, we compute not just the first moment but the whole marginal distribution of the age of information (something important in applications) for two well-performing systems. In neither case do we allow for preemption of the message being processed because this may be difficult to implement in practice. We assume that the arrival process is Poisson and that the messages have independent sizes (service times) with common distribution. We use Palm and Markov-renewal theory to derive explicit results for Laplace transforms. In particular, this approach can be used to analyze more complex last-in-first-out systems with larger buffer sizes.", "venue": "ArXiv", "authors": ["George  Kesidis", "Takis  Konstantopoulos", "Michael A. Zazanis"], "year": 2021, "n_citations": 3}
{"id": 4169129, "s2_id": "1bcb40f861549414ac007ef1bcf0e00c793652c8", "title": "Characterizing data analysis workloads in data centers", "abstract": "As the amount of data explodes rapidly, more and more corporations are using data centers to make effective decisions and gain a competitive edge. Data analysis applications play a significant role in data centers, and hence it has became increasingly important to understand their behaviors in order to further improve the performance of data center computer systems. In this paper, after investigating three most important application domains in terms of page views and daily visitors, we choose eleven representative data analysis workloads and characterize their micro-architectural characteristics by using hardware performance counters, in order to understand the impacts and implications of data analysis workloads on the systems equipped with modern superscalar out-of-order processors. Our study on the workloads reveals that data analysis applications share many inherent characteristics, which place them in a different class from desktop (SPEC CPU2006), HPC (HPCC), and service workloads, including traditional server workloads (SPECweb200S) and scale-out service workloads (four among six benchmarks in CloudSuite), and accordingly we give several recommendations for architecture and system optimizations. On the basis of our workload characterization work, we released a benchmark suite named DCBench for typical datacenter workloads, including data analysis and service workloads, with an open-source license on our project home page on http://prof.ict.ac.cnIDCBench. We hope that DCBench is helpful for performing architecture and small-to-medium scale system researches for datacenter computing.", "venue": "2013 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Zhen  Jia", "Lei  Wang", "Jianfeng  Zhan", "Lixin  Zhang", "Chunjie  Luo"], "year": 2013, "n_citations": 116}
{"id": 4169148, "s2_id": "e7b5fafaf39f2db6ffd2f0f4c8b9c0913005197f", "title": "Performance evaluation of distributed computing environments with Hadoop and Spark frameworks", "abstract": "Recently, due to rapid development of information and communication technologies, the data are created and consumed in the avalanche way. Distributed computing create preconditions for analyzing and processing such Big Data by distributing the computations among a number of compute nodes. In this work, performance of distributed computing environments on the basis of Hadoop and Spark frameworks is estimated for real and virtual versions of clusters. As a test task, we chose the classic use case of word counting in texts of various sizes. It was found that the running times grow very fast with the dataset size and faster than a power function even. As to the real and virtual versions of cluster implementations, this tendency is the similar for both Hadoop and Spark frameworks. Moreover, speedup values decrease significantly with the growth of dataset size, especially for virtual version of cluster configuration. The problem of growing data generated by IoT and multimodal (visual, sound, tactile, neuro and brain-computing, muscle and eye tracking, etc.) interaction channels is presented. In the context of this problem, the current observations as to the running times and speedup on Hadoop and Spark frameworks in real and virtual cluster configurations can be very useful for the proper scaling-up and efficient job management, especially for machine learning and Deep Learning applications, where Big Data are widely present.", "venue": "2017 IEEE International Young Scientists Forum on Applied Physics and Engineering (YSF)", "authors": ["Vladyslav  Taran", "Oleg  Alienin", "Sergii  Stirenko", "Anis  Rojbi", "Yuri  Gordienko"], "year": 2017, "n_citations": 13}
{"id": 4174830, "s2_id": "987a33dd1896c1306473247217be04de866fa5c9", "title": "Predicting the confirmation time of Bitcoin transactions", "abstract": "We study the probabilistic distribution of the confirmation time of Bitcoin transactions, conditional on the current memory pool (i.e., the queue of transactions awaiting confirmation). The results of this paper are particularly interesting for users that want to make a Bitcoin transaction during `heavy-traffic situations', when the transaction demand exceeds the block capacity. In such situations, Bitcoin users tend to bid up the transaction fees, in order to gain priority over other users that pay a lower fee. We argue that the time until a Bitcoin transaction is confirmed can be modelled as a particular stochastic fluid queueing process (to be precise: a Cramer-Lundberg process). We approximate the queueing process in two different ways. The first approach leads to a lower bound on the confirmation probability, which becomes increasingly tight as traffic decreases. The second approach relies on a diffusion approximation with a continuity correction, which becomes increasingly accurate as traffic intensifies. The accuracy of the approximations under different traffic loads are evaluated in a simulation study.", "venue": "ArXiv", "authors": ["David T. Koops"], "year": 2018, "n_citations": 15}
{"id": 4180468, "s2_id": "4506c9cd34a0b6f09fe23a3d9be0bc0962d14540", "title": "ZeRO-infinity: breaking the GPU memory wall for extreme scale deep learning", "abstract": "In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model. In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs (40% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed 1.", "venue": "SC", "authors": ["Samyam  Rajbhandari", "Olatunji  Ruwase", "Jeff  Rasley", "Shaden  Smith", "Yuxiong  He"], "year": 2021, "n_citations": 24}
{"id": 4181780, "s2_id": "c724f501acd836c1cd617b60927659720b3aea0d", "title": "Accurate, Efficient and Scalable Graph Embedding", "abstract": "The Graph Convolutional Network (GCN) model and its variants are powerful graph embedding tools for facilitating classification and clustering on graphs. However, a major challenge is to reduce the complexity of layered GCNs and make them parallelizable and scalable on very large graphs \u2014 state-of the art techniques are unable to achieve scalability without losing accuracy and efficiency. In this paper, we propose novel parallelization techniques for graph sampling-based GCNs that achieve superior scalable performance on very large graphs without compromising accuracy. Specifically, our GCN guarantees work-efficient training and produces order of magnitude savings in computation and communication. To scale GCN training on tightly-coupled shared memory systems, we develop parallelization strategies for the key steps in training: For the graph sampling step, we exploit parallelism within and across multiple sampling instances, and devise an efficient data structure for concurrent accesses that provides theoretical guarantee of near-linear speedup with number of processing units. For the feature propagation step within the sampled graph, we improve cache utilization and reduce DRAM communication by data partitioning. We prove that our partitioning strategy is a 2-approximation for minimizing the communication time compared to the optimal strategy. We demonstrate that our parallel graph embedding outperforms state-of-the-art methods in scalability (with respect to number of processors, graph size and GCN model size), efficiency and accuracy on several large datasets. On a 40-core Xeon platform, our parallel training achieves 64x speedup (with AVX) in the sampling step and 25x speedup in the feature propagation step, compared to the serial implementation, resulting in a net speedup of 21x. Our scalable algorithm enables deeper GCN, as demonstrated by 1306x speedup on a 3-layer GCN compared to Tensorflow implementation of state-of-the-art.", "venue": "2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Hanqing  Zeng", "Hongkuan  Zhou", "Ajitesh  Srivastava", "Rajgopal  Kannan", "Viktor K. Prasanna"], "year": 2019, "n_citations": 31}
{"id": 4184222, "s2_id": "7ecebe0b21e99d18d0e977573a9c1582910f5307", "title": "How Fast Can We Insert? An Empirical Performance Evaluation of Apache Kafka", "abstract": "Message brokers see widespread adoption in modern IT landscapes, with Apache Kafka being one of the most employed platforms. These systems feature well-defined APIs for use and configuration and present flexible solutions for various data storage scenarios. Their ability to scale horizontally enables users to adapt to growing data volumes and changing environments. However, one of the main challenges concerning message brokers is the danger of them becoming a bottleneck within an IT architecture. To prevent this, knowledge about the amount of data a message broker using a specific configuration can handle needs to be available. In this paper, we propose a monitoring architecture for message brokers and similar Java Virtual Machine-based systems. We present a comprehensive performance analysis of the popular Apache Kafka platform using our approach. As part of the benchmark, we study selected data ingestion scenarios with respect to their maximum data ingestion rates. The results show that we can achieve an ingestion rate of about 420,000 messages/second on the used commodity hardware and with the developed data sender tool.", "venue": "2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)", "authors": ["Guenter  Hesse", "Christoph  Matthies", "Matthias  Uflacker"], "year": 2020, "n_citations": 0}
{"id": 4187831, "s2_id": "4ac8b0c6a602b6d16d32a075508ca9614afcbd6d", "title": "Optimizing Large-Scale ODE Simulations", "abstract": "We present a strategy to speed up Runge-Kutta-based ODE simulations of large systems with nearest-neighbor coupling. We identify the cache/memory bandwidth as the crucial performance bottleneck. To reduce the required bandwidth, we introduce a granularity in the simulation and identify the optimal cluster size in a performance study. This leads to a considerable performance increase and transforms the algorithm from bandwidth bound to CPU bound. By additionally employing SIMD instructions we are able to boost the efficiency even further. In the end, a total performance increase of up to a factor three is observed when using cache optimization and SIMD instructions compared to a standard implementation. All simulation codes are written in C++ and made publicly available. By using the modern C++ libraries Boost.odeint and Boost.SIMD, these optimizations can be implemented with minimal programming effort.", "venue": "ArXiv", "authors": ["Mario  Mulansky"], "year": 2014, "n_citations": 2}
{"id": 4189683, "s2_id": "5c92ade3df20e63ddd6a8402aa98fbf688e585f7", "title": "Approximate mechanism for measuring stability of Internet link in aggregated Internet pipe", "abstract": "In this article we propose a method for measuring internet connection stability which is fast and has negligible overhead for the process of its complexity. This method finds a relative value for representing the stability of internet connections and can also be extended for aggregated internet connections. The method is documented with help of a real time implementation and results are shared. This proposed measurement scheme uses HTTP GET method for each connections. The normalized responses to identified sites like gateways of ISPs, google.com etc are used for calculating current link stability. The novelty of the approach is that historic values are used to calculate overall link stability. In this discussion, we also document a method to use the calculated values as a dynamic threshold metric. This is used in routing decisions and for load-balancing each of the connections in an aggregated bandwidth pipe. This scheme is a very popular practice in aggregated internet connections.", "venue": "ArXiv", "authors": ["M.  Vipin", "K. R. Mohamed Imran"], "year": 2009, "n_citations": 0}
{"id": 4194794, "s2_id": "cb416cb86704e3fc6de5ccc8e1e557ca1de5e6d9", "title": "Integration of RTMFP in the OMNeT++ Simulation Environment", "abstract": "This paper introduces the new Real-Time Media Flow Protocol (RTMFP) simulation model for the INET framework for the OMNeT++ simulation environment. RTMFP is a message orientated protocol with a focus on real time peer-to-peer communication. After Adobe Inc. released the specifications, we were able to implement the protocol in INET and compare its performance to the similar Stream Control Transmission Protocol (SCTP) with a focus on congestion control and flow control mechanisms.", "venue": "Simutools 2008", "authors": ["Felix  Weinrank", "Michael  T\u00fcxen", "Erwin P. Rathgeb"], "year": 2008, "n_citations": 4}
{"id": 4195256, "s2_id": "ac4308eecc0079d37275b94cc74db160791dc417", "title": "8 Steps to 3.7 TFLOP/s on NVIDIA V100 GPU: Roofline Analysis and Other Tricks", "abstract": "Performance optimization can be a daunting task especially as the hardware architecture becomes more and more complex. This paper takes a kernel from the Materials Science code BerkeleyGW, and demonstrates a few performance analysis and optimization techniques. Despite challenges such as high register usage, low occupancy, complex data access patterns, and the existence of several long-latency instructions, we have achieved 3.7 TFLOP/s of double-precision performance on an NVIDIA V100 GPU, with 8 optimization steps. This is 55% of the theoretical peak, 6.7 TFLOP/s, at nominal frequency 1312 MHz, and 70% of the more customized peak based on our 58% FMA ratio, 5.3 TFLOP/s. An array of techniques used to analyze this OpenACC kernel and optimize its performance are shown, including the use of hierarchical Roofline performance model and the performance tool Nsight Compute. This kernel exhibits computational characteristics that are commonly seen in many high-performance computing (HPC) applications, and are expected to be very helpful to a general audience of HPC developers and computational scientists, as they pursue more performance on NVIDIA GPUs.", "venue": "ArXiv", "authors": ["Charlene  Yang"], "year": 2020, "n_citations": 4}
{"id": 4196312, "s2_id": "36a5fabe46cd6b2ce875597a41defff8dc4fa0dd", "title": "Sequential File Programming Patterns and Performance with .NET", "abstract": "Programming patterns for sequential file access in the .NET Framework are described and the performance is measured. The default behavior provides excellent performance on a single disk - 50 MBps both reading and writing. Using large request sizes and doing file pre-allocation when possible have quantifiable benefits. When one considers disk arrays, .NET unbuffered IO delivers 800 MBps on a 16-disk array, but buffered IO delivers about 12% of that performance. Consequently, high-performance file and database utilities are still forced to use unbuffered IO for maximum sequential performance. The report is accompanied by downloadable source code that demonstrates the concepts and code that was used to obtain these measurements.", "venue": "ArXiv", "authors": ["Peter  Kukol", "Jim  Gray"], "year": 2005, "n_citations": 1}
{"id": 4207330, "s2_id": "ff5121530f66131ac9e9772feef93b8de2d399a9", "title": "Architectural Impact on Performance of In-memory Data Analytics: Apache Spark Case Study", "abstract": "While cluster computing frameworks are contin-uously evolving to provide real-time data analysis capabilities,Apache Spark has managed to be at the forefront of big data an-alytics for being a unif ...", "venue": "ArXiv", "authors": ["Ahsan Javed Awan", "Mats  Brorsson", "Vladimir  Vlassov", "Eduard  Ayguad\u00e9"], "year": 2016, "n_citations": 6}
{"id": 4207485, "s2_id": "9459778c2c75e08a7d5adfe3840653f38763c458", "title": "FFT, FMM, and Multigrid on the Road to Exascale: performance challenges and opportunities", "abstract": "Abstract FFT, FMM, and multigrid methods are widely used fast and highly scalable solvers for elliptic PDEs. However, emerging large-scale computing systems are introducing challenges in comparison to current petascale computers. Recent efforts (Dongarra et\u00a0al. 2011) have identified several constraints in the design of exascale software that include massive concurrency, resilience management, exploiting the high performance of heterogeneous systems, energy efficiency, and utilizing the deeper and more complex memory hierarchy expected at exascale. In this paper, we perform a model-based comparison of the FFT, FMM, and multigrid methods in the context of these projected constraints. In addition we use performance models to offer predictions about the expected performance on upcoming exascale system configurations based on current technology trends.", "venue": "J. Parallel Distributed Comput.", "authors": ["Huda  Ibeid", "Luke N. Olson", "William  Gropp"], "year": 2020, "n_citations": 8}
{"id": 4209698, "s2_id": "50c08525a3c3f5bf3efcd63771c23bdf4b7fb777", "title": "Improving the Effective Utilization of Supercomputer Resources by Adding Low-Priority Containerized Jobs", "abstract": "We propose an approach to utilize idle computational resources of supercomputers. The idea is to maintain an additional queue of low-priority non-parallel jobs and execute them in containers, using container migration tools to break the execution down into separate intervals. We propose a container management system that can maintain this queue and interact with the supercomputer scheduler. We conducted a series of experiments simulating supercomputer scheduler and the proposed system. The experiments demonstrate that the proposed system increases the effective utilization of supercomputer resources under most of the conditions, in some cases significantly improving the performance.", "venue": "ArXiv", "authors": ["Julia  Dubenskaya", "Stanislav  Polyakov"], "year": 2019, "n_citations": 1}
{"id": 4210149, "s2_id": "53f3d4a5c57a1b70536fadfe197fe2e1eaa91597", "title": "Routing Technique Based on Clustering for Data Duplication Prevention in Wireless Sensor Network", "abstract": "Wireless Sensor Networks is important to nodes energy consumption for long activity of sensor nodes because nodes that compose sensor network are small size, and battery capacity is limited. For energy consumption decrease of sensor nodes, sensor networks routing technique is divided by flat routing and hierarchical routing technique. Specially, hierarchical routing technique is energy efficient routing protocol to pare down energy consumption of whole sensor nodes and to scatter energy consumption of sensor nodes by forming cluster and communicating with cluster head. but though hierarchical routing technique based on clustering is advantage more than flat routing technique, this is not used for reason that is not realistic. The reason that is not realistic is because hierarchical routing technique does not consider data transmission radius of sensor node in actually. so this paper propose realistic routing technique base on clustering.", "venue": "ArXiv", "authors": ["Boseung  Kim", "Huibin  Lim", "Yongtae  Shin"], "year": 2009, "n_citations": 2}
{"id": 4210929, "s2_id": "54a3b4b25a17fe428141babcf0df1ce8a240c9b6", "title": "The distribution of age-of-information performance measures for message processing systems", "abstract": "The idea behind the recently introduced \u201cage-of-information\u201d performance measure of a network message processing system is that it indicates our knowledge regarding the \u201cfreshness\u201d of the most recent piece of information that can be used as a criterion for real-time control. In this foundational paper, we examine two such measures, one that has been extensively studied in the recent literature and a new one that could be more relevant from the point of view of the processor. Considering these measures as stochastic processes in a stationary environment (defined by the arrival processes, message processing times and admission controls in bufferless systems), we characterize their distributions using the Palm inversion formula. Under renewal assumptions, we derive explicit solutions for their Laplace transforms and show some interesting decomposition properties. Previous work has mostly focused on computation of expectations in very particular cases. We argue that using bufferless or very small buffer systems is best and support this by simulation. We also pose some open problems including assessment of enqueueing policies that may be better in cases where one wishes to minimize more general functionals of the age-of-information measures.", "venue": "Queueing Syst. Theory Appl.", "authors": ["George  Kesidis", "Takis  Konstantopoulos", "Michael  Zazanis"], "year": 2020, "n_citations": 4}
{"id": 4211676, "s2_id": "31f39909699d9178c7ffbd8eadee76f1becfa327", "title": "Statistical Regression to Predict Total Cumulative CPU Usage of MapReduce Jobs", "abstract": "Recently, businesses have started using MapReduce as a popular computation framework for processing large amount of data, such as spam detection, and different data mining tasks, in both public and private clouds. Two of the challenging questions in such environments are (1) choosing suitable values for MapReduce configuration parameters e.g., number of mappers, number of reducers, and DFS block size, and (2) predicting the amount of resources that a user should lease from the service provider. Currently, the tasks of both choosing configuration parameters and estimating required resources are solely the users responsibilities. In this paper, we present an approach to provision the total CPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce job, a profile of total CPU usage in clock cycles is built from the job past executions with different values of two configuration parameters e.g., number of mappers, and number of reducers. Then, a polynomial regression is used to model the relation between these configuration parameters and total CPU usage in clock cycles of the job. We also briefly study the influence of input data scaling on measured total CPU usage in clock cycles. This derived model along with the scaling result can then be used to provision the total CPU usage in clock cycles of the same jobs with different input data size. We validate the accuracy of our models using three realistic applications (WordCount, Exim MainLog parsing, and TeraSort). Results show that the predicted total CPU usage in clock cycles of generated resource provisioning options are less than 8% of the measured total CPU usage in clock cycles in our 20-node virtual Hadoop cluster.", "venue": "ArXiv", "authors": ["Nikzad Babaii Rizvandi", "Javid  Taheri", "Reza  Moraveji", "Albert Y. Zomaya"], "year": 2013, "n_citations": 0}
{"id": 4211813, "s2_id": "9b91a7dcc307112e7a95d576a95094c4ad884159", "title": "A Model for Probabilistic Reasoning on Assume/Guarantee Contracts", "abstract": "In this paper, we present a probabilistic adaptation of an Assume/Guarantee contract formalism. For the sake of generality, we assume that the extended state machines used in the contracts and implementations define sets of runs on a given set of variables, that compose by intersection over the common variables. In order to enable probabilistic reasoning, we consider that the contracts dictate how certain input variables will behave, being either non-deterministic, or probabilistic; the introduction of probabilistic variables leading us to tune the notions of implementation, refinement and composition. As shown in the report, this probabilistic adaptation of the Assume/Guarantee contract theory preserves compositionality and therefore allows modular reliability analysis, either with a top-down or a bottom-up approach.", "venue": "ArXiv", "authors": ["Beno\u00eet  Delahaye", "Beno\u00eet  Caillaud"], "year": 2008, "n_citations": 9}
{"id": 4212255, "s2_id": "37a4f26dc38a1faa881aeb2f403c2f21c852055f", "title": "SP^2Bench: A SPARQL Performance Benchmark", "abstract": "Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evaluation strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP^2Bench, a publicly available, language-specific SPARQL performance benchmark. SP^2Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellationsand RDF access patterns. As a proof of concept, we apply SP^2Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results", "venue": "2009 IEEE 25th International Conference on Data Engineering", "authors": ["Michael  Schmidt", "Thomas  Hornung", "Georg  Lausen", "Christoph  Pinkel"], "year": 2009, "n_citations": 370}
{"id": 4212648, "s2_id": "cfecf02ccab3618cad987787fb1763f888e11587", "title": "Multi-GPU Performance Optimization of a CFD Code using OpenACC on Different Platforms", "abstract": "This paper investigates the multi-GPU performance of a 3D buoyancy driven cavity solver using MPI and OpenACC directives on different platforms. The paper shows that decomposing the total problem in different dimensions affects the strong scaling performance significantly for the GPU. Without proper performance optimizations, it is shown that 1D domain decomposition scales poorly on multiple GPUs due to the noncontiguous memory access. The performance using whatever decompositions can be benefited from a series of performance optimizations in the paper. Since the buoyancy driven cavity code is latency-bounded on the clusters examined, a series of optimizations both agnostic and tailored to the platforms are designed to reduce the latency cost and improve memory throughput between hosts and devices efficiently. First, the parallel message packing/unpacking strategy developed for noncontiguous data movement between hosts and devices improves the overall performance by about a factor of 2. Second, transferring different data based on the stencil sizes for different variables further reduces the communication overhead. These two optimizations are general enough to be beneficial to stencil computations having ghost changes on all of the clusters tested. Third, GPUDirect is used to improve the communication on clusters which have the hardware and software support for direct communication between GPUs without staging CPU's memory. Finally, overlapping the communication and computations is shown to be not efficient on multi-GPUs if only using MPI or MPI+OpenACC. Although we believe our implementation has revealed enough overlap, the actual running does not utilize the overlap well due to a lack of asynchronous progression.", "venue": "ArXiv", "authors": ["Weicheng  Xue", "Christopher J. Roy"], "year": 2020, "n_citations": 0}
{"id": 4213460, "s2_id": "2236caf4a8fa55cea386fdfa5698f84e8b2c7055", "title": "Semi-Quantitative Abstraction and Analysis of Chemical Reaction Networks", "abstract": "Analysis of large continuous-time stochastic systems is a computationally intensive task. In this work we focus on population models arising from chemical reaction networks (CRNs), which play a fundamental role in analysis and design of biochemical systems. Many relevant CRNs are particularly challenging for existing techniques due to complex dynamics including stochasticity, stiffness or multimodal population distributions. We propose a novel approach allowing not only to predict, but also to explain both the transient and steady-state behaviour. It focuses on qualitative description of the behaviour and aims at quantitative precision only in orders of magnitude. Firstly, we abstract the CRN into a compact model preserving rough timing information, distinguishing only signifcinatly different populations, but capturing relevant sequences of behaviour. Secondly, we approximately analyse the most probable temporal behaviours of the model through most probable transitions. As demonstrated on complex CRNs from literature, our approach reproduces the known results, but in contrast to the state-of-the-art methods, it runs with virtually no computational cost and thus offers unprecedented~scalability.", "venue": "CAV", "authors": ["Milan  Ceska", "Jan  Kret\u00ednsk\u00fd"], "year": 2019, "n_citations": 4}
{"id": 4213558, "s2_id": "992e6a35032d2c96e3f1270e3a10c1363a43af31", "title": "Performance and Cost Evaluation of Smart Contracts in Collaborative Health Care Environments", "abstract": "Blockchain emerged as a solution for data integrity, non-repudiation, and availability in different applications. Data sensitive scenarios, such as Health Care, can also benefit from these blockchain properties. Consequently, different research proposed the adoption of blockchain in Health Care applications. However, few are discussed about incentive methods to attract new users, as well as to motivate the system or application usage by existing end-users. Also, little is discussed about performance during code execution in blockchains. In order to tackle these issues, this work presents the preliminary evaluation of TokenHealth, an application for collaborative health practice monitoring with gamification and token-based incentives. The proposed solution is implemented through smart contracts using Solidity in the Ethereum blockchain. We evaluated the performance of both in Ropsten test network and in a Private instance. The preliminary results show that the execution of smart contracts takes less than a minute for a full cycle of different smart contracts. Also, we present a discussion about costs for using a Private instance and the public Ethereum main network.", "venue": "ArXiv", "authors": ["Roben Castagna Lunardi", "Henry Cabral Nunes", "Vinicius da Silva Branco", "Bruno Hugentobler Lipper", "Charles Varlei Neu", "Avelino Francisco Zorzo"], "year": 2019, "n_citations": 2}
{"id": 4214294, "s2_id": "4746be9f3fa6d5816438b5a32a2d6db2966d6ee3", "title": "Large Scale Studies of Memory, Storage, and Network Failures in a Modern Data Center", "abstract": "The workloads running in the modern data centers of large scale Internet service providers (such asAlibaba, Amazon, Baidu, Facebook, Google, and Microsoft) support billions of users and span globallydistributed infrastructure. Yet, the devices used in modern data centers fail due to a variety of causes, fromfaulty components to bugs to misconfiguration. Faulty devices make operating large scale data centerschallenging because the workloads running in modern data centers consist of interdependent programsdistributed across many servers, so failures that are isolated to a single device can still have a widespreadeffect on a workload.In this dissertation, we measure and model the device failures in a large scale Internet service company,Facebook. We focus on three device types that form the foundation of Internet service data centerinfrastructure: DRAM for main memory, SSDs for persistent storage, and switches and backbone linksfor network connectivity. For each of these device types, we analyze long term device failure data brokendown by important device attributes and operating conditions, such as age, vendor, and workload. Wealso build and release statistical models of the failure trends for the devices we analyze.For DRAM devices, we analyze the memory errors in the entire fleet of servers at Facebook over thecourse of fourteen months, representing billions of device days of operation. The systems we examinecover a wide range of devices commonly used in modern servers, with DIMMs that use the modernDDR3 communication protocol, manufactured by 4 vendors in capacities ranging from 2GB to 24GB.We observe several new reliability trends for memory systems that have not been discussed before inliterature, develop a model for memory reliability, show how system design choices such as using lowerdensity DIMMs and fewer cores per chip can reduce failure rates of a baseline server by up to 57.7%.We perform the first implementation and real-system analysis of page offlining at scale, on a cluster ofthousands of servers, identify several real-world impediments to the technique, and show that it canreduce memory error rate by 67%. We also examine the efficacy of a new technique to reduce DRAMfaults, physical page randomization, and examine its potential for improving reliability and its overheads.For SSD devices, we perform a large scale study of flash-based SSD reliability at Facebook. We analyzedata collected across a majority of flash-based solid state drives over nearly four years and manymillions of operational hours in order to understand failure properties and trends of flash-based SSDs.Our study considers a variety of SSD characteristics, including: the amount of data written to and readfrom flash chips; how data is mapped within the SSD address space; the amount of data copied, erased,and discarded by the flash controller; and flash board temperature and bus power. Based on our fieldanalysis of how flash memory errors manifest when running modern workloads on modern SSDs, we make several major observations and find that SSD failure rates do not increase monotonically with flashchip wear, but instead they go through several distinct periods corresponding to how failures emerge andare subsequently detected.For network devices, we perform a large scale, longitudinal study of data center network reliabilitybased on operational data collected from the production network infrastructure at Facebook. Our studycovers reliability characteristics of both intra and inter data center networks. For intra data center networks,we study seven years of operation data comprising thousands of network incidents across twodifferent data center network designs, a cluster network design and a state-of-the-art fabric network design.For inter data center networks, we study eighteen months of recent repair tickets from the field tounderstand the reliability of Wide Area Network (WAN) backbones. In contrast to prior work, we studythe effects of network reliability on software systems, and how these reliability characteristics evolve overtime. We discuss the implications of network reliability on the design, implementation, and operation oflarge scale data center systems and how the network affects highly-available web services.Our key conclusion in this dissertation is that we can gain a deep understanding of why devicesfail\u2014and how to predict their failure\u2014using measurement and modeling. We hope that the analysis,techniques, and models we present in this dissertation will enable the community to better measure,understand, and prepare for the hardware reliability challenges we face in the future.", "venue": "ArXiv", "authors": ["Justin  Meza"], "year": 2019, "n_citations": 2}
{"id": 4219740, "s2_id": "af9a9aaefc43e7948e657d8a991fdf97dec9b953", "title": "Multidimensional Intratile Parallelization for Memory-Starved Stencil Computations", "abstract": "Optimizing the performance of stencil algorithms has been the subject of intense research over the last two decades. Since many stencil schemes have low arithmetic intensity, most optimizations foc...", "venue": "ParCo 2017", "authors": ["M  MalasTareq", "HagerGeorg", "LtaiefHatem", "E  KeyesDavid"], "year": 2017, "n_citations": 1}
{"id": 4220190, "s2_id": "8cafd524de65543987418c6a856dc4cb01c25cad", "title": "Optimizing IoT and Web Traffic Using Selective Edge Compression", "abstract": "Internet of Things (IoT) devices and applications are generating and communicating vast quantities of data, and the rate of data collection is increasing rapidly. These high communication volumes are challenging for energy-constrained, data-capped, wireless mobile devices and networked sensors. Compression is commonly used to reduce web traffic, to save energy, and to make network transfers faster. If not used judiciously, however, compression can hurt performance. This work proposes and evaluates mechanisms that employ selective compression at the network\u2019s edge, based on data characteristics and network conditions. This approach (i) improves the performance of network transfers in IoT environments, while (ii) providing significant data savings. We demonstrate that our library speeds up web transfers by an average of 2.18x and 2.03x under fixed and dynamically changing network conditions respectively. Furthermore, it also provides consistent data savings, compacting data down to 19% of the original data size.", "venue": "ArXiv", "authors": ["Themis  Melissaris", "Kelly  Shaw", "Margaret  Martonosi"], "year": 2020, "n_citations": 0}
{"id": 4222123, "s2_id": "e8bdbe162aa4616c648c5b7b93c8366eaed34d39", "title": "Designing a High Performance Parallel Personal Cluster", "abstract": "Today, many scientific and engineering areas require high performance computing to perform computationally intensive experiments. For example, many advances in transport phenomena, thermodynamics, material properties, computational chemistry and physics are possible only because of the availability of such large scale computing infrastructures. Yet many challenges are still open. The cost of energy consumption, cooling, competition for resources have been some of the reasons why the scientific and engineering communities are turning their interests to the possibility of implementing energy-efficient servers utilizing low-power CPUs for computing-intensive tasks. In this paper we introduce a novel approach, which was recently presented at Linux Conference Europe 2015, based on the Beowulf concept and utilizing single board computers (SBC). We present a low-energy consumption architecture capable to tackle heavily demanding scientific computational problems. Additionally, our goal is to provide a low cost personal solution for scientists and engineers. In order to evaluate the performance of the proposed architecture we ran several standard benchmarking tests. Furthermore, we assess the reliability of the machine in real life situations by performing two benchmark tools involving practical TCAD for physicist and engineers in the semiconductor industry.", "venue": "ArXiv", "authors": ["K. G. Kapanova", "Jean Michel D. Sellier"], "year": 2016, "n_citations": 0}
{"id": 4223788, "s2_id": "6954641dc6b8e881f111480b5a7237cd51020163", "title": "Low Latency Datacenter Networking: A Short Survey", "abstract": "Datacenters are the cornerstone of the big data infrastructure supporting numerous online services. The demand for interactivity, which significantly impacts user experience and provider revenue, is translated into stringent timing requirements for flows in datacenter networks. Thus low latency networking is becoming a major concern of both industry and academia. \nWe provide a short survey of recent progress made by the networking community for low latency datacenter networks. We propose a taxonomy to categorize existing work based on four main techniques, reducing queue length, accelerating retransmissions, prioritizing mice flows, and exploiting multi-path. Then we review select papers, highlight the principal ideas, and discuss their pros and cons. We also present our perspectives of the research challenges and opportunities, hoping to aspire more future work in this space.", "venue": "ArXiv", "authors": ["Shuhao  Liu", "Hong  Xu", "Zhiping  Cai"], "year": 2013, "n_citations": 26}
{"id": 4228978, "s2_id": "6ebafe7f774db95dc63a3f8c604ffe2529d3e250", "title": "Stochastic Non-Bipartite Matching Models and Order-Independent Loss Queues", "abstract": "The need for matching items with one another while meeting assignment constraints or preferences gave rise to several well-known problems like the stable marriage and roommate problems. While most of the literature on matching problems focuses on a static setting with a fixed number of items, several recent works incorporated time by considering stochastic models, in which items of different classes arrive according to independent Poisson processes and assignment constraints are described by an undirected non-bipartite graph on the classes. In this paper, we prove that the continuous-time Markov chain associated with this model has the same transition diagram as in a product-form queueing model called an order-independent loss queue. This allows us to adapt existing results on order-independent queues to stochastic matching models, and in particular to derive closed-form expressions for several performance metrics, like the waiting probability or the mean matching time, that can be implemented using dynamic programming. Both these formulas and the numerical results that they allow us to derive are used to gain insight into the impact of parameters on performance. In particular, we characterize performance in a so-called heavy-traffic regime in which the number of items of a subset of the classes goes to infinity while items of other classes become scarce.", "venue": "Stochastic Models", "authors": ["C'eline  Comte"], "year": 2021, "n_citations": 3}
{"id": 4233201, "s2_id": "0fe3afbd3771f849c675608a5f9b6b165dce6e48", "title": "A \"Measure of Transaction Processing\" 20 Years Later", "abstract": "This article quantifies the price-performance improvements on two standard commercial benchmarks (DebitCredit and Sort) from 1985 to 2005. It shows that improvement has exceeded Moore\u2019s law \u2013 largely due to (1) hardware improvements, (2) software improvements, (3) massive parallelism, and (4) changing from mainframe to commodity economics. Price-performance continues to improve faster than Moore\u2019s law but per-processor and peak performance are improving more slowly. The sorting results in particular indicate little progress on per-processor speed in the last decade (probably due to slow improvement in memory bandwidth) and little overall progress since 2000 (when thousands of processors and disks were first used).", "venue": "IEEE Data Eng. Bull.", "authors": ["Jim  Gray"], "year": 2005, "n_citations": 11}
{"id": 4238819, "s2_id": "d028d91f30c71bd4e7b45d995b2338aff07ac775", "title": "Redundancy Suppression In Time-Aware Dynamic Binary Instrumentation", "abstract": "Software tracing techniques are well-established and used by instrumentation tools to extract run-time information for program analysis and debugging. Dynamic binary instrumentation as one tool instruments program binaries to extract information. Unfortunately, instrumentation causes perturbation that is unacceptable for time-sensitive applications. Consequently we developed DIME*, a tool for dynamic binary instrumentation that considers timing constraints. DIME* uses Pin and a rate-based server approach to extract information only as long as user-specified constraints are maintained. Due to the large amount of redundancies in program traces, DIME* reduces the instrumentation overhead by one to three orders of magnitude compared to native Pin while extracting up to 99% of the information. We instrument VLC and PostgreSQL to demonstrate the usability of DIME*.", "venue": "ArXiv", "authors": ["Pansy  Arafa", "Hany  Kashif", "Sebastian  Fischmeister"], "year": 2017, "n_citations": 1}
{"id": 4239046, "s2_id": "e62791276c3f4be470a6d914c58a0787b012b3c7", "title": "Efficient Learning-based Scheduling for Information Freshness in Wireless Networks", "abstract": "Motivated by the recent trend of integrating artificial intelligence into the Internet-of-Things (IoT), we consider the problem of scheduling packets from multiple sensing sources to a central controller over a wireless network. Here, packets from different sensing sources have different values or degrees of importance to the central controller for intelligent decision making. In such a setup, it is critical to provide timely and valuable information for the central controller. In this paper, we develop a parameterized maximum-weight type scheduling policy that combines both the AoI metrics and Upper Confidence Bound (UCB) estimates in its weight measure with parameter \u03b7. Here, UCB estimates balance the tradeoff between exploration and exploitation in learning and are critical for yielding a small cumulative regret. We show that our proposed algorithm yields the running average total age at most by O(N2\u03b7). We also prove that our proposed algorithm achieves the cumulative regret over time horizon T at most by $O(NT/\\eta + \\sqrt {NT\\log T} )$. This reveals a tradeoff between the cumulative regret and the running average total age: when increasing \u03b7, the cumulative regret becomes smaller, but is at the cost of increasing running average total age. Simulation results are provided to evaluate the efficiency of our proposed algorithm.", "venue": "IEEE INFOCOM 2021 - IEEE Conference on Computer Communications", "authors": ["Bin  Li"], "year": 2021, "n_citations": 2}
{"id": 4240163, "s2_id": "c02ad9d361b5d6860118153aa825fe31d0c334da", "title": "On Conditional Branches in Optimal Search Trees", "abstract": "Algorithms for efficiently finding optimal alphabetic decision trees -- such as the Hu-Tucker algorithm -- are well established and commonly used. However, such algorithms generally assume that the cost per decision is uniform and thus independent of the outcome of the decision. The few algorithms without this assumption instead use one cost if the decision outcome is ``less than'' and another cost otherwise. In practice, neither assumption is accurate for software optimized for today's microprocessors. Such software generally has one cost for the more likely decision outcome and a greater cost -- often far greater -- for the less likely decision outcome. This problem and generalizations thereof are thus applicable to hard coding static decision tree instances in software, e.g., for optimizing program bottlenecks or for compiling switch statements. An O(n^3)-time O(n^2)-space dynamic programming algorithm can solve this optimal binary decision tree problem, and this approach has many generalizations that optimize for the behavior of processors with predictive branch capabilities, both static and dynamic. Solutions to this formulation are often faster in practice than ``optimal'' decision trees as formulated in the literature. Different search paradigms can sometimes yield even better performance.", "venue": "ArXiv", "authors": ["Michael B. Baer"], "year": 2006, "n_citations": 1}
{"id": 4245153, "s2_id": "4caf74516c9dc80610c6f1e0fa6bbb653a24bfc6", "title": "Enhanced Performance and Privacy via Resolver-Less DNS", "abstract": "The domain name resolution into IP addresses can significantly delay connection establishments on the web. Moreover, the common use of recursive DNS resolvers presents a privacy risk as they can closely monitor the user\u2019s browsing activities. In this paper, we present a novel HTTP response header allowing web server to provide their clients with relevant DNS records. Our results indicate, that this resolver-less DNS mechanism allows user agents to save the DNS lookup time for subsequent connection establishments. We find, that this proposal saves at least 80 ms per DNS lookup for the one percent of users having the longest round-trip times towards their recursive resolver. Furthermore, our proposal decreases the number of DNS lookups and thus improves the privacy posture of the user towards the used recursive resolver. Comparing the security guarantees of the traditional DNS to our proposal, we find that resolver-less DNS achieves at least the same security properties. In detail, it even improves the user\u2019s resilience against censorship through tampered DNS resolvers.", "venue": "2021 International Conference on Information Networking (ICOIN)", "authors": ["Erik  Sy"], "year": 2021, "n_citations": 1}
{"id": 4246372, "s2_id": "6ed31c000e61baac9225f3d96bc18068c7fe8254", "title": "DESP-C++: a discrete-event simulation package for C++", "abstract": "DESP-C++ is a C++ discrete-event random simulation engine that has been designed to be fast, very easy to use and expand, and valid. DESP-C++ is based on the resource view. Its complete architecture is presented in detail, as well as a short \" user manual \". The validity of DESP-C++ is demonstrated by the simulation of three significant models. In each case, the simulation results obtained with DESP-C++ match those obtained with a validated simulation software: QNAP2. The versatility of DESP-C++ is also illustrated this way, since the modelled systems are very different from each other: a simple production system, the dining philosopher classical deadlock problem, and a complex object-oriented database management system.", "venue": "Softw. Pract. Exp.", "authors": ["J\u00e9r\u00f4me  Darmont"], "year": 2000, "n_citations": 5}
{"id": 4246560, "s2_id": "1d94242db3235fed05d9d0a0bb883d3f27c28fbc", "title": "Diffusing Your Mobile Apps: Extending In-Network Function Virtualization to Mobile Function Offloading", "abstract": "Motivated by the huge disparity between the limited battery capacity of user devices and the ever-growing energy demands of modern mobile apps, we propose INFv. It is the first offloading system able to cache, migrate and dynamically execute on demand functionality from mobile devices in ISP networks. It aims to bridge this gap by extending the promising NFV paradigm to mobile applications in order to exploit in-network resources. In this paper, we present the overall design, state-of-the-art technologies adopted, and various engineering details in the INFv system. We also carefully study the deployment configurations by investigating over 20K Google Play apps, as well as thorough evaluations with realistic settings. In addition to a significant improvement in battery life (up to 6.9x energy reduction) and execution time (up to 4x faster), INFv has two distinct advantages over previous systems: 1) a non-intrusive offloading mechanism transparent to existing apps; 2) an inherent framework support to effectively balance computation load and exploit the proximity of in-network resources. Both advantages together enable a scalable and incremental deployment of computation offloading framework in practical ISPs' networks.", "venue": "ArXiv", "authors": ["Mario  Almeida", "Liang  Wang", "Jeremy  Blackburn", "Konstantina  Papagiannaki", "Jon  Crowcroft"], "year": 2019, "n_citations": 0}
{"id": 4246699, "s2_id": "aee2ef28f2cd948f5a7f55ce0b9a751e72ac84ad", "title": "Performance Evaluation of Different Scheduling Algorithms in WiMAX", "abstract": "Worldwide Interoperability for Microwave Access (WiMAX) networks were expected to be the main Broadband Wireless Access (BWA) technology that provided several services such as data, voice, and video services including different classes of Quality of Services (QoS), which in turn were defined by IEEE 802.16 standard. Scheduling in WiMAX became one of the most challenging issues, since it was responsible for distributing available resources of the network among all users; this leaded to the demand of constructing and designing high efficient scheduling algorithms in order to improve the network utilization, to increase the network throughput, and to minimize the end-to-end delay. In this study, we presenedt a simulation study to measure the performance of several scheduling algorithms in WiMAX, which were Strict Priority algorithm, Round-Robin (RR), Weighted Round Robin (WRR), Weighted Fair Queuing (WFQ), Self-Clocked Fair (SCF), and Diff-Serv Algorithm.", "venue": "ArXiv", "authors": ["Ala'a Z. Al-Howaide", "Ahmad S. Doulat", "Yaser M. Khamayseh"], "year": 2011, "n_citations": 29}
{"id": 4246843, "s2_id": "f4daa8057eaec3f17dcab2938cf846eb65ac974c", "title": "Analysis of buffering effects on hard real-time priority-preemptive wormhole networks", "abstract": "There are several approaches to analyse the worst-case response times of sporadic packets transmitted over priority-preemptive wormhole networks. In this paper, we provide an overview of the different approaches, discuss their strengths and weaknesses, and propose an approach that captures all effects considered by previous approaches while providing tight yet safe upper bounds for packet response times. We specifically address the problems created by buffering and backpressure in wormhole networks, which amplifies the problem of indirect interference in a way that has not been considered by the early analysis approaches. Didactic examples and large-scale experiments with synthetically generated packet flow sets provide evidence of the strength of the proposed approach.", "venue": "ArXiv", "authors": ["Leandro Soares Indrusiak", "Alan  Burns", "Borislav  Nikolic"], "year": 2016, "n_citations": 20}
{"id": 4251369, "s2_id": "c8658532dcd95ca1c69ef727ac7d446e93a24adb", "title": "The Transitional Behavior of Interference in Millimeter Wave Networks and Its Impact on Medium Access Control", "abstract": "Millimeter-wave (mmWave) communication systems use a large number of antenna elements that can potentially overcome severe channel attenuation by narrow beamforming. Narrow-beam operation in mmWave networks also reduces multiuser interference, introducing the concept of noise-limited wireless networks as opposed to interference-limited ones. The noise-limited or interference-limited regime heavily reflects on the medium access control (MAC) layer throughput and on proper resource allocation and interference management strategies. Yet, these regimes are ignored in current approaches to mmWave MAC layer design, with the potential disastrous consequences on the communication performance. In this paper, we investigate these regimes in terms of collision probability and throughput. We derive tractable closed-form expressions for the collision probability and MAC layer throughput of mmWave ad hoc networks, operating under slotted ALOHA. The new analysis reveals that mmWave networks may exhibit a non-negligible transitional behavior from a noise-limited regime to an interference-limited one, depending on the density of the transmitters, density and size of obstacles, transmission probability, operating beamwidth, and transmission power. Such transitional behavior necessitates a new framework of adaptive hybrid resource allocation procedure, containing both contention-based and contention-free phases with on-demand realization of the contention-free phase. Moreover, the conventional collision avoidance procedure in the contention-based phase should be revisited, due to the transitional behavior of interference, to maximize throughput/delay performance of mmWave networks. We conclude that, unless proper hybrid schemes are investigated, the severity of the transitional behavior may significantly reduce throughput/delay performance of mmWave networks.", "venue": "IEEE Transactions on Communications", "authors": ["Hossein Shokri Ghadikolaei", "Carlo  Fischione"], "year": 2016, "n_citations": 73}
{"id": 4255160, "s2_id": "202d985e7286428d1da9d3c1b8d04cf193bf5f6d", "title": "Restructuring expression dags for efficient parallelization", "abstract": "In the field of robust geometric computation it is often necessary to make exact decisions based on inexact floating-point arithmetic. One common approach is to store the computation history in an arithmetic expression dag and to re-evaluate the expression with increasing precision until an exact decision can be made. We show that exact-decisions number types based on expression dags can be evaluated faster in practice through parallelization on multiple cores. We compare the impact of several restructuring methods for the expression dag on its running time in a parallel environment.", "venue": "SEA", "authors": ["Martin  Wilhelm"], "year": 2018, "n_citations": 1}
{"id": 4262520, "s2_id": "0f2cbae7e6008b4f851d6d166978af12962a0fb8", "title": "A QoS-aware workload routing and server speed scaling policy for energy-efficient data centers: a robust queueing theoretic approach", "abstract": "Maintaining energy efficiency in large data centers depends on the ability to manage workload routing and control server speeds according to fluctuating demand. The use of dynamic algorithms often means that management has to install the complicated software or expensive hardware needed to communicate with routers and servers. This paper proposes a static routing and server speed scaling policy that may achieve energy efficiency similar to dynamic algorithms and eliminate the necessity of frequent communications among resources without compromising quality of service (QoS). We use a robust queueing approach to consider the response time constraints, e.g., service level agreements (SLAs). We model each server as a $G/G/1$ processor sharing (PS) queue and use uncertainty sets to define the domain of random variables. A comparison with a dynamic algorithm shows that the proposed static policy provides competitive solutions in terms of energy efficiency and satisfactory QoS.", "venue": "ArXiv", "authors": ["Seung Min Baik", "Young Myoung Ko"], "year": 2019, "n_citations": 1}
{"id": 4268137, "s2_id": "68422eca1c01825871e5a7a2ebcec6189e608200", "title": "Understanding Fairness and its Impact on Quality of Service in IEEE 802.11", "abstract": "The distributed coordination function (DCF) aims at fair and efficient medium access in IEEE 802.11. In face of its success, it is remarkable that there is little consensus on the actual degree of fairness achieved, particularly bearing its impact on quality of service in mind. In this paper we provide an accurate model for the fairness of the DCF. Given M greedy stations we assume fairness if a tagged station contributes a share of 1/M to the overall number of packets transmitted. We derive the probability distribution of fairness deviations and support our analytical results by an extensive set of measurements. We find a closed-form expression for the improvement of long-term over short-term fairness. Regarding the random countdown values we quantify the significance of their distribution whereas we discover that fairness is largely insensitive to the distribution parameters. Based on our findings we view the DCF as emulating an ideal fair queuing system to quantify the deviations from a fair rate allocation. We deduce a stochastic service curve model for the DCF to predict packet delays in IEEE 802.11. We show how a station can estimate its fair bandwidth share from passive measurements of its traffic arrivals and departures.", "venue": "IEEE INFOCOM 2009", "authors": ["Michael  Bredel", "Markus  Fidler"], "year": 2009, "n_citations": 58}
{"id": 4268179, "s2_id": "60ef6b20949031b510b15116f249761c3cd8a296", "title": "Modeling the Internet of Things: a simulation perspective", "abstract": "This paper deals with the problem of properly simulating the Internet of Things (loT). Simulating an loT allows evaluating strategies that can be employed to deploy smart services over different kinds of territories. However, the heterogeneity of scenarios seriously complicates this task. This imposes the use of sophisticated modeling and simulation techniques. We discuss novel approaches for the provision of scalable simulation scenarios, that enable the real-time execution of massively populated IoT environments. Attention is given to novel hybrid and multi-level simulation techniques that, when combined with agent-based, adaptive Parallel and Distributed Simulation (PADS) approaches, can provide means to perform highly detailed simulations on demand. To support this claim, we detail a use case concerned with the simulation of vehicular transportation systems.", "venue": "2017 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Gabriele  D'Angelo", "Stefano  Ferretti", "Vittorio  Ghini"], "year": 2017, "n_citations": 18}
{"id": 4276247, "s2_id": "c404a38efda92d5b7050fb039ce0c040aad838c0", "title": "Revisiting Data Compression in Column-Stores", "abstract": "Data compression is widely used in contemporary columnoriented DBMSes to lower space usage and to speed up query processing. Pioneering systems have introduced compression to tackle the disk bandwidth bottleneck by trading CPU processing power for it. The main issue of this is a trade-off between the compression ratio and the decompression CPU cost. Existing results state that light-weight compression with small decompression costs outperforms heavy-weight compression schemes in column-stores. However, since the time these results were obtained, CPU, RAM, and disk performance have advanced considerably. Moreover, novel compression algorithms have emerged. In this paper, we revisit the problem of compression in disk-based columnstores. More precisely, we study the I/O-RAM compression scheme which implies that there are two types of pages of different size: disk pages (compressed) and in-memory pages (uncompressed). In this scheme, the buffer manager is responsible for decompressing pages as soon as they arrive from disk. This scheme is rather popular as it is easy to implement: several modern column and row-stores use it. We pose and address the following research questions: 1) Are heavyweight compression schemes still inappropriate for disk-based columnstores?, 2) Are new light-weight compression algorithms better than the old ones?, 3) Is there a need for SIMD-employing decompression algorithms in case of a disk-based system? We study these questions experimentally using a columnar query engine and Star Schema Benchmark.", "venue": "MEDI", "authors": ["Alexander  Slesarev", "Evgeniy  Klyuchikov", "Kirill  Smirnov", "George  Chernishev"], "year": 2021, "n_citations": 0}
{"id": 4281295, "s2_id": "37d80e587e64459e8f72b3beff4d44d96cc689cd", "title": "Generalized Analysis of a Distributed Energy Efficient Algorithm for Change Detection", "abstract": "We propose an energy efficient distributed cooperative Change Detection scheme called DualCUSUM based on Page's CUSUM algorithm. In the algorithm, each sensor runs a CUSUM and transmits only when the CUSUM is above some threshold. The transmissions from the sensors are fused at the physical layer. The channel is modeled as a Multiple Access Channel (MAC) corrupted with noise. The fusion center performs another CUSUM to detect the change. The algorithm performs better than several existing schemes when energy is at a premium. We generalize the algorithm to also include nonparametric CUSUM and provide a unified analysis. Our results show that while the false alarm probability is smaller for observation distribution with a lighter tail, the detection delay is asymptotically the same for any distribution. Consequently, we provide a new viewpoint on why parametric CUSUM performs better than nonparametric CUSUM. In the process, we also develop new results on a reflected random walk which can be of independent interest.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Taposh  Banerjee", "Vinod  Sharma"], "year": 2011, "n_citations": 21}
{"id": 4281493, "s2_id": "8e57d54e37b57dc6e21528e98847f494f4bb288d", "title": "Performance Evaluation of Container-based Virtualization for High Performance Computing Environments", "abstract": "Virtualization technologies have evolved along with the development of computational environments. Virtualization offered needed features at that time such as isolation, accountability, resource allocation, resource fair sharing and so on. Novel processor technologies bring to commodity computers the possibility to emulate diverse environments where a wide range of computational scenarios can be run. Along with processors evolution, developers have implemented different virtualization mechanisms exhibiting enhanced performance from previous virtualized environments. Recently, operating system-based virtualization technologies captured the attention of communities abroad because their important improvements on performance area. In this paper, the features of three container-based operating systems virtualization tools (LXC, Docker and Singularity) are presented. LXC, Docker, Singularity and bare metal are put under test through a customized single node HPL-Benchmark and a MPI-based application for the multi node testbed. Also the disk I/O performance, Memory (RAM) performance, Network bandwidth and GPU performance are tested for the COS technologies vs bare metal. Preliminary results and conclusions around them are presented and discussed.", "venue": "Revista UIS Ingenier\u00edas", "authors": ["Carlos  Arango", "R\u00e9my  Dernat", "John  Sanabria"], "year": 2019, "n_citations": 204}
{"id": 4285843, "s2_id": "e2fc071a2bb716c4038ac7eda9bcdbb5333821e3", "title": "Performance Analysis of UAV-Based Mixed RF-UWOC Transmission Systems", "abstract": "In this paper, we investigate the performance of a mixed radio-frequency-underwater wireless optical communication (RF-UWOC) system where an unmanned aerial vehicle (UAV), as a low-altitude mobile aerial base station, transmits information to an autonomous underwater vehicle (AUV) through a fixed-gain amplify-and-forward (AF) or decode-and-forward (DF) relay. The analysis accounts for the main factors that affect the system performance, such as the UAV height, air bubbles, temperature gradient, water salinity variations, detection techniques, and pointing errors. Employing the fixed-gain AF and DF relays, expressions for some key performance metrics are derived, e.g., outage probability (OP), average bit error rate (ABER), and average channel capacity. Moreover, in order to get further insights, asymptotic analyses for the OP and ABER are also carried out. Additionally, for the two relaying systems, we derive analytical expressions for the optimal UAV altitude that minimizes the OP. Simulation results show that the UAV altitude influences the system performance and there is an optimal altitude which ensures a minimum OP. Furthermore, it is demonstrated that the diversity order of the fixed-gain AF relaying depends on the RF link and pointing errors, while the diversity order of the DF relaying depends on the detection techniques and pointing errors.", "venue": "IEEE Transactions on Communications", "authors": ["Sai  Li", "Liang  Yang", "Daniel Benevides da Costa", "Siyuan  Yu"], "year": 2021, "n_citations": 4}
{"id": 4290305, "s2_id": "015f8a9c64da6bdde2102fbd7bdefdd89e6e103a", "title": "Stochastic geometric modelling and simulation of cellular systems for coverage probability characterization", "abstract": "Stochastic geometry (SG) has been successfully used as a modelling tool for cellular networks to characterize the coverage probability in both the downlink (DL) and uplink (UL) systems, under the assumption that the base stations (BS) are deployed as a Poisson point process. In the present article, we extend this use and provide further results for interference limited and Rayleigh fading networks, culminating in a multifaceted contribution. First, we compactly model the two systems at once, allowing parallels to be drawn and contrast to be created. Also, for DL we manage to obtain two closed form expressions for two special cases. Moreover, for UL, notorious for being difficult, we develop a clever approximation that overcomes the difficulty, yielding excellent results. Additionally, we present two efficient Monte Carlo simulation algorithms, designed primarily to validate the models, but can be of great use for SG modelling of communications systems in general. Finally, we prove two theorems at odds with popular belief in cellular communications research. Specifically, we prove that under the SG model, the coverage probability in both DL and UL is independent of BS density. Based on this revelation, a plethora of results in the literature have to be re-examined to rid them of a parameter that has been proven superfluous.", "venue": "ArXiv", "authors": ["Hamed  Nassar", "Gehad  Taher", "El-Sayed  El-Hady"], "year": 2021, "n_citations": 0}
{"id": 4291273, "s2_id": "2147d25f257eeba6ea64bbc66482046b4d30dc46", "title": "A Look at Communication-Intensive Performance in Julia", "abstract": "The Julia programming language continues to gain popularity both for its potential for programmer productivity and for its impressive performance on scientific code. It thus holds potential for large-scale HPC, but we have not yet seen this potential fully realized. While Julia certainly has the machinery to run at scale, and while others have done so for embarrassingly parallel workloads, we have yet to see an analysis of Julia\u2019s performance on communication-intensive codes that are so common in the HPC domain. In this paper we investigate Julia\u2019s performance in this light, first with a suite of microbenchmarks within and without the node, and then using the first Julia port of a standard, HPC benchmarking code, high-performance conjugate gradient (HPCG). We show that if programmers properly balance the computation to communication ratio, Julia can actually outperform C/MPI in a cluster computing environment.", "venue": "ArXiv", "authors": ["Amal  Rizvi", "Kyle C. Hale"], "year": 2021, "n_citations": 0}
{"id": 4294702, "s2_id": "ff2fac32e10bf35f93a4c0a6a94c67d2b4fee390", "title": "Kriptosare.gen, a Dockerized Bitcoin Testbed: Analysis of Server Performance", "abstract": "Bitcoin is a peer-to-peer distributed cryptocurrency system, that keeps all transaction history in a public ledger known as blockchain. The Bitcoin network is implicitly pseudoanonymous and its nodes are controlled by independent entities making network analysis difficult. This calls for the development of a fully controlled testing environment. This paper presents Kriptosare.gen, a dockerized automatized Bitcoin testbed, for deploying full-scale custom Bitcoin networks. The testbed is deployed in a single machine executing four different experiments, each one with different network configuration. We perform a cost analysis to investigate how the resources are related with network parameters and provide experimental data quantifying the amount of computational resources needed to run the different types of simulations. Obtained results demonstrate that it is possible to run the testbed with a configuration similar to a real Bitcoin system.", "venue": "2019 10th IFIP International Conference on New Technologies, Mobility and Security (NTMS)", "authors": ["Francesco  Zola", "Cristina  P\u00e9rez-Sol\u00e0", "Jon Ega\u00f1a Zubia", "Maria  Eguimendia", "Jordi  Herrera-Joancomart\u00ed"], "year": 2019, "n_citations": 1}
{"id": 4294964, "s2_id": "243499ac281a9986e59ed5b29f6cf3ea678f38f4", "title": "Super-speeds with Zero-RAM: Next Generation Large-Scale Optimization in Your Laptop!", "abstract": "This article presents the novel breakthrough general purpose algorithm for large scale optimization problems. The novel algorithm is capable of achieving breakthrough speeds for very large-scale optimization on general purpose laptops and embedded systems. Application of the algorithm to the Griewank function was possible in up to 1 billion decision variables in double precision took only 64485 seconds (~18 hours) to solve, while consuming 7,630 MB (7.6 GB) or RAM on a single threaded laptop CPU. It shows that the algorithm is computationally and memory (space) linearly efficient, and can find the optimal or near-optimal solution in a fraction of the time and memory that many conventional algorithms require. It is envisaged that this will open up new possibilities of real-time large-scale problems on personal laptops and embedded systems.", "venue": "ArXiv", "authors": ["Mark  Amo-Boateng"], "year": 2017, "n_citations": 0}
{"id": 4295182, "s2_id": "a20fbe8bf8231e58ef7f5f3521a416364ff09f0f", "title": "High Performance Code Generation in MLIR: An Early Case Study with GEMM", "abstract": "This article is primarily meant to present an early case study on using MLIR, a new compiler intermediate representation infrastructure, for high-performance code generation. Aspects of MLIR covered in particular include memrefs, the affine dialect, and polyhedral utilities and pass infrastructure surrounding those. This article is also aimed at showing the role compiler infrastructure could play in generating code that is competitive with highly tuned manually developed libraries, albeit in a more modular, reusable, and automatable way.", "venue": "ArXiv", "authors": ["Uday  Bondhugula"], "year": 2020, "n_citations": 9}
{"id": 4296486, "s2_id": "43d61cb32467ee1e8fd881abe1349c14aec5a6ef", "title": "Tensors Come of Age: Why the AI Revolution will help HPC", "abstract": "This article discusses how the automation of tensor algorithms, based on A Mathematics of Arrays and Psi Calculus, and a new way to represent numbers, Unum Arithmetic, enables mechanically provable, scalable, portable, and more numerically accurate software.", "venue": "ArXiv", "authors": ["John L. Gustafson", "Lenore M. Restifo Mullin"], "year": 2017, "n_citations": 2}
{"id": 4308992, "s2_id": "b519da14a75165c635120769869b9f583d1534a6", "title": "Offsite Autotuning Approach - Performance Model Driven Autotuning Applied to Parallel Explicit ODE Methods", "abstract": "Autotuning techniques are a promising approach to minimize the otherwise tedious manual effort of optimizing scientific applications for a specific target platform. Ideally, an autotuning approach is capable of reliably identifying the most efficient implementation variant(s) for a new target system or new characteristics of the input by applying suitable program transformations and analytic models. In this work, we introduce Offsite, an offline autotuning approach which automates this selection process at installation time by rating implementation variants based on an analytic performance model without requiring time-consuming runtime experiments. From abstract multilevel YAML description languages, Offsite automatically derives optimized, platform-specific and problem-specific code of possible implementation variants and applies the performance model to these implementation variants. \nWe apply Offsite to parallel numerical methods for ordinary differential equations (ODEs). In particular, we investigate tuning a specific class of explicit ODE solvers (PIRK methods) for various initial value problems (IVPs) on shared-memory systems. Our experiments demonstrate that Offsite is able to reliably identify a set of the most efficient implementation variants for given test configurations (ODE solver, IVP, platform) and is capable of effectively handling important autotuning scenarios.", "venue": "ArXiv", "authors": ["Johannes  Seiferth", "Matthias  Korch", "Thomas  Rauber"], "year": 2020, "n_citations": 0}
{"id": 4310040, "s2_id": "12277f03fd3227306a89f6e12e89851767646166", "title": "Defence Efficiency", "abstract": "In order to automate actions, such as defences against network attacks, one needs to quantify their efficiency. This can subsequently be used in post-evaluation, learning, etc. In order to quantify the defence efficiency as a function of the impact of the defence and its total cost, we present several natural requirements from such a definition of efficiency and provide a natural definition that complies with these requirements. Next, we precisely characterize our definition of efficiency by the axiomatic approach; namely, we strengthen the original requirements from such a definition and prove that the given definition is the unique definition that satisfies those requirements. Finally, we generalize the definition to the case of any number of input variables in two natural ways, and compare these generalizations.", "venue": "ArXiv", "authors": ["Gleb  Polevoy"], "year": 2019, "n_citations": 2}
{"id": 4311488, "s2_id": "28f07871edc3ddc146635d5a7fd47fa72a4435ed", "title": "Analysis and Approximation of Dual Tandem Queues with Finite Buffer Capacity", "abstract": "Tandem queues with finite buffer capacity commonly exist in practical applications. By viewing a tandem queue as an integrated system, an innovative approach has been developed to analyze its performance through the insight from reduction method. In our approach, the starvation at the bottleneck caused by service time randomness is modeled and captured by interruptions. Fundamental properties of tandem queues with finite buffer capacity are examined. We show that in general system service rate of a dual tandem queue with finite buffer capacity is equal or smaller than its bottleneck service rate, and virtual interruptions, which are the extra idle period at the bottleneck caused by the non-bottlenecks, depend on arrival rates. Hence, system service rate is a function of arrival rate when the buffer capacity of a tandem queue is finite. Approximation for the mean queue time of a dual tandem queue has been developed through the concept of virtual interruptions.", "venue": "ArXiv", "authors": ["Kan  Wu", "Ning  Zhao"], "year": 2014, "n_citations": 1}
{"id": 4312367, "s2_id": "bb685192a14134397943191d8d59014588f04d53", "title": "Performance Evaluation of Components Using a Granularity-based Interface Between Real-Time Calculus and Timed Automata", "abstract": "To analyze complex and heterogeneous real-time embedded systems, recent works have proposed interface techniques between real-time calculus (RTC) and timed automata (TA), in order to take advantage of the strengths of each technique for analyzing various components. But the time to analyze a state-based component modeled by TA may be prohibitively high, due to the state space explosion problem. In this paper, we propose a framework of granularity-based interfacing to speed up the analysis of a TA modeled component. First, we abstract fine models to work with event streams at coarse granularity. We perform analysis of the component at multiple coarse granularities and then based on RTC theory, we derive lower and upper bounds on arrival patterns of the fine output streams using the causality closure algorithm. Our framework can help to achieve tradeoffs between precision and analysis time.", "venue": "QAPL", "authors": ["Karine  Altisen", "Yanhong  Liu", "Matthieu  Moy"], "year": 2010, "n_citations": 13}
{"id": 4313047, "s2_id": "2724fb31aff338bb9c542dc4aa4cc5c1b3f35e54", "title": "Efficient statistical validation with edge cases to evaluate Highly Automated Vehicles", "abstract": "The widescale deployment of Autonomous Vehicles (AV) seems to be imminent despite many safety challenges that are yet to be resolved. It is well known that there are no universally agreed Verification and Validation (VV) methodologies to guarantee absolute safety, which is crucial for the acceptance of this technology. Existing standards focus on deterministic processes where the validation requires only a set of test cases that cover the requirements. Modern autonomous vehicles will undoubtedly include machine learning and probabilistic techniques that require a much more comprehensive testing regime due to the non-deterministic nature of the operating design domain. A rigourous statistical validation process is an essential component required to address this challenge. Most research in this area focuses on evaluating system performance in large scale real-world data gathering exercises (number of miles travelled), or randomised test scenarios in simulation. This paper presents a new approach to compute the statistical characteristics of a system\u2019s behaviour by biasing automatically generated test cases towards the worst case scenarios, identifying potential unsafe edge cases. We use reinforcement learning (RL) to learn the behaviours of simulated actors that cause unsafe behaviour measured by the well established RSS safety metric. We demonstrate that by using the method we can more efficiently validate a system using a smaller number of test cases by focusing the simulation towards the worst case scenario, generating edge cases that correspond to unsafe situations.", "venue": "2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)", "authors": ["Dhanoop  Karunakaran", "Stewart  Worrall", "Eduardo  Nebot"], "year": 2020, "n_citations": 5}
{"id": 4320105, "s2_id": "a8cd0523aca78b2a89d43d346ff66aef83f980cd", "title": "QAOA for Max-Cut requires hundreds of qubits for quantum speed-up", "abstract": "Computational quantum technologies are entering a new phase in which noisy intermediate-scale quantum computers are available, but are still too small to benefit from active error correction. Even with a finite coherence budget to invest in quantum information processing, noisy devices with about 50 qubits are expected to experimentally demonstrate quantum supremacy in the next few years. Defined in terms of artificial tasks, current proposals for quantum supremacy, even if successful, will not help to provide solutions to practical problems. Instead, we believe that future users of quantum computers are interested in actual applications and that noisy quantum devices may still provide value by approximately solving hard combinatorial problems via hybrid classical-quantum algorithms. To lower bound the size of quantum computers with practical utility, we perform realistic simulations of the Quantum Approximate Optimization Algorithm and conclude that quantum speedup will not be attainable, at least for a representative combinatorial problem, until several hundreds of qubits are available.", "venue": "Scientific Reports", "authors": ["Gian Giacomo Guerreschi", "A. Y. Matsuura"], "year": 2019, "n_citations": 87}
{"id": 4320592, "s2_id": "ec39538f797b85549581cd99f33ced8094426ac0", "title": "A Constrained Shortest Path Scheme for Virtual Network Service Management", "abstract": "Virtual network services that span multiple data centers are important to support emerging data-intensive applications in fields such as bioinformatics and retail analytics. Successful virtual network service composition and maintenance requires flexible and scalable \u201cconstrained shortest path management\u201d both in the management plane for virtual network embedding (VNE) or network function virtualization service chaining (NFV-SC), as well as in the data plane for traffic engineering (TE). In this paper, we show analytically and empirically that leveraging constrained shortest paths within recent VNE, NFV-SC and TE algorithms can lead to network utilization gains (of up to 50%) and higher energy efficiency. The management of complex VNE, NFV-SC and TE algorithms can be, however, intractable for large scale substrate networks due to the NP-hardness of the constrained shortest path problem. To address such scalability challenges, we propose a novel, exact constrained shortest path algorithm viz., neighborhoods method (NM). Our NM uses novel search space reduction techniques and has a theoretical quadratic speed-up making it practically faster (by an order of magnitude) than recent branch-and-bound exhaustive search solutions. Finally, we detail our NM-based SDN controller implementation in a real-world testbed to further validate practical NM benefits for virtual network services.", "venue": "IEEE Transactions on Network and Service Management", "authors": ["Dmitrii  Chemodanov", "Flavio  Esposito", "Prasad  Calyam", "Andrei  Sukhov"], "year": 2019, "n_citations": 11}
{"id": 4325641, "s2_id": "e0c4b0f274f354881b4f86a8001f155a6ff049d7", "title": "A comprehensive review of Binary Neural Network", "abstract": "Binary Neural Network (BNN) method is an extreme application of convolutional neural network (CNN) parameter quantization. As opposed to the original CNN methods which employed floating-point computation with full-precision weights and activations, BBN uses 1-bit activations and weights. With BBNs, a significant amount of storage, network complexity, and energy consumption can be reduced, and neural networks can be implemented more efficiently in embedded applications. Unfortunately, binarization causes severe information loss. A gap still exists between full-precision CNN models and their binarized counterparts. The recent developments in BNN have led to a lot of algorithms and solutions that have helped address this issue. This article provides a full overview of recent developments in BNN. The present paper focuses exclusively on 1-bit activations and weights networks, as opposed to previous surveys in which low-bit works are mixed in. In this paper, we conduct a complete investigation of BNN\u2019s development -from their predecessors to the latest BNN algorithms and techniques, presenting a broad design pipeline, and discussing each module\u2019s variants. Along the way, this paper examines BNN (a) purpose: their early successes and challenges; (b) BNN optimization: selected representative works that contain key optimization techniques; (c) deployment: opensource frameworks for BNN modeling and development; (d) terminal: efficient computing architectures and devices for BNN and (e) applications: diverse applications with BNN. Moreover, this paper discusses potential directions and future research opportunities for the latest BNN algorithms and techniques, presents a broad design pipeline, and discusses each module\u2019s variants. Along the way, this paper examines BNN (a) purpose: their early successes and challenges; (b) BNN optimization: selected representative works that contain key optimization techniques; (c) deployment: open-source frameworks for BNN modeling and development; (d) terminal: efficient computing architectures and devices for BNN and (e) applications: diverse applications with BNN. Moreover, this paper discusses potential directions and future research opportunities.", "venue": "ArXiv", "authors": ["Chunyu  Yuan", "Sos S. Agaian"], "year": 2021, "n_citations": 1}
{"id": 4325855, "s2_id": "046bf69a44bac6e62c40e200c6dbb377ea8e3975", "title": "Stochastic modeling of large-scale solid-state storage systems: analysis, design tradeoffs and optimization", "abstract": "Solid state drives (SSDs) have seen wide deployment in mobiles, desktops,and data centers due to their high I/O performance and low energy consumption. As SSDs write data out-of-place, garbage collection (GC) is required to erase and reclaim space with invalid data. However, GC poses additional writes that hinder the I/O performance, while SSD blocks can only endure a finite number of erasures. Thus, there is a performance-durability tradeoff on the design space of GC. To characterize the optimal tradeoff, this paper formulates an analytical model that explores the full optimal design space of any GC algorithm. We first present a stochastic Markov chain model that captures the I/O dynamics of large-scale SSDs, and adapt the mean-field approach to derive the asymptotic steady-state performance. We further prove the model convergence and generalize the model for all types of workload. Inspired by this model, we propose a randomized greedy algorithm (RGA) that can operate along the optimal tradeoff curve with a tunable parameter. Using trace-driven simulation on DiskSim with SSD add-ons, we demonstrate how RGA can be parameterized to realize the performance-durability tradeoff.", "venue": "SIGMETRICS '13", "authors": ["Yongkun  Li", "Patrick P. C. Lee", "John C. S. Lui"], "year": 2013, "n_citations": 45}
{"id": 4326211, "s2_id": "9b341dee891f29a125465d6aa507a4adc41cf67b", "title": "Comparing Hierarchical Data Structures for Sparse Volume Rendering with Empty Space Skipping", "abstract": "Empty space skipping can be efficiently implemented with hierarchical data structures such as k-d trees and bounding volume\nhierarchies. This paper compares several recently published hierarchical data structures with regard to construction and rendering performance. The papers that form our prior work have primarily focused on interactively building the data structures\nand only showed that rendering performance is superior to using simple acceleration data structures such as uniform grids with\nmacro cells. In the area of surface ray tracing, there exists a trade-off between construction and rendering performance of hierarchical data structures. In this paper we present performance comparisons for several empty space skipping data structures\nin order to determine if such a trade-off also exists for volume rendering with uniform data topologies.", "venue": "ArXiv", "authors": ["Stefan  Zellmann"], "year": 2019, "n_citations": 0}
{"id": 4326808, "s2_id": "44910b81fd4669837279c8e3f2ae851a35c92c70", "title": "Spatio-Temporal Correlation of Interference in MANET Under Spatially Correlated Shadowing Environment", "abstract": "Correlation of interference affects spatio-temporal aspects of various wireless mobile systems, such as retransmission, multiple antennas and cooperative relaying. In this paper, we study the spatial and temporal correlation of interference in mobile ad-hoc networks under a correlated shadowing environment. By modeling the node locations as a Poisson point process with an i.i.d. mobility model and considering Gudmundson (1991)'s spatially correlated shadowing model, we theoretically analyze the relationship between the correlation distance of log-normal shadowing and the spatial and temporal correlation coefficients of interference. Since the exact expressions of the correlation coefficients are intractable, we obtain their simple asymptotic expressions as the variance of log-normal shadowing increases. We found in our numerical examples that the asymptotic expansions can be used as tight approximate formulas and useful for modeling general wireless systems under spatially correlated shadowing.", "venue": "IEEE Transactions on Mobile Computing", "authors": ["Tatsuaki  Kimura", "Hiroshi  Saito"], "year": 2021, "n_citations": 1}
{"id": 4326882, "s2_id": "0400fc88e2249c93cc59e1a5cbc61901664c8ec6", "title": "High-Performance Code Generation though Fusion and Vectorization", "abstract": "We present a technique for automatically transforming kernel-based computations in disparate, nested loops into a fused, vectorized form that can reduce intermediate storage needs and lead to improved performance on contemporary hardware. \nWe introduce representations for the abstract relationships and data dependencies of kernels in loop nests and algorithms for manipulating them into more efficient form; we similarly introduce techniques for determining data access patterns for stencil-like array accesses and show how this can be used to elide storage and improve vectorization. \nWe discuss our prototype implementation of these ideas---named HFAV---and its use of a declarative, inference-based front-end to drive transformations, and we present results for some prominent codes in HPC.", "venue": "ArXiv", "authors": ["Jason  Sewall", "Simon J. Pennycook"], "year": 2017, "n_citations": 1}
{"id": 4327184, "s2_id": "4328e3b4ea85ae9ed913afce60dd80ad5565a2ba", "title": "CBinfer: Change-Based Inference for Convolutional Neural Networks on Video Data", "abstract": "Extracting per-frame features using convolutional neural networks for real-time processing of video data is currently mainly performed on powerful GPU-accelerated workstations and compute clusters. However, there are many applications such as smart surveillance cameras that require or would benefit from on-site processing. To this end, we propose and evaluate a novel algorithm for change-based evaluation of CNNs for video data recorded with a static camera setting, exploiting the spatio-temporal sparsity of pixel changes. We achieve an average speed-up of 8.6x over a cuDNN baseline on a realistic benchmark with a negligible accuracy loss of less than 0.1% and no retraining of the network. The resulting energy efficiency is 10x higher than that of per-frame evaluation and reaches an equivalent of 328 GOp/s/W on the Tegra X1 platform.", "venue": "ICDSC", "authors": ["Lukas  Cavigelli", "Philippe  Degen", "Luca  Benini"], "year": 2017, "n_citations": 36}
{"id": 4328273, "s2_id": "63c9896c962491c9ee9a8f48768f991c384c3b78", "title": "AdaptMemBench: Application-Specific MemorySubsystem Benchmarking", "abstract": "Optimizing scientific applications to take full advan-tage of modern memory subsystems is a continual challenge forapplication and compiler developers. Factors beyond working setsize affect performance. A benchmark framework that exploresthe performance in an application-specific manner is essential tocharacterize memory performance and at the same time informmemory-efficient coding practices. We present AdaptMemBench,a configurable benchmark framework that measures achievedmemory performance by emulating application-specific accesspatterns with a set of kernel-independent driver templates. Thisframework can explore the performance characteristics of a widerange of access patterns and can be used as a testbed for potentialoptimizations due to the flexibility of polyhedral code generation.We demonstrate the effectiveness of AdaptMemBench with casestudies on commonly used computational kernels such as triadand multidimensional stencil patterns.", "venue": "ArXiv", "authors": ["Mahesh  Lakshminarasimhan", "Catherine Mills Olschanowsky"], "year": 2018, "n_citations": 1}
{"id": 4331489, "s2_id": "88cc290eda570c9f6556301d0ee679637ca81e99", "title": "ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks", "abstract": "The rapid advancements of computing technology facilitate the development of diverse deep learning applications. Unfortunately, the efficiency of parallel computing infrastructures varies widely with neural network models, which hinders the exploration of the design space to find high-performance neural network architectures on specific computing platforms for a given application. To address such a challenge, we propose a deep learning-based method, ResPerfNet, which trains a residual neural network with representative datasets obtained on the target platform to predict the performance for a deep neural network. Our experimental results show that ResPerfNet can accurately predict the execution time of individual neural network layers and full network models on a variety of platforms. In particular, ResPerfNet achieves 8.4% of mean absolute percentage error for LeNet, AlexNet and VGG16 on the NVIDIA GTX 1080Ti, which is substantially lower than the previously published works.", "venue": "ArXiv", "authors": ["Chuan-Chi  Wang", "Ying-Chiao  Liao", "Chia-Heng  Tu", "Ming-Chang  Kao", "Wen-Yew  Liang", "Shih-Hao  Hung"], "year": 2020, "n_citations": 0}
{"id": 4331836, "s2_id": "5c0206a83cb963875faae92f7c6a6f7ecf4e4696", "title": "Power control for packet streaming with head-of-line deadlines", "abstract": "We consider a mathematical model for streaming media packets (as the motivating key example) from a transmitter buffer to a receiver over a wireless link while controlling the transmitter power (hence, the packet/job processing rate). When each packet comes to the head-of-line (HOL) in the buffer, it is given a deadline D which is the maximum number of times the transmitter can attempt retransmission in order to successfully transmit the packet. If this number of transmission attempts is exhausted, the packet is ejected from the buffer and the next packet comes to the HOL. Costs are incurred in each time slot for holding packets in the buffer, expending transmitter power, and ejecting packets which exceed their deadlines. We investigate how transmission power should be chosen so as to minimize the total cost of transmitting the items in the buffer. We formulate the optimal power control problem in a dynamic programming framework and then hone in on the special case of fixed interference. For this special case, we are able to provide a precise analytic characterization of how the power control should vary with the backlog and how the power control should react to approaching deadlines. In particular, we show monotonicity results for how the transmitter should adapt power levels to the backlog and approaching deadlines. We leverage these analytic results from the special case to build a power control scheme for the general case. Monte Carlo simulations are used to evaluate the performance of the resulting power control scheme as compared to the optimal scheme. The resulting power control scheme is sub-optimal but it provides a low-complexity approximation of the optimal power control. Simulations show that our proposed schemes outperform benchmark algorithms. We also discuss applications of the model to other practical operational scenarios.", "venue": "Perform. Evaluation", "authors": ["Neal  Master", "Nicholas  Bambos"], "year": 2016, "n_citations": 5}
{"id": 4335001, "s2_id": "0d11abb7c6f7009618c78bd5666013ff229ba5f9", "title": "Block iterative eigensolvers for sequences of correlated eigenvalue problems", "abstract": "Abstract In Density Functional Theory simulations based on the LAPW method, each self-consistent field cycle comprises dozens of large dense generalized eigenproblems. In contrast to real-space methods, eigenpairs solving for problems at distinct cycles have either been believed to be independent or at most very loosely connected. In a recent study (Di Napoli et\u00a0al., 2012)\u00a0 [13] , it was demonstrated that, contrary to belief, successive eigenproblems in a sequence are strongly correlated with one another. In particular, by monitoring the subspace angles between eigenvectors of successive eigenproblems, it was shown that these angles decrease noticeably after the first few iterations and become close to collinear. This last result suggests that we can manipulate the eigenvectors, solving for a specific eigenproblem in a sequence, as an approximate solution for the following eigenproblem. In this work we present results that are in line with this intuition. We provide numerical examples where opportunely selected block iterative eigensolvers benefit from the reuse of eigenvectors by achieving a substantial speed-up. The results presented will eventually open the way to a widespread use of block iterative eigensolvers in ab initio electronic structure codes based on the LAPW approach.", "venue": "Comput. Phys. Commun.", "authors": ["Edoardo Di Napoli", "Mario  Berljafa"], "year": 2013, "n_citations": 13}
{"id": 4335901, "s2_id": "1cd70756e00f49690e44a39b3ebeed79460e9482", "title": "A Stochastic Model for File Lifetime and Security in Data Center Networks", "abstract": "Data center networks are an important infrastructure in various applications of modern information technologies. Note that each data center always has a finite lifetime, thus once a data center fails, then it will lose all its storage files and useful information. For this, it is necessary to replicate and copy each important file into other data centers such that this file can increase its lifetime of staying in a data center network. In this paper, we describe a large-scale data center network with a file d-replication policy, which is to replicate each important file into at most \\(d-1\\) other data centers such that this file can maintain in the data center network under a given level of data security in the long-term. To this end, we develop three relevant Markov processes to propose two effective methods for assessing the file lifetime and data security. By using the RG-factorizations, we show that the two methods are used to be able to more effectively evaluate the file lifetime of large-scale data center networks. We hope the methodology and results given in this paper are applicable in the file lifetime and data security study of more general data center networks with replication mechanism.", "venue": "CSoNet", "authors": ["Quan-Lin  Li", "Fan-Qi  Ma", "Jing-Yu  Ma"], "year": 2018, "n_citations": 1}
{"id": 4337237, "s2_id": "003af88548f11f4f3f0e29924a41ea2f1ca6574c", "title": "GPU-Accelerated Mobile Multi-view Style Transfer", "abstract": "An estimated 60% of smartphones sold in 2018 were equipped with multiple rear cameras, enabling a wide variety of 3D-enabled applications such as 3D Photos. The success of 3D Photo platforms (Facebook 3D Photo, Holopix, etc) depend on a steady influx of user generated content. These platforms must provide simple image manipulation tools to facilitate content creation, akin to traditional photo platforms. Artistic neural style transfer, propelled by recent advancements in GPU technology, is one such tool for enhancing traditional photos. However, naively extrapolating single-view neural style transfer to the multi-view scenario produces visually inconsistent results and is prohibitively slow on mobile devices. We present a GPU-accelerated multi-view style transfer pipeline which enforces style consistency between views with on-demand performance on mobile platforms. Our pipeline is modular and creates high quality depth and parallax effects from a stereoscopic image pair.", "venue": "ArXiv", "authors": ["Puneet  Kohli", "Saravana  Gunaseelan", "Jason  Orozco", "Yiwen  Hua", "Edward  Li", "Nicolas  Dahlquist"], "year": 2020, "n_citations": 1}
{"id": 4338176, "s2_id": "80af03f450969e35388a5d7de089039e3c055e3b", "title": "High Performance and Portable Convolution Operators for ARM-based Multicore Processors", "abstract": "The considerable impact of Convolutional Neural Networks on many Artificial Intelligence tasks has led to the development of various high performance algorithms for the convolution operator present in this type of networks. One of these approaches leverages the \\imcol transform followed by a general matrix multiplication (GEMM) in order to take advantage of the highly optimized realizations of the GEMM kernel in many linear algebra libraries. The main problems of this approach are 1) the large memory workspace required to host the intermediate matrices generated by the IM2COL transform; and 2) the time to perform the IM2COL transform, which is not negligible for complex neural networks. This paper presents a portable high performance convolution algorithm based on the BLIS realization of the GEMM kernel that avoids the use of the intermediate memory by taking advantage of the BLIS structure. In addition, the proposed algorithm eliminates the cost of the explicit IM2COL transform, while maintaining the portability and performance of the underlying realization of GEMM in BLIS.", "venue": "ArXiv", "authors": ["Pablo San Juan", "Adri\u00e1n  Castell\u00f3", "Manuel F. Dolz", "Pedro  Alonso-Jord\u00e1", "Enrique S. Quintana-Ort\u0301\u0131"], "year": 2020, "n_citations": 0}
{"id": 4338847, "s2_id": "3514043e41007ea8bd756051f403baf33d504c20", "title": "Comparative Performance Analysis of Intel Xeon Phi, GPU, and CPU", "abstract": "We investigate and characterize the performance of an important class of operations on GPUs and Many Integrated Core (MIC) architectures. Our work is motivated by applications that analyze low-dimensional spatial datasets captured by high resolution sensors, such as image datasets obtained from whole slide tissue specimens using microscopy image scanners. We identify the data access and computation patterns of operations in object segmentation and feature computation categories. We systematically implement and evaluate the performance of these core operations on modern CPUs, GPUs, and MIC systems for a microscopy image analysis application. Our results show that (1) the data access pattern and parallelization strategy employed by the operations strongly affect their performance. While the performance on a MIC of operations that perform regular data access is comparable or sometimes better than that on a GPU; (2) GPUs are significantly more efficient than MICs for operations and algorithms that irregularly access data. This is a result of the low performance of the latter when it comes to random data access; (3) adequate coordinated execution on MICs and CPUs using a performance aware task scheduling strategy improves about 1.29x over a first-come-first-served strategy. The example application attained an efficiency of 84% in an execution with of 192 nodes (3072 CPU cores and 192 MICs).", "venue": "ArXiv", "authors": ["George  Teodoro", "Tahsin M. Kur\u00e7", "Jun  Kong", "Lee A. D. Cooper", "Joel H. Saltz"], "year": 2013, "n_citations": 27}
{"id": 4339677, "s2_id": "3ce68aeedd7fbf56fd7829b642a12e4160e07e72", "title": "Towards a native toplevel for the OCaml language", "abstract": "This paper presents the current state of our work on an interactive toplevel for the OCaml language based on the optimizing native code compiler and runtime. Our native toplevel is up to 100 times faster than the default OCaml toplevel, which is based on the byte code compiler and interpreter. It uses Just-In-Time techniques to compile toplevel phrases to native code at runtime, and currently works with various Unix-like systems running on x86 or x86-64 processors.", "venue": "ArXiv", "authors": ["Marcell  Fischbach", "Benedikt  Meurer"], "year": 2011, "n_citations": 1}
{"id": 4342017, "s2_id": "d68d5fd09304f89b02ca83f558e53220c9fbac8d", "title": "Benchmarking Graph Data Management and Processing Systems: A Survey", "abstract": "The development of scalable, representative, and widely adopted benchmarks for graph data systems have been a question for which answers has been sought for decades. We conduct an in-depth study of the existing literature on benchmarks for graph data management and processing, covering 20 different benchmarks developed during the last 15 years. We categorize the benchmarks into three areas focusing on benchmarks for graph processing systems, graph database benchmarks, and bigdata benchmarks with graph processing workloads. This systematic approach allows us to identify multiple issues existing in this area, including i) few benchmarks exist which can produce high workload scenarios, ii) no significant work done on benchmarking graph stream processing as well as graph based machine learning, iii) benchmarks tend to use conventional metrics despite new meaningful metrics have been around for years, iv) increasing number of big data benchmarks appear with graph processing workloads. Following these observations, we conclude the survey by describing key challenges for future research on graph data systems benchmarking.", "venue": "ArXiv", "authors": ["Miyuru  Dayarathna", "Toyotaro  Suzumura"], "year": 2020, "n_citations": 0}
{"id": 4351122, "s2_id": "34dacd7447574c980cab218d773627b78dbce6cb", "title": "Age of Information in an Overtake- Free Network of Quasi - Reversible Queues", "abstract": "We show how to calculate the Age of Information in an overtake-free network of quasi-reversible queues, with exponential exogenous interarrivals of multiple classes of update packets and exponential service times at all nodes. Results are provided for any number of M/M/1 First-Come-First-Served (FCFS) queues in tandem, and for a network with two classes of update packets, entering through different queues in the network and exiting through the same queue. The main takeaway is that in a network with different classes of update packets, individual classes roughly preserve the ages they would achieve if they were alone in the network, except when shared queues become saturated, in which case the ages increase considerably. The results are extensible for other quasi-reversible queues for which sojourn time distributions are known, such as M/M/c FCFS queues and processor-sharing queues.", "venue": "2020 28th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)", "authors": ["Ioannis  Koukoutsidis"], "year": 2020, "n_citations": 0}
{"id": 4357093, "s2_id": "f34619da731f8d95a63a4c0ee699886ad225cdc1", "title": "Stabilization of Branching Queueing Networks", "abstract": "Queueing networks are gaining attraction for the performance analysis of parallel computer systems. A Jackson network is a set of interconnected servers, where the completion of a job at server i may result in the creation of a new job for server j. We propose to extend Jackson networks by \u201cbranching\u201d and by \u201ccontrol\u201d features. Both extensions are new and substantially expand the modelling power of Jackson networks. On the other hand, the extensions raise computational questions, particularly concerning the stability of the networks, i.e, the ergodicity of the underlying Markov chain. We show for our extended model that it is decidable in polynomial time if there exists a controller that achieves stability. Moreover, if such a controller exists, one can eciently compute a static randomized controller which stabilizes the network in a very strong sense; in particular, all moments of the queue sizes are finite. 1998 ACM Subject Classification G.3 Probability and Statistics", "venue": "STACS", "authors": ["Tom\u00e1s  Br\u00e1zdil", "Stefan  Kiefer"], "year": 2012, "n_citations": 2}
{"id": 4358478, "s2_id": "c0155464ce2c39154f8c7d1d673ec4d2e7c6bd44", "title": "On Transitory Queueing", "abstract": "We introduce a framework and develop a theory of transitory queueing models. These are models that are not only non-stationary and time-varying but also have other features such as the queueing system operates over finite time, or only a finite population arrives. Such models are relevant in many real-world settings, from queues at post-offces, DMV, concert halls and stadia to out-patient departments at hospitals. We develop fluid and diffusion limits for a large class of transitory queueing models. We then introduce three specific models that fit within this framework, namely, the Delta(i)/GI/1 model, the conditioned G/GI/1 model, and an arrival model of scheduled traffic with epoch uncertainty. We show that asymptotically these models are distributionally equivalent, i.e., they have the same fluid and diffusion limits. We note that our framework provides the first ever way of analyzing the standard G/GI/1 model when we condition on the number of arrivals. In obtaining these results, we provide generalizations and extensions of the Glivenko-Cantelli and Donskers Theorem for empirical processes with triangular arrays. Our analysis uses the population acceleration technique that we introduce and develop. This may be useful in analysis of other non-stationary and non-ergodic queuing models.", "venue": "ArXiv", "authors": ["Harsha  Honnappa", "Rahul  Jain", "Amy R. Ward"], "year": 2014, "n_citations": 22}
{"id": 4360471, "s2_id": "ea5470db69cc1c5cd45ce89f4ed5c60688687742", "title": "Monotonically relaxing concurrent data-structure semantics for performance: An efficient 2D design framework", "abstract": "There has been a significant amount of work in the literature proposing semantic relaxation of concurrent data structures for improving scalability and performance. By relaxing the semantics of a data structure, a bigger design space, that allows weaker synchronization and more useful parallelism, is unveiled. Investigating new data structure designs, capable of trading semantics for achieving better performance in a monotonic way, is a major challenge in the area. We algorithmically address this challenge in this paper. We present an efficient, lock-free, concurrent data structure design framework for out-of-order semantic relaxation. Our framework introduces a new two dimensional algorithmic design, that uses multiple instances of a given data structure. The first dimension of our design is the number of data structure instances operations are spread to, in order to benefit from parallelism through disjoint memory access. The second dimension is the number of consecutive operations that try to use the same data structure instance in order to benefit from data locality. Our design can flexibly explore this two-dimensional space to achieve the property of monotonically relaxing concurrent data structure semantics for achieving better throughput performance within a tight deterministic relaxation bound, as we prove in the paper. We show how our framework can instantiate lock-free out-of-order queues, stacks, counters and dequeues. We provide implementations of these relaxed data structures and evaluate their performance and behaviour on two parallel architectures. Experimental evaluation shows that our two-dimensional data structures significantly outperform the respected previous proposed ones with respect to scalability and throughput performance. Moreover, their throughput increases monotonically as relaxation increases.", "venue": "ArXiv", "authors": ["Adones  Rukundo", "Aras  Atalar", "Philippas  Tsigas"], "year": 2019, "n_citations": 2}
{"id": 4361348, "s2_id": "ddcdfbc851dbc372961948565c334eb19c14aec8", "title": "Online Application Guidance for Heterogeneous Memory Systems", "abstract": "As scaling of conventional memory devices has stalled, many high end and next generation computing systems have begun to incorporate alternative memory technologies to meet performance goals. Since these technologies present distinct advantages and tradeoffs compared to conventional DDR* SDRAM, such as higher bandwidth with lower capacity or vice versa, they are typically packaged alongside conventional SDRAM in a heterogeneous memory architecture. To utilize the different types of memory efficiently, new data management strategies are needed to match application usage to the best available memory technology. However, current proposals for managing heterogeneous memories are limited because they either: 1) do not consider high-level application behavior when assigning data to different types of memory, or 2) require separate program execution (with a representative input) to collect information about how the application uses memory resources. This work presents a new data management toolset to address the limitations of existing approaches for managing complex memories. It extends the application runtime layer with automated monitoring and management routines that assign application data to the best tier of memory based on previous usage, without any need for source code modification or a separate profiling run. It evaluates this approach on a state-of-theart server platform with both conventional DDR4 SDRAM and non-volatile Intel\u00ae OptaneTM DC memory, using both memory-intensive high performance computing (HPC) applications as well as standard benchmarks. Overall, the results show that this approach improves program performance significantly compared to a standard unguided approach across a variety of workloads and system configurations. The HPC applications exhibit the largest benefits, with speedups ranging from 1.4x to 7x in the best cases. Additionally, we show that this approach achieves similar performance as a comparable offline profiling-based approach after a short startup period, without requiring separate program execution or offline analysis steps.", "venue": "ArXiv", "authors": ["M. Ben Olson", "Brandon  Kammerdiener", "Kshitij A. Doshi", "Terry  Jones", "Michael R. Jantz"], "year": 2021, "n_citations": 0}
{"id": 4363919, "s2_id": "9acb5bfd83d8591ef1c6bbf6d40fe2244e0b5b96", "title": "Effects of round-to-nearest and stochastic rounding in the numerical solution of the heat equation in low precision", "abstract": "Motivated by the advent of machine learning, the last few years saw the return of hardware-supported low-precision computing. Computations with fewer digits are faster and more memory and energy efficient, but can be extremely susceptible to rounding errors. An application that can largely benefit from the advantages of low-precision computing is the numerical solution of partial differential equations (PDEs), but a careful implementation and rounding error analysis are required to ensure that sensible results can still be obtained. \nIn this paper we study the accumulation of rounding errors in the solution of the heat equation, a proxy for parabolic PDEs, via Runge-Kutta finite difference methods using round-to-nearest (RtN) and stochastic rounding (SR). We demonstrate how to implement the scheme to reduce rounding errors and we derive \\emph{a priori} estimates for local and global rounding errors. Let $u$ be the roundoff unit. While the worst-case local errors are $O(u)$ with respect to the discretization parameters, the RtN and SR error behavior is substantially different. We prove that the RtN solution is discretization, initial condition and precision dependent, and always stagnates for small enough $\\Delta t$. Until stagnation, the global error grows like $O(u\\Delta t^{-1})$. In contrast, we show that the leading order errors introduced by SR are zero-mean, independent in space and mean-independent in time, making SR resilient to stagnation and rounding error accumulation. In fact, we prove that for SR the global rounding errors are only $O(u\\Delta t^{-1/4})$ in 1D and are essentially bounded (up to logarithmic factors) in higher dimensions.", "venue": "ArXiv", "authors": ["Matteo  Croci", "Michael Bryce Giles"], "year": 2020, "n_citations": 3}
{"id": 4364098, "s2_id": "17d29c37f93e2148fa6dd2a650e598bf5bcc6564", "title": "Sensifi: A Wireless Sensing System for Ultra-High-Rate Applications", "abstract": "Wireless Sensor Networks (WSNs) are being used in various applications such as structural health monitoring and industrial control. Since energy efficiency is one of the major design factors, the existing WSNs primarily rely on low-power, low-rate wireless technologies such as 802.15.4 and Bluetooth. In this paper, we strive to tackle the challenges of developing ultra-high-rate WSNs based on 802.11 (WiFi) standard by proposing Sensifi. As an illustrative application of this system, we consider vibration test monitoring of spacecraft and identify system design requirements and challenges. Our main contributions are as follows. First, we propose packet encoding methods to reduce the overhead of assigning accurate timestamps to samples. Second, we propose energy efficiency methods to enhance the system's lifetime. Third, we reduce the overhead of processing outgoing packets through network stack to enhance sampling rate and mitigate sampling rate instability. Fourth, we study and reduce the delay of processing incoming packets through network stack to enhance the accuracy of time synchronization among nodes. Fifth, we propose a low-power node design for ultra-high-rate applications. Sixth, we use our node design to empirically evaluate the system.", "venue": "IEEE Internet of Things Journal", "authors": ["Chia-Chi  Li", "Vikram K. Ramanna", "Daniel  Webber", "Cole  Hunter", "Tyler  Hack", "Behnam  Dezfouli"], "year": 2021, "n_citations": 2}
{"id": 4369061, "s2_id": "bd671fe4054050318dff6099435a22b0eb74a101", "title": "Efficient Implementation of the Overlap Operator on Multi-GPUs", "abstract": "Lattice QCD calculations were one of the first applications to show the potential of GPUs in the area of high performance computing. Our interest is to find ways to effectively use GPUs for lattice calculations using the overlap operator. The large memory footprint of these codes requires the use of multiple GPUs in parallel. In this paper we show the methods we used to implement this operator efficiently. We run our codes both on a GPU cluster and a CPU cluster with similar interconnects. We find that to match performance the CPU cluster requires 20-30 times more CPU cores than GPUs.", "venue": "2011 Symposium on Application Accelerators in High-Performance Computing", "authors": ["Andrei  Alexandru", "Michael  Lujan", "Craig  Pelissier", "Ben  Gamari", "Frank X. Lee"], "year": 2011, "n_citations": 30}
{"id": 4370553, "s2_id": "93498ee275de0b3dd04bb43258c950291894ff69", "title": "Differentiated end-to-end Internet services using a weighted proportional fair sharing TCP", "abstract": "In this document we study the application of weighted proportional fairness to data flows in the Internet. We let the users set the weights of their connections in order to maximise the utility they get from the network. When combined with a pricing scheme where connections are billed by weight and time, such a system is known to maximise the total utility of the network. Our study case is a national Web cache server connected to long distance links. We propose two ways of weighting TCP connections by manipulating some parameters of the protocol and present results from simulations and prototypes. We finally discuss how proportional fairness could be used to implement an Internet with differentiated services.", "venue": "CCRV", "authors": ["Jon  Crowcroft", "Philippe  Oechslin"], "year": 1998, "n_citations": 293}
{"id": 4371639, "s2_id": "bc90082f7847ceb15c58d34b370ecb7218d9f605", "title": "Security Rating Metrics for Distributed Wireless Systems", "abstract": "The paper examines quantitative assessment of wireless distribution system security, as well as an assessment of risks from attacks and security violations. Furthermore, it describes typical security breach and formal attack models and five methods for assessing security. The proposed normalized method for assessing the degree of security assurance operates with at least three characteristics, which allows comparatively analyze heterogeneous information systems. The improved calculating formulas have been proposed for two security assessment methods, and the elements of functional-cost analysis have been applied to calculate the degree of security. To check the results of the analysis, the coefficient of concordance was calculated, which gives opportunity to determine the quality of expert assessment. The simultaneous use of several models to describe attacks and the effectiveness of countering them allows us to create a comprehensive approach to countering modern security threats to information networks at the commercial enterprises and critical infrastructure facilities.", "venue": "MoMLeT", "authors": ["Volodymyr  Buriachok", "Volodymyr  Sokolov", "Pavlo  Skladannyi"], "year": 2019, "n_citations": 1}
{"id": 4381609, "s2_id": "0fc3066cae7c500a5d029eaf9e71034906784a22", "title": "A Transactional Perspective on Execute-order-validate Blockchains", "abstract": "Smart contracts have enabled blockchain systems to evolve from simple cryptocurrency platforms to general transactional systems. A new architecture called execute-order-validate has been proposed in Hyperledger Fabric to support parallel transactions. However, this architecture might render many invalid transactions when serializing them. This problem is further exaggerated as the block formation rate is inherently limited due to other factors beside data processing, such as cryptography and consensus. Inspired by optimistic concurrency control in modern databases, we propose a novel method to enhance the execute-order-validate architecture, by reordering transactions to reduce the abort rate. In contrast to existing blockchains that adopt database's preventive approaches which might over-abort serializable transactions, our method is theoretically more fine-grained: unserializable transactions are aborted before reordering and the rest are guaranteed to be serializable. We implement our method in two blockchains respectively, FabricSharp on top of Hyperledger Fabric, and FastFabricSharp on top of FastFabric. We compare the performance of FabricSharp with vanilla Fabric and three related systems, two of which are respectively implemented with one standard and one state-of-the-art concurrency control techniques from databases. The results demonstrate that FabricSharp achieves 25% higher throughput compared to the other systems in nearly all experimental scenarios. Moreover, the FastFabricSharp's improvement on FastFabric is up to 66%.", "venue": "SIGMOD Conference", "authors": ["Pingcheng  Ruan", "Dumitrel  Loghin", "Quang-Trung  Ta", "Meihui  Zhang", "Gang  Chen", "Beng Chin Ooi"], "year": 2020, "n_citations": 19}
{"id": 4390064, "s2_id": "0a13b0fd290f4fec56fde2dc46b777d254a1d17e", "title": "Optimizing the Age-of-Information for Mobile Users in Adversarial and Stochastic Environments", "abstract": "We study a multi-user downlink scheduling problem for optimizing the freshness of information available to users roaming across multiple cells. We consider both adversarial and stochastic settings and design scheduling policies that optimize two distinct information freshness metrics, namely the average age-of-information and the peak age-of-information. We show that a natural greedy scheduling policy is competitive with the optimal offline policy in the adversarial setting. We also derive fundamental lower bounds to the competitive ratio achievable by any online policy. In the stochastic environment, we show that a Max-Weight scheduling policy that takes into account the channel statistics achieves an approximation factor of $2$ for minimizing the average age of information in two extreme mobility scenarios. We conclude the paper by establishing a large-deviation optimality result achieved by the greedy policy for minimizing the peak age of information for static users situated at a single cell.", "venue": "ArXiv", "authors": ["Abhishek  Sinha", "Rajarshi  Bhattacharjee"], "year": 2020, "n_citations": 1}
{"id": 4391248, "s2_id": "5c8914f3f7035a7f68381193906fa59afb9a24d4", "title": "A simple policy for multiple queues with size-independent service times", "abstract": "We consider a service system with two Poisson arrival queues. A server chooses which queue to serve at each moment. Once a queue is served, all the customers will be served within a fixed amount of time. This model is useful in studying airport shuttling or certain online computing systems. We propose a simple yet optimal state-independent policy for this problem which is not only easy to implement, but also performs very well.", "venue": "Oper. Res. Lett.", "authors": ["Yuhang  Liu", "Zizhuo  Wang"], "year": 2013, "n_citations": 0}
{"id": 4391811, "s2_id": "d067e067dca00bea14c4b492f548ca31df524fea", "title": "Benchmarking Memory-Centric Computing Systems: Analysis of Real Processing-In-Memory Hardware", "abstract": "Many modern workloads such as neural network inference and graph processing are fundamentally memory-bound. For such workloads, data movement between memory and CPU cores imposes a significant overhead in terms of both latency and energy. A major reason is that this communication happens through a narrow bus with high latency and limited bandwidth, and the low data reuse in memory-bound workloads is insufficient to amortize the cost of memory access. Fundamentally addressing this data movement bottleneck requires a paradigm where the memory system assumes an active role in computing by integrating processing capabilities. This paradigm is known as processing-in-memory (PIM). Recent research explores different forms of PIM architectures, motivated by the emergence of new technologies that integrate memory with a logic layer, where processing elements can be easily placed. Past works evaluate these architectures in simulation or, at best, with simplified hardware prototypes. In contrast, the UPMEM company has designed and manufactured the first publicly-available real-world PIM architecture. The UPMEM PIM architecture combines traditional DRAM memory arrays with general-purpose in-order cores, called DRAM Processing Units (DPUs), integrated in the same chip. This paper presents key takeaways from the first comprehensive analysis [1] of the first publicly-available real-world PIM architecture. First, we introduce our experimental characterization of the UPMEM PIM architecture using microbenchmarks, and present PrIM (Processing-In-Memory benchmarks), a benchmark suite of 16 workloads from different application domains (e.g., dense/sparse linear algebra, databases, data analytics, graph processing, neural networks, bioinformatics, image processing), which we identify as memory-bound. Second, we provide four key takeaways about the UPMEM PIM architecture, which stem from our study of the performance and scaling characteristics of PrIM benchmarks on the UPMEM PIM architecture, and their performance and energy consumption comparison to their state-of-the-art CPU and GPU counterparts. More insights about suitability of different workloads to the PIM system, programming recommendations for software designers, and suggestions and hints for hardware and architecture designers of future PIM systems are available in [1].", "venue": "2021 12th International Green and Sustainable Computing Conference (IGSC)", "authors": ["Juan  G'omez-Luna", "Izzat El Hajj", "Ivan  Fernandez", "Christina  Giannoula", "Geraldo F. Oliveira", "Onur  Mutlu"], "year": 2021, "n_citations": 1}
{"id": 4398525, "s2_id": "3209b47495d01fa75e5b78c3c9ee2919eb6771ca", "title": "A Comparative Study on the Performance of the Top DBMS Systems", "abstract": "Database management systems are today\u2019s most reliable mean to organize data into collections that can be searched and updated. However, many DBMS systems are available on the market each having their pros and cons in terms of reliability, usability, security, and performance. This paper presents a comparative study on the performance of the top DBMS systems. They are mainly MS SQL Server 2008, Oracle 11g, IBM DB2, MySQL 5.5, and MS Access 2010. The testing is aimed at executing different SQL queries with different level of complexities over the different five DBMSs under test. This would pave the way to build a head-to-head comparative evaluation that shows the average execution time, memory usage, and CPU utilization of each DBMS after completion of the test.", "venue": "ArXiv", "authors": ["Youssef  Bassil"], "year": 2012, "n_citations": 26}
{"id": 4398824, "s2_id": "29fd8bd77ec35fbadb591f08cfd57672091437c4", "title": "Quiescence of self-stabilizing gossiping among mobile agents in graphs", "abstract": "This paper considers gossiping among mobile agents in graphs: agents move on the graph and have to disseminate their initial information to every other agent. We focus on self-stabilizing solutions for the gossip problem, where agents may start from arbitrary locations in arbitrary states. Self-stabilization requires (some of the) participating agents to keep moving forever, hinting at maximizing the number of agents that could be allowed to stop moving eventually. This paper formalizes the self-stabilizing agent gossip problem, introduces the quiescence number (i.e., the maximum number of eventually stopping agents) of self-stabilizing solutions and investigates the quiescence number with respect to several assumptions related to agent anonymity, synchrony, link duplex capacity, and whiteboard capacity.", "venue": "Theor. Comput. Sci.", "authors": ["Toshimitsu  Masuzawa", "S\u00e9bastien  Tixeuil"], "year": 2010, "n_citations": 2}
{"id": 4402855, "s2_id": "37438a7a92fdf99343e03800f36e6c2ebc15b6d2", "title": "Effect of Packet Delay Variation on Video-Voice over DiffServ-MPLS in IPv4-IPv6 Networks", "abstract": "Over the last years, we have witnessed a rapid deployment of real-time applications on the Internet as well as many research works about Quality of Service (QoS), in particular IPv4 (Internet Protocol version 4). The inevitable exhaustion of the remaining IPv4 address pool has become progressively evident. As the evolution of Internet Protocol (IP) continues, the deployment of IPv6 QoS is underway. Today, there is limited experience in the deployment of QoS for IPv6 traffic in MPLS backbone networks in conjunction with DiffServ (Differentiated Services) support. DiffServ itself does not have the ability to control the traffic which has been taken for end-to-end path while a number of links of the path are congested. In contrast, MPLS Traffic Engineering (TE) is accomplished to control the traffic and can set up end-to-end routing path before data has been forwarded. From the evolution of IPv4 QoS solutions, we know that the integration of DiffServ and MPLS TE satisfies the guaranteed QoS requirement for real-time applications. This paper presents a QoS performance study of real-time applications such as voice and video conferencing in terms of Packet Delay Variation (PDV) over DiffServ with or without MPLS TE in IPv4/IPv6 networks using Optimized Network Engineering Tool (OPNET). We also study the interaction of Expedited Forwarding (EF), Assured Forwarding (AF) traffic aggregation, link congestion, as well as the effect of performance metric such as PDV. The effectiveness of DiffServ and MPLS TE integration in IPv4/IPv6 network is illustrated and analyzed. This paper shows that IPv6 experiences more PDV than their IPv4 counterparts.", "venue": "ArXiv", "authors": ["Md. Tariq Aziz", "Mohammad Saiful Islam", "Md. Nazmul Islam Khan", "Adrian  Popescu"], "year": 2012, "n_citations": 13}
{"id": 4405313, "s2_id": "49ed9041cbbbd899c875807052c4bbcca012f5df", "title": "Fusion-Catalyzed Pruning for Optimizing Deep Learning on Intelligent Edge Devices", "abstract": "The increasing computational cost of deep neural network models limits the applicability of intelligent applications on resource-constrained edge devices. While a number of neural network pruning methods have been proposed to compress the models, prevailing approaches focus only on parametric operators (e.g., convolution), which may miss optimization opportunities. In this article, we present a novel fusion-catalyzed pruning approach, called FuPruner, which simultaneously optimizes the parametric and nonparametric operators for accelerating neural networks. We introduce an aggressive fusion method to equivalently transform a model, which extends the optimization space of pruning and enables nonparametric operators to be pruned in a similar manner as parametric operators, and a dynamic filter pruning method is applied to decrease the computational cost of models while retaining the accuracy requirement. Moreover, FuPruner provides configurable optimization options for controlling fusion and pruning, allowing much more flexible performance-accuracy tradeoffs to be made. Evaluation with state-of-the-art residual neural networks on five representative intelligent edge platforms, Jetson TX2, Jetson Nano, Edge tensor processing unit, neural compute stick, and neural compute stick 2, demonstrates the effectiveness of our approach, which can accelerate the inference of models on CIFAR-10 and ImageNet datasets.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Guangli  Li", "Xiu  Ma", "Xueying  Wang", "Lei  Liu", "Jingling  Xue", "Xiaobing  Feng"], "year": 2020, "n_citations": 2}
{"id": 4408128, "s2_id": "c793f23187645bfbc7424645e8b5e306f354807a", "title": "Exploring performance and power properties of modern multi\u2010core chips via simple machine models", "abstract": "Modern multi\u2010core chips show complex behavior with respect to performance and power. Starting with the Intel Sandy Bridge processor, it has become possible to directly measure the power dissipation of a CPU chip and correlate this data with the performance properties of the running code. Going beyond a simple bottleneck analysis, we employ the recently published Execution\u2010Cache\u2010Memory (ECM) model to describe the single\u2010core and multi\u2010core performance of streaming kernels. The model refines the well\u2010known roofline model, because it can predict the scaling and the saturation behavior of bandwidth\u2010limited loop kernels on a multi\u2010core chip. The saturation point is especially relevant for considerations of energy consumption. From power dissipation measurements of benchmark programs with vastly different requirements to the hardware, we derive a simple, phenomenological power model for the Sandy Bridge processor. Together with the ECM model, we are able to explain many peculiarities in the performance and power behavior of multi\u2010core processors and derive guidelines for energy\u2010efficient execution of parallel programs. Finally, we show that the ECM and power models can be successfully used to describe the scaling and power behavior of a lattice Boltzmann flow solver code. Copyright \u00a9 2013 John Wiley & Sons, Ltd.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Georg  Hager", "Jan  Treibig", "Johannes  Habich", "Gerhard  Wellein"], "year": 2016, "n_citations": 102}
{"id": 4411185, "s2_id": "431378719035d60d13a63b68bd6f7035faebb0c3", "title": "Improving TCP Performance over Wireless Network with Frequent Disconnections", "abstract": "Presented in this paper is the solution to the problem that arises when the TCP/IP protocol suite is used to provide Internet connectivity through mobile terminals over emerging 802.11 wireless links. Taking into consideration the strong drive towards wireless Internet access through mobile terminals, the problem of frequent disconnections causing serial timeouts is examined and analyzed, with the help of extensive simulations. After a detailed review of wireless link loss recovery mechanism and identification of related problems, a new scheme with modifications at link layer and transport layer is proposed. The proposed modifications which depend on interaction between two layers (i) reduce the idle time before transmission at TCP by preventing timeout occurrences and (ii) decouple the congestion control from recovery of the losses due to link failure. Results of simulation based experiments demonstrate considerable performance improvement with the proposed modifications over the conventional TCP, when a wireless sender is experiencing frequent link failures.", "venue": "ArXiv", "authors": ["Purvang  Dalal", "Nikhil  Kothari", "Kankar S. Dasgupta"], "year": 2011, "n_citations": 24}
{"id": 4416134, "s2_id": "773a106186f8c37bc85f44fb9e2997807ea8deba", "title": "Towards rigorous validation of energy optimisation experiments", "abstract": "The optimisation of software energy consumption is of growing importance across all scales of modern computing, i.e., from embedded systems to data-centres. Practitioners in the field of Search-Based Software Engineering and Genetic Improvement of Software acknowledge that optimising software energy consumption is difficult due to noisy and expensive fitness evaluations. However, it is apparent from results to date that more progress needs to be made in rigorously validating optimisation results. This problem is pressing because modern computing platforms have highly complex and variable behaviour with respect to energy consumption. To compare solutions fairly we propose in this paper a new validation approach called R3-validation which exercises software variants in a rotated-round-robin order. Using a case study, we present an in-depth analysis of the impacts of changing system states on software energy usage, and we show how R3-validation mitigates these. We compare it with current validation approaches across multiple devices and operating systems, and we show that it aligns best with actual platform behaviour.", "venue": "GECCO", "authors": ["Mahmoud A. Bokhari", "Brad  Alexander", "Markus  Wagner"], "year": 2020, "n_citations": 3}
{"id": 4419210, "s2_id": "99544e4b075c421ffef5276e6f8185ac4689db99", "title": "Opportunistic relaying in wireless body area networks: Coexistence performance", "abstract": "In this paper, a cooperative two-hop communication scheme, together with opportunistic relaying (OR), is applied within a mobile wireless body area network (WBAN). Its effectiveness in interference mitigation is investigated in a scenario where there are multiple closely-located networks. Due to a typical WBAN's nature, no coordination is used among different WBANs. A suitable time-division-multiple-access (TDMA) scheme is adopted as both an intra-network and also an internetwork access scheme. Extensive on-body and off-body channel gain measurements are employed to gauge performance, which are overlaid to simulate a realistic WBAN working environment. It is found that opportunistic relaying is able to improve the signal-to-interference-plus-noise ratio (SINR) performance at an outage probability of 10% by an average of 5 dB, and it is also shown that it can reduce level crossing rate (LCR) significantly at low SINRs. Furthermore, this scheme is more efficient when on-body channels fade more rapidly.", "venue": "2013 IEEE International Conference on Communications (ICC)", "authors": ["Jie  Dong", "David B. Smith"], "year": 2013, "n_citations": 25}
{"id": 4420365, "s2_id": "97ee283e933e5437f23b679b3ee2f115089736ac", "title": "On the equivalence between multiclass processor sharing and random order scheduling policies", "abstract": "Consider a single server system serving a multiclass population. Some popular scheduling policies for such system are the discriminatory processor sharing (DPS), discriminatory random order service (DROS), generalized processor sharing (GPS) and weighted fair queueing (WFQ). In this paper, we propose two classes of policies, namely MPS (multiclass processor sharing) and MROS (multiclass random order service), that generalize the four policies mentioned above. For the special case when the multiclass population arrive according to Poisson processes and have independent and exponential service requirement with parameter ?, we show that the tail of the sojourn time distribution for a class i customer in a system with the MPS policy is a constant multiple of the tail of the waiting time distribution of a class i customer in a system with the MROS policy. This result implies that for a class i customer, the tail of the sojourn time distribution in a system with the DPS (GPS) scheduling policy is a constant multiple of the tail of the waiting time distribution in a system with the DROS (respectively WFQ) policy.", "venue": "PERV", "authors": ["Konstantin  Avrachenkov", "Tejas  Bodas"], "year": 2018, "n_citations": 1}
{"id": 4421825, "s2_id": "44b5bb6c6e6b5d2afbaf9794d6d48126174eaf90", "title": "Fluid and Diffusion Limits for Bike Sharing Systems", "abstract": "Bike sharing systems have rapidly developed around the world, and they are served as a promising strategy to improve urban traffic congestion and to decrease polluting gas emissions. So far performance analysis of bike sharing systems always exists many difficulties and challenges under some more general factors. In this paper, a more general large-scale bike sharing system is discussed by means of heavy traffic approximation of multiclass closed queueing networks with non-exponential factors. Based on this, the fluid scaled equations and the diffusion scaled equations are established by means of the numbers of bikes both at the stations and on the roads, respectively. Furthermore, the scaling processes for the numbers of bikes both at the stations and on the roads are proved to converge in distribution to a semimartingale reflecting Brownian motion (SRBM) in a $N^{2}$-dimensional box, and also the fluid and diffusion limit theorems are obtained. Furthermore, performance analysis of the bike sharing system is provided. Thus the results and methodology of this paper provide new highlight in the study of more general large-scale bike sharing systems.", "venue": "QTNA", "authors": ["Quan-Lin  Li", "Zhi-Yong  Qian", "Rui-Na  Fan"], "year": 2017, "n_citations": 5}
{"id": 4422085, "s2_id": "d215d57f5588b947f0be72283c8de1e49b2ff3c9", "title": "On Resource Pooling and Separation for LRU Caching", "abstract": "Caching systems using the Least Recently Used (LRU) principle have now become ubiquitous. A fundamental question for these systems is whether the cache space should be pooled together or divided to serve multiple flows of data item requests in order to minimize the miss probabilities. In this paper, we show that there is no straight yes or no answer to this question, depending on complex combinations of critical factors, including, e.g., request rates, overlapped data items across different request flows, data item popularities and their sizes. To this end, we characterize the performance of multiple flows of data item requests under resource pooling and separation for LRU caching when the cache size is large. Analytically, we show that it is asymptotically optimal to jointly serve multiple flows if their data item sizes and popularity distributions are similar and their arrival rates do not differ significantly; the self-organizing property of LRU caching automatically optimizes the resource allocation among them asymptotically. Otherwise, separating these flows could be better, e.g., when data sizes vary significantly. We also quantify critical points beyond which resource pooling is better than separation for each of the flows when the overlapped data items exceed certain levels. Technically, for a broad class of heavy-tailed distributions we derive the asymptotic miss probabilities of multiple flows of requests with varying data item sizes in a shared LRU cache space. It also validates the characteristic time approximation under certain conditions. These results provide new insights on improving the performance of caching systems.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Jian  Tan", "Guocong  Quan", "Kaiyi  Ji", "Ness B. Shroff"], "year": 2018, "n_citations": 16}
{"id": 4424072, "s2_id": "e9c7cd806aa9dcb948fa3bedfe0920a9ded0f388", "title": "Measuring and Comparing the Scaling Behaviour of a High-Performance CFD Code on Different Supercomputing Infrastructures", "abstract": "Parallel code design is a challenging task especially when addressing petascale systems for massive parallel processing (MPP), i.e. parallel computations on several hundreds of thousands of cores. An in-house computational fluid dynamics code, developed by our group, was designed for such high-fidelity runs in order to exhibit excellent scalability values. Basis for this code is an adaptive hierarchical data structure together with an efficient communication and (numerical) computation scheme that supports MPP. For a detailled scalability analysis, we performed several experiments on two of Germany's national supercomputers up to 140,000 processes. In this paper, we will show the results of those experiments and discuss any bottlenecks that could be observed while solving engineering-based problems such as porous media flows or thermal comfort assessments for problem sizes up to several hundred billion degrees of freedom.", "venue": "2015 17th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)", "authors": ["J\u00e9r\u00f4me  Frisch", "Ralf-Peter  Mundani"], "year": 2015, "n_citations": 5}
{"id": 4424548, "s2_id": "af7561c2699407a3b5abb99e05fdf480b19a1ac5", "title": "The M/M/Infinity Service System with Ranked Servers in Heavy Traffic", "abstract": "We consider an M/M/Infinity service system in which an arriving customer is served by the first idle server in an infinite sequence S_1, S_2, ... of servers. We determine the first two terms in the asymptotic expansions of the moments of L as lambda tends to infinity, where L is the index of the server S_L serving a newly arriving customer in equilibrium, and lambda is the ratio of the arrival rate to the service rate. The leading terms of the moments show that L/lambda tends to a uniform distribution on [0,1].", "venue": "ArXiv", "authors": ["Patrick  Eschenfeldt", "Ben  Gross", "Nicholas  Pippenger"], "year": 2011, "n_citations": 1}
{"id": 4424821, "s2_id": "722460994aa0339e84813c784cfc9f9e5b2cf388", "title": "Monitoring Collective Communication Among GPUs", "abstract": "Communication among devices in multi-GPU systems plays an important role in terms of performance and scalability. In order to optimize an application, programmers need to know the type and amount of the communication happening among GPUs. Although there are prior works to gather this information in MPI applications on distributed systems and multi-threaded applications on shared memory systems, there is no tool that identifies communication among GPUs. Our prior work, ComScribe, presents a point-to-point (P2P) communication detection tool for GPUs sharing a common host. In this work, we extend ComScribe to identify communication among GPUs for collective and P2P communication primitives in NVIDIA\u2019s NCCL library. In addition to P2P communications, collective communications are commonly used in HPC and AI workloads thus it is important to monitor the induced data movement due to collectives. Our tool extracts the size and the frequency of data transfers in an application and visualizes them as a communication matrix. To demonstrate the tool in action, we present communication matrices and some statistics for two applications coming from machine translation and image classification domains.", "venue": "ArXiv", "authors": ["Muhammet Abdullah Soyturk", "Palwisha  Akhtar", "Erhan  Tezcan", "Didem  Unat"], "year": 2021, "n_citations": 0}
{"id": 4426640, "s2_id": "3a380bacdc596079a79050a5a0e3bcfa55251be5", "title": "Automatic Mapping Tasks to Cores - Evaluating AMTHA Algorithm in Multicore Architectures", "abstract": "The AMTHA (Automatic Mapping Task on Heterogeneous Architectures) algorithm for task-to-processors assignment and the MPAHA (Model of Parallel Algorithms on Heterogeneous Architectures) model are presented. The use of AMTHA is analyzed for multicore processor-based architectures, considering the communication model among processes in use. The results obtained in the tests carried out are presented, comparing the real execution times on multicores of a set of synthetic applications with the predictions obtained with AMTHA. Finally current lines of research are presented, focusing on clusters of multicores and hybrid programming paradigms.", "venue": "ArXiv", "authors": ["Laura C. De Giusti", "Franco  Chichizola", "Marcelo R. Naiouf", "Armando De Giusti", "Emilio  Luque"], "year": 2010, "n_citations": 18}
{"id": 4427815, "s2_id": "d3ad66028dd7c7a9dd0823900112d9a2560c99ca", "title": "Invited Abstract: A Simulation Package for Energy Consumption of Content Delivery Networks (CDNs)", "abstract": "Content Delivery Networks (CDNs) are becoming an integral part of the future generation Internet. Traditionally, these networks have been designed with the goals of traffic offload and the improvement of users' quality of experience (QoE), but the energy consumption is also becoming an indispensable design factor for CDNs to be a sustainable solution. To study and improve the CDN architectures using this new design metric, we are planning to develop a generic and flexible simulation package in OMNet++. This package is aimed to render a holistic view about the CDN energy consumption behaviour by incorporating the state-of-the-art energy consumption models proposed for the individual elements of CDNs (e.g. servers, routers, wired and wireless links, wireless devices, etc.) and for the various Internet contents (web pages, files, streaming video, etc.). I. INTRODUCTION With the tremendous growth of the Internet traffic and the demand for guaranteed quality of service, the content delivery networks (CDNs) have emerged as a promising solution to offload the traffic and to reduce content access delay, so that both content providers and consumers benefit directly. There is also a new emerging design goal for CDNs, motivated by the present trend of energy consumption in the ICT sector which implies for an urgent need for re-assessing and re-designing Internet systems to achieve energy efficient architectures. In particular, the content delivery networks can play a significant role in harnessing energy consumption per bit delivered to the end users. This is supported by the fact that the Internet is experiencing an unprecedented growth in the type and volume of contents. The most significant phenomenon of this type is video traffic. A recent trend of the Internet usage indicates an increased demand for streaming media (IPTV, Youtube etc.) in the downlink as well as an increased user generated traffic (upload of video to Facebook, Youtube, Vimeo etc.) in the uplink side. Knowing the fact that the delivery of video contents is highly bandwidth and processor demanding, it is of prominent value to understand how the current CDNs behave in terms of energy efficiency, and how to calibrate the existing CDNs or to design new ones by taking into account the energy efficiency as an additional design metric. The study of energy efficiency of the existing CDNs and, more importantly, the design of new CDNs require that the contributions of a tremendous number of factors to the energy consumption is taken into consideration. On one side, the end-users are increasingly inclined to use battery powered mobile devices or portable computers. On the other side, there are a huge number of network elements (e.g. servers, storages, routers, links, etc.) responsible for providing, managing, and carrying the Internet contents to the users. Therefore, the performance study of a given CDN architecture requires that the entire set of network elements with a role in carrying contents from a content provider to the consumers are taken into account. Moreover, contents have different types and properties and users have different preferences over contents and use different devices to access those contents. With this extremely huge set of elements involved in a CDN, simulation as a tool appears as the most affordable way of studying the energy efficiency of the existing CDNs, and to design new CDNs with respect to energy efficiency metric. In the following, we present the building blocks of an envisioned simulation package for the studying of energy consumption behaviour of CDNs.", "venue": "ArXiv", "authors": ["Mohammadhassan  Safavi", "Saeed  Bastani"], "year": 2015, "n_citations": 0}
{"id": 4437754, "s2_id": "9bde9379d81cc29d59be9dd86eed0dfbe956f1ef", "title": "High performance and energy efficient inference for deep learning on ARM processors", "abstract": "We evolve PyDTNN, a framework for distributed parallel training of Deep Neural Networks (DNNs), into an efficient inference tool for convolutional neural networks. Our optimization process on multicore ARM processors involves several high-level transformations of the original framework, such as the development and integration of Cython routines to exploit thread-level parallelism; the design and development of micro-kernels for the matrix multiplication, vectorized with ARM\u2019s NEON intrinsics, that can accommodate layer fusion; and the appropriate selection of several cache configuration parameters tailored to the memory hierarchy of the target ARM processors. Our experiments evaluate both inference throughput (measured in processed images/s) and inference latency (i.e., time-to-response) as well as energy consumption per image when varying the level of thread parallelism and the processor power modes. The experiments with the new inference engine are reported for the ResNet50 v1.5 model on the ImageNet dataset from the MLPerf suite using the ARM v8.2 cores in the NVIDIA Jetson AGX Xavier board. These results show superior performance compared with the well-spread TFLite from Google and slightly inferior results when compared with ArmNN, the native library from ARM for DNN inference.", "venue": "ArXiv", "authors": ["Adri'an  Castell'o", "Sergio  Barrachina", "Manuel F. Dolz", "Enrique S. Quintana-Ort'i", "Pau San Juan"], "year": 2021, "n_citations": 0}
{"id": 4440684, "s2_id": "3c2714565b5b96661e68010fe5221986c70fb3ae", "title": "To Share, or not to Share Online Event Trend Aggregation Over Bursty Event Streams", "abstract": "Complex event processing (CEP) systems continuously evaluate large workloads of pattern queries under tight time constraints. Event trend aggregation queries with Kleene patterns are commonly used to retrieve summarized insights about the recent trends in event streams. State-of-art methods are limited either due to repetitive computations or unnecessary trend construction. Existing shared approaches are guided by statically selected and hence rigid sharing plans that are often sub-optimal under stream fluctuations. In this work, we propose a novel framework Hamlet that is the first to overcome these limitations. Hamlet introduces two key innovations. First, Hamlet adaptively decides at run time whether to share or not to share computations depending on the current stream properties to harvest the maximum sharing benefit. Second, Hamlet is equipped with a highly efficient shared trend aggregation strategy that avoids trend construction. Our experimental study on both real and synthetic data sets demonstrates that Hamlet consistently reduces query latency by up to five orders of magnitude compared to state-of-the-art approaches.", "venue": "SIGMOD Conference", "authors": ["Olga  Poppe", "Chuan  Lei", "Lei  Ma", "Allison  Rozet", "Elke A. Rundensteiner"], "year": 2021, "n_citations": 2}
{"id": 4442398, "s2_id": "03fca2de14efef63bc23d160ad11a6a92f6cdff6", "title": "Modeling the Linux page cache for accurate simulation of data-intensive applications", "abstract": "The emergence of Big Data in recent years has resulted in a growing need for efficient data processing solutions. While infrastructures with sufficient compute power are available, the I/O bottleneck remains. The Linux page cache is an efficient approach to reduce I/O overheads, but few experimental studies of its interactions with Big Data applications exist, partly due to limitations of real-world experiments. Simulation is a popular approach to address these issues, however, existing simulation frameworks do not simulate page caching fully, or even at all. As a result, simulation-based performance studies of data-intensive applications can lead to misleading results and inaccurate conclusions.In this paper, we propose an I/O simulation model that captures the key features of the Linux page cache. We have implemented this model as part of the WRENCH workflow simulation framework, which itself builds on the popular Sim-Grid distributed systems simulation framework. Our model and its implementation enable the simulation of both single-threaded and multithreaded applications, and of both writeback and writethrough caches for local or network-based filesystems. We evaluate the accuracy of our model in different conditions, including sequential and concurrent applications, as well as local and remote I/Os. We find that our page cache model reduces the simulation error by up to an order of magnitude when compared to state-of-the-art, cacheless simulations. Our model is publicly available in the WRENCH framework, making it usable in a wide range of simulation studies.", "venue": "2021 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Hoang-Dung  Do", "Valerie  Hayot-Sasson", "Rafael Ferreira da Silva", "Christopher  Steele", "Henri  Casanova", "Tristan  Glatard"], "year": 2021, "n_citations": 0}
{"id": 4443243, "s2_id": "43e219b557b8a669d2dfda3535c527e6d1ff9f04", "title": "On the Throughput Optimization in Large-Scale Batch-Processing Systems", "abstract": "We analyze a data-processing system with n clients producing jobs which are processed in batches by m parallel servers; the system throughput critically depends on the batch size and a corresponding sub-additive speedup function that arises due to overhead amortization. In practice, throughput optimization relies on numerical searches for the optimal batch size which is computationally cumbersome. In this paper, we model this system in terms of a closed queueing network assuming certain forms of service speedup; a standard Markovian analysis yields the optimal throughput in w n4 time. Our main contribution is a mean-field model that has a unique, globally attractive stationary point, derivable in closed form. This point characterizes the asymptotic throughput as a function of the batch size that can be calculated in O(1) time. Numerical settings from a large commercial system reveal that this asymptotic optimum is accurate in practical finite regimes.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Sounak  Kar", "Robin  Rehrmann", "Arpan  Mukhopadhyay", "Bastian  Alt", "Florin  Ciucu", "Heinz  Koeppl", "Carsten  Binnig", "Amr  Rizk"], "year": 2020, "n_citations": 0}
{"id": 4447217, "s2_id": "9b7a9752472e69b95b9e35cff5c3f8536ad666a6", "title": "The Stabilized Explicit Variable-Load Solver with Machine Learning Acceleration for the Rapid Solution of Stiff Chemical Kinetics", "abstract": "In this study, a fast and stable machine-learned hybrid algorithm implemented in TensorFlow for the integration of stiff chemical kinetics is introduced. Numerical solutions to differential equations are at the core of computational fluid dynamics calculations. As the size and complexity of the simulations grow, so does the need for computational power and time. Many efforts have been made to implement stiff chemistry solvers on GPUs but have not been highly successful because of the logical divergence in traditional stiff solver algorithms. Because of these constrains, a novel Explicit Stabilized Variable-load (STEV) solver has been developed. Overstepping due to the relatively large time steps is prevented by introducing limits to the maximum changes of chemical species per time step. To prevent oscillations, a discrete Fourier transform is introduced to dampen ringing. In contrast to conventional explicit approaches, a variable-load approach is used where each cell in the computational domain is advanced with its unique time step. This approach allows cells to be integrated simultaneously while maintaining warp convergence but finish at different iterations and be removed from the workload. To improve the computational performance of the introduced solver, specific thermodynamic quantities of interest were estimated using shallow neural networks in place of polynomial fits, leading to an additional 10% savings in clock time with minimal training and implementation requirements. However ML specific hardware could increase the time savings to as much as 28%. While the complexity of these particular machine learning models is not high by modern standards, the impact on computational efficiency should not be ignored. The results show a dramatic decrease in total chemistry solution time (over 200 times) while maintaining a similar degree of accuracy.", "venue": "ArXiv", "authors": ["Kyle  Buchheit", "Opeoluwa  Owoyele", "Terry  Jordan", "Dirk Van Essendelft"], "year": 2019, "n_citations": 1}
{"id": 4448762, "s2_id": "0cdc94035a7d2979825b89529093ea478ce714fb", "title": "Automatic timing-coherent transactor generation for mixed-level simulations", "abstract": "In this paper we extend the concept of the traditional transactor, which focuses on correct content transfer, to a new timing-coherent transactor that also accurately aligns the timing of each transaction boundary so that designers can perform precise concurrent system behavior analysis in mixed-abstraction-level system simulations which are essential to increasingly complex system designs. To streamline the process, we also developed an automatic approach for timing-coherent transactor generation. Our approach is actually applied in mixed-level simulations and the results show that it achieves 100% timing accuracy while the conventional approach produces results of 25% to 44% error rate.", "venue": "The 20th Asia and South Pacific Design Automation Conference", "authors": ["Li-Chun  Chen", "Hsin-I  Wu", "Ren-Song  Tsay"], "year": 2015, "n_citations": 0}
{"id": 4448910, "s2_id": "2547a8f3421019b80d589a11dbdeb9aba14aface", "title": "Cellular Connectivity for UAVs: Network Modeling, Performance Analysis, and Design Guidelines", "abstract": "The growing use of aerial user equipments (UEs) in various applications requires ubiquitous and reliable connectivity for safe control and data exchange between these devices and ground stations. Key questions that need to be addressed when planning the deployment of aerial UEs are whether the cellular network is a suitable candidate for enabling such connectivity and how the inclusion of aerial UEs might impact the overall network efficiency. This paper provides an in-depth analysis of user and network-level performance of a cellular network that serves both unmanned aerial vehicles (UAVs) and ground users in the downlink. Our results show that the favorable propagation conditions that UAVs enjoy due to their height often backfire on them, as the increased load-dependent co-channel interference received from neighboring ground base stations (BSs) is not compensated by the improved signal strength. When compared with a ground user in an urban area, our analysis shows that a UAV flying at 100 m can experience a throughput decrease of a factor 10 and a coverage drop from 76% to 30%. Motivated by these findings, we develop UAV and network-based solutions to enable an adequate integration of UAVs into cellular networks. In particular, we show that an optimal tilting of the UAV antenna can increase the coverage from 23% to 89% and throughput from 3.5 to 5.8 b/s/Hz, outperforming ground UEs. Furthermore, our findings reveal that depending on the UAV altitude and its antenna configuration, the aerial user performance can scale with respect to the network density better than that of a ground user. Finally, our results show that network densification and the use of microcells limit the UAV performance. Although UAV usage has the potential to increase the area spectral efficiency (ASE) of cellular networks with a moderate number of cells, they might hamper the development of future ultradense networks.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["M. Mahdi Azari", "Fernando  Rosas", "Sofie  Pollin"], "year": 2019, "n_citations": 81}
{"id": 4453349, "s2_id": "d80c37b6369aa3e45de8a4fb9a8ccbfad9a9ea89", "title": "Using Metrics Suites to Improve the Measurement of Privacy in Graphs", "abstract": "Social graphs are widely used in research (e.g., epidemiology) and business (e.g., recommender systems). However, sharing these graphs poses privacy risks because they contain sensitive information about individuals. Graph anonymization techniques aim to protect individual users in a graph, while graph de-anonymization aims to re-identify users. The effectiveness of anonymization and de-anonymization algorithms is usually evaluated with privacy metrics. However, it is unclear how strong existing privacy metrics are when they are used in graph privacy. In this paper, we study 26 privacy metrics for graph anonymization and de-anonymization and evaluate their strength in terms of three criteria: monotonicity indicates whether the metric indicates lower privacy for stronger adversaries; for within-scenario comparisons, evenness indicates whether metric values are spread evenly; and for between-scenario comparisons, shared value range indicates whether metrics use a consistent value range across scenarios. Our extensive experiments indicate that no single metric fulfills all three criteria perfectly. We therefore use methods from multi-criteria decision analysis to aggregate multiple metrics in a metrics suite, and we show that these metrics suites improve monotonicity compared to the best individual metric. This important result enables more monotonic, and thus more accurate, evaluations of new graph anonymization and de-anonymization algorithms.", "venue": "ArXiv", "authors": ["Isabel  Wagner", "Yuchen  Zhao"], "year": 2019, "n_citations": 4}
{"id": 4454748, "s2_id": "79a66a1ed1edd6c135870d4caae5fe0ed97b893b", "title": "Q-Learning Inspired Self-Tuning for Energy Efficiency in HPC", "abstract": "System self-tuning is a crucial task to lower the energy consumption of computers. Traditional approaches decrease the processor frequency in idle or synchronisation periods. However, in High-Performance Computing (HPC) this is not sufficient: if the executed code is load balanced, there are neither idle nor synchronisation phases that can be exploited. Therefore, alternative self-tuning approaches are needed, which allow exploiting different compute characteristics of HPC programs. The novel notion of application regions based on function call stacks, introduced in the Horizon 2020 Project READEX, allows us to define such a self-tuning approach. In this paper, we combine these regions with the Q-Learning typical state-action maps, which save information about available states, possible actions to take, and the expected rewards. By exploiting the existing processor power interface, we are able to provide direct feedback to the learning process. This approach allows us to save up to 15% energy, while only adding a minor runtime overhead.", "venue": "2019 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Andreas  Gocht", "Robert  Sch\u00f6ne", "Mario  Bielert"], "year": 2019, "n_citations": 1}
{"id": 4460869, "s2_id": "abab5fae4baf70da55698399b75a51ba0b130dfa", "title": "Stay Connected, Leave no Trace: Enhancing Security and Privacy in WiFi via Obfuscating Radiometric Fingerprints", "abstract": "The intrinsic hardware imperfection of WiFi chipsets manifests itself in the transmitted signal, leading to a unique radiometric (radio frequency) fingerprint. This fingerprint can be used as an additional means of authentication to enhance security. In this paper, we prove analytically and experimentally that these solutions are highly vulnerable to impersonation attacks. We also demonstrate that such a unique device-specific signature can be abused to track devices, thus violating privacy. We propose RF-Veil, a radiometric fingerprinting solution that is not only robust against impersonation attacks but also effective in protecting privacy by obfuscating the radiometric fingerprint of the transmitter for non-legitimate receivers. Specifically, we introduce a randomized pattern of phase errors to the transmitted signal such that only the intended receiver can extract the original fingerprint of the transmitter. In a series of experiments and analyses, we expose the vulnerability of adopting naive randomization to statistical attacks and introduce countermeasures. Finally, we show the efficacy of RF-Veil experimentally in protecting user privacy and enhancing security. More importantly, our proposed solution allows communicating with other devices, which do not employ RF-Veil.", "venue": "SIGMETRICS", "authors": ["Luis F. Abanto-Leon", "Andreas  B\u00e4uml", "Gek Hong Sim", "Matthias  Hollick", "Arash  Asadi"], "year": 2021, "n_citations": 5}
{"id": 4460968, "s2_id": "a6860712e221ab8302e424b376b75962887b8224", "title": "A Markov Chain Monte Carlo Approach to Cost Matrix Generation for Scheduling Performance Evaluation", "abstract": "In high performance computing, scheduling of tasks and allocation to machines is very critical especially when we are dealing with heterogeneous execution costs. Simulations can be performed with a large variety of environments and application models. However, this technique is sensitive to bias when it relies on random instances with an uncontrolled distribution. We use methods from the literature to provide formal guarantee on the distribution of the instance. In particular, it is desirable to ensure a uniform distribution among the instances with a given task and machine heterogeneity. In this article, we propose a method that generates instances (cost matrices) with a known distribution where tasks are scheduled on machines with heterogeneous execution costs.", "venue": "2018 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Louis-Claude  Canon", "Mohamad El Sayah", "Pierre-Cyrille  H\u00e9am"], "year": 2018, "n_citations": 3}
{"id": 4462485, "s2_id": "6e696ba72ce3f01462a8757c12505a92a4290d0a", "title": "A Metric for Performance Portability", "abstract": "The term \"performance portability\" has been informally used in computing to refer to a variety of notions which generally include: 1) the ability to run one application across multiple hardware platforms; and 2) achieving some notional level of performance on these platforms. However, there has been a noticeable lack of consensus on the precise meaning of the term, and authors' conclusions regarding their success (or failure) to achieve performance portability have thus been subjective. Comparing one approach to performance portability with another has generally been marked with vague claims and verbose, qualitative explanation of the comparison. This paper presents a concise definition for performance portability, along with a simple metric that accurately captures the performance and portability of an application across different platforms. The utility of this metric is then demonstrated with a retroactive application to previous work.", "venue": "ArXiv", "authors": ["Simon J. Pennycook", "Jason D. Sewall", "Victor W. Lee"], "year": 2016, "n_citations": 34}
{"id": 4470449, "s2_id": "c7995d7c4d9eac081160e0dde0627dda7d5c9afe", "title": "Fast Bitmap Fit: A CPU Cache Line friendly memory allocator for single object allocations", "abstract": "Applications making excessive use of single-object based data structures (such as linked lists, trees, etc...) can see a drop in efficiency over a period of time due to the randomization of nodes in memory. This slow down is due to the ineffective use of the CPU\u2019s L1/L2 cache. We present a novel approach for mitigating this by presenting the design of a single-object memory allocator that preserves memory locality across randomly ordered memory allocations and deallocations.", "venue": "ArXiv", "authors": ["Dhruv  Matani", "Gaurav  Menghani"], "year": 2021, "n_citations": 0}
{"id": 4472706, "s2_id": "ef53026beb1950f0b613724db94bc1e83c7c9d5b", "title": "A New Approach for Improvement Security against DoS Attacks in Vehicular Ad-hoc Network", "abstract": "Vehicular Ad-Hoc Networks (VANET) are a proper subset of mobile wireless networks, where nodes are revulsive, the vehicles are armed with special electronic devices on the motherboard OBU (On Board Unit) which enables them to trasmit and receive messages from other vehicles in the VANET. Furthermore the communication between the vehicles, the VANET interface is donated by the contact points with road infrastructure. VANET is a subgroup of MANETs. Unlike the MANETs nodes, VANET nodes are moving very fast. Impound a permanent route for the dissemination of emergency messages and alerts from a danger zone is a very challenging task. Therefore, routing plays a significant duty in VANETs. decreasing network overhead, avoiding network congestion, increasing traffic congestion and packet delivery ratio are the most important issues associated with routing in VANETs. In addition, VANET network is subject to various security attacks. In base VANET systems, an algorithm is used to dicover attacks at the time of confirmation in which overhead delay occurs. This paper proposes (P-Secure) approach which is used for the detection of DoS attacks before the confirmation time. This reduces the overhead delays for processing and increasing the security in VANETs. Simulation results show that the P-Secure approach, is more efficient than OBUmodelVaNET approach in terms of PDR, e2e_delay, throughput and drop packet rate.", "venue": "ArXiv", "authors": ["Reza  Fotohi", "Yaser  Ebazadeh", "Mohammad Seyyar Geshlag"], "year": 2020, "n_citations": 38}
{"id": 4476802, "s2_id": "16222d2f1e19dc0687f8477ad373a170fba48565", "title": "Fast and Accurate 3D Medical Image Segmentation with Data-swapping Method", "abstract": "Deep neural network models used for medical image segmentation are large because they are trained with high-resolution three-dimensional (3D) images. Graphics processing units (GPUs) are widely used to accelerate the trainings. However, the memory on a GPU is not large enough to train the models. A popular approach to tackling this problem is patch-based method, which divides a large image into small patches and trains the models with these small patches. However, this method would degrade the segmentation quality if a target object spans multiple patches. In this paper, we propose a novel approach for 3D medical image segmentation that utilizes the data-swapping, which swaps out intermediate data from GPU memory to CPU memory to enlarge the effective GPU memory size, for training high-resolution 3D medical images without patching. We carefully tuned parameters in the data-swapping method to obtain the best training performance for 3D U-Net, a widely used deep neural network model for medical image segmentation. We applied our tuning to train 3D U-Net with full-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result, communication overhead, which is the most important issue, was reduced by 17.1%. Compared with the patch-based method for patches of 128 x 128 x 128 voxels, our training for full-size images achieved improvement on the mean Dice score by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core sub-region, respectively. The total training time was reduced from 164 hours to 47 hours, resulting in 3.53 times of acceleration.", "venue": "ArXiv", "authors": ["Haruki  Imai", "Samuel  Matzek", "Tung D. Le", "Yasushi  Negishi", "Kiyokuni  Kawachiya"], "year": 2018, "n_citations": 8}
{"id": 4478963, "s2_id": "c0fb4db6351276d24a574aa455eedf45b7b1cdb6", "title": "The Bitlet Model: Defining a Litmus Test for the Bitwise Processing-in-Memory Paradigm", "abstract": "This paper describes an analytical modeling tool called Bitlet that can be used, in a parameterized fashion, to understand the affinity of workloads to processing-in-memory (PIM) as opposed to traditional computing. The tool uncovers interesting trade-offs between operation complexity (cycles required to perform an operation through PIM) and other key parameters, such as system memory bandwidth, data transfer size, the extent of data alignment, and effective memory capacity involved in PIM computations. Despite its simplicity, the model has already proven useful. In the future, we intend to extend and refine Bitlet to further increase its utility.", "venue": "ArXiv", "authors": ["Kunal  Korgaonkar", "Ronny  Ronen", "Anupam  Chattopadhyay", "Shahar  Kvatinsky"], "year": 2019, "n_citations": 3}
{"id": 4481225, "s2_id": "e6f4702b28118c387766b7abd2bb69be86306def", "title": "AdEle: An Adaptive Congestion-and-Energy-Aware Elevator Selection for Partially Connected 3D NoCs", "abstract": "By lowering the number of vertical connections in fully connected 3D networks-on-chip (NoCs), partially connected 3D NoCs (PC-3DNoCs) help alleviate reliability and fabrication issues. This paper proposes a novel, adaptive congestion- and energy-aware elevator-selection scheme called AdEle to improve the traffic distribution in PC-3DNoCs. AdEle employs an offline multi-objective simulated-annealing-based algorithm to find good elevator subsets and an online elevator selection policy to enhance elevator selection during routing. Compared to the state-of-the-art techniques under different real-application traffics and configuration scenarios, AdEle improves the network latency by 10.9% on average (up to 14.6%) with less than 6.9% energy consumption overhead.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Ebadollah  Taheri", "Ryan G. Kim", "Mahdi  Nikdast"], "year": 2021, "n_citations": 2}
{"id": 4481334, "s2_id": "ac504fb7daf7e6a8bb93423eb1cfde5fbeb477c8", "title": "Energy harvesting wireless networks with correlated energy sources", "abstract": "This work considers a system with two energy harvesting (EH) nodes transmitting to a common destination over a random access channel. The amount of harvested energy is assumed to be random and independent over time, but correlated among the nodes possibly with respect to their relative position. A threshold-based transmission policy is developed for the maximization of the expected aggregate network throughput. Assuming that there is no a priori channel state or EH information available to the nodes, the aggregate network throughput is obtained. The optimal thresholds are determined for two practically important special cases: i) at any time only one of the sensors harvests energy due to, for example, physical separation of the nodes; ii) the nodes are spatially close, and at any time, either both nodes or none of them harvests energy.", "venue": "2016 IEEE Wireless Communications and Networking Conference", "authors": ["Mehdi Salehi Heydar Abad", "Deniz  G\u00fcnd\u00fcz", "\u00d6zg\u00fcr  Er\u00e7etin"], "year": 2016, "n_citations": 4}
{"id": 4484746, "s2_id": "a876b9385a086f22e9bf3b1d06c1c05a675f0193", "title": "Latency Analysis of Multiple Classes of AVB Traffic in TSN With Standard Credit Behavior Using Network Calculus", "abstract": "Time-sensitive networking (TSN) is a set of amendments that extend Ethernet to support distributed safety-critical and real-time applications in the industrial automation, aerospace, and automotive areas. TSN integrates multiple traffic types and supports interactions in several combinations. In this article, we consider the configuration supporting scheduled traffic (ST) based on gate-control lists, audio\u2013video bridging (AVB) traffic according to IEEE 802.1BA that has bounded latencies, and best-effort traffic, for which no guarantees are provided. This article extends the timing analysis method to multiple AVB classes and proofs the credit bounds for multiple classes of AVB traffic, respectively, under frozen and nonfrozen behaviors of credit during guard band (GB). They are prerequisites for nonoverflow credits of credit-based shaper (CBS) and preventing starvation of AVB traffic. Moreover, this article proposes an improved timing analysis method reducing the pessimism for the worst-case end-to-end delays of AVB traffic by considering the limitations from the physical link rate and the output of CBS. Finally, we evaluate the improved analysis method on both synthetic and real-world test cases, showing the significant reduction of pessimism on latency bounds compared to related work and presenting the correctness validation compared with simulation results. We also compare the AVB latency bounds in the case of frozen and nonfrozen credit during GB. Additionally, we evaluate the scalability of our method with variation of the load of ST flows and of the bandwidth reservation for AVB traffic.", "venue": "IEEE Transactions on Industrial Electronics", "authors": ["Luxi  Zhao", "Paul  Pop", "Zhong  Zheng", "Hugo  Daigmorte", "Marc  Boyer"], "year": 2021, "n_citations": 6}
{"id": 4488699, "s2_id": "4e4344a3f66f15ff34cde020487e9bb96eca0f1f", "title": "Speeding Up BigClam Implementation on SNAP", "abstract": "We perform a detailed analysis of the C++ implementation of the Cluster Affiliation Model for Big Networks (BigClam) on the Stanford Network Analysis Project (SNAP). BigClam is a popular graph mining algorithm that is capable of finding overlapping communities in networks containing millions of nodes. Our analysis shows a key stage of the algorithm - determining if a node belongs to a community - dominates the runtime of the implementation, yet the computation is not parallelized. We show that by parallelizing computations across multiple threads using OpenMP we can speed up the algorithm by 5.3 times when solving large networks for communities, while preserving the integrity of the program and the result.", "venue": "ICCSW", "authors": ["C. H. Bryan Liu", "Benjamin Paul Chamberlain"], "year": 2018, "n_citations": 0}
{"id": 4490307, "s2_id": "ae94e04f02379e13fecb4584e3b0c523e7860b3b", "title": "LP-Based Power Grid Enhancement Methodology", "abstract": "In this paper, we explored the opportunity to enhance power grid robustness after routing stage, and propose a linear programming based algorithm that maximizes the improvement of power grid strengthening with given available routing resource. We further discussed some techniques to leverage tradeoffs between runtime and optimality of the solutions. Experimental results show substantial power integrity improvement with \"zero cost\".", "venue": "ArXiv", "authors": ["Tapio  Bohn", "Paul  Salmi", "Albert  Milner"], "year": 2017, "n_citations": 0}
{"id": 4493897, "s2_id": "19bd999a9482663bff08ffc4dfe9f5807472cbd8", "title": "Deficit Round-Robin: A Second Network Calculus Analysis", "abstract": "Deficit Round-Robin (DRR) is a widespread scheduling algorithm that provides fair queueing with variable-length packets. Bounds on worst-case delays obtained with DRR were found by Boyer et al. They used a rigorous network calculus approach and characterized the service obtained by one flow of interest by means of a strict service curve. These bounds do not make any assumptions on the interfering traffic flows hence are pessimistic when the interfering traffic is constrained by some arrival curves. For such cases, Soni et al. improved the worstcase delay bounds by a correction term that accounts for arrival curve constraints of interfering traffic, using a semi-rigorous approach. Unfortunately, these latter bounds are incorrect, as we show by exhibiting a counter-example. Then we derive new service curves for DRR, which are rigorously proven, and we account for arrival curve constraints of interfering traffic. Hence, the resulting delay bounds are guaranteed to be correct. Furthermore, we find numerically that they are smaller than the incorrect ones obtained with the method of Soni et al. These bounds also improve on the results by Boyer et al. when there is no constraint on interfering traffic. Therefore, as of today, they are the best known delay bounds for DRR. Our results are obtained by applying the method of the pseudo-inverse.", "venue": "2021 IEEE 27th Real-Time and Embedded Technology and Applications Symposium (RTAS)", "authors": ["Seyed Mohammadhossein Tabatabaee", "Jean-Yves Le Boudec"], "year": 2021, "n_citations": 1}
{"id": 4494714, "s2_id": "3c743916baf11f9121915c58799060e9197db364", "title": "Exact Coverage Analysis of Intelligent Reflecting Surfaces With Nakagami-M Channels", "abstract": "Intelligent Reflecting Surfaces (IRS) are a promising solution to enhance the coverage of future wireless networks by tuning low-cost passive reflecting elements (referred to as metasurfaces), thereby constructing a favorable wireless propagation environment. Different from prior works, which assume Rayleigh fading channels and do not consider the direct link between a base station and a user, this article develops a framework based on moment generation functions (MGF) to characterize the coverage probability of a user in an IRS-aided wireless systems with generic Nakagami-m fading channels in the presence of direct links. In addition, we demonstrate that the proposed framework is tractable for both finite and asymptotically large values of the metasurfaces. Furthermore, we derive the channel hardening factor as a function of the shape factor of Nakagami-m fading channel and the number of IRS elements. Finally, we derive a closed-form expression to calculate the maximum coverage range of the IRS for given network parameters. Numerical results obtained from Monte-Carlo simulations validate the derived analytical results.", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Hazem  Ibrahim", "Hina  Tabassum", "Uyen  T. Nguyen"], "year": 2021, "n_citations": 10}
{"id": 4498548, "s2_id": "870dc8ae3213c4a78bbbe545b7781540537054de", "title": "The Multi-Source Preemptive M/PH/1/1 Queue with Packet Errors: Exact Distribution of the Age of Information and Its Peak", "abstract": "Age of Information (AoI) and Peak AoI (PAoI) and their analytical models have recently drawn substantial amount of attention in information theory and wireless communications disciplines, in the context of qualitative assessment of information freshness in status update systems. We take a queueing-theoretic approach and study a probabilistically preemptive bufferless $M/PH/1/1$ queueing system with arrivals stemming from $N$ separate information sources, with the aim of modeling a generic status update system. In this model, a new information packet arrival from source $m$ is allowed to preempt a packet from source $n$ in service, with a probability depending on $n$ and $m$. To make the model even more general than the existing ones, for each of the information sources, we assume a distinct PH-type service time distribution and a distinct packet error probability. Subsequently, we obtain the exact distributions of the AoI and PAoI for each of the information sources using matrix-analytical algorithms and in particular the theory of Markov fluid queues and sample path arguments. This is in contrast with existing methods that rely on Stochastic Hybrid Systems (SHS) which obtain only the average values and in less general settings. Numerical examples are provided to validate the proposed approach as well as to give engineering insight on the impact of preemption probabilities on certain AoI and PAoI performance figures.", "venue": "ArXiv", "authors": ["Ozancan  Dogan", "Nail  Akar"], "year": 2020, "n_citations": 5}
{"id": 4499697, "s2_id": "2842ffeb146579a5186edb8f90b7e25aeec0e0ed", "title": "Proof of Convergence and Performance Analysis for Sparse Recovery via Zero-Point Attracting Projection", "abstract": "A recursive algorithm named zero-point attracting projection (ZAP) is proposed recently for sparse signal reconstruction. Compared with the reference algorithms, ZAP demonstrates rather good performance in recovery precision and robustness. However, any theoretical analysis about the mentioned algorithm, even a proof on its convergence, is not available. In this work, a strict proof on the convergence of ZAP is provided and the condition of convergence is put forward. Based on the theoretical analysis, it is further proved that ZAP is non-biased and can approach the sparse solution to any extent, with the proper choice of step-size. Furthermore, the case of inaccurate measurements in noisy scenario is also discussed. It is proved that disturbance power linearly reduces the recovery precision, which is predictable but not preventable. The reconstruction deviation of -compressible signal is also provided. Finally, numerical simulations are performed to verify the theoretical analysis.", "venue": "IEEE Transactions on Signal Processing", "authors": ["Xiaohan  Wang", "Yuantao  Gu", "Laming  Chen"], "year": 2012, "n_citations": 16}
{"id": 4499810, "s2_id": "57e55f33e5bd21d73b2f0ccd7965d348e016dda1", "title": "An Experimental Evaluation and Characterization of VoIP Over an LTE-A Network", "abstract": "Mobile telecommunications are converging towards all-IP solutions. This is the case of the Long Term Evolution (LTE) technology that, having no circuit-switched bearer to support voice traffic, needs a dedicated VoIP infrastructure, which often relies on the IP Multimedia Subsystem architecture. Most telecom operators implement LTE-A, an advanced version of LTE often marketed as $4{G} +$ , which achieves data rate peaks of 300 Mbps. Yet, although such novel technology boosts the access to advanced multimedia contents and services, telco operators continue to consider the VoIP market as the major revenue for their business. In this work, the authors propose a detailed performance assessment of VoIP traffic by carrying out experimental trials across a real LTE-A environment. The experimental campaign consists of two stages. First, we characterize VoIP calls between fixed and mobile terminals, based on a data-set that includes more than 750,000 data-voice packets. We analyze quality-of-service metrics such as round-trip time (RTT) and jitter, to capture the influence of uncontrolled factors that typically appear in real-world settings. In the second stage, we further consider VoIP flows across a range of codecs, looking at the trade-offs between quality and bandwidth consumption. Moreover, we propose a statistical characterization of jitter and RTT (representing the most critical parameters), identifying the optimal approximating distribution, namely the Generalized Extreme Value (GEV). The estimation of parameters through the Maximum Likelihood criterion, leads us to reveal both the short- and long-tail behaviour for jitter and RTT, respectively.", "venue": "IEEE Transactions on Network and Service Management", "authors": ["Mario  Di Mauro", "Antonio  Liotta"], "year": 2020, "n_citations": 5}
{"id": 4504198, "s2_id": "7fbbbfe8a765adaf60c46e8d80d4a554c571c327", "title": "On Linear Learning with Manycore Processors", "abstract": "A new generation of manycore processors is on the rise that offers dozens and more cores on a chip and, in a sense, fuses host processor and accelerator. In this paper we target the efficient training of generalized linear models on these machines. We propose a novel approach for achieving parallelism which we call Heterogeneous Tasks on Homogeneous Cores (HTHC). It divides the problem into multiple fundamentally different tasks, which themselves are parallelized. For evaluation, we design a detailed, architecture-cognizant implementation of our scheme on a recent 72-core Knights Landing processor that is adaptive to the cache, memory, and core structure. Our library efficiently supports dense and sparse datasets as well as 4-bit quantized data for further possible gains in performance. We show benchmarks for Lasso and SVM with different data sets against straightforward parallel implementations and prior software. In particular, for Lasso on dense data, we improve the state-of-the-art by an order of magnitude.", "venue": "2019 IEEE 26th International Conference on High Performance Computing, Data, and Analytics (HiPC)", "authors": ["Eliza  Wszola", "Celestine  Mendler-D\u00fcnner", "Martin  Jaggi", "Markus  P\u00fcschel"], "year": 2019, "n_citations": 1}
{"id": 4505073, "s2_id": "49a6a237a5c97e4dfb6d63ac91df559ee3c86a13", "title": "Design Heuristic for Parallel Many Server Systems under FCFS-ALIS", "abstract": "We study a parallel service queueing system with servers of types $s_1,\\ldots,s_J$, customers of types $c_1,\\ldots,c_I$, bipartite compatibility graph $\\mathcal{G}$, where arc $(c_i, s_j)$ indicates that server type $s_j$ can serve customer type $c_i$, and service policy of first come first served FCFS, assign longest idle server ALIS. For a general renewal stream of arriving customers and general service time distributions, the behavior of such systems is very complicated, in particular the calculation of matching rates $r_{c_i,s_j}$, the fraction of services of customers of type $c_i$ by servers of type $s_j$, is intractable. We suggest through a heuristic argument that if the number of servers becomes large, the matching rates are well approximated by matching rates calculated from the tractable FCFS bipartite infinite matching model. We present simulation evidence to support this heuristic argument, and show how this can be used to design systems for given performance requirements.", "venue": "ArXiv", "authors": ["Ivo J. B. F. Adan", "Marko A. A. Boon", "Gideon  Weiss"], "year": 2016, "n_citations": 1}
{"id": 4507143, "s2_id": "ef2422bd46d1250182e93e62164787f5cf5ea946", "title": "Practical Bounds on Optimal Caching with Variable Object Sizes", "abstract": "Many recent caching systems aim to improve miss ratios, but there is no good sense among practitioners of how much further miss ratios can be improved. In other words, should the systems community continue working on this problem? Currently, there is no principled answer to this question. In practice, object sizes often vary by several orders of magnitude, where computing the optimal miss ratio (OPT) is known to be NP-hard. The few known results on caching with variable object sizes provide very weak bounds and are impractical to compute on traces of realistic length. We propose a new method to compute upper and lower bounds on OPT. Our key insight is to represent caching as a min-cost flow problem, hence we call our method the flow-based offline optimal (FOO). We prove that, under simple independence assumptions, FOO's bounds become tight as the number of objects goes to infinity. Indeed, FOO's error over 10M requests of production CDN and storage traces is negligible: at most 0.3%. FOO thus reveals, for the first time, the limits of caching with variable object sizes. While FOO is very accurate, it is computationally impractical on traces with hundreds of millions of requests. We therefore extend FOO to obtain more efficient bounds on OPT, which we call practical flow-based offline optimal (PFOO). We evaluate PFOO on several full production traces and use it to compare OPT to prior online policies. This analysis shows that current caching systems are in fact still far from optimal, suffering 11-43% more cache misses than OPT, whereas the best prior offline bounds suggest that there is essentially no room for improvement.", "venue": "SIGMETRICS", "authors": ["Daniel S. Berger", "Nathan  Beckmann", "Mor  Harchol-Balter"], "year": 2018, "n_citations": 1}
{"id": 4507518, "s2_id": "51f132deb99bc2f243f83a113ca7281a7d4747b1", "title": "An Alternating Direction Method Approach to Cloud Traffic Management", "abstract": "In this paper, we introduce a unified framework for studying various cloud traffic management problems, ranging from geographical load balancing to backbone traffic engineering. We first abstract these real-world problems as a multi-facility resource allocation problem, and then present two distributed optimization algorithms by exploiting the special structure of the problem. Our algorithms are inspired by Alternating Direction Method of Multipliers (ADMM), enjoying a number of unique features. Compared to dual decomposition, they converge with non-strictly convex objective functions; compared to other ADMM-type algorithms, they not only achieve faster convergence under weaker assumptions, but also have lower computational complexity and lower message-passing overhead. The simulation results not only confirm these desirable features of our algorithms, but also highlight several additional advantages, such as scalability and fault-tolerance.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Chen  Feng", "Hong  Xu", "Baochun  Li"], "year": 2017, "n_citations": 30}
{"id": 4509506, "s2_id": "b98c684947a2da5b7a7e020ff8a7c4d80c5a8d3b", "title": "Accelerated Nearest Neighbor Search with Quick ADC", "abstract": "Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a foundation of many multimedia retrieval systems. Because it offers low responses times, Product Quantization (PQ) is a popular solution. PQ compresses high-dimensional vectors into short codes using several sub-quantizers, which enables in-RAM storage of large databases. This allows fast answers to NN queries, without accessing the SSD or HDD. The key feature of PQ is that it can compute distances between short codes and high-dimensional vectors using cache-resident lookup tables. The efficiency of this technique, named Asymmetric Distance Computation (ADC), remains limited because it performs many cache accesses. In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to 6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD) units available in current CPUs. Efficiently exploiting SIMD requires algorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key modifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard 8-bit sub-quantizers and (ii) the quantization of floating-point distances. This allows Quick ADC to exceed the performance of state-of-the-art systems, e.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors (128-bit codes).", "venue": "ICMR", "authors": ["Fabien  Andr\u00e9", "Anne-Marie  Kermarrec", "Nicolas Le Scouarnec"], "year": 2017, "n_citations": 13}
{"id": 4509508, "s2_id": "6fe4590f09f295361ad2921a22ce60eade2388e8", "title": "Low Power Reversible Parallel Binary Adder/Subtractor", "abstract": "In recent years, Reversible Logic is becoming more and more prominent technology having its applications in Low Power CMOS, Quantum Computing, Nanotechnology, and Optical Computing. Reversibility plays an important role when energy efficient computations are considered. In this paper, Reversible eight-bit Parallel Binary Adder/Subtractor with Design I, Design II and Design III are proposed. In all the three design approaches, the full Adder and Subtractors are realized in a single unit as compared to only full Subtractor in the existing design. The performance analysis is verified using number reversible gates, Garbage input/outputs and Quantum Cost. It is observed that Reversible eight-bit Parallel Binary Adder/Subtractor with Design III is efficient compared to Design I, Design II and existing design.", "venue": "VLSIC 2010", "authors": ["H. G. Rangaraju", "U.  Venugopal", "K. N. Muralidhara", "K. B. Raja"], "year": 2010, "n_citations": 78}
{"id": 4514184, "s2_id": "e7d3349770602516eb95ef6c9c1b20e39ff4c934", "title": "Structural analysis of network traffic matrix via relaxed principal component pursuit", "abstract": "The network traffic matrix is widely used in network operation and management. It is therefore of crucial importance to analyze the components and the structure of the network traffic matrix, for which several mathematical approaches such as Principal Component Analysis (PCA) were proposed. In this paper, we first argue that PCA performs poorly for analyzing traffic matrix that is polluted by large volume anomalies, and then propose a new decomposition model for the network traffic matrix. According to this model, we carry out the structural analysis by decomposing the network traffic matrix into three sub-matrices, namely, the deterministic traffic, the anomaly traffic and the noise traffic matrix, which is similar to the Robust Principal Component Analysis (RPCA) problem previously studied in [13]. Based on the Relaxed Principal Component Pursuit (Relaxed PCP) method and the Accelerated Proximal Gradient (APG) algorithm, we present an iterative approach for decomposing a traffic matrix, and demonstrate its efficiency and flexibility by experimental results. Finally, we further discuss several features of the deterministic and noise traffic. Our study develops a novel method for the problem of structural analysis of the traffic matrix, which is robust against pollution of large volume anomalies.", "venue": "Comput. Networks", "authors": ["Zhe  Wang", "Kai  Hu", "Ke  Xu", "Baolin  Yin", "Xiaowen  Dong"], "year": 2012, "n_citations": 33}
{"id": 4517031, "s2_id": "4745453b7ff75319dd7fff55b21d9662531743db", "title": "A Quick Look at SATA Disk Performance", "abstract": "We have been investigating the use of low-cost, commodity components for multi-terabyte SQL Server databases. Dubbed storage bricks, these servers are white box PCs containing the largest ATA drives, value-priced AMD or Intel processors, and inexpensive ECC memory. One issue has been the wiring mess, air flow problems, length restrictions, and connector failures created by seven or more parallel ATA (PATA) ribbon cables and drives in]a tower or 3U rack-mount chassis. Large capacity Serial ATA (SATA) drives have recently become widely available for the PC environment at a reasonable price. In addition to being faster, the SATA connectors seem more reliable, have a more reasonable length restriction (1m) and allow better airflow. We tested two drive brands along with two RAID controllers to evaluate SATA drive performance and reliablility. This paper documents our results so far.", "venue": "ArXiv", "authors": ["Tom  Barclay", "Wyman  Chong", "Jim  Gray"], "year": 2004, "n_citations": 4}
{"id": 4517295, "s2_id": "ec79cc479e743b6264ef521b202c7ebda3a904c3", "title": "Energy efficiency of finite difference algorithms on multicore CPUs, GPUs, and Intel Xeon Phi processors", "abstract": "In addition to hardware wall-time restrictions commonly seen in high-performance computing systems, it is likely that future systems will also be constrained by energy budgets. In the present work, finite difference algorithms of varying computational and memory intensity are evaluated with respect to both energy efficiency and runtime on an Intel Ivy Bridge CPU node, an Intel Xeon Phi Knights Landing processor, and an NVIDIA Tesla K40c GPU. The conventional way of storing the discretised derivatives to global arrays for solution advancement is found to be inefficient in terms of energy consumption and runtime. In contrast, a class of algorithms in which the discretised derivatives are evaluated on-the-fly or stored as thread-/process-local variables (yielding high compute intensity) is optimal both with respect to energy consumption and runtime. On all three hardware architectures considered, a speed-up of ~2 and an energy saving of ~2 are observed for the high compute intensive algorithms compared to the memory intensive algorithm. The energy consumption is found to be proportional to runtime, irrespective of the power consumed and the GPU has an energy saving of ~5 compared to the same algorithm on a CPU node.", "venue": "ArXiv", "authors": ["Satya P. Jammy", "Christian T. Jacobs", "David J. Lusher", "Neil D. Sandham"], "year": 2017, "n_citations": 2}
{"id": 4520035, "s2_id": "e7570e7910159bd35dc56f480fd9984440eefd30", "title": "Improving OpenCL Performance by Specializing Compiler Phase Selection and Ordering", "abstract": "Automatic compiler phase selection/ordering has traditionally been focused on CPUs and, to a lesser extent, FPGAs. We present experiments regarding compiler phase ordering specialization of OpenCL kernels targeting a GPU. We use iterative exploration to specialize LLVM phase orders on 15 OpenCL benchmarks to an NVIDIA GPU. We analyze the generated NVIDIA PTX code for the various versions to identify the main causes of the most significant improvements and present results of a set of experiments that demonstrate the importance of using specific phase orders. Using specialized compiler phase orders, we were able to achieve geometric mean improvements of 1.54x (up to 5.48x) and 1.65x (up to 5.7x) over PTX generated by the NVIDIA CUDA compiler from CUDA versions of the same kernels, and over execution of the OpenCL kernels compiled from source with the NVIDIA OpenCL driver, respectively. We also evaluate the use of code-features in the OpenCL kernels. More specifically, we evaluate an approach that achieves geometric mean improvements of 1.49x and 1.56x over the same OpenCL baseline, by using the compiler sequences of the 1 or 3 most similar benchmarks, respectively.", "venue": "ArXiv", "authors": ["Ricardo  Nobre", "Lu\u00eds  Reis", "Jo\u00e3o M. P. Cardoso"], "year": 2018, "n_citations": 0}
{"id": 4521494, "s2_id": "cf66efc963280c9883a0596422b4998c9a93f65b", "title": "Custom Tailored Suite of Random Forests for Prefetcher Adaptation", "abstract": "To close the gap between memory and processors, and in turn improve performance, there has been an abundance of work in the area of data/instruction prefetcher designs. Prefetchers are deployed in each level of the memory hierarchy, but typically, each prefetcher gets designed without comprehensively accounting for other prefetchers in the system. As a result, these individual prefetcher designs do not always complement each other, and that leads to low average performance gains and/or many negative outliers. In this work, we propose SuitAP (Suite of random forests for Adaptation of Prefetcher system configuration), which is a hardware prefetcher adapter that uses a suite of random forests to determine at runtime which prefetcher should be ON at each memory level, such that they complement each other. Compared to a design with no prefetchers, using SuitAP we improve IPC by 46% on average across traces generated from SPEC2017 suite with 12KB overhead. Moreover, we also reduce negative outliers using SuitAP.", "venue": "ArXiv", "authors": ["Furkan  Eris", "Sadullah  Canakci", "Cansu  Demirkiran", "Ajay  Joshi"], "year": 2020, "n_citations": 0}
{"id": 4527949, "s2_id": "073e1eea3e5aeeadeb403b858f3f1a9fb8223403", "title": "An infinite dimensional model for a single server priority queue", "abstract": "We consider a Markovian single server queue in which customers are preemptively scheduled by exogenously assigned priority levels. The novelty in our model is that the priority levels are randomly assigned from a continuous probability measure rather than a discrete one. Because the priority levels are drawn from a continuum, the queue is modeled by a measure-valued stochastic process. We analyze the steady state behavior of this process and provide several results. We derive a measure that describes the average distribution of customer priority levels in the system; we provide a formula for the expected sojourn time of a customer as a function of his priority level; and we provide a formula for the expected waiting time of a customer as a function of his priority level. We interpret these quantitative results and give a qualitative understanding of how the priority levels affect individual customers as well as how they affect the system as a whole. The theoretical analysis is verified by simulation. We also discuss some directions of future work.", "venue": "2017 American Control Conference (ACC)", "authors": ["Neal  Master", "Zhengyuan  Zhou", "Nicholas  Bambos"], "year": 2017, "n_citations": 4}
{"id": 4529505, "s2_id": "9e5ea07585b24c11defe57720269dc97b3fad9f0", "title": "Infinite server queueing networks with deadline based routing", "abstract": "Motivated by timeouts in Internet services, we consider networks of infinite server queues in which routing decisions are based on deadlines. Specifically, at each node in the network, the total service time equals the minimum of several independent service times (e.g. the minimum of the amount of time required to complete a transaction and a deadline). Furthermore, routing decisions depend on which of the independent service times achieves the minimum (e.g. exceeding a deadline will require the customer to be routed so they can re-attempt the transaction). Because current routing decisions are dependent on past service times, much of the existing theory on product-form queueing networks does not apply. In spite of this, we are able to show that such networks have product-form equilibrium distributions. We verify our analytic characterization with a simulation of a simple network. We also discuss extensions of this work to more general settings.", "venue": "2016 IEEE 55th Conference on Decision and Control (CDC)", "authors": ["Neal  Master", "Nicholas  Bambos"], "year": 2016, "n_citations": 1}
{"id": 4530138, "s2_id": "8fd049b8b50e4a3f85f351208d90872ff67135ee", "title": "A Proof of Concept for Optimizing Task Parallelism by Locality Queues", "abstract": "Task parallelism as employed by the OpenMP task construct, although ideal for tackling irregular problems or typical producer/consumer schemes, bears some potential for performance bottlenecks if locality of data access is important, which is typically the case for memory-bound code on ccNUMA systems. We present a programming technique which ameliorates adverse effects of dynamic task distribution by sorting tasks into locality queues, each of which is preferably processed by threads that belong to the same locality domain. Dynamic scheduling is fully preserved inside each domain, and is preferred over possible load imbalance even if non-local access is required. The effectiveness of the approach is demonstrated using a blocked six-point stencil solver as a toy model.", "venue": "ArXiv", "authors": ["Markus  Wittmann", "Georg  Hager"], "year": 2009, "n_citations": 2}
{"id": 4533006, "s2_id": "f4994df2d8f16cb54aad04ef29bc181814748aab", "title": "Towards energy efficiency and maximum computational intensity for stencil algorithms using wavefront diamond temporal blocking", "abstract": "We study the impact of tunable parameters on computational intensity (i.e., inverse code balance) and energy consumption of multicore-optimized wavefront diamond temporal blocking (MWD) applied to different stencil-based update schemes. MWD combines the concepts of diamond tiling and multicore-aware wavefront blocking in order to achieve lower cache size requirements than standard single-core wavefront temporal blocking. We analyze the impact of the cache block size on the theoretical and observed code balance, introduce loop tiling in the leading dimension to widen the range of applicable diamond sizes, and show performance results on a contemporary Intel CPU. The impact of code balance on power dissipation on the CPU and in the DRAM is investigated and shows that DRAM power is a decisive factor for energy consumption, which is strongly influenced by the code balance. Furthermore we show that highest performance does not necessarily lead to lowest energy even if the clock speed is fixed.", "venue": "ArXiv", "authors": ["Tareq M. Malas", "Georg  Hager", "Hatem  Ltaief", "David E. Keyes"], "year": 2014, "n_citations": 9}
{"id": 4535012, "s2_id": "76f2e77d98f9f243e4ab65318b00e04e4ee6078a", "title": "IoTDataBench: Extending TPCx-IoT for Compression and Scalability", "abstract": "We present a record-breaking result and lessons learned in practicing TPCx-IoT benchmarking for a real-world use case. We find that more system characteristics need to be benchmarked for its application to real-world use cases. We introduce an extension to the TPCxIoT benchmark, covering fundamental requirements of time-series data management for IoT infrastructure. We characterize them as data compression and system scalability. To evaluate these two important features of IoT databases, we propose IoTDataBench and update four aspects of TPCx-IoT, i.e., data generation, workloads, metrics and test procedures. Preliminary evaluation results show systems that fail to effectively compress data or flexibly scale can negatively affect the redesigned metrics, while systems with high compression ratios and linear scalability are rewarded in the final metrics. Such systems have the ability to scale up computing resources on demand and can thus save dollar costs.", "venue": "ArXiv", "authors": ["Yuqing  Zhu", "Yanzhe  An", "Yuan  Zi", "Yu  Feng", "Jianmin  Wang"], "year": 2021, "n_citations": 0}
{"id": 4536616, "s2_id": "f6d36e69e5e2fff3e581b3d8046d21ea21ac3d7c", "title": "Fast Nonblocking Persistence for Concurrent Data Structures", "abstract": "We present a fully lock-free variant of our recent Montage system for persistent data structures. The variant, nbMontage, adds persistence to almost any nonblocking concurrent structure without introducing significant overhead or blocking of any kind. Like its predecessor, nbMontage is buffered durably linearizable: it guarantees that the state recovered in the wake of a crash will represent a consistent prefix of pre-crash execution. Unlike its predecessor, nbMontage ensures wait-free progress of the persistence frontier, thereby bounding the number of recent updates that may be lost on a crash, and allowing a thread to force an update of the frontier (i.e., to perform a sync operation) without the risk of blocking. As an extra benefit, the helping mechanism employed by our wait-free sync significantly reduces its latency. Performance results for nonblocking queues, skip lists, trees, and hash tables rival custom data structures in the literature \u2013 dramatically faster than achieved with prior general-purpose systems, and generally within 50% of equivalent non-persistent structures placed in DRAM. 2012 ACM Subject Classification Computing methodologies \u2192 Concurrent algorithms; Computer systems organization \u2192 Reliability; Theory of computation \u2192 Parallel computing models", "venue": "DISC", "authors": ["Wentao  Cai", "Haosen  Wen", "Vladimir  Maksimovski", "Mingzhe  Du", "Rafaello  Sanna", "Shreif  Abdallah", "Michael L. Scott"], "year": 2021, "n_citations": 1}
{"id": 4538216, "s2_id": "93cfeb7d5bf63abbf9ee9242eed734b8652a610c", "title": "The Potential of Synergistic Static, Dynamic and Speculative Loop Nest Optimizations for Automatic Parallelization", "abstract": "Research in automatic parallelization of loop-centric programs started with static analysis, then broadened its arsenal to include dynamic inspection-execution and speculative execution, the best results involving hybrid static-dynamic schemes. Beyond the detection of parallelism in a sequential program, scalable parallelization on many-core processors involves hard and interesting parallelism adaptation and mapping challenges. These challenges include tailoring data locality to the memory hierarchy, structuring independent tasks hierarchically to exploit multiple levels of parallelism, tuning the synchronization grain, balancing the execution load, decoupling the execution into thread-level pipelines, and leveraging heterogeneous hardware with specialized accelerators. The polyhedral framework allows to model, construct and apply very complex loop nest transformations addressing most of the parallelism adaptation and mapping challenges. But apart from hardware-specific, back-end oriented transformations (if-conversion, trace scheduling, value prediction), loop nest optimization has essentially ignored dynamic and speculative techniques. Research in polyhedral compilation recently reached a significant milestone towards the support of dynamic, data-dependent control flow. This opens a large avenue for blending dynamic analyses and speculative techniques with advanced loop nest optimizations. Selecting real-world examples from SPEC benchmarks and numerical kernels, we make a case for the design of synergistic static, dynamic and speculative loop transformation techniques. We also sketch the embedding of dynamic information, including speculative assumptions, in the heart of affine transformation search spaces.", "venue": "ArXiv", "authors": ["Riyadh  Baghdadi", "Albert  Cohen", "C\u00e9dric  Bastoul", "Louis-No\u00ebl  Pouchet", "Lawrence  Rauchwerger"], "year": 2011, "n_citations": 5}
{"id": 4539494, "s2_id": "46cf29eabc7059a70c580d226f24171b85849397", "title": "BigOP: Generating Comprehensive Big Data Workloads as a Benchmarking Framework", "abstract": "Big Data is considered proprietary asset of companies, organizations, and even nations. Turning big data into real treasure requires the support of big data systems. A variety of commercial and open source products have been unleashed for big data storage and processing. While big data users are facing the choice of which system best suits their needs, big data system developers are facing the question of how to evaluate their systems with regard to general big data processing needs. System benchmarking is the classic way of meeting the above demands. However, existent big data benchmarks either fail to represent the variety of big data processing requirements, or target only one specific platform, e.g. Hadoop. \nIn this paper, with our industrial partners, we present BigOP, an end-to-end system benchmarking framework, featuring the abstraction of representative Operation sets, workload Patterns, and prescribed tests. BigOP is part of an open-source big data benchmarking project, BigDataBench. BigOP's abstraction model not only guides the development of BigDataBench, but also enables automatic generation of tests with comprehensive workloads. \nWe illustrate the feasibility of BigOP by implementing an automatic test generation tool and benchmarking against three widely used big data processing systems, i.e. Hadoop, Spark and MySQL Cluster. Three tests targeting three different application scenarios are prescribed. The tests involve relational data, text data and graph data, as well as all operations and workload patterns. We report results following test specifications.", "venue": "DASFAA", "authors": ["Yuqing  Zhu", "Jianfeng  Zhan", "Chuliang  Weng", "Raghunath Othayoth Nambiar", "Jinchao  Zhang", "Xingzhen  Chen", "Lei  Wang"], "year": 2014, "n_citations": 14}
{"id": 4540413, "s2_id": "1d1f83023686d43fd4e8805c8e517dffb02d118c", "title": "Compiler Enhanced Scheduling for OpenMP for Heterogeneous Multiprocessors", "abstract": "Scheduling in Asymmetric Multicore Processors (AMP), a special case of Heterogeneous Multiprocessors, is a widely studied topic. The scheduling techniques which are mostly runtime do not usually consider parallel programming pattern used in parallel programming frameworks like OpenMP. On the other hand, current compilers for these parallel programming platforms are hardware oblivious which prevent any compile-time optimization for platforms like big.LITTLE and has to completely rely on runtime optimization. In this paper, we propose a hardware-aware Compiler Enhanced Scheduling (CES) where the common compiler transformations are coupled with compiler added scheduling commands to take advantage of the hardware asymmetry and improve the runtime efficiency. We implement a compiler for OpenMP and demonstrate its efficiency in Samsung Exynos with big.LITTLE architecture. On an average, we see 18% reduction in runtime and 14% reduction in energy consumption in standard NPB and FSU benchmarks with CES across multiple frequencies and core configurations in big.LITTLE.", "venue": "ArXiv", "authors": ["S  JyothiKrishnaV.", "Shankar  Balachandran"], "year": 2018, "n_citations": 3}
{"id": 4541807, "s2_id": "ea108479cc97674171dd9fa57338f4c4ac39f9ae", "title": "ClassyTune: A Performance Auto-Tuner for Systems in the Cloud", "abstract": "Performance tuning can improve the system performance and thus enable the reduction of cloud computing resources needed to support an application. Due to the ever increasing number of parameters and complexity of systems, there is a necessity to automate performance tuning for the complicated systems in the cloud. The state-of-the-art tuning methods are adopting either the experience-driven tuning approach or the data-driven one. Data-driven tuning is attracting increasing attentions, as it has wider applicability. But existing data-driven methods cannot fully address the challenges of sample scarcity and high dimensionality simultaneously. We present ClassyTune, a data-driven automatic configuration tuning tool for cloud systems. ClassyTune exploits the machine learning model of classification for auto-tuning. This exploitation enables the induction of more training samples without increasing the input dimension. Experiments on seven popular systems in the cloud show that ClassyTune can effectively tune system performance to seven times higher for high-dimensional configuration space, outperforming expert tuning and the state-of-the-art auto-tuning solutions. We also describe a use case in which performance tuning enables the reduction of 33% computing resources needed to run an online stateless service.", "venue": "IEEE Transactions on Cloud Computing", "authors": ["Yuqing  Zhu", "Jianxun  Liu"], "year": 2019, "n_citations": 5}
{"id": 4546082, "s2_id": "f6f0b94880de796e4100479c073ce1cb0f001689", "title": "Automating Large-Scale Simulation and Data Analysis with OMNeT++: Lession Learned and Future Perspectives", "abstract": "Simulation is widely adopted in the study of modern computer networks. In this context, OMNeT++ provides a set of very effective tools that span from the definition of the network, to the automation of simulation execution and quick result representation. However, as network models become more and more complex to cope with the evolution of network systems, the amount of simulation factors, the number of simulated nodes and the size of results grow consequently, leading to simulations with larger scale. In this work, we perform a critical analysis of the tools provided by OMNeT++ in case of such large-scale simulations. We then propose a unified and flexible software architecture to support simulation automation.", "venue": "ArXiv", "authors": ["Antonio  Virdis", "Carlo  Vallati", "Giovanni  Nardini"], "year": 2016, "n_citations": 3}
{"id": 4547843, "s2_id": "216b078e16076382b9c4a40298f7c2a227a2f826", "title": "RDMAvisor: Toward Deploying Scalable and Simple RDMA as a Service in Datacenters", "abstract": "RDMA is increasingly adopted by cloud computing platforms to provide low CPU overhead, low latency, high throughput network services. On the other hand, however, it is still challenging for developers to realize fast deployment of RDMA-aware applications in the datacenter, since the performance is highly related to many lowlevel details of RDMA operations. To address this problem, we present a simple and scalable RDMA as Service (RaaS) to mitigate the impact of RDMA operational details. RaaS provides careful message buffer management to improve CPU/memory utilization and improve the scalability of RDMA operations. These optimized designs lead to simple and flexible programming model for common and knowledgeable users. We have implemented a prototype of RaaS, named RDMAvisor, and evaluated its performance on a cluster with a large number of connections. Our experiment results demonstrate that RDMAvisor achieves high throughput for thousand of connections and maintains low CPU and memory overhead through adaptive RDMA transport selection.", "venue": "ArXiv", "authors": ["Zhi  Wang", "Xiaoliang  Wang", "Zhuzhong  Qian", "Baoliu  Ye", "Sanglu  Lu"], "year": 2018, "n_citations": 5}
{"id": 4548369, "s2_id": "4d044f0c84be360b26398b58027e618d5f08a83f", "title": "Towards a property graph generator for benchmarking", "abstract": "The use of synthetic graph generators is a common practice among graph-oriented benchmark designers, as it allows obtaining graphs with the required scale and characteristics. However, finding a graph generator that accurately fits the needs of a given benchmark is very difficult, thus practitioners end up creating ad-hoc ones. Such a task is usually time-consuming, and often leads to reinventing the wheel. In this paper, we introduce the conceptual design of DataSynth, a framework for property graphs generation with customizable schemas and characteristics. The goal of DataSynth is to assist benchmark designers in generating graphs efficiently and at scale, saving from implementing their own generators. Additionally, DataSynth introduces novel features barely explored so far, such as modeling the correlation between properties and the structure of the graph. This is achieved by a novel property-to-node matching algorithm for which we present preliminary promising results.", "venue": "GRADES@SIGMOD/PODS", "authors": ["Arnau  Prat-P\u00e9rez", "Joan  Guisado-G\u00e1mez", "Xavier Fern\u00e1ndez Salas", "Petr  Koupy", "Siegfried  Depner", "Davide Basilio Bartolini"], "year": 2017, "n_citations": 12}
{"id": 4553298, "s2_id": "48d3a9a0552bdbc339dc609f275add322a4845fc", "title": "Benchmarking SciDB data import on HPC systems", "abstract": "SciDB is a scalable, computational database management system that uses an array model for data storage. The array data model of SciDB makes it ideally suited for storing and managing large amounts of imaging data. SciDB is designed to support advanced analytics in database, thus reducing the need for extracting data for analysis. It is designed to be massively parallel and can run on commodity hardware in a high performance computing (HPC) environment. In this paper, we present the performance of SciDB using simulated image data. The Dynamic Distributed Dimensional Data Model (D4M) software is used to implement the benchmark on a cluster running the MIT SuperCloud software stack. A peak performance of 2.2M database inserts per second was achieved on a single node of this system. We also show that SciDB and the D4M toolbox provide more efficient ways to access random sub-volumes of massive datasets compared to the traditional approaches of reading volumetric data from individual files. This work describes the D4M and SciDB tools we developed and presents the initial performance results. This performance was achieved by using parallel inserts, a in-database merging of arrays as well as supercomputing techniques, such as distributed arrays and single-program-multiple-data programming.", "venue": "2016 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Siddharth  Samsi", "Laura J. Brattain", "William  Arcand", "David  Bestor", "Bill  Bergeron", "Chansup  Byun", "Vijay  Gadepally", "Matthew  Hubbell", "Michael  Jones", "Anna  Klein", "Peter  Michaleas", "Lauren  Milechin", "Julie  Mullen", "Andrew  Prout", "Antonio  Rosa", "Charles  Yee", "Jeremy  Kepner", "Albert  Reuther"], "year": 2016, "n_citations": 10}
{"id": 4554191, "s2_id": "949c4006aa7ce804107ad7c9bcc8c479d678cecd", "title": "Unasssuming View-Size Estimation Techniques in OLAP", "abstract": "Even if storage was infinite, a data warehouse could not materialize all possible views due to the running time and update requirements. Therefore, it is necessary to estimate quickly, accurately, and reliably the size of views. Many available techniques make particular statistical assumptions and their error can be quite large. Unassuming techniques exist, but typically assume we have independent hashing for which there is no known practical implementation. We adapt an unassuming estimator due to Gibbons and Tirthapura: its theoretical bounds do not make unpractical assumptions. We compare this technique experimentally with stochastic probabilistic counting, LogLog probabilistic counting, and multifractal statistical models. Our experiments show that we can reliably and accurately (within 10%, 19 times out 20) estimate view sizes over large data sets (1.5 GB) within minutes, using almost no memory. However, only Gibbons-Tirthapura provides universally tight estimates irrespective of the size of the view. For large views, probabilistic counting has a small edge in accuracy, whereas the competitive sampling-based method (multifractal) we tested is an order of magnitude faster but can sometimes provide poor estimates (relative error of 100%). In our tests, LogLog probabilistic counting is not competitive. Experimental validation on the US Census 1990 data set and on the Transaction Processing Performance (TPC H) data set is provided.", "venue": "ArXiv", "authors": ["Kamel  Aouiche", "Daniel  Lemire"], "year": 2007, "n_citations": 3}
{"id": 4555570, "s2_id": "8c1fcd37b6e76bad8fdc10cbc873fe6891a53a79", "title": "Heavy Traffic Limits for GI/H/n Queues: Theory and Application", "abstract": "We consider a GI/H/n queueing system. In this system, there are multiple servers in the queue. The inter-arrival time is general and independent, and the service time follows hyper-exponential distribution. Instead of stochastic differential equations, we propose two heavy traffic limits for this system, which can be easily applied in practical systems. In applications, we show how to use these heavy traffic limits to design a power efficient cloud computing environment based on different QoS requirements.", "venue": "ArXiv", "authors": ["Yousi  Zheng", "Ness B. Shroff", "Prasun  Sinha"], "year": 2014, "n_citations": 4}
{"id": 4559146, "s2_id": "67b11e6c40d36760e197e58b9944ceba8858b838", "title": "Using Propagation for Solving Complex Arithmetic Constraints", "abstract": "Solving a system of nonlinear inequalities is an important problem for which conventional numerical analysis has no satisfactory method. With a box-consistency algorithm one can compute a cover for the solution set to arbitrarily close approximation. Because of difficulties in the use of propagation for complex arithmetic expressions, box consistency is computed with interval arithmetic. In this paper we present theorems that support a simple modification of propagation that allows complex arithmetic expressions to be handled efficiently. The version of box consistency that is obtained in this way is stronger than when interval arithmetic is used.", "venue": "ArXiv", "authors": ["M. H. van Emden", "Belaid  Moa"], "year": 2003, "n_citations": 1}
{"id": 4559155, "s2_id": "6abbc3d834a4341aec82228aac9fc0cf3fc0931c", "title": "Exploiting Parallelism Opportunities with Deep Learning Frameworks", "abstract": "State-of-the-art machine learning frameworks support a wide variety of design features to enable a flexible machine learning programming interface and to ease the programmability burden on machine learning developers. Identifying and using a performance-optimal setting in feature-rich frameworks, however, involves a non-trivial amount of performance profiling efforts and often relies on domain-specific knowledge. This article takes a deep dive into analyzing the performance impact of key design features in a machine learning framework and quantifies the role of parallelism. The observations and insights distill into a simple set of guidelines that one can use to achieve much higher training and inference speedup. Across a diverse set of real-world deep learning models, the evaluation results show that the proposed performance tuning guidelines outperform the Intel and TensorFlow recommended settings by 1.30\u00d7 and 1.38\u00d7, respectively.", "venue": "ACM Trans. Archit. Code Optim.", "authors": ["Yu Emma Wang", "Carole-Jean  Wu", "Xiaodong  Wang", "Kim  Hazelwood", "David  Brooks"], "year": 2021, "n_citations": 9}
{"id": 4559945, "s2_id": "ccc39a4b9345f0052210aac746a81536a825a1dc", "title": "An Empirical Evaluation of Allgatherv on Multi-GPU Systems", "abstract": "Applications for deep learning and big data analytics have compute and memory requirements that exceed the limits of a single GPU. However, effectively scaling out an application to multiple GPUs is challenging due to the complexities of communication between the GPUs, particularly for collective communication with irregular message sizes. In this work, we provide a performance evaluation of the Allgatherv routine on multi-GPU systems, focusing on GPU network topology and the communication library used. We present results from the OSU-micro benchmark as well as conduct a case study for sparse tensor factorization, one application that uses Allgatherv with highly irregular message sizes. We extend our existing tensor factorization tool to run on systems with different node counts and varying number of GPUs per node. We then evaluate the communication performance of our tool when using traditional MPI, CUDA-aware MVAPICH and NCCL across a suite of real-world data sets on three different systems: a 16-node cluster with one GPU per node, NVIDIA's DGX-1 with 8 GPUs and Cray's CS-Storm with 16 GPUs. Our results show that irregularity in the tensor data sets produce trends that contradict those in the OSU micro-benchmark, as well as trends that are absent from the benchmark.", "venue": "2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)", "authors": ["Thomas B. Rolinger", "Tyler A. Simon", "Christopher D. Krieger"], "year": 2018, "n_citations": 2}
{"id": 4561422, "s2_id": "663475b0ea12e123afff6d7ccd75488cc19ea79c", "title": "Slim graph: practical lossy graph compression for approximate graph processing, storage, and analytics", "abstract": "We propose Slim Graph: the first programming model and framework for practical lossy graph compression that facilitates high-performance approximate graph processing, storage, and analytics. Slim Graph enables the developer to express numerous compression schemes using small and programmable compression kernels that can access and modify local parts of input graphs. Such kernels are executed in parallel by the underlying engine, isolating developers from complexities of parallel programming. Our kernels implement novel graph compression schemes that preserve numerous graph properties, for example connected components, minimum spanning trees, or graph spectra. Finally, Slim Graph uses statistical divergences and other metrics to analyze the accuracy of lossy graph compression. We illustrate both theoretically and empirically that Slim Graph accelerates numerous graph algorithms, reduces storage used by graph datasets, and ensures high accuracy of results. Slim Graph may become the common ground for developing, executing, and analyzing emerging lossy graph compression schemes.", "venue": "SC", "authors": ["Maciej  Besta", "Simon  Weber", "Lukas  Gianinazzi", "Robert  Gerstenberger", "Andrey  Ivanov", "Yishai  Oltchik", "Torsten  Hoefler"], "year": 2019, "n_citations": 21}
{"id": 4564188, "s2_id": "5e3222023d88fac8c47cf7952b47346defa852aa", "title": "Asymptotically Optimal Load Balancing in Large-scale Heterogeneous Systems with Multiple Dispatchers", "abstract": "We consider the load balancing problem in large-scale heterogeneous systems with multiple dispatchers. We introduce a general framework called Local-Estimation-Driven (LED). Under this framework, each dispatcher keeps local (possibly outdated) estimates of the queue lengths for all the servers, and the dispatching decision is made purely based on these local estimates. The local estimates are updated via infrequent communications between dispatchers and servers. We derive sufficient conditions for LED policies to achieve throughput optimality and delay optimality in heavy-traffic, respectively. These conditions directly imply delay optimality for many previous local-memory based policies in heavy traffic. Moreover, the results enable us to design new delay optimal policies for heterogeneous systems with multiple dispatchers. Finally, the heavy-traffic delay optimality of the LED framework also sheds light on a recent open question on how to design optimal load balancing schemes using delayed information.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Xingyu  Zhou", "Ness B. Shroff", "Adam  Wierman"], "year": 2020, "n_citations": 2}
{"id": 4564604, "s2_id": "798bf33081864158a576ce8ce76409e539940040", "title": "Performance Characterization of In-Memory Data Analytics on a Modern Cloud Server", "abstract": "In last decade, data analytics have rapidly progressed from traditional disk-based processing to modern in-memory processing. However, little effort has been devoted at enhancing performance at micro-architecture level. This paper characterizes the performance of in-memory data analytics using Apache Spark framework. We use a single node NUMA machine and identify the bottlenecks hampering the scalability of workloads. We also quantify the inefficiencies at micro-architecture level for various data analysis workloads. Through empirical evaluation, we show that spark workloads do not scale linearly beyond twelve threads, due to work time inflation and thread level load imbalance. Further, at the micro-architecture level, we observe memory bound latency to be the major cause of work time inflation.", "venue": "2015 IEEE Fifth International Conference on Big Data and Cloud Computing", "authors": ["Ahsan Javed Awan", "Mats  Brorsson", "Vladimir  Vlassov", "Eduard  Ayguad\u00e9"], "year": 2015, "n_citations": 44}
{"id": 4564714, "s2_id": "fce8241e28bb06efcc5da42ea6bf93f078874175", "title": "Quantifying the effect of matrix structure on multithreaded performance of the SpMV kernel", "abstract": "Sparse matrix-vector multiplication (SpMV) is the core operation in many common network and graph analytics, but poor performance of the SpMV kernel handicaps these applications. This work quantifies the effect of matrix structure on SpMV performance, using Intel's VTune tool for the Sandy Bridge architecture. Two types of sparse matrices are considered: finite difference (FD) matrices, which are structured, and R-MAT matrices, which are unstructured. Analysis of cache behavior and prefetcher activity reveals that the SpMV kernel performs far worse with R-MAT matrices than with FD matrices, due to the difference in matrix structure. To address the problems caused by unstructured matrices, novel architecture improvements are proposed.", "venue": "2014 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Daniel  Kimball", "Elizabeth  Michel", "Paul  Keltcher", "Michael M. Wolf"], "year": 2014, "n_citations": 2}
{"id": 4566303, "s2_id": "598141f2cb8369fac51b08e216ae3dbd97e9988e", "title": "Maximizing Cloud Providers' Revenues via Energy Aware Allocation Policies", "abstract": "Cloud providers, like Amazon, offer their data centers' computational and storage capacities for lease to paying customers. High electricity consumption, associated with running a data center, not only reflects on its carbon footprint, but also increases the costs of running the data center itself. This paper addresses the problem of maximizing the revenues of Cloud providers by trimming down their electricity costs. As a solution allocation policies which are based on the dynamic powering servers on and off are introduced and evaluated. The policies aim at satisfying the conflicting goals of maximizing the users' experience while minimizing the amount of consumed electricity. The results of numerical experiments and simulations are described, showing that the proposed scheme performs well under different traffic conditions.", "venue": "2010 IEEE 3rd International Conference on Cloud Computing", "authors": ["Michele  Mazzucco", "Dmytro  Dyachuk", "Ralph  Deters"], "year": 2010, "n_citations": 150}
{"id": 4566884, "s2_id": "11ae03d73ad4bd138c1e63b333bba4bcca65e77c", "title": "A lightweight optimization selection method for Sparse Matrix-Vector Multiplication", "abstract": "In this paper, we propose an optimization selection methodology for the ubiquitous sparse matrix-vector multiplication (SpMV) kernel. We propose two models that attempt to identify the major performance bottleneck of the kernel for every instance of the problem and then select an appropriate optimization to tackle it. Our first model requires online profiling of the input matrix in order to detect its most prevailing performance issue, while our second model only uses comprehensive structural features of the sparse matrix. Our method delivers high performance stability for SpMV across different platforms and sparse matrices, due to its application and architecture awareness. Our experimental results demonstrate that a) our approach is able to distinguish and appropriately optimize special matrices in multicore platforms that fall out of the standard class of memory bandwidth bound matrices, and b) lead to a significant performance gain of 29% in a manycore platform compared to an architecture-centric optimization, as a result of the successful selection of the appropriate optimization for the great majority of the matrices. With a runtime overhead equivalent to a couple dozen SpMV operations, our approach is practical for use in iterative numerical solvers of real-life applications.", "venue": "ArXiv", "authors": ["Athena  Elafrou", "Georgios I. Goumas", "Nectarios  Koziris"], "year": 2015, "n_citations": 6}
{"id": 4573208, "s2_id": "d25da45400be4bbb51d650c18ebde4d21837252d", "title": "Maneuver Identification Challenge", "abstract": "AI algorithms that identify maneuvers from trajectory data could play an important role in improving flight safety and pilot training. AI challenges allow diverse teams to work together to solve hard problems and are an effective tool for developing AI solutions. AI challenges are also a key driver of AI computational requirements. The Maneuver Identification Challenge hosted at maneuver-id.mit.edu provides thousands of trajectories collected from pilots practicing in flight simulators, descriptions of maneuvers, and examples of these maneuvers performed by experienced pilots. Each trajectory consists of positions, velocities, and aircraft orientations normalized to a common coordinate system. Construction of the data set required significant data architecture to transform flight simulator logs into AI ready data, which included using a supercomputer for deduplication and data conditioning. There are three proposed challenges. The first challenge is separating physically plausible (good) trajectories from unfeasible (bad) trajectories. Human labeled good and bad trajectories are provided to aid in this task. Subsequent challenges are to label trajectories with their intended maneuvers and to assess the quality of those maneuvers.", "venue": "2021 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Kaira  Samuel", "Vijay  Gadepally", "David  Jacobs", "Michael  Jones", "Kyle  McAlpin", "Kyle  Palko", "Ben  Paulk", "Sid  Samsi", "Ho Chit Siu", "Charles  Yee", "Jeremy  Kepner"], "year": 2021, "n_citations": 0}
{"id": 4574393, "s2_id": "158fd0ff5701b15ddea63f06f4e10592f069a947", "title": "FPGA-based Multi-Chip Module for High-Performance Computing", "abstract": "Current integration, architectural design and manufacturing technologies are not suited for the computing density and power efficiency requested by Exascale computing. New approaches in hardware architecture are thus needed to overcome the technological barriers preventing the transition to the Exascale era. In that scope, we report successful fabrication of first ExaNoDe's MCM prototypes dedicated to Exascale computing applications. Each MCM was composed of 2 Xilinx Zynq Ultrascale+ MPSoC, assembled on advanced 68.5 mm x 55 mm laminate substrates specifically designed and fabricated for the project. Acoustic microscopy, x-ray, cross-section and Thermo-Moire investigations revealed no voids, shorts, delamination, cracks or warpage issues. Two MCMs were mounted on a daughter board by FORTH for testing purposes. The DDR memories on the 4 SODIMMs of the daughter board were successfully tested by running extensive Xilinx memory tests with clock frequencies of 1866 MHz and 2133 MHz. All 4 FPGAs were programmed with the Xilinx integrated bit error ratio test (IBERT) tailored for this board for links testing. All intra-board high-speed links between all FPGAs were stable at 10 Gbps, even under the more demanding 31-bit PRBS (Pseudorandom Binary Sequence) tests.", "venue": "ArXiv", "authors": ["Yann  Beilliard", "Maxime  Godard", "Aggelos  Ioannou", "Astrinos  Damianakis", "Michael  Ligerakis", "Iakovos  Mavroidis", "Pierre-Yves  Martinez", "David  Danovitch", "Julien  Sylvestre", "Dominique  Drouin"], "year": 2019, "n_citations": 0}
{"id": 4577276, "s2_id": "b6c80ba455f3657195e0aadab1fd3950706b6cec", "title": "Scalable Load Balancing in the Presence of Heterogeneous Servers", "abstract": "In large-scale computer systems, deciding how to dispatch arriving jobs to servers is a primary factor affecting system performance. Consequently, there is a wealth of literature on designing, analyzing, and evaluating the performance of load balancing policies. For analytical tractability, most existing work on dispatching in large-scale systems makes a key assumption: that the servers are homogeneous, meaning that they all have the same speeds, capabilities, and available resources. But this assumption is not accurate in practice. Modern computer systems are instead heterogeneous: server farms may consist of multiple generations of hardware, servers with varied resources, or even virtual machines running in a cloud environment. Given the ubiquity of heterogeneity in today's systems, it is critically important to develop load balancing policies that perform well in heterogeneous environments. In this paper, we focus on systems in which server speeds are heterogeneous.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Kristen  Gardner", "Jazeem Abdul Jaleel", "Alexander  Wickeham", "Sherwin  Doroudi"], "year": 2020, "n_citations": 2}
{"id": 4577943, "s2_id": "298465c5e711ddb20dad0a6d2c60cbcc21cd9859", "title": "Enhancing Spectral Utilization by Maximizing the Reuse in LTE Network", "abstract": "Need for increased spectral efficiency is key to improve the quality of experience for next-generation wireless applications like online gaming, HD Video, etc.,. In our work, we consider an LTE Device-to-device (D2D) network where LTE UEs have primary access to the spectrum and D2D pairs have secondary access. To enhance spectral efficiency, BS can offload the traffic by activating multiple D2D pairs within the serving cell. This ensures that the same radio resource will be reused across the primary LTE UEs and different D2D pairs. In this context, we propose to enable more D2D secondary users in the serving cell, by utilizing neighboring BS spectrum to fairly co-exist with neighboring LTE primary users. We model the system and show via extensive simulations, that the above configuration guarantees good throughput for the D2D pairs in the serving cell while ensuring that the primary LTE throughput demand is not compromised.", "venue": "ArXiv", "authors": ["S. Yuva Kumar", "Vanlin  Sathya", "Sreenath  Ramanath"], "year": 2019, "n_citations": 2}
{"id": 4578241, "s2_id": "fc115df2982a3083ece7027d81bd08b2935b24fd", "title": "Reliability of Broadcast Communications Under Sparse Random Linear Network Coding", "abstract": "Ultrareliable point-to-multipoint communications are expected to become pivotal in networks offering future dependable services for smart cities. In this regard, sparse random linear network coding techniques have been widely employed to provide an efficient way to improve the reliability of broadcast and multicast data streams. This paper addresses the pressing concern of providing a tight approximation to the probability of a user recovering a data stream protected by this kind of coding technique. In particular, by exploiting the Stein\u2013Chen method, we provide a novel and general performance framework applicable to any combination of system and service parameters, such as finite field sizes, lengths of the data stream, and level of sparsity. The deviation of the proposed approximation from Monte Carlo simulations is negligible, improving significantly on the state-of-the-art performance bounds.", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Suzie  Brown", "Oliver  Johnson", "Andrea  Tassi"], "year": 2018, "n_citations": 16}
{"id": 4578537, "s2_id": "1167aa4a059567a7e147b451dd8b68109126eec7", "title": "An approach to define Very High Capacity Networks with improved quality at an affordable cost", "abstract": "This paper aims to propose one possible approach in the setting of VHCNs (Very High Capacity Networks) performance targets that should be capable of promoting efficient investments for operators and, at the same time, improving the benefits for end-users. To this aim, we suggest relying on some specific KPIs (Key Performance Indicators), especially throughput - i.e., the bandwidth as perceived by the customer - valid at the application layer, instead of the physical layer data-rate. In this regard, the paper underlines that the bandwidth perceived is strictly linked to the latency. The most important implication is that some of the most demanding services envisaged for the future (e.g., mobile virtual and augmented reality, tactile internet) cannot be met by merely increasing the low-level protocol data-rate. Therefore, for the VHCNs reducing latency through Edge Cloud Computing (ECC) is a mandatory pre-requisite.", "venue": "2020 Global Information Infrastructure and Networking Symposium (GIIS)", "authors": ["Giovanni  Santella", "Francesco  Vatalaro"], "year": 2020, "n_citations": 0}
{"id": 4580535, "s2_id": "24fa70a670a4b63173c0622743769e06ef6986ff", "title": "A fast analytical model of fully associative caches", "abstract": "While the cost of computation is an easy to understand local property, the cost of data movement on cached architectures depends on global state, does not compose, and is hard to predict. As a result, programmers often fail to consider the cost of data movement. Existing cache models and simulators provide the missing information but are computationally expensive. We present a lightweight cache model for fully associative caches with least recently used (LRU) replacement policy that gives fast and accurate results. We count the cache misses without explicit enumeration of all memory accesses by using symbolic counting techniques twice: 1) to derive the stack distance for each memory access and 2) to count the memory accesses with stack distance larger than the cache size. While this technique seems infeasible in theory, due to non-linearities after the first round of counting, we show that the counting problems are sufficiently linear in practice. Our cache model often computes the results within seconds and contrary to simulation the execution time is mostly problem size independent. Our evaluation measures modeling errors below 0.6% on real hardware. By providing accurate data placement information we enable memory hierarchy aware software development.", "venue": "PLDI", "authors": ["Tobias  Gysi", "Tobias  Grosser", "Laurin  Brandner", "Torsten  Hoefler"], "year": 2019, "n_citations": 19}
{"id": 4591553, "s2_id": "86e2c860d2e93759f05a3a1597921d49e3ee1493", "title": "Performability of Network Service Chains: Stochastic Modeling and Assessment of Softwarized IP Multimedia Subsystem", "abstract": "Service provisioning mechanisms implemented across 5G infrastructures take broadly into use the network service chain concept. Typically, it is coupled with Network Function Virtualization (NFV) paradigm, and consists in defining a pre-determined path traversed by a set of softwarized network nodes to provide specific services. A well known chain-like framework is the IP Multimedia Subsystem (IMS), a key infrastructure of 5G networks, that we characterize both by a performance and an availability perspective. Precisely, supported by a designed from scratch testbed realized through Clearwater platform, we perform a stochastic assessment of a softwarized IMS (softIMS) architecture where two main stages stand out: i) a performance analysis, where, exploiting the queueing network decomposition method, we formalize an optimization problem of resource allocation by modeling each softIMS node as an M/G/c system; ii) an availability assessment, where, adopting the Stochastic Reward Net methodology, we are able to characterize the behavior of softIMS in terms of failure/repair events, and to derive a set of optimal configurations satisfying a given availability requirement (e.g. five nines) while minimizing deployment costs. Two routines dubbed OptCNT and OptSearchChain have been devised to govern the performance and availability analyses, respectively.", "venue": "ArXiv", "authors": ["Mario Di Mauro", "Giovanni  Galatro", "Fabio  Postiglione", "Marco  Tambasco"], "year": 2021, "n_citations": 0}
{"id": 4594346, "s2_id": "a0d09bed5c61e0be12b2b9c2482b4e3319e97788", "title": "Delay optimal power aware opportunistic scheduling with mutual information accumulation", "abstract": "This paper considers optimization of power and delay in a time-varying wireless link using rateless codes. The link serves a sequence of variable-length packets. Each packet is coded and transmitted over multiple slots. Channel conditions can change from slot to slot and are unknown to the transmitter. The amount of mutual information accumulated on each slot depends on the random channel realization and the power used. The goal is to minimize average service delay subject to an average power constraint. We formulate this problem as a frame-based stochastic optimization problem and solve it via an online algorithm. We show that the subproblem within each frame is a simple integer program which can be effectively solved using a dynamic program. The optimality of this online algorithm is proved using the frame-based Lyapunov drift analysis.", "venue": "2016 14th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)", "authors": ["Xiaohan  Wei", "Michael J. Neely"], "year": 2016, "n_citations": 0}
{"id": 4594464, "s2_id": "d7fd2f039fdf93e8e381ae898d30cc188a64be41", "title": "Deep Reinforcement Learning-Aided RAN Slicing Enforcement for B5G Latency Sensitive Services", "abstract": "The combination of cloud computing capabilities at the network edge and artificial intelligence promise to turn future mobile networks into serviceand radio-aware entities, able to address the requirements of upcoming latency-sensitive applications. In this context, a challenging research goal is to exploit edge intelligence to dynamically and optimally manage the Radio Access Network Slicing (that is a less mature and more complex technology than fifth-generation Network Slicing) and Radio Resource Management, which is a very complex task due to the mostly unpredictably nature of the wireless channel. This paper presents a novel architecture that leverages Deep Reinforcement Learning at the edge of the network in order to address Radio Access Network Slicing and Radio Resource Management optimization supporting latency-sensitive applications. The effectiveness of our proposal against baseline methodologies is investigated through computer simulation, by considering an autonomous-driving use-case.", "venue": "ArXiv", "authors": ["Sergio  Martiradonna", "Andrea  Abrardo", "Marco  Moretti", "Giuseppe  Piro", "Gennaro  Boggia"], "year": 2021, "n_citations": 0}
{"id": 4596180, "s2_id": "73d22076fd4a734b06c769f86d862e4a1b2ecf38", "title": "Analysis of AeroMACS Data Link for Unmanned Aircraft Vehicles", "abstract": "Aeronautical Mobile Airport Communications System (AeroMACS) is based on the IEEE 802.16e mobile wireless standard commonly known as WiMAX. It is expected to be a main part of next-generation aviation communication system to support fixed and mobile services for manned and unmanned applications. AeroMACS will be an essential technology helping pave the way toward full integration of Unmanned Aircraft Vehicle (UAV) into the national airspace. A number of practical tests and analyses have been done so far for AeroMACS. The main contribution of this paper is to consider the theoretical concepts behind its features and discuss their suitability for UAVs\u2019 applications. Mathematical analyses of AeroMACS physical layer framework is provided to show the theoretical trade-offs. We mainly focus on the analysis of AeroMACS OFDMA structure, which affects the speed limits, coverage cell, channel estimation requirements and inter-carrier interference.", "venue": "2018 International Conference on Unmanned Aircraft Systems (ICUAS)", "authors": ["Maede  Zolanvari", "Marcio A. Teixeira", "Raj  Jain"], "year": 2018, "n_citations": 5}
{"id": 4598242, "s2_id": "10364a5d13902228751e7de150371a8185d676ed", "title": "Measurement-Based Coexistence Studies of LAA & Wi-Fi Deployments in Chicago", "abstract": "LTE-Licensed Assisted Access (LAA) networks are beginning to be deployed widely in major metropolitan areas in the U.S. in the unlicensed 5 GHz bands, which have existing dense deployments of Wi-Fi as well. Various aspects of the coexistence scenarios such deployments give rise to have been considered in a vast body of academic and industry research. However, there is very little data and research on how these coexisting networks will behave in practice. The question of \u201cfair coexistence\u201d between Wi-Fi and LAA has moved from a theoretical question to reality. The recent roll-out of LAA deployments provides an opportunity to collect data on the operation of these networks as well as studying coexistence issues on the ground. In this article we describe the first results of a measurement campaign conducted over many months, using custom apps as well as off-the-shelf tools, in several areas of Chicago where the major carriers have been expanding LAA deployments. The measurements reveal that coexistence between LAA and Wi-Fi in dense urban environments where both systems aggregate multiple channels continues to be a challenging problem that requires further research.", "venue": "IEEE Wireless Communications", "authors": ["Vanlin  Sathya", "Muhammad Iqbal Rochman", "Monisha  Ghosh"], "year": 2021, "n_citations": 11}
{"id": 4600392, "s2_id": "604e909836066f8230350aeb5fdb04cdfaa0ff57", "title": "Competitive Algorithms for Minimizing the Maximum Age-of-Information", "abstract": "In this short paper, we consider the problem of designing a near-optimal competitive scheduling policy to maximize the freshness of available information uniformly across N mobile users. Motivated by the unreliability and non-stationarity of the emerging 5G-mmWave channels for high-speed users, we forego of any statistical modeling assumptions of the wireless channels and user-mobility. Instead, we allow the channel states and the mobility patterns to be dictated by an omniscient adversary. It is not difficult to see that no competitive scheduling policy can exist for the corresponding throughput-maximization problem in this adversarial model. Surprisingly, we show that there exists a simple online distributed scheduling policy with a finite competitive ratio for maximizing the freshness of information in this model. We also prove that the proposed policy is competitively optimal up to an O(logN) factor.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Rajarshi  Bhattacharjee", "Abhishek  Sinha"], "year": 2020, "n_citations": 2}
{"id": 4600507, "s2_id": "22e7af5579777c1df29530b55cace34fc8243fea", "title": "Message Scheduling for Performant, Many-Core Belief Propagation", "abstract": "Belief Propagation (BP) is a message-passing algorithm for approximate inference over Probabilistic Graphical Models (PGMs), finding many applications such as computer vision, error-correcting codes, and protein-folding. While general, the convergence and speed of the algorithm has limited its practical use on difficult inference problems. As an algorithm that is highly amenable to parallelization, many-core Graphical Processing Units (GPUs) could significantly improve BP performance. Improving BP through many-core systems is non-trivial: the scheduling of messages in the algorithm strongly affects performance. We present a study of message scheduling for BP on GPUs. We demonstrate that BP exhibits a tradeoff between speed and convergence based on parallelism and show that existing message schedulings are not able to utilize this tradeoff. To this end, we present a novel randomized message scheduling approach, Randomized BP (RnBP), which outperforms existing methods on the GPU.", "venue": "2019 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Mark Van der Merwe", "Vinu  Joseph", "Ganesh  Gopalakrishnan"], "year": 2019, "n_citations": 2}
{"id": 4601055, "s2_id": "0e7c88e6f5facbe307e2ce8fa059bd2120311ea2", "title": "Communication-Aware Scheduling of Precedence-Constrained Tasks on Related Machines", "abstract": "Scheduling precedence-constrained tasks is a classical problem that has been studied for more than fifty years. However, little progress has been made in the setting where there are communication delays between tasks. Results for the case of identical machines were derived nearly thirty years ago, and yet no results for related machines have followed. In this work, we propose a new scheduler, Generalized Earliest Time First (GETF), and provide the first provable, worst-case approximation guarantees for the goals of minimizing both the makespan and total weighted completion time of tasks with precedence constraints on related machines with machine-dependent communication times.", "venue": "ArXiv", "authors": ["Yu  Su", "Xiaoqi  Ren", "Shai  Vardi", "Adam  Wierman"], "year": 2020, "n_citations": 0}
{"id": 4601713, "s2_id": "45858fd5a8e651280647a1340c24ff7ffd24f86f", "title": "Sketch-Based Estimation of Subpopulation-Weight", "abstract": "Summaries of massive data sets support approximate query processing over the original data. A basic aggregate over a set of records is the weight of subpopulations specified as a predicate over records' attributes. Bottom-k sketches are a powerful summarization format of weighted items that includes priority sampling and the classic weighted sampling without replacement. They can be computed efficiently for many representations of the data including distributed databases and data streams. \nWe derive novel unbiased estimators and efficient confidence bounds for subpopulation weight. Our estimators and bounds are tailored by distinguishing between applications (such as data streams) where the total weight of the sketched set can be computed by the summarization algorithm without a significant use of additional resources, and applications (such as sketches of network neighborhoods) where this is not the case. \nOur rigorous derivations are based on clever applications of the Horvitz-Thompson estimator, and are complemented by efficient computational methods. We demonstrate their benefit on a wide range of Pareto distributions.", "venue": "ArXiv", "authors": ["Edith  Cohen", "Haim  Kaplan"], "year": 2008, "n_citations": 5}
{"id": 4604138, "s2_id": "cdcc0ed2164ef9a748c748e99e8a7e3528588611", "title": "DLFusion: An Auto-Tuning Compiler for Layer Fusion on Deep Neural Network Accelerator", "abstract": "Many hardware vendors have introduced specialized deep neural networks (DNN) accelerators owing to their superior performance and efficiency. As such, how to generate and optimize the code for the hardware accelerator becomes an important yet less explored problem. In this paper, we perform the compiler-stage optimization study using a novel and representative Cambricon DNN accelerator and demonstrate that the code optimization knobs play an important role in unleashing the potential of hardware computational horsepower. However, even only two studied code optimization knobs, namely the number of cores and layer fusion scheme, present an enormous search space that prevents the naive brute-force search. This work introduces a joint, auto-tuning optimization framework to address this challenge. We first use a set of synthesized DNN layers to study the interplay between the hardware performance and layer characteristics. Based on the insights, we extract the operation count and feature map channel size as each layer's characteristics and derive a joint optimization strategy to decide the performance-optimal core number and fusion scheme. We evaluate the performance of the proposed approach using a set of representative DNN models and show that it achieves the minimal of 3.6x and the maximal of 7.9x performance speedup compared to no optimization baseline. We also show that the achieved speedup is close to the oracle case that is based on a reduced brute-force search but with much less search time.", "venue": "2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)", "authors": ["Zihan  Liu", "Jingwen  Leng", "Quan  Chen", "Chao  Li", "Wenli  Zheng", "Li  Li", "Minyi  Guo"], "year": 2020, "n_citations": 0}
{"id": 4606359, "s2_id": "e3cf24d8cfdd41700f80ea2827dbe42fda5953f3", "title": "Towards Latency-aware DNN Optimization with GPU Runtime Analysis and Tail Effect Elimination", "abstract": "Despite the superb performance of State-Of-The-Art (SOTA) DNNs, the increasing computational cost makes them very challenging to meet real-time latency and accuracy requirements. Although DNN runtime latency is dictated by model property (e.g., architecture, operations), hardware property (e.g., utilization, throughput), and more importantly, the effective mapping between these two, many existing approaches focus only on optimizing model property such as FLOPS reduction and overlook the mismatch between DNN model and hardware properties. In this work, we show that the mismatch between the varied DNN computation workloads and GPU capacity can cause the idle GPU tail effect, leading to GPU under-utilization and low throughput. As a result, the FLOPs reduction cannot bring effective latency reduction, which causes sub-optimal accuracy versus latency trade-offs. Motivated by this, we propose a GPU runtime-aware DNN optimization methodology to eliminate such GPU tail effect adaptively on GPU platforms. Our methodology can be applied on top of existing SOTA DNN optimization approaches to achieve better latency and accuracy trade-offs. Experiments show 11%-27% latency reduction and 2.5%-4.0% accuracy improvement over several SOTA DNN pruning and NAS methods, respectively", "venue": "ArXiv", "authors": ["Fuxun  Yu", "Zirui  Xu", "Tong  Shen", "Dimitrios  Stamoulis", "Longfei  Shangguan", "Di  Wang", "Rishi  Madhok", "Chunshui  Zhao", "Xin  Li", "Nikolaos  Karianakis", "Dimitrios  Lymberopoulos", "Ang  Li", "ChenChen  Liu", "Yiran  Chen", "Xiang  Chen"], "year": 2020, "n_citations": 2}
{"id": 4612854, "s2_id": "6908ae82d96b29547faa2bb45b585fc4aa93095b", "title": "ALEA: Fine-Grain Energy Profiling with Basic Block Sampling", "abstract": "Energy efficiency is an essential requirement for all contemporary computing systems. We thus need tools to measure the energy consumption of computing systems and to understand how workloads affect it. Significant recent research effort has targeted direct power measurements on production computing systems using on-board sensors or external instruments. These direct methods have in turn guided studies of software techniques to reduce energy consumption via workload allocation and scaling. Unfortunately, direct energymeasurementsarehamperedbythelowpowersampling frequency of power sensors. The coarse granularity of power sensing limits our understanding of how power is allocated in systems and our ability to optimize energy efficiency via workload allocation. We present ALEA, a tool to measure power and energy consumption at the granularity of basic blocks, using a probabilistic approach. ALEA provides fine-grained energy profiling via statistical sampling, which overcomes the limitations of power sensing instruments. Compared to state-of-the-art energy measurement tools, ALEA provides finer granularity without sacrificing accuracy. ALEA achieves low overhead energy measurements with mean error rates between 1.4% and 3.5% in 14 sequential and parallel benchmarks tested on both Intel and ARM platforms. The sampling method caps execution time overhead at approximately 1%. ALEA is thus suitable for online energy monitoring and optimization. Finally, ALEA is a user-space tool with a portable, machine-independent sampling method. We demonstrate three use cases of ALEA, where we reduce the energy consumption of a k-means computational kernel by 37%, an ocean modeling code by 33%, and a ray tracing code by 6% compared to high-performance execution baselines, by varying the power optimization strategy between basic blocks.", "venue": "2015 International Conference on Parallel Architecture and Compilation (PACT)", "authors": ["Lev  Mukhanov", "Dimitrios S. Nikolopoulos", "Bronis R. de Supinski"], "year": 2015, "n_citations": 25}
{"id": 4618538, "s2_id": "8d36a6edf0c4a44109a9721629902837e9c7b22a", "title": "Randomized Assignment of Jobs to Servers in Heterogeneous Clusters of Shared Servers for Low Delay", "abstract": "We consider the job assignment problem in a multi-server system consisting of $N$ parallel processor sharing servers, categorized into $M$ ($\\ll N$) different types according to their processing capacity or speed. Jobs of random sizes arrive at the system according to a Poisson process with rate $N \\lambda$. Upon each arrival, a small number of servers from each type is sampled uniformly at random. The job is then assigned to one of the sampled servers based on a selection rule. We propose two schemes, each corresponding to a specific selection rule that aims at reducing the mean sojourn time of jobs in the system. \nWe first show that both methods achieve the maximal stability region. We then analyze the system operating under the proposed schemes as $N \\to \\infty$ which corresponds to the mean field. Our results show that asymptotic independence among servers holds even when $M$ is finite and exchangeability holds only within servers of the same type. We further establish the existence and uniqueness of stationary solution of the mean field and show that the tail distribution of server occupancy decays doubly exponentially for each server type. When the estimates of arrival rates are not available, the proposed schemes offer simpler alternatives to achieving lower mean sojourn time of jobs, as shown by our numerical studies.", "venue": "ArXiv", "authors": ["Arpan  Mukhopadhyay", "A.  Karthik", "Ravi  Mazumdar"], "year": 2015, "n_citations": 25}
{"id": 4623427, "s2_id": "31d2f11fe12cba6ff32ed162e702872b59cb63a2", "title": "Usage Scenarios for Byte-Addressable Persistent Memory in High-Performance and Data Intensive Computing", "abstract": "Byte-addressable persistent memory (B-APM) presents a new opportunity to bridge the performance gap between main memory and storage. In this paper, we present the usage scenarios for this new technology, based on the capabilities of Intel\u2019s DCPMM. We outline some of the basic performance characteristics of DCPMM, and explain how it can be configured and used to address the needs of memory and I/O intensive applications in the HPC (high-performance computing) and data intensive domains. Two decision trees are presented to advise on the configuration options for B-APM; their use is illustrated with two examples. We show that the flexibility of the technology has the potential to be truly disruptive, not only because of the performance improvements it can deliver, but also because it allows systems to cater for wider range of applications on homogeneous hardware.", "venue": "J. Comput. Sci. Technol.", "authors": ["Michele  Weiland", "Bernhard  Homoelle"], "year": 2021, "n_citations": 1}
{"id": 4624406, "s2_id": "31cc47d919588c06eefd1763a9c0bd4e5130d019", "title": "Coordinated dynamic spectrum management of LTE-U and Wi-Fi networks", "abstract": "This paper investigates the co-existence of Wi-Fi and LTE networks in emerging unlicensed frequency bands which are intended to accommodate multiple radio access technologies. Wi-Fi and LTE are the two most prominent wireless access technologies being deployed today, motivating further study of the inter-system interference arising in such shared spectrum scenarios as well as possible techniques for enabling improved co-existence. An analytical model for evaluating the baseline performance of co-existing Wi-Fi and LTE networks is developed and used to obtain baseline performance measures. The results show that both Wi-Fi and LTE networks cause significant interference to each other and that the degradation is dependent on a number of factors such as power levels and physical topology. The model-based results are partially validated via experimental evaluations using USRP-based SDR platforms on the ORBIT testbed. Further, inter-network coordination with logically centralized radio resource management across Wi-Fi and LTE systems is proposed as a possible solution for improved co-existence. Numerical results are presented showing significant gains in both Wi-Fi and LTE performance with the proposed inter-network coordination approach.", "venue": "2015 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)", "authors": ["Shweta  Sagari", "Samuel  Baysting", "Dola  Saha", "Ivan  Seskar", "Wade  Trappe", "Dipankar  Raychaudhuri"], "year": 2015, "n_citations": 135}
{"id": 4624908, "s2_id": "df0653c66997f7dcce84aac9fb89fc3e1f89f5ec", "title": "XSP: Across-Stack Profiling and Analysis of Machine Learning Models on GPUs", "abstract": "There has been a rapid proliferation of machine learning/deep learning (ML) models and wide adoption of them in many application domains. This has made profiling and characterization of ML model performance an increasingly pressing task for both hardware designers and system providers, as they would like to offer the best possible system to serve ML models with the target latency, throughput, cost, and energy requirements while maximizing resource utilization. Such an endeavor is challenging as the characteristics of an ML model depend on the interplay between the model, framework, system libraries, and the hardware (or the HW/SW stack). Existing profiling tools are disjoint, however, and only focus on profiling within a particular level of the stack, which limits the thoroughness and usefulness of the profiling results.This paper proposes XSP \u2014 an across-stack profiling design that gives a holistic and hierarchical view of ML model execution. XSP leverages distributed tracing to aggregate and correlate profile data from different sources. XSP introduces a leveled and iterative measurement approach that accurately captures the latencies at all levels of the HW/SW stack in spite of the profiling overhead. We couple the profiling design with an automated analysis pipeline to systematically analyze 65 state-of-the-art ML models. We demonstrate that XSP provides insights which would be difficult to discern otherwise.", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Cheng  Li", "Abdul  Dakkak", "Jinjun  Xiong", "Wei  Wei", "Lingjie  Xu", "Wen-mei  Hwu"], "year": 2020, "n_citations": 13}
{"id": 4626875, "s2_id": "2df127d38c607f6dc01ca53907d538fc0f10861a", "title": "PMEvo: portable inference of port mappings for out-of-order processors by evolutionary optimization", "abstract": "Achieving peak performance in a computer system requires optimizations in every layer of the system, be it hardware or software. A detailed understanding of the underlying hardware, and especially the processor, is crucial to optimize software. One key criterion for the performance of a processor is its ability to exploit instruction-level parallelism. This ability is determined by the port mapping of the processor, which describes the execution units of the processor for each instruction. Processor manufacturers usually do not share the port mappings of their microarchitectures. While approaches to automatically infer port mappings from experiments exist, they are based on processor-specific hardware performance counters that are not available on every platform. We present PMEvo, a framework to automatically infer port mappings solely based on the measurement of the execution time of short instruction sequences. PMEvo uses an evolutionary algorithm that evaluates the fitness of candidate mappings with an analytical throughput model formulated as a linear program. Our prototype implementation infers a port mapping for Intel's Skylake architecture that predicts measured instruction throughput with an accuracy that is competitive to existing work. Furthermore, it finds port mappings for AMD's Zen+ architecture and the ARM Cortex-A72 architecture, which are out of scope of existing techniques.", "venue": "PLDI", "authors": ["Fabian  Ritter", "Sebastian  Hack"], "year": 2020, "n_citations": 5}
{"id": 4634153, "s2_id": "b07b19def0b0ce61314e35c7b1c9ac19fd8b17ec", "title": "Delay-Optimal Data Forwarding in Vehicular Sensor Networks", "abstract": "The vehicular sensor network (VSN) is emerging as a new solution for monitoring urban environments such as intelligent transportation systems and air pollution. One of the crucial factors that determine the service quality of urban monitoring applications is the delivery delay of sensing data packets in the VSN. In this paper, we study the problem of routing data packets with minimum delay in the VSN by exploiting 1) vehicle traffic statistics, 2) anycast routing, and 3) knowledge of future trajectories of vehicles such as busses. We first introduce a novel road network graph model that incorporates the three factors into the routing metric. We then characterize the packet delay on each edge as a function of the vehicle density, speed, and the length of the edge. Based on the network model and delay function, we formulate the packet routing problem as a Markov decision process (MDP) and develop an optimal routing policy by solving the MDP. Evaluations using real vehicle traces in a city show that our routing policy significantly improves the delay performance compared with existing routing protocols. Specifically, optimal VSN data forwarding (OVDF) yields, on average, 96% better delivery ratio and 72% less delivery delay than existing algorithms in some areas distant from destinations.", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Okyoung  Choi", "Seokhyun  Kim", "Jaeseong  Jeong", "Hyang-Won  Lee", "Song  Chong"], "year": 2016, "n_citations": 42}
{"id": 4638035, "s2_id": "8c446477d6368965cda8bd2512871fc0da16d558", "title": "On Optimization of Network-coded Scalable Multimedia Service Multicasting", "abstract": "In the near future, the delivery of multimedia multicast services over next-generation networks is likely to become one of the main pillars of future cellular networks. In this extended abstract, we address the issue of efficiently multicasting layered video services by defining a novel optimization paradigm that is based on an Unequal Error Protection implementation of Random Linear Network Coding, and aims to ensure target service coverages by using a limited amount of radio resources.", "venue": "ArXiv", "authors": ["Andrea  Tassi", "Ioannis  Chatzigeorgiou", "Dejan  Vukobratovic"], "year": 2015, "n_citations": 0}
{"id": 4639563, "s2_id": "6f5b3175340d7ad036a8297a6d3ade6f15897687", "title": "Function delivery network: Extending serverless computing for heterogeneous platforms", "abstract": "Serverless computing has rapidly grown following the launch of Amazon's Lambda platform. Function\u2010as\u2010a\u2010Service (FaaS) a key enabler of serverless computing allows an application to be decomposed into simple, standalone functions that are executed on a FaaS platform. The FaaS platform is responsible for deploying and facilitating resources to the functions. Several of today's cloud applications spread over heterogeneous connected computing resources and are highly dynamic in their structure and resource requirements. However, FaaS platforms are limited to homogeneous clusters and homogeneous functions and do not account for the data access behavior of functions before scheduling. We introduce an extension of FaaS to heterogeneous clusters and to support heterogeneous functions through a network of distributed heterogeneous target platforms called Function Delivery Network (FDN). A target platform is a combination of a cluster of homogeneous nodes and a FaaS platform on top of it. FDN provides Function\u2010Delivery\u2010as\u2010a\u2010Service (FDaaS), delivering the function to the right target platform. We showcase the opportunities such as varied target platform's characteristics, possibility of collaborative execution between multiple target platforms, and localization of data that the FDN offers in fulfilling two objectives: Service Level Objective (SLO) requirements and energy efficiency when scheduling functions by evaluating over five distributed target platforms using the FDNInspector, a tool developed by us for benchmarking distributed target platforms. Scheduling functions on an edge target platform in our evaluation reduced the overall energy consumption by 17\u00d7 without violating the SLO requirements in comparison to scheduling on a high\u2010end target platform.", "venue": "Softw. Pract. Exp.", "authors": ["Anshul  Jindal", "Michael  Gerndt", "Mohak  Chadha", "Vladimir  Podolskiy", "Pengfei  Chen"], "year": 2021, "n_citations": 10}
{"id": 4639841, "s2_id": "d724dcb4ab67971aaa10fe8c3c3f347ff445cc7d", "title": "DTLS Performance - How Expensive is Security?", "abstract": "Secure communication is an integral feature of many Internet services. The widely deployed TLS protects reliable transport protocols. DTLS extends TLS security services to protocols relying on plain UDP packet transport, such as VoIP or IoT applications. In this paper, we construct a model to determine the performance of generic DTLS-enabled applications. Our model considers basic network characteristics, e.g., number of connections, and the chosen security parameters, e.g., the encryption algorithm in use. Measurements are presented demonstrating the applicability of our model. These experiments are performed using a high-performance DTLS-enabled VPN gateway built on top of the well-established libraries DPDK and OpenSSL. This VPN solution represents the most essential parts of DTLS, creating a DTLS performance baseline. Using this baseline the model can be extended to predict even more complex DTLS protocols besides the measured VPN. Code and measured data used in this paper are publicly available at this https URL and this https URL.", "venue": "ArXiv", "authors": ["Sebastian  Gallenm\u00fcller", "Dominik  Sch\u00f6ffmann", "Dominik  Scholz", "Fabien  Geyer", "Georg  Carle"], "year": 2019, "n_citations": 2}
{"id": 4640471, "s2_id": "7183c79e72f319815b3373360bbd231237d534bb", "title": "Traffic Rate Network Tomography with Higher-Order Cumulants", "abstract": "Network tomography aims at estimating source-destination traffic rates from link traffic measurements. This inverse problem was formulated by Vardi in 1996 for Poisson traffic over networks operating under deterministic as well as random routing regimes. In this paper we expand Vardi's second-order moment matching rate estimation approach to higher-order cumulant matching with the goal of increasing the column rank of the mapping and consequently improving the rate estimation accuracy. We develop a systematic set of linear cumulant matching equations and express them compactly in terms of the Khatri-Rao product. Both least squares estimation and iterative minimum I-divergence estimation are considered. We develop an upper bound on the mean squared error (MSE) in least squares rate estimation from empirical cumulants. We demonstrate for the NSFnet that supplementing Vardi's approach with third-order empirical cumulant reduces its averaged normalized MSE relative to the theoretical minimum of the second-order moment matching approach by about 12%-18%. This minimum MSE is obtained when Vardi's second-order moment matching approach is based on the theoretical rather than the empirical moments.", "venue": "ArXiv", "authors": ["Hanoch  Lev-Ari", "Yariv  Ephraim", "Brian L. Mark"], "year": 2020, "n_citations": 1}
{"id": 4640619, "s2_id": "0be2921f644274f0d983a0f1ad52a78846373f3b", "title": "Experiences with advanced CORBA services", "abstract": "The Common Object Request Broker Architecture (CORBA) is successfully used in many control systems (CS) for data transfer and device modeling. Communication rates below 1 millisecond, high reliability, scalability, language independence and other features make it very attractive. For common types of applications like error logging, alarm messaging or slow monitoring, one can benefit from standard CORBA services that are implemented by third parties and save tremendous amount of developing time. We have started using few CORBA services on our previous CORBA-based control system for the light source ANKA [1] and use now several CORBA services for the ALMA Common Software (ACS) [2], the core of the control system of the Atacama Large Millimeter Array. Our experiences with the interface repository (IFR), the implementation repository, the naming service, the property service, telecom log service and the notify service from different vendors are presented. Performance and scalability benchmarks have been performed.", "venue": "ArXiv", "authors": ["G.  Milcinski", "M.  Plesko", "M.  Sekoranja"], "year": 2001, "n_citations": 1}
{"id": 4643534, "s2_id": "4f8695713a98f2fb163d490b2e41eb5df8bb52d9", "title": "On the Computation of the Higher-Order Statistics of the Channel Capacity over Generalized Fading Channels", "abstract": "The higher-order statistics (HOS) of the channel capacity \u03bcn = E[ logn(1+\u03b3end)], where n \u2208 N denotes the order of the statistics, has received relatively little attention in the literature, due in part to the intractability of its analysis. In this letter, we propose a novel and unified analysis, which is based on the moment generating function (MGF) technique, to exactly compute the HOS of the channel capacity. More precisely, our mathematical formalism can be readily applied to maximal-ratio-combining (MRC) receivers operating in generalized fading environments. The mathematical formalism is illustrated by some numerical examples focusing on the correlated generalized fading environments.", "venue": "IEEE Wireless Communications Letters", "authors": ["Ferkan  Yilmaz", "Mohamed-Slim  Alouini"], "year": 2012, "n_citations": 15}
{"id": 4643632, "s2_id": "124798a62b96122a0e503a6ac9a7e1cbe1c8de74", "title": "Porting a sparse linear algebra math library to Intel GPUs", "abstract": "With the announcement that the Aurora Supercomputer will be composed of general purpose Intel CPUs complemented by discrete high performance Intel GPUs, and the deployment of the oneAPI ecosystem, Intel has committed to enter the arena of discrete high performance GPUs. A central requirement for the scientific computing community is the availability of production-ready software stacks and a glimpse of the performance they can expect to see on Intel high performance GPUs. In this paper, we present the first platform-portable open source math library supporting Intel GPUs via the DPC++ programming environment. We also benchmark some of the developed sparse linear algebra functionality on different Intel GPUs to assess the efficiency of the DPC++ programming ecosystem to translate raw performance into application performance. Aside from quantifying the efficiency within the hardware-specific roofline model, we also compare against routines providing the same functionality that ship with Intel\u2019s oneMKL vendor library.", "venue": "ArXiv", "authors": ["Yuhsiang M. Tsai", "Terry  Cojean", "Hartwig  Anzt"], "year": 2021, "n_citations": 1}
{"id": 4644305, "s2_id": "ae464dc54a594de682fddd59479736b1e65bbf52", "title": "TENET: A Framework for Modeling Tensor Dataflow Based on Relation-centric Notation", "abstract": "Accelerating tensor applications on spatial architectures provides high performance and energy-efficiency, but requires accurate performance models for evaluating various dataflow alternatives. Such modeling relies on the notation of tensor dataflow and the formulation of performance metrics. Recent proposed compute-centric and data-centric notations describe the dataflow using imperative directives. However, these two notations are less expressive and thus lead to limited optimization opportunities and inaccurate performance models.In this paper, we propose a framework TENET that models hardware dataflow of tensor applications. We start by introducing a relation-centric notation, which formally describes the hardware dataflow for tensor computation. The relation-centric notation specifies the hardware dataflow, PE interconnection, and data assignment in a uniform manner using relations. The relation-centric notation is more expressive than the compute-centric and data-centric notations by using more sophisticated affine transformations. Another advantage of relation-centric notation is that it inherently supports accurate metrics estimation, including data reuse, bandwidth, latency, and energy. TENET computes each performance metric by counting the relations using integer set structures and operators. Overall, TENET achieves 37.4% and 51.4% latency reduction for CONV and GEMM kernels compared with the state-of-the-art data-centric notation by identifying more sophisticated hardware dataflows.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Liqiang  Lu", "Naiqing  Guan", "Yuyue  Wang", "Liancheng  Jia", "Zizhang  Luo", "Jieming  Yin", "Jason  Cong", "Yun  Liang"], "year": 2021, "n_citations": 5}
{"id": 4648080, "s2_id": "9153d009b6be6ed2bf26353f95d38423925b2484", "title": "Broadcasting in Prefix Space: P2P Data Dissemination with Predictable Performance", "abstract": "A broadcast mode may augment peer-to-peer overlay networks with an efficient, scalable data replication function, but may also give rise to a virtual link layer in VPN-type solutions. We introduce a generic, simple broadcasting mechanism that operates in the prefix space of distributed hash tables without signaling. This paper concentrates on the performance analysis of the prefix flooding scheme. Starting from simple models of recursive $k$-ary trees, we analytically derive distributions of hop counts and the replication load. Further on, extensive simulation results are presented based on an implementation within the OverSim framework. Comparisons are drawn to Scribe, taken as a general reference model for group communication according to the shared, rendezvous-point-centered distribution paradigm. The prefix flooding scheme thereby confirmed its widely predictable performance and consistently outperformed Scribe in all metrics. Reverse path selection in overlays is identified as a major cause of performance degradation.", "venue": "2009 Fourth International Conference on Internet and Web Applications and Services", "authors": ["Matthias  W\u00e4hlisch", "Thomas C. Schmidt", "Georg  Wittenburg"], "year": 2009, "n_citations": 13}
{"id": 4654974, "s2_id": "64931f9c69f94c521af0b0c199d2e764796bef3c", "title": "DISCO Verification: Division of Input Space into COnvex polytopes for neural network verification", "abstract": "The impressive results of modern neural networks partly come from their non linear behaviour. Unfortunately, this property makes it very difficult to apply formal verification tools, even if we restrict ourselves to networks with a piecewise linear structure. However, such networks yields subregions that are linear and thus simpler to analyse independently. In this paper, we propose a method to simplify the verification problem by operating a partitionning into multiple linear subproblems. To evaluate the feasibility of such an approach, we perform an empirical analysis of neural networks to estimate the number of linear regions, and compare them to the bounds currently known. We also present the impact of a technique aiming at reducing the number of linear regions during training.", "venue": "ArXiv", "authors": ["Julien  Girard-Satabin", "Aymeric  Varasse", "Marc  Schoenauer", "Guillaume  Charpiat", "Zakaria  Chihani"], "year": 2021, "n_citations": 1}
{"id": 4655637, "s2_id": "4ad53420d0c63213cc5adcc5aa19b6d1c12fa112", "title": "A Survey of Miss-Ratio Curve Construction Techniques", "abstract": "Miss-ratio curve (MRC), or equivalently hit-ratio curve (HRC), construction techniques have recently gathered the attention of many researchers. Recent advancements have allowed for approximating these curves in constant time, allowing for online working-set-size (WSS) measurement. Techniques span the algorithmic design paradigm from classic dynamic programming to artificial intelligence inspired techniques. Our survey produces broad classification of the current techniques primarily based on \\emph{what} locality metric is being recorded and \\emph{how} that metric is stored for processing. \nApplications of theses curves span from dynamic cache partitioning in the processor, to improving block allocation at the operating system level. Our survey will give an overview of the historical, exact MRC construction methods, and compare them with the state-of-the-art methods present in today's literature. In addition, we will show where there are still open areas of research and remain excited to see what this domain can produce with a strong theoretical background.", "venue": "ArXiv", "authors": ["Daniel  Byrne"], "year": 2018, "n_citations": 1}
{"id": 4658296, "s2_id": "d483d5720e5277744b896a8f61a6fa28edd015bd", "title": "Comprehensive Resource Measurement and Analysis for HPC Systems with TACC_Stats", "abstract": "High-performance computing (HPC) systems are a complex combination of software, processors, memory, networks, and storage systems characterized by frequent disruptive technological advances. Anomalous behavior has to be manually diagnosed and remedied with incomplete and sparse data. It also has been effort-intensive for users to assess the effectiveness with which they are using the available resources. The data available for system level analyses appear from multiple sources and in disparate formats (from Linux \"sysstat\" and accounting to scheduler/kernel logs). Sysstat does not resolve its measurements by job so that job-oriented analyses require individual measurements. There are many user-oriented performance instrumentation and profiling tools but they require extensive system knowledge, code changes and recompilation, and thus are not widely used. To address this issue, we develop TACC_Stats, a job-oriented and logically structured version of the conventional Linux \"sysstat/sar\" system-wide performance monitor. We use TACC_Stats-collected data from a supercomputer \"Ranger\" to demonstrate its effectiveness in two case studies.", "venue": "ArXiv", "authors": ["Charng-Da  Lu"], "year": 2013, "n_citations": 0}
{"id": 4659283, "s2_id": "46bb5162da1ad9b3591a9e056550135e7a50bc2b", "title": "Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect", "abstract": "High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink's topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Ang  Li", "Shuaiwen  Song", "Jieyang  Chen", "Jiajia  Li", "Xu  Liu", "Nathan R. Tallent", "Kevin J. Barker"], "year": 2020, "n_citations": 69}
{"id": 4663926, "s2_id": "0e5d529befc3ca2e3e3371a0c39dc05731c1d5b7", "title": "Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking", "abstract": "Every year, novel NVIDIA GPU designs are introduced. This rapid architectural and technological progression, coupled with a reluctance by manufacturers to disclose low-level details, makes it difficult for even the most proficient GPU software designers to remain up-to-date with the technological advances at a microarchitectural level. To address this dearth of public, microarchitectural-level information on the novel NVIDIA GPUs, independent researchers have resorted to microbenchmarks-based dissection and discovery. This has led to a prolific line of publications that shed light on instruction encoding, and memory hierarchy's geometry and features at each level. Namely, research that describes the performance and behavior of the Kepler, Maxwell and Pascal architectures. In this technical report, we continue this line of research by presenting the microarchitectural details of the NVIDIA Volta architecture, discovered through microbenchmarks and instruction set disassembly. Additionally, we compare quantitatively our Volta findings against its predecessors, Kepler, Maxwell and Pascal.", "venue": "ArXiv", "authors": ["Zhe  Jia", "Marco  Maggioni", "Benjamin  Staiger", "Daniele Paolo Scarpazza"], "year": 2018, "n_citations": 161}
{"id": 4666944, "s2_id": "c06767fdb048485dac1e90d94b94ed8376370b93", "title": "Loop Tiling in Large-Scale Stencil Codes at Run-Time with OPS", "abstract": "The key common bottleneck in most stencil codes is data movement, and prior research has shown that improving data locality through optimisations that optimise across loops do particularly well. However, in many large PDE applications it is not possible to apply such optimisations through compilers because there are many options, execution paths and data per grid point, many dependent on run-time parameters, and the code is distributed across different compilation units. In this paper, we adapt the data locality improving optimisation called tiling for use in large OPS applications both in shared-memory and distributed-memory systems, relying on run-time analysis and delayed execution. We evaluate our approach on a number of applications, observing speedups of 2<inline-formula> <tex-math notation=\"LaTeX\">$\\times$</tex-math><alternatives><inline-graphic xlink:href=\"reguly-ieq1-2778161.gif\"/> </alternatives></inline-formula> on the Cloverleaf 2D/3D proxy applications, which contain 83(2D)/141(3D) loops, <inline-formula><tex-math notation=\"LaTeX\">$3.5\\times$</tex-math><alternatives> <inline-graphic xlink:href=\"reguly-ieq2-2778161.gif\"/></alternatives></inline-formula> on the linear solver TeaLeaf, and <inline-formula><tex-math notation=\"LaTeX\">$1.7\\times$</tex-math><alternatives> <inline-graphic xlink:href=\"reguly-ieq3-2778161.gif\"/></alternatives></inline-formula> on the compressible Navier-Stokes solver OpenSBLI. We demonstrate strong and weak scalability on up to 4608 cores of CINECA's Marconi supercomputer. We also evaluate our algorithms on Intel's Knights Landing, demonstrating maintained throughput as the problem size grows beyond 16GB, and we do scaling studies up to 8704 cores. The approach is generally applicable to any stencil DSL that provides per loop nest data access information.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Istv\u00e1n Z. Reguly", "Gihan R. Mudalige", "Michael B. Giles"], "year": 2018, "n_citations": 27}
{"id": 4668192, "s2_id": "e8965640f4f55931afd9453739245e049d2ba6fe", "title": "On the equivalence of holding cost and response time for evaluating performance of queues", "abstract": "This self-contained discussion relates the long-run average holding cost per unit time to the long-run average response time per customer in a G/G/1 queue with no assumption made on the order of service. The only restriction established is that the system be ergodic. This is achieved using standard queuing theory. The practical relevance of such a result is discussed in the context of simulation output analysis as well as through an application to formulating a Markov Decision Process that minimises long-run average response time per customer.", "venue": "ArXiv", "authors": ["Dylan  Solms"], "year": 2021, "n_citations": 0}
{"id": 4672207, "s2_id": "efc2cd3f4a6b837ce1ff3fb698f62b4448e0a29b", "title": "The missing piece syndrome in peer-to-peer communication", "abstract": "Typical protocols for peer-to-peer file sharing over the Internet divide files to be shared into pieces. New peers strive to obtain a complete collection of pieces from other peers and from a seed. In this paper we identify a problem that can occur if the seeding rate is not large enough. The problem is that, even if the statistics of the system are symmetric in the pieces, there can be symmetry breaking, with one piece becoming very rare. If peers depart after obtaining a complete collection, they can tend to leave before helping other peers receive the rare piece. 1", "venue": "2010 IEEE International Symposium on Information Theory", "authors": ["Bruce E. Hajek", "Ji  Zhu"], "year": 2010, "n_citations": 46}
{"id": 4674817, "s2_id": "921dbe9496130e566e14ca90d0d6879a63548282", "title": "Automated and Distributed Statistical Analysis of Economic Agent-Based Models", "abstract": "We propose a novel approach to the statistical analysis of simulation models and, especially, agent-based models (ABMs). Our main goal is to provide a fully automated and model-independent tool-kit to inspect simulations and perform counter-factual analysis. Our approach: (i) is easy-to-use by the modeller, (ii) improves reproducibility of results, (iii) optimizes running time given the modeller\u2019s machine, (iv) automatically chooses the number of required simulations and simulation steps to reach user-specified statistical confidence, and (v) automatically performs a variety of statistical tests. In particular, our framework is designed to distinguish the transient dynamics of the model from its steady state behaviour (if any), estimate properties of the model in both \u201cphases\u201d, and provide indications on the ergodic (or non-ergodic) nature of the simulated processes \u2013 which, in turns allows one to gauge the reliability of a steady state analysis. Estimates are equipped with statistical guarantees, allowing for robust comparisons across computational experiments. To demonstrate the effectiveness of our approach, we apply it to two models from the literature: a large scale macro-financial ABM and a small scale prediction market model. Compared to prior analyses of these models, we obtain new insights and we are able to identify and fix some erroneous conclusions.", "venue": "ArXiv", "authors": ["Andrea  Vandin", "Daniele  Giachini", "Francesco  Lamperti", "Francesca  Chiaromonte"], "year": 2021, "n_citations": 2}
{"id": 4675189, "s2_id": "5ee63bf66633a4cf0af64e04c7291a56febecbfe", "title": "EVM as generic QoS trigger for heterogeneous wieless overlay network", "abstract": "Fourth Generation (4G) Wireless System will integrate heterogeneous wireless overlay systems i.e. interworking of WLAN/ GSM/ CDMA/ WiMAX/ LTE/ etc with guaranteed Quality of Service (QoS) and Experience (QoE).QoS(E) vary from network to network and is application sensitive. User needs an optimal mobility solution while roaming in Overlaid wireless environment i.e. user could seamlessly transfer his session/ call to a best available network bearing guaranteed Quality of Experience. And If this Seamless transfer of session is executed between two networks having different access standards then it is called Vertical Handover (VHO). Contemporary VHO decision algorithms are based on generic QoS metrics viz. SNR, bandwidth, jitter, BER and delay. In this paper, Error Vector Magnitude (EVM) is proposed to be a generic QoS trigger for VHO execution. EVM is defined as the deviation of inphase/ quadrature (I/Q) values from ideal signal states and thus provides a measure of signal quality. In 4G Interoperable environment, OFDM is the leading Modulation scheme (more prone to multi-path fading). EVM (modulation error) properly characterises the wireless link/ channel for accurate VHO decision. EVM depends on the inherent transmission impairments viz. frequency offset, phase noise, non-linearimpairment, skewness etc. for a given wireless link. Paper provides an insight to the analytical aspect of EVM & measures EVM (%) for key management subframes like association/re-association/disassociation/ probe request/response frames. EVM relation is explored for different possible NAV-Network Allocation Vectors (frame duration). Finally EVM is compared with SNR, BER and investigation concludes EVM as a promising QoS trigger for OFDM based emerging wireless standards.", "venue": "ArXiv", "authors": ["Rajender  Kumar", "Brahmjit  Singh"], "year": 2010, "n_citations": 5}
{"id": 4676614, "s2_id": "9f69c00a542d52e9ba5dce4b8598bca1611bcd1f", "title": "Stability Analysis of a Quantum Network with Max-Weight Scheduling", "abstract": "We study a quantum network that distributes entangled quantum states to multiple sets of users that are connected to the network. Each user is connected to a switch of the network via a link. All the links of the network generate bipartite Bell-state entangled states in each time-slot with certain probabilities, and each end node stores one qubit of the entanglement generated by the link. To create shared entanglements for a set of users, measurement operations are performed on qubits of link-level entanglements on a set of related links, and these operations are probabilistic in nature and are successful with certain probabilities. Requests arrive to the system seeking shared entanglements for different sets of users. Each request is for the creation of shared entanglements for a fixed set of users using link-level entanglements on a fixed set of links. Requests are processed according to First-Come-First-Served service discipline and unserved requests are stored in buffers. Once a request is selected for service, measurement operations are performed on qubits of link-level entanglements on related links to create a shared entanglement. For given set of request arrival rates and link-level entanglement generation rates, we obtain necessary conditions for the stability of queues of requests. In each time-slot, the scheduler has to schedule entanglement swapping operations for different sets of users to stabilize the network. Next, we propose a Max-Weight scheduling policy and show that this policy stabilizes the network for all feasible arrival rates. We also provide numerical results to support our analysis. The analysis of a single quantum switch that creates multipartite entanglements for different sets of users is a special case of our work.", "venue": "ArXiv", "authors": ["Thirupathaiah  Vasantam", "Don  Towsley"], "year": 2021, "n_citations": 1}
{"id": 4679539, "s2_id": "f2786944293162926243b6e318edf89aaa655146", "title": "Stress-SGX: Load and Stress your Enclaves for Fun and Profit", "abstract": "T hi s is a po st -p ee rre vi ew ,p re -c op ye di t ve rs io n of an ar ti cl e pu bl is he d in \u201cN et w or ke d Sy st em s\u201d .T he fin al au th en ti ca te d ve rs io n is av ai la bl e on lin e at ht tp s: // do i.o rg /1 0. 10 07 /9 78 -3 -0 30 -0 55 29 -5 _ 24 . The latest generation of Intel processors supports Software Guard Extensions (SGX), a set of instructions that implements a Trusted Execution Environment (TEE) right inside the CPU, by means of socalled enclaves. This paper presents Stress-SGX, an easy-to-use stresstest tool to evaluate the performance of SGX-enabled nodes. We build on top of the popular Stress-ng tool, while only keeping the workload injectors (stressors) that are meaningful in the SGX context. We report on several insights and lessons learned about porting legacy code to run inside an SGX enclave, as well as the limitations introduced by this process. Finally, we use Stress-SGX to conduct a study comparing the performance of different SGX-enabled machines.", "venue": "ArXiv", "authors": ["S\u00e9bastien  Vaucher", "Valerio  Schiavoni", "Pascal  Felber"], "year": 2019, "n_citations": 4}
{"id": 4686528, "s2_id": "fce55d6e12a835b7fb0a6ae096d9a12d7f9c3558", "title": "On Non-Markovian Performance Models", "abstract": "We present an approach that can be useful when the network or system performance is described by a model that is not Markovian. Although most performance models are based on Markov chains or Markov processes, in some cases the Markov property does not hold. This can occur, for example, when the system exhibits long range dependencies. For such situations, and other non-Markovian cases, our method may provide useful help.", "venue": "ArXiv", "authors": ["Andras  Farago"], "year": 2020, "n_citations": 0}
{"id": 4686953, "s2_id": "85955c2f36ec71475373cf33636f071135657e74", "title": "Efficiently Reclaiming Space in a Log Structured Store", "abstract": "Modern storage devices do not support update-in-place. Rather, flash and shingled disks, are forms of log structured stores. Such a store writes a number of diverse and non-contiguous logical pages into a unit of contiguous storage we call a segment instead of using a write I/O to update each page in place. The result is that pages need to be relocated and remapped on every write. Log structuring was invented for and used initially to improve performance in file systems. Segments need to be garbage collected, but can be only when they no longer house any current pages. A process of \"cleaning\" produces an empty segment by, when necessary, moving (re-writing) still current pages of the segment to another location. Cleaning effectiveness has a major impact on the performance of modern storage devices, and for flash, impacts the rate of wear and hence the lifetime of the device. We analyze cleaning performance and introduce a cleaning strategy that uses a new way to prioritize the order in which segments are cleaned. Our cleaning strategy approximates an \"optimal cleaning strategy\". Simulation studies confirm the results of the analysis. This strategy is a significant improvement over previous cleaning strategies.", "venue": "2021 IEEE 37th International Conference on Data Engineering (ICDE)", "authors": ["David  Lomet", "Chen  Luo"], "year": 2021, "n_citations": 1}
{"id": 4687547, "s2_id": "dda3f2975435f885aed0ba1714910617d5e2a349", "title": "Automated Inference System for End-To-End Diagnosis of Network Performance Issues in Client-Terminal Devices", "abstract": "Traditional network diagnosis methods of Client-Terminal Device (CTD) problems tend to be laborintensive, time consuming, and contribute to increased customer dissatisfaction. In this paper, we propose an automated solution for rapidly diagnose the root causes of network performance issues in CTD. Based on a new intelligent inference technique, we create the Intelligent Automated Client Diagnostic (IACD) system, which only relies on collection of Transmission Control Protocol (TCP) packet traces. Using soft-margin Support Vector Machine (SVM) classifiers, the system (i) distinguishes link problems from client problems and (ii) identifies characteristics unique to the specific fault to report the root cause. The modular design of the system enables support for new access link and fault types. Experimental evaluation demonstrated the capability of the IACD system to distinguish between faulty and healthy links and to diagnose the client faults with 98% accuracy. The system can perform fault diagnosis independent of the user\u2019s specific TCP implementation, enabling diagnosis of diverse range of client devices.", "venue": "ArXiv", "authors": ["Chathuranga  Widanapathirana", "Y. Ahmet Sekercioglu", "Milosh V. Ivanovich", "Paul G. Fitzpatrick", "Jonathan C. Li"], "year": 2012, "n_citations": 6}
{"id": 4697622, "s2_id": "e729f98c7c813acf1fd4fbb3a8168df687431965", "title": "Is Your Load Generator Launching Web Requests in Bunches?", "abstract": "One problem with load test quality, almost always overlooked, is the potential for the load generator's user thread pool to sync up and dispatch queries in bunches rather than independently from each other like real users initiate their requests. A spiky launch pattern misrepresents workload flow as well as yields erroneous application response time statistics. This paper describes what a real user request timing pattern looks like, illustrates how to identify it in the load generation environment, and exercises a free downloadable tool which measures how well the load generator is mimicking the timing pattern of real web user requests.", "venue": "ArXiv", "authors": ["James F. Brady"], "year": 2018, "n_citations": 0}
{"id": 4700458, "s2_id": "828012662ba3917e3c7fc054a59a516d72766ab2", "title": "Modelling Computational Resources for Next Generation Sequencing Bioinformatics Analysis of 16S rRNA Samples", "abstract": "In the rapidly evolving domain of next generation sequencing and bioinformatics analysis, data generation is one aspect that is increasing at a concomitant rate. The burden associated with processing large amounts of sequencing data has emphasised the need to allocate sufficient computing resources to complete analyses in the shortest possible time with manageable and predictable costs. A novel method for predicting time to completion for a popular bioinformatics software (QIIME), was developed using key variables characteristic of the input data assumed to impact processing time. Multiple Linear Regression models were developed to determine run time for two denoising algorithms and a general bioinformatics pipeline. The models were able to accurately predict clock time for denoising sequences from a naturally assembled community dataset, but not an artificial community. Speedup and efficiency tests for AmpliconNoise also highlighted that caution was needed when allocating resources for parallel processing of data. Accurate modelling of computational processing time using easily measurable predictors can assist NGS analysts in determining resource requirements for bioinformatics software and pipelines. Whilst demonstrated on a specific group of scripts, the methodology can be extended to encompass other packages running on multiple architectures, either in parallel or sequentially.", "venue": "ArXiv", "authors": ["Matthew J. Wade", "Thomas P. Curtis", "Russell J. Davenport"], "year": 2015, "n_citations": 1}
{"id": 4700462, "s2_id": "8865096a8461ec2492cfb984a2401554f485e7cb", "title": "An Algebra of Synchronous Scheduling Interfaces", "abstract": "In this paper we propose an algebra of synchronous scheduling interfaces which combines the expressiveness of Boolean algebra for logical and functional behaviour with the min-max-plus arithmetic for quantifying the non-functional aspects of synchronous interfaces. The interface theory arises from a realisability interpretation of intuitionistic modal logic (also known as Curry-Howard-Isomorphism or propositions-as-types principle). The resulting algebra of interface types aims to provide a general setting for specifying type-directed and compositional analyses of worst-case scheduling bounds. It covers synchronous control flow under concurrent, multi-processing or multi-threading execution and permits precise statements about exactness and coverage of the analyses supporting a variety of abstractions. The paper illustrates the expressiveness of the algebra by way of some examples taken from network flow problems, shortest-path, task scheduling and worst-case reaction times in synchronous programming.", "venue": "FIT", "authors": ["Michael  Mendler"], "year": 2010, "n_citations": 1}
{"id": 4702223, "s2_id": "cd55bd6836f8268784bb1bb2d94d31a3d73189df", "title": "On the performance overhead tradeoff of distributed principal component analysis via data partitioning", "abstract": "Principal component analysis (PCA) is not only a fundamental dimension reduction method, but is also a widely used network anomaly detection technique. Traditionally, PCA is performed in a centralized manner, which has poor scalability for large distributed systems, on account of the large network bandwidth cost required to gather the distributed state at a fusion center. Consequently, several recent works have proposed various distributed PCA algorithms aiming to reduce the communication overhead incurred by PCA without losing its inferential power. This paper evaluates the tradeoff between communication cost and solution quality of two distributed PCA algorithms on a real domain name system (DNS) query dataset from a large network. We also apply the distributed PCA algorithm in the area of network anomaly detection and demonstrate that the detection accuracy of both distributed PCA-based methods has little degradation in quality, yet achieves significant savings in communication bandwidth.", "venue": "2016 Annual Conference on Information Science and Systems (CISS)", "authors": ["Ni  An", "Steven P. Weber"], "year": 2016, "n_citations": 2}
{"id": 4704461, "s2_id": "573087ff349afefc3525aaf352331c57d463cab7", "title": "Methodologies of Link-Level Simulator and System-Level Simulator for C-V2X Communication", "abstract": "At the time of the development, standardization, and further improvement are vital to the modern cellular systems such as the next generation wireless communication (5G). Simulations are essential to test and optimize algorithms and procedures prior to their implementation process of the equipment manufactures. In order to evaluate system performance at different levels, accurate simulations of simple setups, as well as simulations of more complex systems via abstracted models are necessary. In this work, two new simulators for the sidelink Cooperative- Vehicle-to-Everything (C-V2X) communication have been implemented and carried out on both the physical layer (Link-Level (LL)) and network layer (System-Level (SL)). Detailed methodologies of the LL and SL simulators for C-V2X communication have been illustrated. In the LL simulator, we get the mapping curves of BLER and Signal-to-Noise-Ratio (SNR), which are used as a baseline for measuring the performance of the LL simulation. In addition, these mapping curves are used as the important Link-to-System (L2S) interfaces. The SL simulator is utilized for measuring the performance of cell networking and simulating large networks comprising of multiple eNBs and UEs. Finally, the simulation results of both simulators for C-V2X communication are presented, which shows that different objectives can be met by using LL or SL simulations types.", "venue": "2019 IEEE 2nd International Conference on Electronics and Communication Engineering (ICECE)", "authors": ["Donglin  Wang", "Raja  Sattiraju", "Qiu  Anjie", "Sanket  Partani", "Hans D. Schotten"], "year": 2019, "n_citations": 1}
{"id": 4704537, "s2_id": "8f97fed095571f31c865373e54d3e0c10a4468b7", "title": "A Simple Performance Analysis of a Core Node in an Optical Burst Switched Network", "abstract": "This paper has been withdrawn", "venue": "ArXiv", "authors": ["Mohamed H. S. Morsy", "Mohammad Y. S. Sowailem", "Hossam M. H. Shalaby"], "year": 2008, "n_citations": 2}
{"id": 4708680, "s2_id": "f189ec045225127febb113a6ef011941825c9376", "title": "Performance Analysis of Linear-Equality-Constrained Least-Squares Estimation", "abstract": "We analyze the performance of a linear-equality-constrained least-squares (CLS) algorithm and its relaxed version, called rCLS, that is obtained via the method of weighting. The rCLS algorithm solves an unconstrained least-squares problem that is augmented by incorporating a weighted form of the linear constraints. As a result, unlike the CLS algorithm, the rCLS algorithm is amenable to our approach to performance analysis presented here, which is akin to the energy-conservation-based methodology. Therefore, we initially inspect the convergence properties and evaluate the precision of estimation as well as satisfaction of the constraints for the rCLS algorithm in both mean and mean-square senses. Afterwards, we examine the performance of the CLS algorithm by evaluating the limiting performance of the rCLS algorithm as the relaxation parameter (weight) approaches infinity. Numerical examples verify the accuracy of the theoretical findings.", "venue": "IEEE Transactions on Signal Processing", "authors": ["Reza  Arablouei", "Kutluyil  Dogan\u00e7ay"], "year": 2015, "n_citations": 12}
{"id": 4709053, "s2_id": "5aaedaf595baa0dd61f7cf7e77eb65576924cf11", "title": "Scheduling With Inexact Job Sizes: The Merits of Shortest Processing Time First", "abstract": "It is well known that size-based scheduling policies, which take into account job size (i.e., the time it takes to run them), can perform very desirably in terms of both response time and fairness. Unfortunately, the requirement of knowing a priori the exact job size is a major obstacle which is frequently insurmountable in practice. Often, it is possible to get a coarse estimation of job size, but unfortunately analytical results with inexact job sizes are challenging to obtain, and simulation-based studies show that several size-based algorithm are severely impacted by job estimation errors. For example, Shortest Remaining Processing Time (SRPT), which yields optimal mean sojourn time when job sizes are known exactly, can drastically underperform when it is fed inexact job sizes. \nSome algorithms have been proposed to better handle size estimation errors, but they are somewhat complex and this makes their analysis challenging. We consider Shortest Processing Time (SPT), a simplification of SRPT that skips the update of \"remaining\" job size and results in a preemptive algorithm that simply schedules the job with the shortest estimated processing time. When job size is inexact, SPT performs comparably to the best known algorithms in the presence of errors, while being definitely simpler. In this work, SPT is evaluated through simulation, showing near-optimal performance in many cases, with the hope that its simplicity can open the way to analytical evaluation even when inexact inputs are considered.", "venue": "ArXiv", "authors": ["Matteo  Dell'Amico"], "year": 2019, "n_citations": 3}
{"id": 4711950, "s2_id": "d560b9e75425e3534f0ec6e04a1144ee7bdb6e57", "title": "Enabling URLLC for Low-Cost IoT Devices via Diversity Combining Schemes", "abstract": "Supporting Ultra-Reliable Low-Latency Communication (URLLC) in the Internet of Things (IoT) era is challenging due to stringent constraints on latency and reliability combined with the simple circuitry of IoT nodes. Diversity is usually required for sustaining the reliability levels of URLLC, but there is an additional delay associated to auxiliary procedures to be considered, specially when communication includes low-cost IoT devices. Herein, we analyze Selection Combining (SC) and Switch and Stay Combining (SSC) diversity schemes as plausible solutions for enabling ultra-reliable low-latency downlink communications to low-cost IoT devices. We demonstrate the necessity of considering the time spent in auxiliary procedures, which has not been traditionally taken into account, while we show its impact on the reliability performance. We show there is an optimum number of receive antennas, which suggests that under certain conditions it might be required to turn off some of them, specially under the SC operation. We highlight the superiority of SSC with respect to SC as long the associated Signal-to-Noise Ratio threshold is properly selected. We propose using a fixed threshold relying only on long-term channel fading statistics, which leads to near-optimum results.", "venue": "2020 IEEE International Conference on Communications Workshops (ICC Workshops)", "authors": ["Onel L. Alcaraz L'opez", "Nurul Huda Mahmood", "Hirley  Alves"], "year": 2020, "n_citations": 3}
{"id": 4712820, "s2_id": "f5fa97f455278637c46f566f889ea77249bf78d8", "title": "StreamBlocks: A compiler for heterogeneous dataflow computing (technical report)", "abstract": "To increase performance and efficiency, systems use FPGAs as reconfigurable accelerators. A key challenge in designing these systems is partitioning computation between processors and an FPGA. An appropriate division of labor may be difficult to predict in advance and require experiments and measurements. When an investigation requires rewriting part of the system in a new language or with a new programming model, its high cost can retard the study of different configurations. A single-language system with an appropriate programming model and compiler that targets both platforms simplifies this exploration to a simple recompile with new compiler directives. This work introduces StreamBlocks, an open-source compiler and runtime that uses the CAL dataflow programming language to partition computations across heterogeneous (CPU/accelerator) platforms. Because of the dataflow model\u2019s semantics and the CAL language, StreamBlocks can exploit both thread parallelism in multi-core CPUs and the inherent parallelism of FPGAs. StreamBlocks supports exploring the design space with a profile-guided tool that helps identify the best hardware-software partitions.", "venue": "ArXiv", "authors": ["Endri  Bezati", "Mahyar  Emami", "Jorn  Janneck", "James  Larus"], "year": 2021, "n_citations": 0}
{"id": 4713015, "s2_id": "5c390a004dc0eab97f4803e01b012db09bdd51f3", "title": "How Crisp is the Crease? A Subjective Study on Web Browsing Perception of Above-The-Fold", "abstract": "Quality of Experience (QoE) for various types of websites has gained significant attention in recent years. In order to design and evaluate websites, a metric that can estimate a user's experienced quality robustly for diverse content is necessary. SpeedIndex (SI) has been widely adopted to estimate perceived web page loading progress. It measures the speed of rendering pixels for the webpage that is visible in the browser window. This is termed Above-The-Fold (ATF). The influence of animated content on the perception of ATF has been less comprehensively explored. In this paper, we present an experimental design and methodology to measure ATF perception for websites with and without animated elements for various page content categories. We found that pages with animated elements caused people to have more varied perceptions of ATF under different network conditions. Animated content also impacts the page load estimation accuracy of SI for websites. We discuss how the difference in the perception of ATF will impact the QoE management of web applications. We explain the necessity of revisiting the visual assessment of ATF to include the animated contents and improve the robustness of metrics like SI.", "venue": "2020 6th IEEE Conference on Network Softwarization (NetSoft)", "authors": ["Hamed Z. Jahromi", "Declan T. Delaney", "Andrew  Hines"], "year": 2020, "n_citations": 1}
{"id": 4719179, "s2_id": "24875d52a7c9de664ce992475aaf034f6eeccf94", "title": "Power-of-d-Choices with Memory: Fluid Limit and Optimality", "abstract": "In multiserver distributed queueing systems, the access of stochastically arriving jobs to resources is often regulated by a dispatcher, also known as a load balancer. A fundamental problem consist...", "venue": "Math. Oper. Res.", "authors": ["Jonatha  Anselmi", "Fran\u00e7ois  Dufour"], "year": 2020, "n_citations": 12}
{"id": 4719554, "s2_id": "ea46a66d87c00d34e50b861b87c7a7dcee58b542", "title": "Discrete-Time Queueing Model of Age of Information With Multiple Information Sources", "abstract": "Information freshness in IoT-based status update systems has recently been studied through the Age of Information (AoI) and Peak AoI (PAoI) performance metrics. In this article, we study a discrete-time server arising in multisource IoT systems, which accepts incoming information packets from multiple information sources so as to be forwarded to a remote monitor for status update purposes. Under the assumption of Bernoulli information packet arrivals and a common general discrete phase-type service time distribution across all the sources, we numerically obtain the exact per-source distributions of AoI and PAoI in matrix-geometric form for three different queueing disciplines: 1) nonpreemptive bufferless; 2) preemptive bufferless; and 3) nonpreemptive single buffer with replacement. The proposed numerical algorithm employs the theory of discrete-time Markov chains of quasi-birth-death type and is matrix analytical. Numerical examples are provided to validate the accuracy and effectiveness of the proposed queueing model. We also present a numerical example on the optimum choice of the Bernoulli parameters in a practical IoT system with two sources with diverse AoI requirements.", "venue": "IEEE Internet of Things Journal", "authors": ["Nail  Akar", "Ozancan  Dogan"], "year": 2021, "n_citations": 4}
{"id": 4721117, "s2_id": "80c5cc243a63153bb3e194f8b8af56e6bd808eaf", "title": "A Comprehensive Study and Performance Comparison of M-ary Modulation Schemes for an Efficient Wireless Mobile Communication System", "abstract": "Wireless communications has become one of the fastest growing areas in our modern life and creates enormous impact on nearly every feature of our daily life. In this paper, the performance of M-ary modulations schemes (MPSK, MQAM, MFSK) based wireless communication system on audio signal transmission over Additive Gaussian Noise (AWGN) channel are analyzed in terms of bit error probability as a function of SNR. Based on the results obtained in the present study, MPSK and MQAM are showing better performance for lower modulation order whereas these are inferior with higher M. The BER value is smaller in MFSK for higher M, but it is worse due to the distortion in the reproduce signal at the receiver end. The lossless reproduction of recorded voice signal can be achieved at the receiver end with a lower modulation order.", "venue": "ArXiv", "authors": ["Md. Emdadul Haque", "Md. Golam Rashed", "M. Hasnat Kabir"], "year": 2012, "n_citations": 13}
{"id": 4722422, "s2_id": "57980255798f84b38c616a126d61f7e38892816e", "title": "uPredict: A User-Level Profiler-Based Predictive Framework for Single VM Applications in Multi-Tenant Clouds", "abstract": "Most existing studies on performance prediction for virtual machines (VMs) in multi-tenant clouds are at system level and generally require access to performance counters in Hypervisors. In this work, we propose uPredict, a user-level profiler-based performance predictive framework for single-VM applications in multi-tenant clouds. Here, three micro-benchmarks are specially devised to assess the contention of CPUs, memory and disks in a VM, respectively. Based on measured performance of an application and micro-benchmarks, the application and VM-specific predictive models can be derived by exploiting various regression and neural network based techniques. These models can then be used to predict the application's performance using the in-situ profiled resource contention with the micro-benchmarks. We evaluated uPredict extensively with representative benchmarks from PARSEC, NAS Parallel Benchmarks and CloudSuite, on both a private cloud and two public clouds. The results show that the average prediction errors are between 9.8% to 17% for various predictive models on the private cloud with high resource contention, while the errors are within 4% on public clouds. A smart load-balancing scheme powered by uPredict is presented and can effectively reduce the execution and turnaround times of the considered application by 19% and 10%, respectively.", "venue": "ArXiv", "authors": ["Hamidreza  Moradi", "Wei  Wang", "Amanda  Fernandez", "Dakai  Zhu"], "year": 2019, "n_citations": 7}
{"id": 4723049, "s2_id": "cc91b7acd728159a065718c30aff41397d80956a", "title": "Asymptotic optimality of the static frequency caching in the presence of correlated requests", "abstract": "It is well known that the static caching algorithm that keeps the most frequently requested documents in the cache is optimal in case when documents are of the same size and requests are independent and identically distributed. However, it is hard to develop explicit and provably optimal caching algorithms when requests are statistically correlated. In this paper, we show that keeping the most frequently requested documents in the cache is still optimal for large cache sizes even if the requests are strongly correlated.", "venue": "Oper. Res. Lett.", "authors": ["Predrag R. Jelenkovic", "Ana  Radovanovic"], "year": 2009, "n_citations": 7}
{"id": 4726485, "s2_id": "13e30c777f6d97db7147b2bf242cb7d4bad3d7af", "title": "Queuing Analysis of Opportunistic Cognitive Radio IoT Network with Imperfect Sensing", "abstract": "In this paper, we analyze a Cognitive Radio-based Internet-of-Things (CR-IoT) network comprising a Primary Network Provider (PNP) and an IoT operator. The PNP uses its licensed spectrum to serve its users. The IoT operator identifies the white-space in the licensed band at regular intervals and opportunistically exploits them to serve the IoT nodes under its coverage. IoT nodes are battery-operated devices that require periodical energy replenishment. We employ the Microwave Power Transfer (MPT) technique for its superior energy transfer efficiency over long-distance. The white-space detection process is not always perfect and the IoT operator may jeopardize the PNP\u2019s transmissions due to misdetection. To reduce the possibility of such interferences, some of the spectrum holes must remain unutilized, even when the IoT nodes have data to transmit. The IoT operator needs to decide what percentage of the whitespace to keep unutilized and how to judiciously use the rest for data transmission and energy-replenishment to maintain an optimal balance between the average interference inflicted on PNP\u2019s users and the Quality-of-Service (QoS) experienced by IoT nodes. Due to the periodic nature of the spectrum-sensing process, Discrete Time Markov Chain (DTMC) method can realistically model this framework. In literature, activities of the PNP and IoT operator are assumed to be mutually exclusive, for ease of analysis. Our model incorporates possible overlaps between these activities, making the analysis more realistic. Using our model, the sustainability region of the CR-IoT network can be obtained. The accuracy of our analysis is demonstrated via extensive simulation.", "venue": "ArXiv", "authors": ["Asif Ahmed Sardar", "Dibbendu  Roy", "Washim Uddin Mondal", "Goutam  Das"], "year": 2021, "n_citations": 0}
{"id": 4728266, "s2_id": "df47fb11c9d04a68e3d316955a5a39409a8cc9d0", "title": "Reinforcement Learning Assisted Load Test Generation for E-Commerce Applications", "abstract": "Background: End-user satisfaction is not only dependent on the correct functioning of the software systems but is also heavily dependent on how well those functions are performed. Therefore, perfor ...", "venue": "ArXiv", "authors": ["Golrokh  Hamidi"], "year": 2020, "n_citations": 0}
{"id": 4731696, "s2_id": "e6f09975bdd664a6b0f4e95301431ed22184e2bf", "title": "Demonstrating 100 Gbps in and out of the public Clouds", "abstract": "There is increased awareness and recognition that public Cloud providers do provide capabilities not found elsewhere, with elasticity being a major driver. The value of elastic scaling is however tightly coupled to the capabilities of the networks that connect all involved resources, both in the public Clouds and at the various research institutions. This paper presents results of measurements involving file transfers inside public Cloud providers, fetching data from on-prem resources into public Cloud instances and fetching data from public Cloud storage into on-prem nodes. The networking of the three major Cloud providers, namely Amazon Web Services, Microsoft Azure and the Google Cloud Platform, has been benchmarked. The on-prem nodes were managed by either the Pacific Research Platform or located at the University of Wisconsin \u2013 Madison. The observed sustained throughput was of the order of 100 Gbps in all the tests moving data in and out of the public Clouds and throughput reaching into the Tbps range for data movements inside the public Cloud providers themselves. All the tests used HTTP as the transfer protocol.", "venue": "PEARC", "authors": ["Igor  Sfiligoi"], "year": 2020, "n_citations": 2}
{"id": 4733129, "s2_id": "27a6e54a5145c6d987919d6f48d43beaf139a51a", "title": "A Survey on the Evolution of Stream Processing Systems", "abstract": "Stream processing has been an active research field for more than 20 years, but it is now witnessing its prime time due to recent successful efforts by the research community and numerous worldwide open-source communities. This survey provides a comprehensive overview of fundamental aspects of stream processing systems and their evolution in the functional areas of out-of-order data management, state management, fault tolerance, high availability, load management, elasticity, and reconfiguration. We review noteworthy past research findings, outline the similarities and differences between early ('00-'10) and modern ('11-'18) streaming systems, and discuss recent trends and open problems.", "venue": "ArXiv", "authors": ["Marios  Fragkoulis", "Paris  Carbone", "Vasiliki  Kalavri", "Asterios  Katsifodimos"], "year": 2020, "n_citations": 6}
{"id": 4733582, "s2_id": "461e3e7f2f26052f73fb131aaa098875a8597a3c", "title": "Traffic optimization for TCP-based Massive Multiplayer Online Games", "abstract": "This paper studies the use of a traffic optimization technique named TCM (Tunneling, Compressing and Multiplexing) to reduce the bandwidth of MMORPGs (Massively Multiplayer Online Role-Playing Games), which employ TCP to provide a soft real-time service. In order to optimize the traffic and to improve bandwidth efficiency, TCM can be applied when the packets of a number of players share the same link, which occurs in some scenarios, as e.g. the traffic between proxies and servers of game-supporting infrastructures. First, TCP/IP headers are compressed using standard algorithms that avoid sending repeated fields; next, a number of packets are blended into a bigger one and finally, they are sent using a tunnel. The expected compressed header size has been obtained using traffic traces of a real game. Next, simulations using a traffic model of a popular MMORPG have been performed in order to estimate the expected bandwidth savings and the reduction in packets per second. The obtained bandwidth saving is about 60 percent. Packets per second are also significantly reduced. In addition, the added delays are shown to be small enough so as not to impair players' experienced quality.", "venue": "2012 International Symposium on Performance Evaluation of Computer & Telecommunication Systems (SPECTS)", "authors": ["Jose  Saldana", "Luis  Sequeira", "Juli\u00e1n  Fern\u00e1ndez-Navajas", "Jos\u00e9  Ru\u00edz"], "year": 2012, "n_citations": 8}
{"id": 4740637, "s2_id": "86de1e4181293760c9a16e223ad0dd2bf7056b10", "title": "Analytical Cost Metrics : Days of Future Past", "abstract": "As we move towards the exascale era, the new architectures must be capable of running the massive computational problems efficiently. Scientists and researchers are continuously investing in tuning the performance of extreme-scale computational problems. These problems arise in almost all areas of computing, ranging from big data analytics, artificial intelligence, search, machine learning, virtual/augmented reality, computer vision, image/signal processing to computational science and bioinformatics. With Moore's law driving the evolution of hardware platforms towards exascale, the dominant performance metric (time efficiency) has now expanded to also incorporate power/energy efficiency. Therefore, the major challenge that we face in computing systems research is: \"how to solve massive-scale computational problems in the most time/power/energy efficient manner?\" \nThe architectures are constantly evolving making the current performance optimizing strategies less applicable and new strategies to be invented. The solution is for the new architectures, new programming models, and applications to go forward together. Doing this is, however, extremely hard. There are too many design choices in too many dimensions. We propose the following strategy to solve the problem: (i) Models - Develop accurate analytical models (e.g. execution time, energy, silicon area) to predict the cost of executing a given program, and (ii) Complete System Design - Simultaneously optimize all the cost models for the programs (computational problems) to obtain the most time/area/power/energy efficient solution. Such an optimization problem evokes the notion of codesign.", "venue": "ArXiv", "authors": ["Nirmal  Prajapati", "Sanjay V. Rajopadhye", "Hristo  Djidjev"], "year": 2018, "n_citations": 0}
{"id": 4740750, "s2_id": "ec7e9698490ce8935e7773d6a4a18eb42d5e3c1e", "title": "Impact of Traditional Sparse Optimizations on a Migratory Thread Architecture", "abstract": "Achieving high performance for sparse applications is challenging due to irregular access patterns and weak locality. These properties preclude many static optimizations and degrade cache performance on traditional systems. To address these challenges, novel systems such as the Emu architecture have been proposed. The Emu design uses light-weight migratory threads, narrow memory, and near-memory processing capabilities to address weak locality and reduce the total load on the memory system. Because the Emu architecture is fundamentally different than cache based hierarchical memory systems, it is crucial to understand the cost-benefit tradeoffs of standard sparse algorithm optimizations on Emu hardware. In this work, we explore sparse matrix-vector multiplication (SpMV) on the Emu architecture. We investigate the effects of different sparse optimizations such as dense vector data layouts, work distributions, and matrix reorderings. Our study finds that initially distributing work evenly across the system is inadequate to maintain load balancing over time due to the migratory nature of Emu threads. In severe cases, matrix sparsity patterns produce hot-spots as many migratory threads converge on a single resource. We demonstrate that known matrix reordering techniques can improve SpMV performance on the Emu architecture by as much as 70% by encouraging more consistent load balancing. This can be compared with a performance gain of no more than 16% on a cache-memory based system.", "venue": "2018 IEEE/ACM 8th Workshop on Irregular Applications: Architectures and Algorithms (IA3)", "authors": ["Thomas B. Rolinger", "Christopher D. Krieger"], "year": 2018, "n_citations": 5}
{"id": 4742430, "s2_id": "cb1349c517878a2a2e405c9bf9d684dac82e3359", "title": "Fast Product-Matrix Regenerating Codes", "abstract": "Distributed storage systems support failures of individual devices by the use of replication or erasure correcting codes. While erasure correcting codes offer a better storage efficiency than replication for similar fault tolerance, they incur higher CPU consumption, higher network consumption and higher disk I/Os. To address these issues, codes specific to storage systems have been designed. Their main feature is the ability to repair a single lost disk efficiently. In this paper, we focus on one such class of codes that minimize network consumption during repair, namely regenerating codes. We implement the original Product-Matrix Regenerating codes as well as a new optimization we propose and show that the resulting optimized codes allow achieving 790 MB/s for encoding in typical settings. Reported speeds are significantly higher than previous studies, highlighting that regenerating codes can be used with little CPU penalty.", "venue": "ArXiv", "authors": ["Nicolas Le Scouarnec"], "year": 2014, "n_citations": 1}
{"id": 4745050, "s2_id": "49bdb4bce2b8d80d7198b2d5ab9d575fd93f7b18", "title": "Sharon: Shared Online Event Sequence Aggregation", "abstract": "Streaming systems evaluate massive workloads of event sequence aggregation queries. State-of-the-art approaches suffer from long delays caused by not sharing intermediate results of similar queries and by constructing event sequences prior to their aggregation. To overcome these limitations, our Shared Online Event Sequence Aggregation (Sharon) approach shares intermediate aggregates among multiple queries while avoiding the expensive construction of event sequences. Our Sharon optimizer faces two challenges. One, a sharing decision is not always beneficial. Two, a sharing decision may exclude other sharing opportunities. To guide our Sharon optimizer, we compactly encode sharing candidates, their benefits, and conflicts among candidates into the Sharon graph. Based on the graph, we map our problem of finding an optimal sharing plan to the Maximum Weight Independent Set (MWIS) problem. We then use the guaranteed weight of a greedy algorithm for the MWIS problem to prune the search of our sharing plan finder without sacrificing its optimality. The Sharon optimizer is shown to produce sharing plans that achieve up to an 18-fold speed-up compared to state-of-the-art approaches.", "venue": "2018 IEEE 34th International Conference on Data Engineering (ICDE)", "authors": ["Olga  Poppe", "Allison  Rozet", "Chuan  Lei", "Elke A. Rundensteiner", "David  Maier"], "year": 2018, "n_citations": 3}
{"id": 4749099, "s2_id": "ffa912ba4feaa4102e7caa98402c3dfc7963b89e", "title": "Ripple : Simplified Large-Scale Computation on Heterogeneous Architectures with Polymorphic Data Layout", "abstract": "GPUs are now used for a wide range of problems within HPC. However, making efficient use of the computational power available with multiple GPUs is challenging. The main challenges in achieving good performance are memory layout, affecting memory bandwidth, effective use of the memory spaces with a GPU, inter-GPU communication, and synchronization. We address these problems with the Ripple library, which provides a unified view of the computational space across multiple dimensions and multiple GPUs, allows polymorphic data layout, and provides a simple graph interface to describe an algorithm from which interGPU data transfers can be optimally scheduled. We describe the abstractions provided by Ripple to allow complex computations to be described simply, and to execute efficiently across many GPUs with minimal overhead. We show performance results for a number of examples, from particle motion to finite-volume methods and the eikonal equation, as well as showing good strong and weak scaling results across multiple GPUs.", "venue": "ArXiv", "authors": ["Robert  Clucas", "Philip  Blakely", "Nikolaos  Nikiforakis"], "year": 2021, "n_citations": 0}
{"id": 4749970, "s2_id": "febbe6482e4aacba81c2fbe9af501e7949bee575", "title": "Depth First Always On Routing Trace Algorithm", "abstract": "In this paper, we discussed current limitation in the electronic-design-automotation (EDA) tool on tracing the always on routing. We developed an algorithm to efficiently track the secondary power routing and accurately estimate the routing quality using approximate voltage drop as the criteria. The fast check can identify potential hotspot issues without going through sign-off checks. It helps designers to capture issues at early stages and fix the issues with less design effort. We also discussed some limitations to our algorithm.", "venue": "ArXiv", "authors": ["Anthony  Kim", "Sung Hyun Chen", "Chen  Zheng"], "year": 2017, "n_citations": 0}
{"id": 4751662, "s2_id": "037006fd28b89f0581e7dbb3f1bc68846a32dba2", "title": "Towards Transactional Load over XtreemFS", "abstract": "We propose using trace-based assessment of the performance of distributed file systems (DFS) under transactional IO load. The assessment includes simulations and experiments using the IO traces. Our experiments suggest that DFS, and specifically XtreemFS have a good potential to support transactional IO load in distributed environments: they demonstrate good performance, high availability and scalability, while at the same time opening the way to TCO reduction.", "venue": "ArXiv", "authors": ["Roman  Talyansky", "Bernd  Scheuermann", "Bjorn  Kolbeck", "Jan  Stender"], "year": 2010, "n_citations": 2}
{"id": 4754517, "s2_id": "9a9b420d7409f580bf377457407814f928bf774c", "title": "Understanding BitTorrent through real measurements", "abstract": "In this paper, we present the results of the BitTorrent measurement study. Two sources of BitTorrent data were utilised: metadata files and the logs of one of the currently most popular BitTorrent clients - \u03bcTorrent. Experimental data were collected for fifteen days from the popular torrent-discovery site thepiratebay.org (more than 30 000 torrents were captured and analysed). During this period the activity and logs of an unmodified version of \u03bcTorrent client downloading sessions were also captured. The obtained experimental results are swarm-oriented, which allows us to look at BitTorrent and its users from an exchanged resources perspective. Moreover, comparative analysis of the clients' connections with and without the \u03bcTP protocol is carried out to verify the extent to which \u03bcTP improves BitTorrent transmissions. To the authors' best knowledge, none of the previous studies have addressed these issues.", "venue": "China Communications", "authors": ["Wojciech  Mazurczyk", "Pawel  Kopiczko"], "year": 2013, "n_citations": 4}
{"id": 4757877, "s2_id": "e9a5d4bfd2478e98d7a7735f60ab4ee02d0394dc", "title": "On the Performance Evaluation and Analysis of the Hybridised Bittorrent Protocol with Partial Mobility Characteristics", "abstract": "Engaging mobility with file sharing is considered very promising in today's run Anywhere, Anytime, Anything (3As) environments. The Bittorrent file sharing protocol can be rarely combined with the mobility scenario framework since resources are not available due to the dynamically changing topology network. As a result, mobility in P2P-oriented file sharing platforms, degrades the end-to-end efficiency and the system's performance. This work proposes a new hybridized model, which takes into account the mobility characteristics of the combined Bittorrent protocol in a centralized manner enabling partial mobility characteristics, where the clients of the network use a distinct technique to differentiate between mobile and static nodes. Many parameters were taken into consideration like the round trip delays, the diffusion process, and the seeding techniques, targeting the maximization of the average throughput in the clustered swarms containing mobile peers. Partial mobility characteristics are set in a peer-tracker and peer-peer communication enhancement schema with partial mobility, allowing an optimistic approach to attain high availability and throughput response as simulation results show.", "venue": "AP2PS 2010", "authors": ["George C. Violaris", "Constandinos X. Mavromoustakis"], "year": 2010, "n_citations": 5}
{"id": 4762500, "s2_id": "e622665bab560e10a08b1a9c3fb9962db0925180", "title": "Search Algorithms for Automated Hyper-Parameter Tuning", "abstract": "Machine learning is a powerful method for modeling in different fields such as education. Its capability to accurately predict students\u2019 success makes it an ideal tool for decision-making tasks related to higher education. The accuracy of machine learning models depends on selecting the proper hyper-parameters. However, it is not an easy task because it requires time and expertise to tune the hyper-parameters to fit the machine learning model. In this paper, we examine the effectiveness of automated hyper-parameter tuning techniques to the realm of students\u2019 success. Therefore, we develop two automated Hyper-Parameter Optimization methods, namely grid search and random search, to assess and improve a previous study\u2019s performance. The experiment results show that applying random search and grid search on machine learning algorithms improves accuracy. We empirically show automated methods\u2019 superiority on a real-world educational data (MIDFIELD) for tuning HPs of conventional machine learning classifiers. This work emphasizes the effectiveness of automated hyper-parameter optimization while applying machine learning in the education field to aid faculties, directors\u2019, or non-expert users\u2019 decisions to improve students\u2019 success.", "venue": "ArXiv", "authors": ["Leila  Zahedi", "Farid Ghareh Mohammadi", "Shabnam  Rezapour", "Matthew W. Ohland", "M. Hadi Amini"], "year": 2021, "n_citations": 2}
{"id": 4765297, "s2_id": "ba77a3e3b93e8f616dcf93cd7641631a73de2686", "title": "On Bootstrapping Machine Learning Performance Predictors via Analytical Models", "abstract": "Performance modeling typically relies on two antithetic methodologies: white box models, which exploit knowledge on system's internals and capture its dynamics using analytical approaches, and black box techniques, which infer relations among the input and output variables of a system based on the evidences gathered during an initial training phase. In this paper we investigate a technique, which we name Bootstrapping, which aims at reconciling these two methodologies and at compensating the cons of the one with the pros of the other. We thoroughly analyze the design space of this gray box modeling technique, and identify a number of algorithmic and parametric trade-offs which we evaluate via two realistic case studies, a Key-Value Store and a Total Order Broadcast service.", "venue": "ArXiv", "authors": ["Diego  Didona", "Paolo  Romano"], "year": 2014, "n_citations": 18}
{"id": 4766193, "s2_id": "d917afd10fba45a23070cb0ae04e452bd75331d6", "title": "Theoretical Evaluation of Offloading through Wireless LANs", "abstract": "Offloading of cellular traffic through a wireless local area network (WLAN) is theoretically evaluated. First, empirical data sets of the locations of WLAN internet access points are analyzed and an inhomogeneous Poisson process consisting of high, normal, and low density regions is proposed as a spatial point process model for these configurations. Second, performance metrics, such as mean available bandwidth for a user and the number of vertical handovers, are evaluated for the proposed model through geometric analysis. Explicit formulas are derived for the metrics, although they depend on many parameters such as the number of WLAN access points, the shape of each WLAN coverage region, the location of each WLAN access point, the available bandwidth (bps) of the WLAN, and the shape and available bandwidth (bps) of each subregion identified by the channel quality indicator in a cell of the cellular network. Explicit formulas strongly suggest that the bandwidth a user experiences does not depend on the user mobility. This is because the bandwidth available by a user who does not move and that available by a user who moves are the same or approximately the same as a probabilistic distribution. Numerical examples show that parameters, such as the size of regions where placement of WLAN access points is not allowed and the mean density of WLANs in high density regions, have a large impact on performance metrics. In particular, a homogeneous Poisson process model as the WLAN access point location model largely overestimates the mean available bandwidth for a user and the number of vertical handovers. The overestimated mean available bandwidth is, for example, about 50% in a certain condition.", "venue": "ArXiv", "authors": ["Hiroshi  Saito", "Ryoichi  Kawahara"], "year": 2014, "n_citations": 0}
{"id": 4768515, "s2_id": "4f229608122bea0734479460eddf0ed68bdcde43", "title": "First Experiences Optimizing Smith-Waterman on Intel's Knights Landing Processor", "abstract": "The well-known Smith-Waterman (SW) algorithm is the most commonly used method for local sequence alignments. However, SW is very computationally demanding for large protein databases. There exist several implementations that take advantage of computing parallelization on many-cores, FPGAs or GPUs, in order to increase the alignment throughtput. In this paper, we have explored SW acceleration on Intel KNL processor. The novelty of this architecture requires the revision of previous programming and optimization techniques on many-core architectures. To the best of authors knowledge, this is the first KNL architecture assessment for SW algorithm. Our evaluation, using the renowned Environmental NR database as benchmark, has shown that multi-threading and SIMD exploitation reports competitive performance (351 GCUPS) in comparison with other implementations.", "venue": "ArXiv", "authors": ["Enzo  Rucci", "Carlos  Garc\u00eda", "Guillermo Botella Juan", "Armando De Giusti", "Marcelo R. Naiouf", "Manuel  Prieto"], "year": 2017, "n_citations": 3}
{"id": 4768935, "s2_id": "84918ccb615083daf49f02c460d667cc5c0ca8d2", "title": "An Autonomous Performance Testing Framework using Self-Adaptive Fuzzy Reinforcement Learning", "abstract": "Test automation brings the potential to reduce costs and human effort, but several aspects of software testing remain challenging to automate. One such example is automated performance testing to find performance breaking points. Current approaches to tackle automated generation of performance test cases mainly involve using source code or system model analysis or use-case based techniques. However, source code and system models might not always be available at testing time. On the other hand, if the optimal performance testing policy for the intended objective in a testing process instead could be learned by the testing system, then test automation without advanced performance models could be possible. Furthermore, the learned policy could later be reused for similar software systems under test, thus leading to higher test efficiency. We propose SaFReL, a self-adaptive fuzzy reinforcement learning-based performance testing framework. SaFReL learns the optimal policy to generate performance test cases through an initial learning phase, then reuses it during a transfer learning phase, while keeping the learning running and updating the policy in the long term. Through multiple experiments on a simulated environment, we demonstrate that our approach generates the target performance test cases for different programs more efficiently than a typical testing process, and performs adaptively without access to source code and performance models.", "venue": "ArXiv", "authors": ["Mahshid Helali Moghadam", "Mehrdad  Saadatmand", "Markus  Borg", "Markus  Bohlin", "Bjorn  Lisper"], "year": 2019, "n_citations": 1}
{"id": 4773125, "s2_id": "47be4ffa42e52704d614d314b6cb76110ef5a650", "title": "Training and Profiling a Pediatric Emotion Recognition Classifier on Mobile Devices", "abstract": "Implementing automated emotion recognition on mobile devices could provide an accessible diagnostic and therapeutic tool for those who struggle to recognize emotion, including children with developmental behavioral conditions such as autism. Although recent advances have been made in building more accurate emotion classifiers, existing models are too computationally expensive to be deployed on mobile devices. In this study, we optimized and profiled various machine learning models designed for inference on edge devices and were able to match previous state of the art results for emotion recognition on children. Our best model, a MobileNet-V2 network pre-trained on ImageNet, achieved 65.11% balanced accuracy and 64.19% F1-score on CAFE, while achieving a 45-millisecond inference latency on a Motorola Moto G6 phone. This balanced accuracy is only 1.79% less than the current state of the art for CAFE, which used a model that contains 26.62x more parameters and was unable to run on the Moto G6, even when fully optimized. This work validates that with specialized design and optimization techniques, machine learning models can become lightweight enough for deployment on mobile devices and still achieve high accuracies on difficult image classification tasks. Keywords\u2014edge computing, autism spectrum disorder, mobile health, computer vision", "venue": "ArXiv", "authors": ["Agnik  Banerjee", "Peter  Washington", "Cezmi  Mutlu", "Aaron  Kline", "Dennis P. Wall"], "year": 2021, "n_citations": 0}
{"id": 4774119, "s2_id": "6c24d03d9747770951f2c3157ca888afe41b47b3", "title": "On the tradeoff of average delay, average service cost, and average utility for single server queues with monotone policies", "abstract": "In this thesis, we study the optimal tradeoff of average delay, average service cost, and average utility for single server queueing models, with and without admission control. The continuous time and discrete time queueing models that we consider are motivated by cross-layer models for noisy point-to-point links, with random packet arrivals. We study the above tradeoff problem for a class of admissible policies, which are monotone and stationary and obtain an asymptotic characterization of the minimum average delay as a function of the average service cost and average utility constraints.", "venue": "ArXiv", "authors": ["Vineeth Bala Sukumaran"], "year": 2015, "n_citations": 3}
{"id": 4775289, "s2_id": "295e40a7b6017260feeafc58c49b3293fe9c66db", "title": "Efficient Neighbor-Finding on Space-Filling Curves", "abstract": "Space-filling curves (SFC, also known as FASS-curves) are a useful tool in scientific computing and other areas of computer science to sequentialize multidimensional grids in a cache-efficient and parallelization-friendly way for storage in an array. Many algorithms, for example grid-based numerical PDE solvers, have to access all neighbor cells of each grid cell during a grid traversal. While the array indices of neighbors can be stored in a cell, they still have to be computed for initialization or when the grid is adaptively refined. A fast neighbor-finding algorithm can thus significantly improve the runtime of computations on multidimensional grids. \nIn this thesis, we show how neighbors on many regular grids ordered by space-filling curves can be found in an average-case time complexity of $O(1)$. In general, this assumes that the local orientation (i.e. a variable of a describing grammar) of the SFC inside the grid cell is known in advance, which can be efficiently realized during traversals. Supported SFCs include Hilbert, Peano and Sierpinski curves in arbitrary dimensions. We assume that integer arithmetic operations can be performed in $O(1)$, i.e. independent of the size of the integer. We do not deal with the case of adaptively refined grids here. However, it appears that a generalization of the algorithm to suitable adaptive grids is possible. To formulate the neighbor-finding algorithm and prove its correctness and runtime properties, a modeling framework is introduced. This framework extends the idea of vertex-labeling to a description using grammars and matrices. With the sfcpp library, we provide a C++ implementation to render SFCs generated by such models and automatically compute all lookup tables needed for the neighbor-finding algorithm. Furthermore, optimized neighbor-finding implementations for various SFCs are included for which we provide runtime measurements.", "venue": "ArXiv", "authors": ["David  Holzm\u00fcller"], "year": 2017, "n_citations": 5}
{"id": 4778326, "s2_id": "512b52e8330ae3468ca30d87c44e0b54675d7d70", "title": "Adaptive Scheduling of Data Paths using Uppaal Tiga", "abstract": "We apply Uppaal Tiga to automatically compute adaptive scheduling strategies for an industrial casestudy dealing with a state-of-the-art image processing pipeline of a printer. As far as we know,this is the \ufb01rst application of timed automata technology to an industrial scheduling problem withuncertainty in job arrivals.", "venue": "FM 2009", "authors": ["Israa  AlAttili", "Fred  Houben", "Georgeta  Igna", "Steffen  Michels", "Feng  Zhu", "Frits  Vaandrager"], "year": 2009, "n_citations": 12}
{"id": 4778567, "s2_id": "782caf239f0555756c2ad5f5f4cacbcbe578787f", "title": "A note on integrating products of linear forms over the unit simplex", "abstract": "Integrating a product of linear forms over the unit simplex can be done in polynomial time if the number of variables n is fixed (V. Baldoni et al., 2011). In this note, we highlight that this problem is equivalent to obtaining the normalizing constant of state probabilities for a popular class of Markov processes used in queueing network theory. In light of this equivalence, we survey existing computational algorithms developed in queueing theory that can be used for exact integration. For example, under some regularity conditions, queueing theory algorithms can exactly integrate a product of linear forms of total degree N by solving N systems of linear equations.", "venue": "ArXiv", "authors": ["Giuliano  Casale"], "year": 2017, "n_citations": 0}
{"id": 4780308, "s2_id": "503c39ad31b763d345c53f6f46553105cd3e8066", "title": "Improving BLE Beacon Proximity Estimation Accuracy Through Bayesian Filtering", "abstract": "The interconnectedness of all things is continuously expanding which has allowed every individual to increase their level of interaction with their surroundings. Internet of Things (IoT) devices are used in a plethora of context-aware application, such as proximity-based services (PBSs), and location-based services (LBSs). For these systems to perform, it is essential to have reliable hardware and predict a user\u2019s position in the area with high accuracy in order to differentiate between individuals in a small area. A variety of wireless solutions that utilize received signal strength indicators (RSSIs) have been proposed to provide PBS and LBS for indoor environments, though each solution presents its own drawbacks. In this article, Bluetooth low energy (BLE) beacons are examined in terms of their accuracy in proximity estimation. Specifically, a mobile application is developed along with three Bayesian filtering techniques to improve the BLE beacon proximity estimation accuracy. This includes a Kalman filter, a particle filter, and a nonparametric information (NI) filter. Since the RSSI is heavily influenced by the environment, experiments were conducted to examine the performance of beacons from three popular vendors in two different environments. The error is compared in terms of mean absolute error (MAE) and root mean squared error (RMSE). According to the experimental results, Bayesian filters can improve proximity estimation accuracy up to 30% in comparison with traditional filtering, when the beacon and the receiver are within 3 m.", "venue": "IEEE Internet of Things Journal", "authors": ["Andrew  Mackey", "Petros  Spachos", "Liang  Song", "Konstantinos N. Plataniotis"], "year": 2020, "n_citations": 29}
{"id": 4781357, "s2_id": "4abe45828118c57f19b7f81e56b9fc75effdfecf", "title": "Characterization and architectural implications of big data workloads", "abstract": "The previous major efforts on big data benchmark either propose a large amount of workloads (e.g. a recent comprehensive big data benchmark suite - BigDataBench [4]), which impose cognitive difficulty on workload characterization and serious benchmarking cost; or only select a few workloads according to so-called popularity[1], which lead to partial or biased observations.", "venue": "2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Lei  Wang", "Rui  Ren", "Jianfeng  Zhan", "Zhen  Jia"], "year": 2016, "n_citations": 7}
{"id": 4782472, "s2_id": "4f030bb8326461c52d1a587302169daa66ac48eb", "title": "Data-parallel programming with Intel Array Building Blocks (ArBB)", "abstract": "Intel Array Building Blocks is a high-level data-parallel programming environment designed to produce scalable and portable results on existing and upcoming multi- and many-core platforms. \nWe have chosen several mathematical kernels - a dense matrix-matrix multiplication, a sparse matrix-vector multiplication, a 1-D complex FFT and a conjugate gradients solver - as synthetic benchmarks and representatives of scientific codes and ported them to ArBB. This whitepaper describes the ArBB ports and presents performance and scaling measurements on the Westmere-EX based system SuperMIG at LRZ in comparison with OpenMP and MKL.", "venue": "ArXiv", "authors": ["Volker  Weinberg"], "year": 2012, "n_citations": 1}
{"id": 4783461, "s2_id": "2f56189676d7fa4f96af8d227f06a0ffd579b007", "title": "Correcting for Non-Markovian Asymptotic Effects using Markovian Representation", "abstract": "Asymptotic properties of Markov Processes, such as steady state probabilities or hazard rate for absorbing states can be efficiently calculated by means of linear algebra even for large-scale problems. This paper discusses the methods for adjusting parameters of the Markov models to account for non-constant transition rates. In particular, transitions with fixed delays are considered along with the transitions that follow Weibull and lognormal distributions. Procedures for both steady-state solutions in the absence of an absorbing state, and for hazard rates to an absorbing state are provided and demonstrated on several examples.", "venue": "ArXiv", "authors": ["Vitali  Volovoi"], "year": 2017, "n_citations": 0}
{"id": 4789303, "s2_id": "ee294075bd74382480987f7966dd842b9e4c4723", "title": "Parallel Sparse Tensor Decomposition in Chapel", "abstract": "In big-data analytics, using tensor decomposition to extract patterns from large, sparse multivariate data is a popular technique. Many challenges exist for designing parallel, high performance tensor decomposition algorithms due to irregular data accesses and the growing size of tensors that are processed. There have been many efforts at implementing shared-memory algorithms for tensor decomposition, most of which have focused on the traditional C/C++ with OpenMP framework. However, Chapel is becoming an increasingly popular programing language due to its expressiveness and simplicity for writing scalable parallel programs. In this work, we port a state of the art C/OpenMP parallel sparse tensor decomposition tool, SPLATT, to Chapel. We present a performance study that investigates bottlenecks in our Chapel code and discusses approaches for improving its performance. Also, we discuss features in Chapel that would have been beneficial to our porting effort. We demonstrate that our Chapel code is competitive with the C/OpenMP code for both runtime and scalability, achieving 83%-96% performance of the original code and near linear scalability up to 32 cores.", "venue": "2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Thomas B. Rolinger", "Tyler A. Simon", "Christopher D. Krieger"], "year": 2018, "n_citations": 1}
{"id": 4790080, "s2_id": "c5977212004d938a77a7ac51697259596059cfda", "title": "Towards Models for Availability and Security Evaluation of Cloud Computing with Moving Target Defense", "abstract": "Security is one of the most relevant concerns in cloud computing. With the evolution of cyber-security threats, developing innovative techniques to thwart attacks is of utmost importance. One recent method to improve cloud computing security is Moving Target Defense (MTD). MTD makes use of dynamic reconfiguration in virtualized environments to \"confuse\" attackers or to nullify their knowledge about the system state. However, there is still no consolidated mechanism to evaluate the trade-offs between availability and security when using MTD on cloud computing. The evaluation through measurements is complex as one needs to deal with unexpected events as failures and attacks. To overcome this challenge, we intend to propose a set of models to evaluate the availability and security of MTD in cloud computing environments. The expected results include the quantification of availability and security levels under different conditions (e.g., different software aging rates, varying workloads, different attack intensities).", "venue": "ArXiv", "authors": ["Matheus  Torquato", "Marco  Vieira"], "year": 2019, "n_citations": 2}
{"id": 4790562, "s2_id": "3df2e6f33855ae2883956bba5d6d56d11c93208c", "title": "Transmission Capacity of Wireless Networks", "abstract": "Transmission capacity (TC) is a performance metric for wireless networks that measures the spatial intensity of successful transmissions per unit area, subject to a constraint on the permissible outage probability (where outage occurs when the SINR at a receiver is below a threshold). This volume gives a unified treatment of the TC framework that has been developed by the authors and their collaborators over the past decade. The mathematical framework underlying the analysis (reviewed in Ch. 2) is stochastic geometry: Poisson point processes model the locations of interferers, and (stable) shot noise processes represent the aggregate interference seen at a receiver. Ch. 3 presents TC results (exact, asymptotic, and bounds) on a simple model in order to illustrate a key strength of the framework: analytical tractability yields explicit performance dependence upon key model parameters. Ch. 4 presents enhancements to this basic model --- channel fading, variable link distances, and multi-hop. Ch. 5 presents four network design case studies well-suited to TC: i) spectrum management, ii) interference cancellation, iii) signal threshold transmission scheduling, and iv) power control. Ch. 6 studies the TC when nodes have multiple antennas, which provides a contrast vs. classical results that ignore interference.", "venue": "Found. Trends Netw.", "authors": ["Steven P. Weber", "Jeffrey G. Andrews"], "year": 2011, "n_citations": 127}
{"id": 4793991, "s2_id": "54839a8af130571d1d9ca53317d4403d4cc65544", "title": "Speculative Container Scheduling for Deep Learning Applications in a Kubernetes Cluster", "abstract": "In the past decade, we have witnessed a dramatically increasing volume of data collected from varied sources. The explosion of data has transformed the world as more information is available for collection and analysis than ever before. To maximize the utilization, various machine and deep learning models have been developed, e.g. CNN [1] and RNN [2], to study data and extract valuable information from different perspectives. While data-driven applications improve countless products, training models for hyperparameter tuning is still a time-consuming and resource-intensive process. Cloud computing provides infrastructure support for the training of deep learning applications. The cloud service providers, such as Amazon Web Services [3], create an isolated virtual environment (virtual machines and containers) for clients, who share physical resources, e.g., CPU and memory. On the cloud, resource management schemes are implemented to enable better sharing among users and boost the system-wide performance. However, general scheduling approaches, such as spread priority and balanced resource schedulers, do not work well with deep learning workloads. In this project, we propose SpeCon, a novel container scheduler that is optimized for shortlived deep learning applications. Based on virtualized containers, such as Kubernetes [4] and Docker [5], SpeCon analyzes the common characteristics of training processes. We design a suite of algorithms to monitor the progress of the training and speculatively migrate the slow-growing models to release resources for fast-growing ones. Specifically, the extensive experiments demonstrate that SpeCon improves the completion time of an individual job by up to 41.5%, 14.8% system-wide and 24.7% in terms of makespan.", "venue": "ArXiv", "authors": ["Ying  Mao", "Yuqi  Fu", "Wenjia  Zheng", "Long  Cheng", "Qingzhi  Liu", "Dingwen  Tao"], "year": 2020, "n_citations": 9}
{"id": 4796156, "s2_id": "ca92d9f1517193624f003f181438ba1fe9ba8145", "title": "HTBQueue: A Hierarchical Token Bucket Implementation for the OMNeT++/INET Framework", "abstract": "The hierarchical token bucket (HTB) algorithm allows to specify per-flow bitrate guarantees and enables excess bandwidth sharing between flows of the same class. Additionally, it provides capabilities to prioritize the traffic of specific flows, potentially considering their delay demands. HTB hence constitutes a powerful mechanism to enforce QoS requirements hierarchically and on a fine granular per-flow level, making it an appropriate choice in numerous use-cases. In this paper, we present HTBQueue, our implementation of a compound module for HTB support in the discrete event simulator OMNeT++. We validate HTBQueue\u2019s functionality in terms of rate conformance and fair bandwidth sharing behavior between competing flows. We furthermore demonstrate its support for flow prioritization.", "venue": "ArXiv", "authors": ["Marcin  Bosk", "Marija  Gaji'c", "Susanna  Schwarzmann", "Stanislav  Lange", "Thomas  Zinner"], "year": 2021, "n_citations": 0}
{"id": 4805748, "s2_id": "a363e0c464e2ae1928b5f696a80fbc90cb679fbb", "title": "Anytime diagnosis for reconfiguration", "abstract": "Many domains require scalable algorithms that help to determine diagnoses efficiently and often within predefined time limits. Anytime diagnosis is able to determine solutions in such a way and thus is especially useful in real-time scenarios such as production scheduling, robot control, and communication networks management where diagnosis and corresponding reconfiguration capabilities play a major role. Anytime diagnosis in many cases comes along with a trade-off between diagnosis quality and the efficiency of diagnostic reasoning. In this paper we introduce and analyze FlexDiag which is an anytime direct diagnosis approach. We evaluate the algorithm with regard to performance and diagnosis quality using a configuration benchmark from the domain of feature models and an industrial configuration knowledge base from the automotive domain. Results show that FlexDiag helps to significantly increase the performance of direct diagnosis search with corresponding quality tradeoffs in terms of minimality and accuracy.", "venue": "Journal of Intelligent Information Systems", "authors": ["Alexander  Felfernig", "Rouven  Walter", "Jos\u00e9 A. Galindo", "David  Benavides", "Seda Polat Erdeniz", "M\u00fcsl\u00fcm  Atas", "Stefan  Reiterer"], "year": 2017, "n_citations": 20}
{"id": 4807527, "s2_id": "643419164cda7c87f0bed6bf52e0d5cfbf66a65d", "title": "Asymptotically Optimal Load Balancing Topologies", "abstract": "We consider a system of N ~servers inter-connected by some underlying graph topology~G N . Tasks with unit-mean exponential processing times arrive at the various servers as independent Poisson processes of rate lambda. Each incoming task is irrevocably assigned to whichever server has the smallest number of tasks among the one where it appears and its neighbors in G N . The above model arises in the context of load balancing in large-scale cloud networks and data centers, and has been extensively investigated in the case G N is a clique. Since the servers are exchangeable in that case, mean-field limits apply, and in particular it has been proved that for any lambda < 1, the fraction of servers with two or more tasks vanishes in the limit as N -> \u0131nfty. For an arbitrary graph G N , mean-field techniques break down, complicating the analysis, and the queue length process tends to be worse than for a clique. Accordingly, a graph G N is said to be N -optimal or \u221eN-optimal when the queue length process on G N is equivalent to that on a clique on an N -scale or \u221eN-scale, respectively. We prove that if G N is an Erdos-R\u00e9nyi random graph with average degree d(N), then with high probability it is N -optimal and \u221eN-optimal if d(N) -> \u0131nfty$ and d(N) / (\u221eN \u0142og(N)) -> \u0131nfty as N -> \u0131nfty, respectively. This demonstrates that optimality can be maintained at N -scale and \u221eN-scale while reducing the number of connections by nearly a factor N and \u221eN/ \u0142og(N) compared to a clique, provided the topology is suitably random. It is further shown that if G N contains \u0398(N) bounded-degree nodes, then it cannot be N -optimal. In addition, we establish that an arbitrary graph G N is N -optimal when its minimum degree is N - o(N), and may not be N -optimal even when its minimum degree is c N + o(N) for any 0 < c < 1/2. Simulation experiments are conducted for various scenarios to corroborate the asymptotic results.", "venue": "SIGMETRICS", "authors": ["Debankur  Mukherjee", "Sem C. Borst", "Johan van Leeuwaarden"], "year": 2018, "n_citations": 0}
{"id": 4807850, "s2_id": "7565a2d1843f5625f7a35d4991b7e504b1d69e39", "title": "Automatic Differentiation for Adjoint Stencil Loops", "abstract": "Stencil loops are a common motif in computations including convolutional neural networks, structured-mesh solvers for partial differential equations, and image processing. Stencil loops are easy to parallelise, and their fast execution is aided by compilers, libraries, and domain-specific languages. Reverse-mode automatic differentiation, also known as algorithmic differentiation, autodiff, adjoint differentiation, or back-propagation, is sometimes used to obtain gradients of programs that contain stencil loops. Unfortunately, conventional automatic differentiation results in a memory access pattern that is not stencil-like and not easily parallelisable. In this paper we present a novel combination of automatic differentiation and loop transformations that preserves the structure and memory access pattern of stencil loops, while computing fully consistent derivatives. The generated loops can be parallelised and optimised for performance in the same way and using the same tools as the original computation. We have implemented this new technique in the Python tool PerforAD, which we release with this paper along with test cases derived from seismic imaging and computational fluid dynamics applications.", "venue": "ICPP", "authors": ["Jan  H\u00fcckelheim", "Navjot  Kukreja", "Sri Hari Krishna Narayanan", "Fabio  Luporini", "Gerard  Gorman", "Paul D. Hovland"], "year": 2019, "n_citations": 8}
{"id": 4807998, "s2_id": "dbc560a158f8e782b388912add132915f13bd3f6", "title": "A Fast-rate WLAN Measurement Tool for Improved Miss-rate in Indoor Navigation", "abstract": "Recently, location-based services (LBS) have steered attention to indoor positioning systems (IPS). WLAN-based IPSs relying on received signal strength (RSS) measurements such as fingerprinting are gaining popularity due to proven high accuracy of their results. Typically, sets of RSS measurements at selected locations from several WLAN access points (APs) are used to calibrate the system. Retrieval of such measurements from WLAN cards are commonly at one-Hz rate. Such measurement collection is needed for offline radio-map surveying stage which aligns fingerprints to locations, and for online navigation stage, when collected measurements are associated with the radio-map for user navigation. As WLAN network is not originally designed for positioning, an RSS measurement miss could have a high impact on the fingerprinting system. Additionally, measurement fluctuations require laborious signal processing, and surveying process can be very time consuming. This paper proposes a fast-rate measurement collection method that addresses previously mentioned problems by achieving a higher probability of RSS measurement collection during a given one-second window. This translates to more data for statistical processing and faster surveying. The fast-rate collection approach is analyzed against the conventional measurement rate in a proposed testing methodology that mimics real-life scenarios related to IPS surveying and online navigation.", "venue": "Proceedings of the 31st International Technical Meeting of The Satellite Division of the Institute of Navigation (ION GNSS+ 2018)", "authors": ["Erick  Schmidt", "David  Akopian"], "year": 2018, "n_citations": 0}
{"id": 4810708, "s2_id": "7c8fe0fa7ae5850834f4d296309dcdd4ba26feeb", "title": "Effectiveness of Garbage Collection in MIT/GNU Scheme", "abstract": "Scheme uses garbage collection for heap memory management. Ideally, garbage collectors should be able to reclaim all dead objects, i.e. objects that will not be used in future. However, garbage collectors collect only those dead objects that are not reachable from any program variable. Dead objects that are reachable from program variables are not reclaimed. \nIn this paper we describe our experiments to measure the effectiveness of garbage collection in MIT/GNU Scheme. We compute the drag time of objects, i.e. the time for which an object remains in heap memory after its last use. The number of dead objects and the drag time together indicate opportunities for improving garbage collection. Our experiments reveal that up to 26% of dead objects remain in memory. The average drag time is up to 37% of execution time. Overall, we observe memory saving potential ranging from 9% to 65%.", "venue": "ArXiv", "authors": ["Amey  Karkare", "Amitabha  Sanyal", "Uday P. Khedker"], "year": 2006, "n_citations": 7}
{"id": 4813702, "s2_id": "d4f465337297f156e5ab41efec13e7e4da631d5a", "title": "Distributed Multiuser Sequential Channel Sensing Schemes in Multichannel Cognitive Radio Networks", "abstract": "Effective spectrum sensing strategies enable cognitive radios (CRs) to identify and opportunistically transmit on under-utilized spectral resources. In this paper, sequential channel sensing problems for single and multiple secondary users (SUs) networks are effectively modeled through finite state Markovian processes. More specifically, a model for single user case is introduced and its performance is validated through analytical analysis. Then, in order to address multiple SUs case, this model is extended to include the modified p-persistent access (MPPA) protocol. Since the scheme utilized experiences a high level of collision among the SUs, to mitigate the problem appropriately, p-persistent random access (PPRA) protocol is considered, which offers higher average throughput for SUs by statistically distributing their loads among all channels. The structure and performance of the proposed schemes are discussed in detail, and a set of illustrative numerical results is presented to validate and compare the performance of the proposed sense-access strategies.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Hossein Shokri Ghadikolaei", "Fatemeh  Sheikholeslami", "Masoumeh  Nasiri-Kenari"], "year": 2013, "n_citations": 28}
{"id": 4814692, "s2_id": "20da0dffc78e8430e46166901f5ba394c0d6de38", "title": "Real-Time Welfare-Maximizing Regulation Allocation in Dynamic Aggregator-EVs System", "abstract": "The concept of vehicle-to-grid (V2G) has gained recent interest as more and more electric vehicles (EVs) are put to use. In this paper, we consider a dynamic aggregator-EVs system, where an aggregator centrally coordinates a large number of dynamic EVs to provide regulation service. We propose a Welfare-Maximizing Regulation Allocation (WMRA) algorithm for the aggregator to fairly allocate the regulation amount among the EVs. Compared with previous works, WMRA accommodates a wide spectrum of vital system characteristics, including dynamics of EV, limited EV battery size, EV battery degradation cost, and the cost of using external energy sources for the aggregator. The algorithm operates in real time and does not require any prior knowledge of the statistical information of the system. Theoretically, we demonstrate that WMRA is away from the optimum by O(1/V), where V is a controlling parameter depending on EVs' battery size. In addition, our simulation results indicate that WMRA can substantially outperform a suboptimal greedy algorithm.", "venue": "IEEE Transactions on Smart Grid", "authors": ["Sun  Sun", "Min  Dong", "Ben  Liang"], "year": 2014, "n_citations": 59}
{"id": 4817476, "s2_id": "c30e990bdc761eb1314e65166ae1ae863579c0cc", "title": "Accelerated Neural Networks on OpenCL Devices Using SYCL-DNN", "abstract": "Over the past few years machine learning has seen a renewed explosion of interest, following a number of studies showing the effectiveness of neural networks in a range of tasks which had previously been considered incredibly hard. Neural networks' effectiveness in the fields of image recognition and natural language processing stems primarily from the vast amounts of data available to companies and researchers, coupled with the huge amounts of compute power available in modern accelerators such as GPUs, FPGAs and ASICs. There are a number of approaches available to developers for utilizing GPGPU technologies such as SYCL, OpenCL and CUDA, however many applications require the same low level mathematical routines. Libraries dedicated to accelerating these common routines allow developers to easily make full use of the available hardware without requiring low level knowledge of the hardware themselves, however such libraries are often provided by hardware manufacturers for specific hardware such as cuDNN [9] for Nvidia hardware or MIOpen [5] for AMD hardware. SYCL-DNN is a new open-source library dedicated to providing accelerated routines for neural network operations which are hardware and vendor agnostic. Built on top of the SYCL open standard and written entirely in standard C++, SYCL-DNN allows a user to easily accelerate neural network code for a wide range of hardware using a modern C++ interface. The library is tested on AMD's OpenCL for GPU, Intel's OpenCL for CPU and GPU, ARM's OpenCL for Mali GPUs as well as ComputeAorta's OpenCL for R-Car CV engine and host CPU. In this talk we will present performance figures for SYCL-DNN on this range of hardware, and discuss how high performance was achieved on such a varied set of accelerators with such different hardware features.", "venue": "IWOCL", "authors": ["Rod  Burns", "John  Lawson", "Duncan  McBain", "Daniel  Soutar"], "year": 2019, "n_citations": 8}
{"id": 4822772, "s2_id": "8406e4116f283e17953aba73210e70645b80bdd8", "title": "Product-form solutions for integrated services packet networks and cloud computing systems", "abstract": "We iteratively derive the product-form solutions of stationary distributions of priority multiclass queueing networks with multi-sever stations. The networks are Markovian with exponential interarrival and service time distributions. These solutions can be used to conduct performance analysis or as comparison criteria for approximation and simulation studies of large scale networks with multi-processor shared-memory switches and cloud computing systems with parallel-server stations. Numerical comparisons with existing Brownian approximating model are provided to indicate the effectiveness of our algorithm.", "venue": "ArXiv", "authors": ["Wanyang  Dai"], "year": 2013, "n_citations": 3}
{"id": 4826641, "s2_id": "bd6dbc68ac1e7bf5cbe3cc2d49527033c5c8b9c1", "title": "Hybrid behaviour of Markov population models", "abstract": "We investigate the behaviour of population models written in Stochastic Concurrent Constraint Programming (sCCP), a stochastic extension of Concurrent Constraint Programming. In particular, we focus on models from which we can define a semantics of sCCP both in terms of Continuous Time Markov Chains (CTMC) and in terms of Stochastic Hybrid Systems, in which some populations are approximated continuously, while others are kept discrete. We will prove the correctness of the hybrid semantics from the point of view of the limiting behaviour of a sequence of models for increasing population size. More specifically, we prove that, under suitable regularity conditions, the sequence of CTMC constructed from sCCP programs for increasing population size converges to the hybrid system constructed by means of the hybrid semantics. We investigate in particular what happens for sCCP models in which some transitions are guarded by boolean predicates or in the presence of instantaneous transitions.", "venue": "Inf. Comput.", "authors": ["Luca  Bortolussi"], "year": 2016, "n_citations": 25}
{"id": 4827543, "s2_id": "f03e22b4ca8a6bd6a89b4621bb7a13b2d646fa36", "title": "Profiling parallel Mercury programs with ThreadScope", "abstract": "The behavior of parallel programs is even harder to understand than the behavior of sequential programs. Parallel programs may suffer from any of the performance problems affecting sequential programs, as well as from several problems unique to parallel systems. Many of these problems are quite hard (or even practically impossible) to diagnose without help from specialized tools. We present a proposal for a tool for profiling the parallel execution of Mercury programs, a proposal whose implementation we have already started. This tool is an adaptation and extension of the ThreadScope profiler that was first built to help programmers visualize the execution of parallel Haskell programs.", "venue": "ArXiv", "authors": ["Paul  Bone", "Zoltan  Somogyi"], "year": 2011, "n_citations": 3}
{"id": 4828129, "s2_id": "bec524b7d9424709cb402eaa72132f789a390410", "title": "Optimal Multiserver Scheduling with Unknown Job Sizes in Heavy Traffic", "abstract": "We consider scheduling to minimize mean response time of the M/G/k queue with unknown job sizes. In the singleserver k = 1 case, the optimal policy is the Gittins policy, but it is not known whether Gittins or any other policy is optimal in the multiserver case. Exactly analyzing the M/G/k under any scheduling policy is intractable, and Gittins is a particularly complicated policy that is hard to analyze even in the single-server case.", "venue": "SIGMETRICS Perform. Evaluation Rev.", "authors": ["Ziv  Scully", "Isaac  Grosof", "Mor  Harchol-Balter"], "year": 2020, "n_citations": 0}
{"id": 4834965, "s2_id": "8b26b901881df657d85cc57f826719240298b0f4", "title": "On the Distribution of AoI for the GI/GI/1/1 and GI/GI/1/2* Systems: Exact Expressions and Bounds", "abstract": "Since Age of Information (AoI) has been proposed as a metric that quantifies the freshness of information updates in a communication system, there has been a constant effort in understanding and optimizing different statistics of the AoI process for classical queueing systems. In addition to classical queuing systems, more recently, systems with no queue or a unit capacity queue storing the latest packet have been gaining importance as storing and transmitting older packets do not reduce AoI at the receiver. Following this line of research, we study the distribution of AoI for the GI/GI/1/1 and GI/GI/1/2* systems, under non-preemptive scheduling. For any single-source-single-server queueing system, we derive, using sample path analysis, a fundamental result that characterizes the AoI violation probability, and use it to obtain closed-form expressions for D/GI/1/1, M/GI/1/1 as well as systems that use zero-wait policy. Further, when exact results are not tractable, we present a simple methodology for obtaining upper bounds for the violation probability for both GI/GI/1/1 and GI/GI/1/2* systems. An interesting feature of the proposed upper bounds is that, if the departure rate is given, they overestimate the violation probability by at most a value that decreases with the arrival rate. Thus, given the departure rate and for a fixed average service, the bounds are tighter at higher utilization.", "venue": "IEEE INFOCOM 2019 - IEEE Conference on Computer Communications", "authors": ["Jaya Prakash Champati", "Hussein  Al-Zubaidy", "James  Gross"], "year": 2019, "n_citations": 30}
{"id": 4838624, "s2_id": "544e4f3db3727fe7bb8e79712ea2b0aa7c2da64f", "title": "Cluster Based Hierarchical Routing Protocol for Wireless Sensor Network", "abstract": "The efficient use of energy source in a sensor node is most desirable criteria for prolong the life time of wireless sensor network. In this paper, we propose a two layer hierarchical routing protocol called Cluster Based Hierarchical Routing Protocol (CBHRP). We introduce a new concept called head-set, consists of one active cluster head and some other associate cluster heads within a cluster. The head-set members are responsible for control and management of the network. Results show that this protocol reduces energy consumption quite significantly and prolongs the life time of sensor network as compared to LEACH.", "venue": "ArXiv", "authors": ["Md. Golam Rashed", "M. Hasnat Kabir", "Muhammad Sajjadur Rahim", "Shaikh Enayet Ullah"], "year": 2012, "n_citations": 22}
{"id": 4844554, "s2_id": "54bed824610c8ba3a22973424a4ea96e3a48075e", "title": "DeLag: Detecting Latency Degradation Patterns in Service-based Systems", "abstract": "Performance debugging in production is a fundamental activity in modern service-based systems. The diagnosis of performance issues is often time-consuming, since it requires thorough inspection of large volumes of traces and performance indices. In this paper we present DeLag, a novel automated search-based approach for diagnosing performance issues in service-based systems. DeLag identifies subsets of requests that show, in the combination of their Remote Procedure Call execution times, symptoms of potentially relevant performance issues. We call such symptoms Latency Degradation Patterns. DeLag simultaneously search for multiple latency degradation patterns while optimizing precision, recall and latency dissimilarity. Experimentation on 700 datasets of requests generated from two microservice-based systems shows that our approach provide better and more stable effectiveness than three state-of-the-art approaches and general purpose machine learning clustering algorithms. Moreover, DeLag outperforms in terms of efficiency the second and the third most effective baseline techniques on the largest datasets used in our evaluation.", "venue": "ArXiv", "authors": ["Luca  Traini", "Vittorio  Cortellessa"], "year": 2021, "n_citations": 0}
{"id": 4845122, "s2_id": "b764d9fa854badad14ee4f24cd63646276378ac7", "title": "Accelerating Gradient-based Meta Learner", "abstract": "Meta Learning has been in focus in recent years due to the metalearner model\u2019s ability to adapt well and generalize to new tasks, thus, reducing both the time and data requirements for learning. However, a major drawback of meta learner is that, to reach to a state from where learning new tasks becomes feasible with less data, it requires a large number of iterations and a lot of time. We address this issue by proposing various acceleration techniques to speed up meta learning algorithms such as MAML (Model Agnostic Meta Learning). We present 3.73X acceleration on a well known RNN optimizer based meta learner proposed in literature [11]. We introduce a novel method of training tasks in clusters, which not only accelerates the meta learning process but also improves model accuracy performance.", "venue": "ArXiv", "authors": ["Varad  Pimpalkhute", "Amey  Pandit", "Mayank  Mishra", "Rekha  Singhal"], "year": 2021, "n_citations": 0}
{"id": 4845239, "s2_id": "4292039c2b83ddfae5de8311acbb4eeb257fa760", "title": "Pigeonring: A Principle for Faster Thresholded Similarity Search", "abstract": "The pigeonhole principle states that if $n$ items are contained in $m$ boxes, then at least one box has no more than $n / m$ items. It is utilized to solve many data management problems, especially for thresholded similarity searches. Despite many pigeonhole principle-based solutions proposed in the last few decades, the condition stated by the principle is weak. It only constrains the number of items in a single box. By organizing the boxes in a ring, we propose a new principle, called the pigeonring principle, which constrains the number of items in multiple boxes and yields stronger conditions. To utilize the new principle, we focus on problems defined in the form of identifying data objects whose similarities or distances to the query is constrained by a threshold. Many solutions to these problems utilize the pigeonhole principle to find candidates that satisfy a filtering condition. By the new principle, stronger filtering conditions can be established. We show that the pigeonhole principle is a special case of the new principle. This suggests that all the pigeonhole principle-based solutions are possible to be accelerated by the new principle. A universal filtering framework is introduced to encompass the solutions to these problems based on the new principle. Besides, we discuss how to quickly find candidates specified by the new principle. The implementation requires only minor modifications on top of existing pigeonhole principle-based algorithms. Experimental results on real datasets demonstrate the applicability of the new principle as well as the superior performance of the algorithms based on the new principle.", "venue": "Proc. VLDB Endow.", "authors": ["Jianbin  Qin", "Chuan  Xiao"], "year": 2018, "n_citations": 19}
{"id": 4847511, "s2_id": "59e4f0f0e436364543558fdc88973b397235826c", "title": "OMI4papps: Optimisation, Modelling and Implementation for Highly Parallel Applications", "abstract": "This article reports on first results of the KONWIHR-II project OMI4papps at the Leibniz Supercomputing Centre (LRZ). The first part describes Apex-MAP, a tunable synthetic benchmark designed to simulate the performance of typical scientific applications. Apex-MAP mimics common memory access patterns and different computational intensity of scientific codes. An approach for modelling LRZ\u2019s application mix is given which makes use of performance counter measurements of real applications running on \u201cHLRB II\u201d, an SGI Altix system based on 9728 Intel Montecito dual-cores. The second part will show how the Apex-MAP benchmark could be used to simulate the performance of two mathematical kernels frequently used in scientific applications: a dense matrix-matrix multiplication and a sparse matrix-vector multiplication. The performance of both kernels has been intensively studied on x86 cores and hardware accelerators. We will compare the predicted performance with measured data to validate our Apex-MAP approach.", "venue": "ArXiv", "authors": ["Volker  Weinberg", "Matthias  Brehm", "Iris  Christadler"], "year": 2010, "n_citations": 2}
{"id": 4848632, "s2_id": "c6b276f2f14c5f74b549cc4c41f764ee401df631", "title": "Analysis of a trunk reservation policy in the framework of fog computing", "abstract": "We analyze in this paper a system composed of two data centers with limited capacity in terms of servers. When one request for a single server is blocked at the first data center, this request is forwarded to the second one. To protect the single server requests originally assigned to the second data center, a trunk reservation policy is introduced (i.e., a redirected request is accepted only if there is a sufficient number of free servers at the second data center). After rescaling the system by assuming that there are many servers in both data centers and high request arrival rates, we are led to analyze a random walk in the quarter plane, which has the particularity of having non constant reflecting conditions on one boundary of the quarter plane. Contrary to usual reflected random walks, to compute the stationary distribution of the presented random walk, we have to determine three unknown functions, one polynomial and two infinite generating functions. We show that the coefficients of the polynomial are solutions to a linear system. After solving this linear system, we are able to compute the two other unknown functions and the blocking probabilities at both data centers. Numerical experiments are eventually performed to estimate the gain achieved by the trunk reservation policy.", "venue": "ArXiv", "authors": ["Fabrice  Guillemin", "Guilherme  Thompson"], "year": 2016, "n_citations": 2}
{"id": 4852027, "s2_id": "f8d6be9607e5a8bc0dd6dd09607ef26cee48c9bf", "title": "Intelligent colocation of HPC workloads", "abstract": "Many HPC applications suffer from a bottleneck in the shared caches, instruction execution units, I/O or memory bandwidth, even though the remaining resources may be underutilized. It is hard for developers and runtime systems to ensure that all critical resources are fully exploited by a single application, so an attractive technique for increasing HPC system utilization is to colocate multiple applications on the same server. When applications share critical resources, however, contention on shared resources may lead to reduced application performance. In this paper, we show that server efficiency can be improved by first modeling the expected performance degradation of colocated applications based on measured hardware performance counters, and then exploiting the model to determine an optimized mix of colocated applications. This paper presents a new intelligent resource manager and makes the following contributions: (1) a new machine learning model to predict the performance degradation of colocated applications based on hardware counters and (2) an intelligent scheduling scheme deployed on an existing resource manager to enable application co-scheduling with minimum performance degradation. Our results show that our approach achieves performance improvements of 7 % (avg) and 12 % (max) compared to the standard policy commonly used by existing job managers.", "venue": "J. Parallel Distributed Comput.", "authors": ["Felippe V. Zacarias", "Vinicius  Petrucci", "Rajiv  Nishtala", "Paul  Carpenter", "Daniel Moss'e Universidade Federal da Bahia", "Universitat Politecnica de Catalunya", "Barcelona Supercomputing Center", "Coop", "NorwayNorwegian University of Science", "Technology", "Norway", "University of Pittsburgh"], "year": 2021, "n_citations": 1}
{"id": 4854725, "s2_id": "61f094a41e0dd9db5fcf9127de90d9543ca09a0f", "title": "Optimal control policies for resource allocation in the Cloud: comparison between Markov decision process and heuristic approaches", "abstract": "We consider an auto-scaling technique in a cloud system where virtual machines hosted on a physical node are turned on and off depending on the queue\u2019s occupation (or thresholds), in order to minimise a global cost integrating both energy consumption and performance. We propose several efficient optimisation methods to find threshold values minimising this global cost: local search heuristics coupled with aggregation of Markov chain and with queues approximation techniques to reduce the execution time and improve the accuracy. The second approach tackles the problem with a Markov Decision Process (MDP) for which we proceed to a theoretical study and provide theoretical comparison with the first approach. We also develop structured MDP algorithms integrating hysteresis properties. We show that MDP algorithms (value iteration, policy iteration) and especially structured MDP algorithms outperform the devised heuristics, in terms of time execution and accuracy. Finally, we propose a cost model for a real scenario of a cloud system to apply our optimisation algorithms and show their relevance.", "venue": "ArXiv", "authors": ["Thomas  Tournaire", "Hind  Castel-Taleb", "Emmanuel  Hyon"], "year": 2021, "n_citations": 0}
{"id": 4856032, "s2_id": "62c4172e829ae2964fc794dbe030f9a2da701b97", "title": "Joint Program and Layout Transformations to Enable Convolutional Operators on Specialized Hardware Based on Constraint Programming", "abstract": "The success of Deep Artificial Neural Networks (DNNs) in many domains created a rich body of research concerned with hardware accelerators for compute-intensive DNN operators. However, implementing such operators efficiently with complex hardware intrinsics such as matrix multiply is a task not yet automated gracefully. Solving this task often requires joint program and data layout transformations. First solutions to this problem have been proposed, such as TVM, UNIT, or ISAMIR, which work on a loop-level representation of operators and specify data layout and possible program transformations before the embedding into the operator is performed. This top-down approach creates a tension between exploration range and search space complexity, especially when also exploring data layout transformations such as im2col, channel packing, or padding.\n In this work, we propose a new approach to this problem. We created a bottom-up method that allows the joint transformation of both computation and data layout based on the found embedding. By formulating the embedding as a constraint satisfaction problem over the scalar dataflow, every possible embedding solution is contained in the search space. Adding additional constraints and optimization targets to the solver generates the subset of preferable solutions.\n An evaluation using the VTA hardware accelerator with the Baidu DeepBench inference benchmark shows that our approach can automatically generate code competitive to reference implementations. Further, we show that dynamically determining the data layout based on intrinsic and workload is beneficial for hardware utilization and performance. In cases where the reference implementation has low hardware utilization due to its fixed deployment strategy, we achieve a geomean speedup of up to \u00d7 2.813, while individual operators can improve as much as \u00d7 170.", "venue": "ACM Transactions on Architecture and Code Optimization", "authors": ["Dennis  Rieber", "Axel  Acosta", "Holger  Froning"], "year": 2022, "n_citations": 0}
{"id": 4856898, "s2_id": "2ded9340bf432a5a323286a700dc30d8c89e482e", "title": "Structured in Space, Randomized in Time: Leveraging Dropout in RNNs for Efficient Training", "abstract": "Recurrent Neural Networks (RNNs), more specifically their Long Short-Term Memory (LSTM) variants, have been widely used as a deep learning tool for tackling sequence-based learning tasks in text and speech. Training of such LSTM applications is computationally intensive due to the recurrent nature of hidden state computation that repeats for each time step. While sparsity in Deep Neural Nets has been widely seen as an opportunity for reducing computation time in both training and inference phases, the usage of non-ReLU activation in LSTM RNNs renders the opportunities for such dynamic sparsity associated with neuron activation and gradient values to be limited or non-existent. In this work, we identify dropout induced sparsity for LSTMs as a suitable mode of computation reduction. Dropout is a widely used regularization mechanism, which randomly drops computed neuron values during each iteration of training. We propose to structure dropout patterns, by dropping out the same set of physical neurons within a batch, resulting in column (row) level hidden state sparsity, which are well amenable to computation reduction at run-time in general-purpose SIMD hardware as well as systolic arrays. We provide a detailed analysis of how the dropoutinduced sparsity propagates through the different stages of network training and how it can be leveraged in each stage. More importantly, our proposed approach works as a direct replacement for existing dropout-based application settings. We conduct our experiments for three representative NLP tasks: language modelling on the PTB dataset, OpenNMT based machine translation using the IWSLT De-En and En-Vi datasets, and named entity recognition sequence labelling using the CoNLL-2003 shared task. We demonstrate that our proposed approach can be used to translate dropout-based computation reduction into reduced training time, with improvement ranging from 1.23\u00d7 to 1.64\u00d7, without sacrificing the target metric.", "venue": "ArXiv", "authors": ["Anup  Sarma", "Sonali  Singh", "Huaipan  Jiang", "Rui  Zhang", "Mahmut T Kandemir", "Chita R Das"], "year": 2021, "n_citations": 0}
{"id": 4861575, "s2_id": "e51617c0189704787566d0de07a5b13383b59784", "title": "Reinforcement Learning based Orchestration for Elastic Services", "abstract": "Due to the highly variable execution context in which edge services run, adapting their behavior to the execution context is crucial to comply with their requirements. However, adapting service behavior is a challenging task because it is hard to anticipate the execution contexts in which it will be deployed, as well as assessing the impact that each behavior change will produce. In order to provide this adaptation efficiently, we propose a Reinforcement Learning (RL) based Orchestration for Elastic Services. We implement and evaluate this approach by adapting an elastic service in different simulated execution contexts and comparing its performance to a Heuristics based approach. We show that elastic services achieve high precision and requirement satisfaction rates while creating an overhead of less than 0.5% to the overall service. In particular, the RL approach proves to be more efficient than its rule-based counterpart; yielding a 10 to 25% higher precision while being 25% less computationally expensive.", "venue": "2019 IEEE 5th World Forum on Internet of Things (WF-IoT)", "authors": ["Mauricio Fadel Argerich", "Bin  Cheng", "Jonathan  F\u00fcrst"], "year": 2019, "n_citations": 5}
{"id": 4861654, "s2_id": "6fce68d0448fb3d47844997617fa31bea9aef6ad", "title": "Enhancing Cloud Storage with Shareable Instances for Social Computing", "abstract": "Cloud storage plays an important role in social computing. This paper aims to develop a cloud storage management system for mobile devices to support an extended set of file operations. Because of the limit of storage, bandwidth, power consumption and other resource restrictions, most existing cloud storage apps for smartphones do not keep local copies of files. This efficient design, however, limits the application capacities. In this paper, we attempt to extend the available file operations for cloud storage service to better serve smartphone users. We develop an efficient and secure file management system, Skyfiles, to support more advanced file operations. The basic idea of our design is to utilize cloud instances to assist file operations. Particularly, Skyfiles supports downloading, compressing, encrypting, and converting operations, as well as file transfer between two smartphone users\u2019 cloud storage spaces. In addition, we propose a protocol for users to share their idle instances. All file operations supported by Skyfiles can be efficiently and securely accomplished with either a self-created instance or shared instance.", "venue": "ArXiv", "authors": ["Ying  Mao", "Peizhao  Hu"], "year": 2020, "n_citations": 0}
{"id": 4862409, "s2_id": "c5ff4810c5b9087142064ceca34b6188c416a8e4", "title": "Quality of Service Improvement for High-Speed Railway Communications", "abstract": "With the fast development of high-speed railways, a call for fulfilling the notion of communication at \"anytime, anywhere\" for high-speed train passengers in the Train Operating Control System is on the way. In order to make a realization of that, new railway wireless communication networks are needed. The most promising one is the Long Term Evolution for Railway which will provide broadband access, fast handover, and reliable communication for high mobility users. However, with the increase of speed, the system is subjected to high bit error rate, Doppler frequency shift and handover failure just like other system does. This paper is trying to solve these problems by employing MIMO technique. Specifically, the goal is to provide higher data rate, higher reliability, less delay, and other relative quality of services for passengers. MIMO performance analysis, resource allocation, and access control for handover and various services in a two-hop model are proposed in this paper. Analytical results and simulation results show that the proposed model and schemes perform well in improving the system performances.", "venue": "ArXiv", "authors": ["Yuzhe  Zhou", "Bo  Ai"], "year": 2014, "n_citations": 3}
{"id": 4864361, "s2_id": "17d0aa8bb9c38263581b7891a848c245a96d4265", "title": "Yelp Dataset Analysis using Scalable Big Data", "abstract": "Yelp has served and will continue to serve as a data-driven application. Yelp has published a datas et containing business information, reviews, user info rmation, and check-in information. This paper will examine this dataset to provide descriptive analytics to underst and business performance, geo-spatial distribution of b usinesses, reviewers' rating and other characteristics, and te mporal distribution of check-ins in business premises. Wit h these analysis we are able to establish that yelp reviews , tip , elite users and check ins have started to plummet over th e years. Coincidentally, the paper also establishes that Can adi s have a more stable star ratings as well as sentimen t rati gs when compared to Americans.", "venue": "ArXiv", "authors": ["Mohsen  Alam", "Benjamin  Cevallos", "Oscar  Flores", "Randall  Lunetto", "Kotaro  Yayoshi", "Jongwook  Woo"], "year": 2021, "n_citations": 0}
{"id": 4869944, "s2_id": "a08c03b1b18881f75365b0a35fe700ffd8330d88", "title": "An ODE for an Overloaded X Model Involving a Stochastic Averaging Principle", "abstract": "We study an ordinary differential equation (ODE) arising as the many-server heavy-traffic fluid limit of a sequence of overloaded Markovian queueing models with two customer classes and two service pools. The system, known as the X model in the call-center literature, operates under the fixed-queue-ratio-with-thresholds (FQR-T) control, which we proposed in a recent paper as a way for one service system to help another in face of an unanticipated overload. Each pool serves only its own class until a threshold is exceeded; then one-way sharing is activated with all customer-server assignments then driving the two queues toward a fixed ratio. For large systems, that fixed ratio is achieved approximately. The ODE describes system performance during an overload. The control is driven by a queue-difference stochastic process, which operates in a faster time scale than the queueing processes themselves, thus achieving a time-dependent steady state instantaneously in the limit. As a result, for the ODE, the driv...", "venue": "ArXiv", "authors": ["Ohad  Perry", "Ward  Whitt"], "year": 2010, "n_citations": 28}
{"id": 4875497, "s2_id": "88f008846c31cb5e750d666a925cfacffcabea87", "title": "Scaling Structured Multigrid to 500K+ Cores through Coarse-Grid Redistribution", "abstract": "The efficient solution of sparse, linear systems resulting from the discretization of partial differential equations is crucial to the performance of many physics-based simulations. The algorithmic optimality of multilevel approaches for common discretizations makes them a good candidate for an efficient parallel solver. Yet, modern architectures for high-performance computing systems continue to challenge the parallel scalability of multilevel solvers. While algebraic multigrid methods are robust for solving a variety of problems, the increasing importance of data locality and cost of data movement in modern architectures motivates the need to carefully exploit structure in the problem. \nRobust logically structured variational multigrid methods, such as Black Box Multigrid (BoxMG), maintain structure throughout the multigrid hierarchy. This avoids indirection and increased coarse-grid communication costs typical in parallel algebraic multigrid. Nevertheless, the parallel scalability of structured multigrid is challenged by coarse-grid problems where the overhead in communication dominates computation. In this paper, an algorithm is introduced for redistributing coarse-grid problems through incremental agglomeration. Guided by a predictive performance model, this algorithm provides robust redistribution decisions for structured multilevel solvers. \nA two-dimensional diffusion problem is used to demonstrate the significant gain in performance of this algorithm over the previous approach that used agglomeration to one processor. In addition, the parallel scalability of this approach is demonstrated on two large-scale computing systems, with solves on up to 500K+ cores.", "venue": "SIAM J. Sci. Comput.", "authors": ["Andrew  Reisner", "Luke N. Olson", "J. David Moulton"], "year": 2018, "n_citations": 5}
{"id": 4881434, "s2_id": "c9e406bf2042b5e1a0612b45be8d64b5503b0787", "title": "Graph-Based Minimum Dwell Time and Average Dwell Time Computations for Discrete-Time Switched Linear Systems", "abstract": "Discrete-time switched linear systems where switchings are governed by a digraph are considered. The minimum (or average) dwell time that guarantees the asymptotic stability can be computed by calculating the maximum cycle ratio (or maximum cycle mean) of a doubly weighted digraph where weights depend on the eigenvalues and eigenvectors of subsystem matrices. The graph-based method is applied to systems with defective subsystem matrices using Jordan decomposition. In the case of bimodal switched systems scaling algorithms that minimizes the condition number can be used to give a better minimum (or average) dwell time estimates.", "venue": "ArXiv", "authors": ["Ferruh  Ilhan", "\u00d6zkan  Karabacak"], "year": 2014, "n_citations": 0}
{"id": 4885204, "s2_id": "daebf10ba6f9749445a53d287e7750ccfa175cdb", "title": "Performance of wireless network coding: motivating small encoding numbers", "abstract": "This paper focuses on a particular transmission scheme called local network coding, which has been reported to provide significant performance gains in practical wireless networks. The performance of this scheme strongly depends on the network topology and thus on the locations of the wireless nodes. Also, it has been shown previously that finding the encoding strategy, which achieves maximum performance, requires complex calculations to be undertaken by the wireless node in real-time. \nBoth deterministic and random point pattern are explored and using the Boolean connectivity model we provide upper bounds for the maximum coding number, i.e., the number of packets that can be combined such that the corresponding receivers are able to decode. For the models studied, this upper bound is of order of $\\sqrt{N}$, where $N$ denotes the (mean) number of neighbors. Moreover, achievable coding numbers are provided for grid-like networks. We also calculate the multiplicative constants that determine the gain in case of a small network. Building on the above results, we provide an analytic expression for the upper bound of the efficiency of local network coding. The conveyed message is that it is favorable to reduce computational complexity by relying only on small encoding numbers since the resulting expected throughput loss is negligible.", "venue": "ArXiv", "authors": ["Petteri  Mannersalo", "Georgios S. Paschos", "Lazaros  Gkatzikis"], "year": 2010, "n_citations": 3}
{"id": 4886640, "s2_id": "7ad64d03a67a4604d4033389bdc63ef66659b360", "title": "On Delay-Optimal Scheduling in Queueing Systems with Replications", "abstract": "In modern computer systems, jobs are divided into short tasks and executed in parallel. Empirical observations in practical systems suggest that the task service times are highly random and the job service time is bottlenecked by the slowest straggling task. One common solution for straggler mitigation is to replicate a task on multiple servers and wait for one replica of the task to finish early. The delay performance of replications depends heavily on the scheduling decisions of when to replicate, which servers to replicate on, and which job to serve first. So far, little is understood on how to optimize these scheduling decisions for minimizing the delay to complete the jobs. In this paper, we present a comprehensive study on delay-optimal scheduling of replications in both centralized and distributed multi-server systems. Low-complexity scheduling policies are designed and are proven to be delay-optimal or near delay-optimal in stochastic ordering among all causal and non-preemptive policies. These theoretical results are established for general system settings and delay metrics that allow for arbitrary arrival processes, arbitrary job sizes, arbitrary due times, and heterogeneous servers with data locality constraints. Novel sample-path tools are developed to prove these results.", "venue": "ArXiv", "authors": ["Yin  Sun", "Can Emre Koksal", "Ness B. Shroff"], "year": 2016, "n_citations": 27}
{"id": 4887946, "s2_id": "76603f2fc161ef5776dc8024637c049557409c7e", "title": "Service Rate Region: A New Aspect of Coded Distributed System Design", "abstract": "Erasure coding has been recognized as a powerful method to mitigate delays due to slow or straggling nodes in distributed systems. This work shows that erasure coding of data objects can flexibly handle skews in the request rates. Coding can help boost the service rate region, that is, increase the overall volume of data access requests that the system can handle. This paper aims to postulate the service rate region as an important consideration in the design of erasure-coded distributed systems. We highlight several open problems that can be grouped into two broad threads: 1) characterizing the service rate region of a given code and finding the optimal request allocation, and 2) designing the underlying erasure code for a given service rate region. As contributions along the first thread, we find the rate regions of maximum-distance-separable, locally repairable, and simplex codes. We show the effectiveness of hybrid codes that combine replication and erasure coding in terms of code design. We also discover fundamental connections between multi-set batch codes and the problem of maximizing the service rate region.", "venue": "IEEE Transactions on Information Theory", "authors": ["Mehmet  Akta\u015f", "Gauri  Joshi", "Swanand  Kadhe", "Fatemeh  Kazemi", "Emina  Soljanin"], "year": 2021, "n_citations": 8}
{"id": 4889104, "s2_id": "277acf0b7e72febc198dd52762725bd996277fa9", "title": "Performance Analysis of Reliable Video Streaming with Strict Playout Deadline in Multi-Hop Wireless Networks", "abstract": "Motivated by emerging vision-based intelligent services, we consider the problem of rate adaptation for high quality and low delay visual information delivery over wireless networks using scalable video coding. Rate adaptation in this setting is inherently challenging due to the interplay between the variability of the wireless channels, the queuing at the network nodes and the frame-based decoding and playback of the video content at the receiver at very short time scales. To address the problem, we propose a low-complexity, model-based rate adaptation algorithm for scalable video streaming systems, building on a novel performance model based on stochastic network calculus. We validate the model using extensive simulations. We show that it allows fast, near optimal rate adaptation for fixed transmission paths, as well as cross-layer optimized routing and video rate adaptation in mesh networks, with less than $10$\\% quality degradation compared to the best achievable performance.", "venue": "ArXiv", "authors": ["Hussein  Al-Zubaidy", "Viktoria  Fodor", "Gy\u00f6rgy  D\u00e1n", "Markus  Flierl"], "year": 2017, "n_citations": 0}
{"id": 4889715, "s2_id": "16d3c1bff374f00213dee85690aa01519fb597d8", "title": "Exploring mutexes, the Oracle RDBMS retrial spinlocks", "abstract": "Spinlocks are widely used in database engines for processes synchronization. KGX mutexes is new retrial spinlocks appeared in contemporary Oracle versions for submicrosecond synchronization. The mutex contention is frequently observed in highly concurrent OLTP environments. \nThis work explores how Oracle mutexes operate, spin, and sleep. It develops predictive mathematical model and discusses parameters and statistics related to mutex performance tuning, as well as results of contention experiments.", "venue": "ArXiv", "authors": ["Andrey  Nikolaev"], "year": 2012, "n_citations": 0}
{"id": 4894188, "s2_id": "617aebac7057e82724a01817813be0061b63173f", "title": "Exploiting Parallelism in Optical Network Systems: A Case Study of Random Linear Network Coding (RLNC) in Ethernet-over-Optical Networks", "abstract": "As parallelism becomes critically important in the semiconductor technology, high-performance computing, and cloud applications, parallel network systems will increasingly follow suit. Today, parallelism is an essential architectural feature of 40/100/400 Gigabit Ethernet standards, whereby high speed Ethernet systems are equipped with multiple parallel network interfaces. This creates new network topology abstractions and new technology requirements: instead of a single high capacity network link, multiple Ethernet end-points and interfaces need to be considered together with multiple links in form of discrete parallel paths. This new paradigm is enabling implementations of various new features to improve overall system performance. In this paper, we analyze the performance of parallel network systems with network coding. In particular, by using random LNC (RLNC), - a code without the need for decoding, we can make use of the fact that we have codes that are both distributed (removing the need for coordination or optimization of resources) and composable (without the need to exchange code information), leading to a fully stateless operation. We propose a novel theoretical modeling framework, including derivation of the upper and lower bounds as well as an expected value of the differential delay of parallel paths, and the resulting queue size at the receiver. The results show a great promise of network system parallelism in combination with RLNC: with a proper set of design parameters, the differential delay and the buffer size at the Ethernet receiver can be reduced significantly, while the cross-layer design and routing can be greatly simplified.", "venue": "ArXiv", "authors": ["Anna  Engelmann", "Wolfgang  Bziuk", "Admela  Jukan", "Muriel  M\u00e9dard"], "year": 2017, "n_citations": 1}
{"id": 4903941, "s2_id": "e4341086af4fc13d5bf604ec5f414f4860b446d5", "title": "Pick the Right Edge Device: Towards Power and Performance Estimation of CUDA-based CNNs on GPGPUs", "abstract": "The emergence of Machine Learning (ML) as a powerful technique has been helping nearly all fields of business to increase operational efficiency or to develop new value propositions. Besides the challenges of deploying and maintaining ML models, picking the right edge device (e.g., GPGPUs) to run these models (e.g., CNN with the massive computational process) is one of the most pressing challenges faced by organizations today. As the cost of renting (on Cloud) or purchasing an edge device is directly connected to the cost of final products or services, choosing the most efficient device is essential. However, this decision making requires deep knowledge about performance and power consumption of the ML models running on edge devices that must be identified at the early stage of ML workflow. In this paper, we present a novel ML-based approach that provides ML engineers with the early estimation of both power consumption and performance of CUDA-based CNNs on GPGPUs. The proposed approach empowers ML engineers to pick the most efficient GPGPU for a given CNN model at the early stage of development.", "venue": "ArXiv", "authors": ["Christopher A. Metz", "Mehran  Goli", "Rolf  Drechsler"], "year": 2021, "n_citations": 1}
{"id": 4911555, "s2_id": "88cdd95b64ad757022e3973a942ca891e8fe1b6b", "title": "Workload-Aware Opportunistic Energy Efficiency in Multi-FPGA Platforms", "abstract": "The continuous growth of big data applications with high computational and scalability demands has resulted in increasing popularity of cloud computing. Optimizing the performance and power consumption of cloud resources is therefore crucial to relieve the costs of data centers. In recent years, multi-FPGA platforms have gained traction in data centers as low-cost yet high-performance solutions particularly as acceleration engines, thanks to the high degree of parallelism they provide. Nonetheless, the size of data centers workloads varies during service time, leading to significant underutilization of computing resources while consuming a large amount of power, which turns out as a key factor of data center inefficiency, regardless of the underlying hardware structure. In this paper, we propose an efficient framework to throttle the power consumption of multi-FPGA platforms by dynamically scaling the voltage and hereby frequency during runtime according to prediction of, and adjustment to the workload level, while maintaining the desired Quality of Service (QoS). This is in contrast to, and more efficient than, conventional approaches that merely scale (i.e., power-gate) the computing nodes or frequency. The proposed framework carefully exploits a pre-characterized library of delay-voltage, and power-voltage information of FPGA resources, which we show is indispensable to obtain the efficient operating point due to the different sensitivity of resources w.r.t. voltage scaling, particularly considering multiple power rails residing in these devices. Our evaluations by implementing state-of-the-art deep neural network accelerators revealed that, providing an average power reduction of 4.0\u00d7, the proposed framework surpasses the previous works by 33.6% (up to 83%).", "venue": "2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Sahand  Salamat", "Behnam  Khaleghi", "Mohsen  Imani", "Tajana  Simunic"], "year": 2019, "n_citations": 14}
{"id": 4911935, "s2_id": "6f93cd7ab35faeb5aea1379b6e01a1a176bce4bd", "title": "Energy of Computing on Multicore CPUs: Predictive Models and Energy Conservation Law", "abstract": "Energy is now a first-class design constraint along with performance in all computing settings. Energy predictive modelling based on performance monitoring counts (PMCs) is the leading method used for prediction of energy consumption during an application execution. We use a model-theoretic approach to formulate the assumed properties of existing models in a mathematical form. We extend the formalism by adding properties, heretofore unconsidered, that account for a limited form of energy conservation law. The extended formalism defines our theory of energy of computing. By applying the basic practical implications of the theory, we improve the prediction accuracy of state-of-the-art energy models from 31% to 18%. We also demonstrate that use of state-of-the-art measurement tools for energy optimisation may lead to significant losses of energy (ranging from 56% to 65% for applications used in experiments) since they do not take into account the energy conservation properties.", "venue": "ArXiv", "authors": ["Arsalan  Shahid", "Muhammad  Fahad", "Ravi  Reddy", "Alexey L. Lastovetsky"], "year": 2019, "n_citations": 3}
{"id": 4917674, "s2_id": "a7cdac5f92ebfa5c42399cb131e97c35cbd934c0", "title": "Faster Concurrent Range Queries with Contention Adapting Search Trees Using Immutable Data", "abstract": "The need for scalable concurrent ordered set data structures with linearizable range query support is increasing due to the rise of multicore computers, data processing platforms and in-memory databases. This paper presents a new concurrent ordered set with linearizable range query support. The new data structure is based on the contention adapting search tree and an immutable data structure. Experimental results show that the new data structure is as much as three times faster compared to related data structures. The data structure scales well due to its ability to adapt the sizes of its immutable parts to the contention level and the sizes of the range queries.", "venue": "ICCSW", "authors": ["Kjell  Winblad"], "year": 2017, "n_citations": 6}
{"id": 4917807, "s2_id": "0cac1522c5679ca51ea7514e3be02223e1eb219e", "title": "Quality Control Methodology for Simulation Models of Computer Network Protocols", "abstract": "This paper summarizes know-how about modeling and simulation of computer networking protocols we contributed to the OMNeT++ community. We propose a methodology aiming to set a reliable ground truth for the quality of simulation models of networking protocols. We demonstrate the application of this methodology on our EIGRP source code pull-requested to the INET framework.", "venue": "ArXiv", "authors": ["Vladim\u00edr  Vesel\u00fd", "Jan  Zavrel"], "year": 2021, "n_citations": 0}
{"id": 4922665, "s2_id": "fa3c6e29c5449a6ece0889ac60db9a36907eb461", "title": "Trust Computational Heuristic for Social Internet of Things: A Machine Learning-based Approach", "abstract": "The Internet of Things (IoT) is an evolving network of billions of interconnected physical objects, such as, numerous sensors, smartphones, wearables, and embedded devices. These physical objects, generally referred to as the smart objects, when deployed in real-world aggregates useful information from their surrounding environment. As-of-late, this notion of IoT has been extended to incorporate the social networking facets which have led to the promising paradigm of the \u2018Social Internet of Things\u2019 (SIoT). In SIoT, the devices operate as an autonomous agent and provide an exchange of information and services discovery in an intelligent manner by establishing social relationships among them with respect to their owners. Trust plays an important role in establishing trustworthy relationships among the physical objects and reduces probable risks in the decision making process. In this paper, a trust computational model is proposed to extract individual trust features in a SIoT environment. Furthermore, a machine learning-based heuristic is used to aggregate all the trust features in order to ascertain an aggregate trust score. Simulation results illustrate that the proposed trust-based model isolates the trustworthy and untrustworthy nodes within the network in an efficient manner.", "venue": "ICC 2020 - 2020 IEEE International Conference on Communications (ICC)", "authors": ["Subhash  Sagar", "Adnan  Mahmood", "Quan Z. Sheng", "Wei Emma Zhang"], "year": 2020, "n_citations": 4}
{"id": 4923099, "s2_id": "9f4f059c9a509a7e509615c255d44fb8f1fa641b", "title": "Enabling highly scalable remote memory access programming with MPI-3 one sided", "abstract": "Modern high-performance networks offer remote direct memory access (RDMA) that exposes a process' virtual address space to other processes in the network. The Message Passing Interface (MPI) specification has recently been extended with a programming interface called MPI-3 Remote Memory Access (MPI-3 RMA) for efficiently exploiting state-of-the-art RDMA features. MPI-3 RMA enables a powerful programming model that alleviates many message passing downsides. In this work, we design and develop bufferless protocols that demonstrate how to implement this interface and support scaling to millions of cores with negligible memory consumption while providing highest performance and minimal overheads. To arm programmers, we provide a spectrum of performance models for RMA functions that enable rigorous mathematical analysis of application performance and facilitate the development of codes that solve given tasks within specified time and energy budgets. We validate the usability of our library and models with several application studies with up to half a million processes. In a wider sense, our work illustrates how to use RMA principles to accelerate computation- and data-intensive codes.", "venue": "Commun. ACM", "authors": ["Robert  Gerstenberger", "Maciej  Besta", "Torsten  Hoefler"], "year": 2018, "n_citations": 13}
{"id": 4926830, "s2_id": "c91c3f7a793bccf037436f51d2cc524fecb347b0", "title": "The LDBC Social Network Benchmark", "abstract": "The Linked Data Benchmark Council's Social Network Benchmark (LDBC SNB) is an effort intended to test various functionalities of systems used for graph-like data management. For this, LDBC SNB uses the recognizable scenario of operating a social network, characterized by its graph-shaped data. LDBC SNB consists of two workloads that focus on different functionalities: the Interactive workload (interactive transactional queries) and the Business Intelligence workload (analytical queries). This document contains the definition of the Interactive Workload and the first draft of the Business Intelligence Workload. This includes a detailed explanation of the data used in the LDBC SNB benchmark, a detailed description for all queries, and instructions on how to generate the data and run the benchmark with the provided software.", "venue": "ArXiv", "authors": ["Renzo  Angles", "J\u00e1nos Benjamin Antal", "Alex  Averbuch", "Peter A. Boncz", "Orri  Erling", "Andrey  Gubichev", "Vlad  Haprian", "Moritz  Kaufmann", "Josep-Llu\u00eds  Larriba-Pey", "Norbert  Mart\u00ednez-Bazan", "J\u00f3zsef  Marton", "Marcus  Paradies", "Minh-Duc  Pham", "Arnau  Prat-P\u00e9rez", "Mirko  Spasic", "Benjamin A. Steer", "G\u00e1bor  Sz\u00e1rnyas", "Jack  Waudby"], "year": 2020, "n_citations": 11}
{"id": 4933131, "s2_id": "1cf0522b83f523487b09dadeba2762549fcc4921", "title": "Toward a Standard Interface for User-Defined Scheduling in OpenMP", "abstract": "Parallel loops are an important part of OpenMP programs. Efficient scheduling of parallel loops can improve performance of the programs. The current OpenMP specification only offers three options for loop scheduling, which are insufficient in certain instances. Given the large number of other possible scheduling strategies, it is infeasible to standardize each one. A more viable approach is to extend the OpenMP standard to allow for users to define loop scheduling strategies. The approach will enable standard-compliant application-specific scheduling. This work analyzes the principal components required by user-defined scheduling and proposes two competing interfaces as candidates for the OpenMP standard. We conceptually compare the two proposed interfaces with respect to the three host languages of OpenMP, i.e., C, C++, and Fortran. These interfaces serve the OpenMP community as a basis for discussion and prototype implementation for user-defined scheduling.", "venue": "IWOMP", "authors": ["Vivek  Kale", "Christian  Iwainsky", "Michael  Klemm", "Jonas H. M\u00fcller Kornd\u00f6rfer", "Florina M. Ciorba"], "year": 2019, "n_citations": 7}
{"id": 4940022, "s2_id": "5d0c37a847e27fdc5d37be629e066fb71e5bc1ba", "title": "Efficient cache use for stencil operations on structured discretization grids", "abstract": "We derive tight bounds on the cache misses for evaluation of explicit stencil operators on structured grids. Our lower bound is based on the isoperimetrical property of the discrete octahedron. Our upper bound is based on a good surface to volume ratio of a parallelepiped spanned by a reduced basis of the interference lattice of a grid. Measurements show that our algorithm typically reduces the number of cache misses by a factor of three, relative to a compiler optimized code. We show that stencil calculations on grids whose interference lattice have a short vector feature abnormally high numbers of cache misses. We call such grids unfavorable and suggest to avoid these in computations by appropriate padding. By direct measurements on a MIPS R10000 processor we show a good correlation between abnormally high numbers of cache misses and unfavorable three-dimensional grids.", "venue": "ArXiv", "authors": ["Michael A. Frumkin", "Rob F. Van der Wijngaart"], "year": 2000, "n_citations": 8}
{"id": 4949057, "s2_id": "aa144531c5b69f0ac7481cec01da7503424b2c1c", "title": "Stochastic Automata Network for Performance Evaluation of Heterogeneous SoC Communication", "abstract": "To meet ever increasing demand for performance of emerging System-on-Chip (SoC) applications, designer employ techniques for concurrent communication between components. Hence communication architecture becomes complex and major performance bottleneck. An early performance evaluation of communication architecture is the key to reduce design time, time-to-market and consequently cost of the system. Moreover, it helps to optimize system performance by selecting appropriate communication architecture. However, performance model of concurrent communication is complex to describe and hard to solve. In this paper, we propose methodology for performance evaluation of bus based communication architectures, modeling for which is based on modular Stochastic Automata Network (SAN). We employ Generalized Semi Markov Process (GSMP) model for each module of the SAN that emulates dynamic behavior of a Processing Element (PE) of an SoC architecture. The proposed modeling approach provides an early estimation of performance parameters viz. memory bandwidth, average queue length at memory and average waiting time seen by a processing element; while we provide parameters viz. number of processing elements, the mean computation time of processing elements and the first and second moments of connection time between processing elements and memories, as input to the model.", "venue": "2008 NORCHIP", "authors": ["U.  Deshmukh", "V.  Sahula"], "year": 2008, "n_citations": 1}
{"id": 4950447, "s2_id": "c786643fbf92f8b41da13b5cb630459f475f904f", "title": "A Tighter Real-Time Communication Analysis for Wormhole-Switched Priority-Preemptive NoCs", "abstract": "Simulations and runtime measurements are some of the methods which can be used to evaluate whether a given NoC-based platform can accommodate application workload and fulfil its timing requirements. Yet, these techniques are often time-consuming, and hence can evaluate only a limited set of scenarios. Therefore, these approaches are not suitable for safety-critical and hard real-time systems, where one of the fundamental requirements is to provide strong guarantees that all timing requirements will always be met, even in the worst-case conditions. For such systems the analytic-based real-time analysis is the only viable approach. \nIn this paper the focus is on the real-time communication analysis for wormhole-switched priority-preemptive NoCs. First, we elaborate on the existing analysis and identify one source of pessimism. Then, we propose an extension to the analysis, which efficiently overcomes this limitation, and allows for a less pessimistic analysis. Finally, through a comprehensive experimental evaluation, we compare the newly proposed approach against the existing one, and also observe how the trends change with different traffic parameters.", "venue": "ArXiv", "authors": ["Borislav  Nikolic", "Leandro Soares Indrusiak", "Stefan M. Petters"], "year": 2016, "n_citations": 6}
{"id": 4951465, "s2_id": "4af23e70a01e7a814d350c7f147ade713ab130c1", "title": "Matching Impatient and Heterogeneous Demand and Supply", "abstract": "Service platforms must determine rules for matching heterogeneous demand (customers) and supply (workers) that arrive randomly over time and may be lost if forced to wait too long for a match. We show how to balance the trade-off between making a less good match quickly and waiting for a better match, at the risk of losing impatient customers and/or workers. When the objective is to maximize the cumulative value of matches over a finite-time horizon, we propose discrete-review matching policies, both for the case in which the platform has access to arrival rate parameter information and the case in which the platform does not. We show that both the blind and nonblind policies are asymptotically optimal in a high-volume setting. However, the blind policy requires frequent re-solving of a linear program. For that reason, we also investigate a blind policy that makes decisions in a greedy manner, and we are able to establish an asymptotic lower bound for the greedy, blind policy that depends on the matching values and is always higher than half of the value of an optimal policy. Next, we develop a fluid model that approximates the evolution of the stochastic model and captures explicitly the nonlinear dependence between the amount of demand and supply waiting and the distribution of their patience times. We use the fluid model to propose a policy for a more general objective that additionally penalizes queue build-up. We run numerous simulations to investigate the performance of the aforementioned proposed matching policies.", "venue": "ArXiv", "authors": ["Angelos  Aveklouris", "Levi  DeValve", "Amy R. Ward", "Xiaofan  Wu"], "year": 2021, "n_citations": 4}
{"id": 4952324, "s2_id": "a5fc4f3d8678b29e9653aec87d7eae079a002864", "title": "MLPerf Inference Benchmark", "abstract": "Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark\u2019s flexibility and adaptability.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Vijay Janapa Reddi", "Christine  Cheng", "David  Kanter", "Peter  Mattson", "Guenther  Schmuelling", "Carole-Jean  Wu", "Brian  Anderson", "Maximilien  Breughe", "Mark  Charlebois", "William  Chou", "Ramesh  Chukka", "Cody  Coleman", "Sam  Davis", "Pan  Deng", "Greg  Diamos", "Jared  Duke", "Dave  Fick", "J. Scott Gardner", "Itay  Hubara", "Sachin  Idgunji", "Thomas B. Jablin", "Jeff  Jiao", "Tom St. John", "Pankaj  Kanwar", "David  Lee", "Jeffery  Liao", "Anton  Lokhmotov", "Francisco  Massa", "Peng  Meng", "Paulius  Micikevicius", "Colin  Osborne", "Gennady  Pekhimenko", "Arun Tejusve Raghunath Rajan", "Dilip  Sequeira", "Ashish  Sirasao", "Fei  Sun", "Hanlin  Tang", "Michael  Thomson", "Frank  Wei", "Ephrem  Wu", "Lingjie  Xu", "Koichi  Yamada", "Bing  Yu", "George  Yuan", "Aaron  Zhong", "Peizhao  Zhang", "Yuchen  Zhou"], "year": 2020, "n_citations": 143}
{"id": 4953318, "s2_id": "097db1009c5d0be3ff666647e5421626ec27b60f", "title": "On Modelling and Prediction of Total CPU Usage for Applications in MapReduce Environments", "abstract": "Recently, businesses have started using MapReduce as a popular computation framework for processing large amount of data, such as spam detection, and different data mining tasks, in both public and private clouds. Two of the challenging questions in such environments are (1) choosing suitable values for MapReduce configuration parameters --- e.g., number of mappers, number of reducers, and DFS block size---, and (2) predicting the amount of resources that a user should lease from the service provider. Currently, the tasks of both choosing configuration parameters and estimating required resources are solely the users' responsibilities. In this paper, we present an approach to provision the total CPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce job, a profile of total CPU usage in clock cycles is built from the job past executions with different values of two configuration parameters e.g., number of mappers, and number of reducers. Then, a polynomial regression is used to model the relation between these configuration parameters and total CPU usage in clock cycles of the job. We also briefly study the influence of input data scaling on measured total CPU usage in clock cycles. This derived model along with the scaling result can then be used to provision the total CPU usage in clock cycles of the same jobs with different input data size. We validate the accuracy of our models using three realistic applications (WordCount, Exim MainLog parsing, and TeraSort). Results show that the predicted total CPU usage in clock cycles of generated resource provisioning options are less than 8% of the measured total CPU usage in clock cycles in our 20-node virtual Hadoop cluster.", "venue": "ICA3PP", "authors": ["Nikzad Babaii Rizvandi", "Javid  Taheri", "Reza  Moraveji", "Albert Y. Zomaya"], "year": 2012, "n_citations": 29}
{"id": 4953995, "s2_id": "c7b19e45399f320f1e9a8fe1767a8786be52e603", "title": "Performance Analysis of Load Balancing Policies with Memory", "abstract": "Joining the shortest or least loaded queue among d randomly selected queues are two fundamental load balancing policies. Under both policies the dispatcher does not maintain any information on the queue length or load of the servers. In this paper we analyze the performance of these policies when the dispatcher has some memory available to store the ids of some of the idle servers. We consider methods where the dispatcher discovers idle servers as well as methods where idle servers inform the dispatcher about their state. We focus on large-scale systems and our analysis uses the cavity method. The main insight provided is that the performance measures obtained via the cavity method for a load balancing policy with memory reduce to the performance measures for the same policy without memory provided that the arrival rate is properly scaled. Thus, we can study the performance of load balancers with memory in the same manner as load balancers without memory. In particular this entails closed form solutions for joining the shortest or least loaded queue among d randomly selected queues with memory in case of exponential job sizes. We present simulation results that support our belief that the approximation obtained by the cavity method becomes exact as the number of servers tends to infinity.", "venue": "VALUETOOLS", "authors": ["Tim  Hellemans", "Benny Van Houdt"], "year": 2020, "n_citations": 4}
{"id": 4957744, "s2_id": "5d97e72a019fac33f325e7c02ebeee14df3777cf", "title": "Kerncraft: A Tool for Analytic Performance Modeling of Loop Kernels", "abstract": "Achieving optimal program performance requires deep insight into the interaction between hardware and software. For software developers without an in-depth background in computer architecture, understanding and fully utilizing modern architectures is close to impossible. Analytic loop performance modeling is a useful way to understand the relevant bottlenecks of code execution based on simple machine models. The Roofline Model and the Execution-Cache-Memory (ECM) model are proven approaches to performance modeling of loop nests. In comparison to the Roofline model, the ECM model can also describes the single-core performance and saturation behavior on a multicore chip.We give an introduction to the Roofline and ECM models, and to stencil performance modeling using layer conditions (LC). We then present Kerncraft, a tool that can automatically construct Roofline and ECM models for loop nests by performing the required code, data transfer, and LC analysis. The layer condition analysis allows to predict optimal spatial blocking factors for loop nests. Together with the models it enables an ab-initio estimate of the potential benefits of loop blocking optimizations and of useful block sizes. In cases where LC analysis is not easily possible, Kerncraft supports a cache simulator as a fallback option. Using a 25-point long-range stencil we demonstrate the usefulness and predictive power of the Kerncraft tool.", "venue": "ArXiv", "authors": ["Julian  Hammer", "Jan  Eitzinger", "Georg  Hager", "Gerhard  Wellein"], "year": 2017, "n_citations": 29}
{"id": 4958277, "s2_id": "b60e894f732e3ae92ba2efc649a53deb1db1ded5", "title": "GOSH: Task Scheduling using Deep Surrogate Models in Fog Computing Environments", "abstract": "Recently, intelligent scheduling approaches using surrogate models have been proposed to efficiently allocate volatile tasks in heterogeneous fog environments. Advances like deterministic surrogate models, deep neural networks (DNN) and gradient-based optimization allow low energy consumption and response times to be reached. However, deterministic surrogate models, which estimate objective values for optimization, do not consider the uncertainties in the distribution of the Quality of Service (QoS) objective function that can lead to high Service Level Agreement (SLA) violation rates. Moreover, the brittle nature of DNN training and the limited exploration with low agility in gradient-based optimization prevent such models from reaching minimal energy or response times. To overcome these difficulties, we present a novel scheduler that we call GOSH for Gradient Based Optimization using Second Order derivatives and Heteroscedastic Deep Surrogate Models. GOSH uses a second-order gradient based optimization approach to obtain better QoS and reduce the number of iterations to converge to a scheduling decision, subsequently lowering the scheduling time. Instead of a vanilla DNN, GOSH uses a Natural Parameter Network (NPN) to approximate objective scores. Further, a Lower Confidence Bound (LCB) optimization approach allows GOSH to find an optimal trade-off between greedy minimization of the mean latency and uncertainty reduction by employing error-based exploration. Thus, GOSH and its co-simulation based extension GOSH*, can adapt quickly and reach better objective scores than baseline methods. We show that GOSH* reaches better objective scores than GOSH, but it is suitable only for high resource availability settings, whereas GOSH is apt for limited resource settings. Real system experiments for both GOSH and GOSH* show significant improvements against the state-of-the-art in terms of energy consumption, response time and SLA violations by up to 18, 27 and 82 percent, respectively.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Shreshth  Tuli", "Giuliano  Casale", "Nicholas R. Jennings"], "year": 2021, "n_citations": 0}
{"id": 4958385, "s2_id": "087c0601dd4d6a600d55f3c4d4de6c0cbdc0870d", "title": "Optimal Multiserver Scheduling with Unknown Job Sizes in Heavy Traffic", "abstract": "We consider scheduling to minimize mean response time of the M/G/k queue with unknown job sizes. In the single-server case, the optimal policy is the Gittins policy, but it is not known whether Gittins or any other policy is optimal in the multiserver case. Exactly analyzing the M/G/k under any scheduling policy is intractable, and Gittins is a particularly complicated policy that is hard to analyze even in the single-server case. \nIn this work we introduce monotonic Gittins (M-Gittins), a new variation of the Gittins policy, and show that it minimizes mean response time in the heavy-traffic M/G/k for a wide class of finite-variance job size distributions. We also show that the monotonic shortest expected remaining processing time (M-SERPT) policy, which is simpler than M-Gittins, is a 2-approximation for mean response time in the heavy traffic M/G/k under similar conditions. These results constitute the most general optimality results to date for the M/G/k with unknown job sizes. Our techniques build upon work by Grosof et al., who study simple policies, such as SRPT, in the M/G/k; Bansal et al., Kamphorst and Zwart, and Lin et al., who analyze mean response time scaling of simple policies in the heavy-traffic M/G/1; and Aalto et al. and Scully et al., who characterize and analyze the Gittins policy in the M/G/1.", "venue": "Perform. Evaluation", "authors": ["Ziv  Scully", "Isaac  Grosof", "Mor  Harchol-Balter"], "year": 2021, "n_citations": 12}
{"id": 4959372, "s2_id": "cf6395084ca596e4d0fbcccf139a0cb384d69ab0", "title": "On partially homogeneous nearest-neighbour random walks in the quarter plane and their application in the analysis of two-dimensional queues with limited state-dependency", "abstract": "This work deals with the stationary analysis of two-dimensional partially homogeneous nearest-neighbour random walks. Such type of random walks are characterized by the fact that the one-step transition probabilities are functions of the state-space. We show that its stationary behaviour is investigated by solving a finite system of linear equations, two matrix functional equations, and a functional equation with the aid of the theory of Riemann (-Hilbert) boundary value problems. This work is strongly motivated by emerging applications in flow level performance of wireless networks that give rise in queueing models with scalable service capacity, as well as in queue-based random access protocols, where the network's parameters are functions of the queue lengths. A simple numerical illustration, along with some details on the numerical implementation are also presented.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Ioannis  Dimitriou"], "year": 2021, "n_citations": 0}
{"id": 4959373, "s2_id": "ec783917b751c43cb4d3de1bee2671ff0905eca1", "title": "Applying stochastic network calculus to 802.11 backlog and delay analysis", "abstract": "Stochastic network calculus provides an elegant way to characterize traffic and service processes. However, little effort has been made on applying it to multi-access communication systems such as 802.11. In this paper, we take the first step to apply it to the backlog and delay analysis of an 802.11 wireless local network. We found the derived bounds are very loose in comparison to ns-2 simulation results especially for heavy arriving traffic, indicating that improvements are needed for the current version of stochastic network calculus.", "venue": "2011 IEEE Nineteenth IEEE International Workshop on Quality of Service", "authors": ["Yue  Wang", "Tianmei  Wang"], "year": 2011, "n_citations": 8}
{"id": 4963407, "s2_id": "fb185ce68fd443e640671a0ffd73577ccbb06098", "title": "Roaming Real-Time Applications - Mobility Services in IPv6 Networks", "abstract": "Emerging mobility standards within the next generation Internet Protocol, IPv6, promise to continuously operate devices roaming between IP networks. Associated with the paradigm of ubiquitous computing and communication, network technology is on the spot to deliver voice and videoconferencing as a standard internet solution. However, current roaming procedures are too slow, to remain seamless for real-time applications. Multicast mobility still waits for a convincing design. This paper investigates the temporal behaviour of mobile IPv6 with dedicated focus on topological impacts. Extending the hierarchical mobile IPv6 approach we suggest protocol improvements for a continuous handover, which may serve bidirectional multicast communication, as well. Along this line a multicast mobility concept is introduced as a service for clients and sources, as they are of dedicated importance in multipoint conferencing applications. The mechanisms introduced do not rely on assumptions of any specific multicast routing protocol in use.", "venue": "ArXiv", "authors": ["Thomas C. Schmidt", "Matthias  W\u00e4hlisch"], "year": 2004, "n_citations": 14}
{"id": 4964868, "s2_id": "34c5a6b061995a4455111dcb7eb04fa8d91b8a10", "title": "Quality and Cost of Deterministic Network Calculus: Design and Evaluation of an Accurate and Fast Analysis", "abstract": "Networks are integral parts of modern safety-critical systems and certification demands the provision of guarantees for data transmissions. Deterministic Network Calculus (DNC) can compute a worst-case bound on a data flow's end-to-end delay. Accuracy of DNC results has been improved steadily, resulting in two DNC branches: the classical algebraic analysis (algDNC) and the more recent optimization-based analysis (optDNC). The optimization-based branch provides a theoretical solution for tight bounds. Its computational cost grows, however, (possibly super-)exponentially with the network size. Consequently, a heuristic optimization formulation trading accuracy against computational costs was proposed. In this paper, we challenge optimization-based DNC with a novel algebraic DNC algorithm. We show that: (1) no current optimization formulation scales well with the network size and (2) algebraic DNC can be considerably improved in both aspects, accuracy and computational cost. To that end, we contribute a novel DNC algorithm that transfers the optimization's search for best attainable delay bounds to algebraic DNC. It achieves a high degree of accuracy and our novel efficiency improvements reduce the cost of the analysis dramatically. In extensive numerical experiments, we observe that our delay bounds deviate from the optimization-based ones by only 1.142% on average while computation times simultaneously decrease by several orders of magnitude.", "venue": "SIGMETRICS 2017", "authors": ["Steffen  Bondorf", "Paul  Nikolaus", "Jens B. Schmitt"], "year": 2016, "n_citations": 24}
{"id": 4966506, "s2_id": "296a6e321f4f19f1fa8ef0a78a14d4a521955d67", "title": "Explainable Multi-class Classification of Medical Data", "abstract": "Machine Learning applications have brought new insights into a secondary analysis of medical data. Machine Learning helps to develop new drugs, define populations susceptible to certain illnesses, identify predictors of many common diseases. At the same time, Machine Learning results depend on convolution of many factors, including feature selection, class (im)balance, algorithm preference, and performance metrics. In this paper, we present explainable multi-class classification of a large medical data set. We in details discuss knowledge-based feature engineering, data set balancing, best model selection, and parameter tuning. Six algorithms are used in this study: Support Vector Machine (SVM), Na\u00efve Bayes, Gradient Boosting, Decision Trees, Random Forest, and Logistic Regression. Our empirical evaluation is done on the UCI Diabetes 130-US hospitals for years 1999-2008 dataset, with the task to classify patient hospital re-admission stay into three classes: 0 days, <30 days, or > 30 days. Our results show that using 23 medication features in learning experiments improves Recall of five out of the six applied learning algorithms. This is a new result that expands the previous studies conducted on the same data. Gradient Boosting and Random Forest outperformed other algorithms in terms of the three-class classification Accuracy.", "venue": "ArXiv", "authors": ["YuanZheng  Hu", "Marina  Sokolova"], "year": 2020, "n_citations": 2}
{"id": 4967602, "s2_id": "021897eebed666449bf2a08643d99211a9ade08b", "title": "Asymptotic analysis of the sojourn time of a batch in an M[X]/M/1 Processor Sharing Queue", "abstract": "In this paper, we exploit results obtained in an earlier study for the Laplace transform of the sojourn time \u03a9 of an entire batch in the M [X]/M/1 Processor Sharing (PS) queue in order to derive the asymptotic behavior of the complementary probability distribution function of this random variable, namely the behavior of P(\u03a9 > x) when x tends to infinity. We precisely show that up to a multiplying factor, the behavior of P(\u03a9 > x) for large x is of the same order of magnitude as P(\u03c9 > x), where \u03c9 is the sojourn time of an arbitrary job is the system. From a practical point of view, this means that if a system has to be dimensioned to guarantee processing time for jobs then the system can also guarantee processing times for entire batches by introducing a marginal amount of processing capacity.", "venue": "ArXiv", "authors": ["Fabrice  Guillemin", "Alain  Simonian", "Ridha  Nasri", "Veronica Quintuna Rodriguez"], "year": 2021, "n_citations": 2}
{"id": 4973317, "s2_id": "c0c3772806496bb2470fb3e2bc147265910d650a", "title": "FPDetect: Efficient Reasoning About Stencil Programs Using Selective Direct Evaluation", "abstract": "We present FPDetect, a low overhead approach for detecting logical errors and soft errors affecting stencil computations without generating false positives. We develop an offline analysis that tightly estimates the number of floating-point bits preserved across stencil applications. This estimate rigorously bounds the values expected in the data space of the computation. Violations of this bound can be attributed with certainty to errors. FPDetect helps synthesize error detectors customized for user-specified levels of accuracy and coverage. FPDetect also enables overhead reduction techniques based on deploying these detectors coarsely in space and time. Experimental evaluations demonstrate the practicality of our approach.", "venue": "ACM Trans. Archit. Code Optim.", "authors": ["Arnab  Das", "Sriram  Krishnamoorthy", "Ian  Briggs", "Ganesh  Gopalakrishnan", "Ramakrishna  Tipireddy"], "year": 2020, "n_citations": 3}
{"id": 4975832, "s2_id": "c7ee830908a21473a0b2d1d198cd1cce79271f57", "title": "Runtime Performances Benchmark for Knowledge Graph Embedding Methods", "abstract": "This paper wants to focus on providing a characterization of the runtime performances of state-of-the-art implementations of KGE alghoritms, in terms of memory footprint and execution time. Despite the rapidly growing interest in KGE methods, so far little attention has been devoted to their comparison and evaluation; in particular, previous work mainly focused on performance in terms of accuracy in specific tasks, such as link prediction. To this extent, a framework is proposed for evaluating available KGE implementations against graphs with different properties, with a particular focus on the effectiveness of the adopted optimization strategies. Graphs and models have been trained leveraging different architectures, in order to enlighten features and properties of both models and the architectures they have been trained on. Some results enlightened with experiments in this document are the fact that multithreading is efficient, but benefit deacreases as the number of threads grows in case of CPU. GPU proves to be the best architecture for the given task, even if CPU with some vectorized instructions still behaves well. Finally, RAM utilization for the loading of the graph never changes between different architectures and depends only on the type of graph, not on the model.", "venue": "ArXiv", "authors": ["Angelica Sofia Valeriani"], "year": 2020, "n_citations": 0}
{"id": 4979573, "s2_id": "df26076f79067e0ab2663ed074c933f9aa0ef534", "title": "Revisiting 802.11 Rate Adaptation from Energy Consumption's Perspective", "abstract": "Rate adaptation in 802.11 WLANs has received a lot of attention from the research community, with most of the proposals aiming at maximising throughput based on network conditions. Considering energy consumption, an implicit assumption is that optimality in throughput implies optimality in energy efficiency, but this assumption has been recently put into question. In this paper, we address via analysis and experimentation the relation between throughput performance and energy efficiency in multi-rate 802.11 scenarios. We demonstrate the trade-off between these performance figures, confirming that they may not be simultaneously optimised, and analyse their sensitivity towards the energy consumption parameters of the device. Our results provide the means to design novel rate adaptation schemes that takes energy consumption into account.", "venue": "MSWiM", "authors": ["I\u00f1aki  Ucar", "Carlos  Donato", "Pablo  Serrano", "Andres  Garcia-Saavedra", "Arturo  Azcorra", "Albert  Banchs"], "year": 2016, "n_citations": 7}
{"id": 4983192, "s2_id": "94654b4fe3dc2a410c70e2262d394769fe06e8cf", "title": "Memory transfer optimization for a lattice Boltzmann solver on Kepler architecture nVidia GPUs", "abstract": "Abstract The Lattice Boltzmann method (LBM) for solving fluid flow is naturally well suited to an efficient implementation for massively parallel computing, due to the prevalence of local operations in the algorithm. This paper presents and analyses the performance of a 3D lattice Boltzmann solver, optimized for third generation nVidia GPU hardware, also known as \u2018Kepler\u2019. We provide a review of previous optimization strategies and analyse data read/write times for different memory types. In LBM, the time propagation step (known as streaming), involves shifting data to adjacent locations and is central to parallel performance; here we examine three approaches which make use of different hardware options. Two of which make use of \u2018performance enhancing\u2019 features of the GPU; shared memory and the new shuffle instruction found in Kepler based GPUs. These are compared to a standard transfer of data which relies instead on optimized storage to increase coalesced access. It is shown that the more simple approach is most efficient; since the need for large numbers of registers per thread in LBM limits the block size and thus the efficiency of these special features is reduced. Detailed results are obtained for a D3Q19 LBM solver, which is benchmarked on nVidia K5000M and K20C GPUs. In the latter case the use of a read-only data cache is explored, and peak performance of over 1036 Million Lattice Updates Per Second (MLUPS) is achieved. The appearance of a periodic bottleneck in the solver performance is also reported, believed to be hardware related; spikes in iteration-time occur with a frequency of around 11\u00a0Hz for both GPUs, independent of the size of the problem.", "venue": "Comput. Phys. Commun.", "authors": ["Mark  Mawson", "Alistair J. Revell"], "year": 2014, "n_citations": 36}
{"id": 4985215, "s2_id": "4a29d4c34fd22f0b4b1c6d688f501e956ebefaa6", "title": "Pipelined Iterative Solvers with Kernel Fusion for Graphics Processing Units", "abstract": "We revisit the implementation of iterative solvers on discrete graphics processing units and demonstrate the benefit of implementations using extensive kernel fusion for pipelined formulations over conventional implementations of classical formulations. The proposed implementations with both CUDA and OpenCL are freely available in ViennaCL and are shown to be competitive with or even superior to other solver packages for graphics processing units. The highest-performance gains are obtained for small to medium-sized systems, while our implementations are on par with vendor-tuned implementations for very large systems. Our results are especially beneficial for transient problems, where many small to medium-sized systems instead of a single big system need to be solved.", "venue": "ACM Trans. Math. Softw.", "authors": ["Karl  Rupp", "Josef  Weinbub", "Ansgar  J\u00fcngel", "Tibor  Grasser"], "year": 2016, "n_citations": 17}
{"id": 4986994, "s2_id": "9b09b7569b9fca3a3177dc43441f9e4537374a67", "title": "A deep dive into the accuracy of IP Geolocation Databases and its impact on online advertising", "abstract": "The quest for every time more personalized Internet experience relies on the enriched contextual information about each user. Online advertising also follows this approach. Among the context information that advertising stakeholders leverage, location information is certainly one of them. However, when this information is not directly available from the end users, advertising stakeholders infer it using geolocation databases, matching IP addresses to a position on earth. The accuracy of this approach has often been questioned in the past: however, the reality check on an advertising DSP shows that this technique accounts for a large fraction of the served advertisements. In this paper, we revisit the work in the field, that is mostly from almost one decade ago, through the lenses of big data. More specifically, we, i) benchmark two commercial Internet geolocation databases, evaluate the quality of their information using a ground truth database of user positions containing more than 2 billion samples, ii) analyze the internals of these databases, devising a theoretical upper bound for the quality of the Internet geolocation approach, and iii) we run an empirical study that unveils the monetary impact of this technology by considering the costs associated with a real-world ad impressions dataset. We show that when factoring cost in, IP geolocation technology may be, under certain campaign characteristics, a better alternative than GPS from an economic point of view, despite its inferior performance.", "venue": "ArXiv", "authors": ["Patricia  Callejo", "Marco  Gramaglia", "Rub\u00e9n Cuevas Rum\u00edn", "\u00c1ngel  Cuevas"], "year": 2021, "n_citations": 0}
{"id": 4987380, "s2_id": "0e4b570e2ed43146e9a13e864ba052c8462e48d8", "title": "Self-optimizing mechanisms for EMF reduction in heterogeneous networks", "abstract": "This paper focuses on the exposure to Radio Frequency (RF) Electromagnetic Fields (EMF) and on optimization methods to reduce it. Within the FP7 LEXNET project, an Exposure Index (EI) has been defined that aggregates the essential components that impact exposure to EMF. The EI includes, among other, downlink (DL) exposure induced by the base stations (BSs) and access points, the uplink (UL) exposure induced by the devices in communication, and the corresponding exposure time. Motivated by the EI definition, this paper develops stochastic approximation based self-optimizing algorithm that dynamically adapts the network to reduce the EI in a heterogeneous network with macro- and small cells. It is argued that the increase of the small cells' coverage can, to a certain extent, reduce the EI, but above a certain limit, will deteriorate DL QoS. A load balancing algorithm is formulated that adapts the small cell' coverage based on UL loads and a DL QoS indicator. The proof of convergence of the algorithm is provided and its performance in terms of EI reduction is illustrated through extensive numerical simulations.", "venue": "2014 12th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)", "authors": ["Habib B. A. Sidi", "Zwi  Altman", "Abdoulaye  Tall"], "year": 2014, "n_citations": 9}
{"id": 4989647, "s2_id": "b3c35bc4fc65f2525792a3dab977cf35682058cc", "title": "Performance Analysis of Online Social Platforms", "abstract": "We introduce an original mathematical model to analyze the diffusion of posts within a generic online social platform. Each user of such a platform has his own Wall and Newsfeed, as well as his own self-posting and re-posting activity. As a main result, using our developed model, we derive in closed form the probabilities that posts originating from a given user are found on the Wall and Newsfeed of any other. These probabilities are the solution of a linear system of equations. Conditions of existence of the solution are provided, and two ways of solving the system are proposed, one using matrix inversion and another using fixed-point iteration. Comparisons with simulations show the accuracy of our model and its robustness with respect to the modeling assumptions. Hence, this article introduces a novel measure which allows to rank users by their influence on the social platform, by taking into account not only the social graph structure, but also the platform design, user activity (self-and re-posting), as well as competition among posts.", "venue": "IEEE INFOCOM 2019 - IEEE Conference on Computer Communications", "authors": ["Anastasios  Giovanidis", "Bruno  Baynat", "Antoine  Vendeville"], "year": 2019, "n_citations": 5}
{"id": 4994516, "s2_id": "f70fee48a2a962f86e2c23f0882964738f1d956a", "title": "Improving Adversarial Robustness via Guided Complement Entropy", "abstract": "Adversarial robustness has emerged as an important topic in deep learning as carefully crafted attack samples can significantly disturb the performance of a model. Many recent methods have proposed to improve adversarial robustness by utilizing adversarial training or model distillation, which adds additional procedures to model training. In this paper, we propose a new training paradigm called Guided Complement Entropy (GCE) that is capable of achieving \"adversarial defense for free,\" which involves no additional procedures in the process of improving adversarial robustness. In addition to maximizing model probabilities on the ground-truth class like cross-entropy, we neutralize its probabilities on the incorrect classes along with a \"guided\" term to balance between these two terms. We show in the experiments that our method achieves better model robustness with even better performance compared to the commonly used cross-entropy training objective. We also show that our method can be used orthogonal to adversarial training across well-known methods with noticeable robustness gain. To the best of our knowledge, our approach is the first one that improves model robustness without compromising performance.", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "authors": ["Hao-Yun  Chen", "Jhao-Hong  Liang", "Shih-Chieh  Chang", "Jia-Yu  Pan", "Yu-Ting  Chen", "Wei  Wei", "Da-Cheng  Juan"], "year": 2019, "n_citations": 19}
{"id": 4995345, "s2_id": "64e0d6e62d64cac7103452758739fc047e9b3d20", "title": "Optimal Message Bundling with Delay and Synchronization Constraints in Wireless Sensor Networks", "abstract": "Energy efficiency and end-to-end delay are two of the major requirements for the monitoring and detection applications based on resource-constrained wireless sensor networks (WSNs). As new advanced technologies for accurate monitoring and detection\u2014such as device-free wireless sensing schemes for human activity and gesture recognition\u2014have been developed, time synchronization accuracy becomes an important requirement for those WSN applications too. Message bundling is considered one of the effective methods to reduce the energy consumption for message transmissions in WSNs, but bundling more messages increases the transmission interval of bundled messages and thereby their end-to-end delays; the end-to-end delays need to be maintained within a certain value for time-sensitive applications like factory monitoring and disaster prevention, while the message transmission interval affects time synchronization accuracy when the bundling includes synchronization messages as well. Taking as an example a novel WSN time synchronization scheme recently proposed for energy efficiency, we investigate an optimal approach for message bundling to reduce the number of message transmissions while maintaining the user-defined requirements on end-to-end delay and time synchronization accuracy. Formulating the optimal message bundling problem as integer linear programming, we compute a set of optimal bundling numbers for the sensor nodes to constrain their link-level delays, thereby achieving and maintaining the required end-to-end delay and synchronization accuracy. Extensive experimental results based on a real WSN testbed using TelosB sensor nodes demonstrate that the proposed optimal bundling could reduce the number of message transmissions about 70% while simultaneously maintaining the required end-to-end delay and time synchronization accuracy.", "venue": "Sensors", "authors": ["Xintao  Huan", "Kyeong Soo Kim"], "year": 2019, "n_citations": 1}
{"id": 4999989, "s2_id": "c556198a414ecadd8e4c4a430d2bb40d94aa9790", "title": "Proceedings of the 4th OMNeT++ Community Summit, University of Bremen - Germany, September 7-8, 2017", "abstract": "These are the Proceedings of the 4th OMNeT++ Community Summit, which was held at the University of Bremen - Germany - on September 7-8, 2017.", "venue": "ArXiv", "authors": ["Anna  F\u00f6rster", "Asanga  Udugama", "Andreas  K\u00f6nsgen", "Antonio  Virdis", "Michael  Kirsche"], "year": 2017, "n_citations": 0}
{"id": 5000827, "s2_id": "76d7eee6acc69c07d36a29e26f8c62f4ce08ee21", "title": "A refined mean field approximation of synchronous discrete-time population models", "abstract": "Mean field approximation is a popular method to study the behaviour of stochastic models composed of a large number of interacting objects. When the objects are asynchronous, the mean field approximation of a population model can be expressed as an ordinary differential equation. When the objects are (clock-) synchronous the mean field approximation is a discrete time dynamical system. We focus on the latter.We study the accuracy of mean field approximation when this approximation is a discrete-time dynamical system. We extend a result that was shown for the continuous time case and we prove that expected performance indicators estimated by mean field approximation are $O(1/N)$-accurate. We provide simple expressions to effectively compute the asymptotic error of mean field approximation, for finite time-horizon and steady-state, and we use this computed error to propose what we call a \\emph{refined} mean field approximation. We show, by using a few numerical examples, that this technique improves the quality of approximation compared to the classical mean field approximation, especially for relatively small population sizes.", "venue": "Perform. Evaluation", "authors": ["Nicolas  Gast", "Diego  Latella", "Mieke  Massink"], "year": 2018, "n_citations": 9}
{"id": 5001104, "s2_id": "16d14632891c4a94c8a3cab9b696879356ce96ca", "title": "Simplified Ray Tracing for the Millimeter Wave Channel: A Performance Evaluation", "abstract": "Millimeter-wave (mmWave) communication is one of the cornerstone innovations of fifth-generation (5G) wireless networks, thanks to the massive bandwidth available in these frequency bands. To correctly assess the performance of such systems, however, it is essential to have reliable channel models, based on a deep understanding of the propagation characteristics of the mmWave signal. In this respect, ray tracers can provide high accuracy, at the expense of a significant computational complexity, which limits the scalability of simulations. To address this issue, in this paper we present possible simplifications that can reduce the complexity of ray tracing in the mmWave environment, without significantly affecting the accuracy of the model. We evaluate the effect of such simplifications on linklevel metrics, testing different configuration parameters and propagation scenarios.", "venue": "2020 Information Theory and Applications Workshop (ITA)", "authors": ["Mattia  Lecci", "Paolo  Testolina", "Marco  Giordani", "Michele  Polese", "Tanguy  Ropitault", "Camillo  Gentile", "Neeraj  Varshney", "Anuraag  Bodi", "Michele  Zorzi"], "year": 2020, "n_citations": 12}
{"id": 5002356, "s2_id": "b38e9d76a4e664e5ef873cc282297bf22e21415e", "title": "Adaptive Point-to-Multipoint Transmission for Multimedia Broadcast Multicast Services in LTE", "abstract": "This paper investigates point-to-multipoint (PTM) transmission supporting adaptive modulation and coding (AMC) as well as retransmissions based on incremental redundancy. In contrast to the classical PTM transmission which was introduced by the Multimedia Broadcast Multicast Service (MBMS), the adaptiveness requires user individual feedback channels that allow the receivers to report their radio conditions and send positive or negative acknowledgments (ACK/NACK) for a Layer 1 transport block to the eNodeB. In this work, an adaptive PTM scheme based on feedback from multiple users is presented and evaluated. Furthermore, a simple NACK-oriented feedback mechanism is introduced to relieve the feedback channel that is used in the uplink. Finally, the performance of different single-cell MBMS transmission modes is evaluated by dynamic radio network simulations. It is shown that adaptive PTM transmission outperforms the conventional MBMS configurations in terms of radio resource consumption and user satisfaction rate.", "venue": "ArXiv", "authors": ["Mai-Anh  Phan", "J\u00f6rg  Huschke"], "year": 2009, "n_citations": 7}
{"id": 5006663, "s2_id": "6a6eeb2e8c7e356975769da71693672d68cb0e06", "title": "Performance Modeling for Dense Linear Algebra", "abstract": "It is well known that the behavior of dense linear algebra algorithms is greatly influenced by factors like target architecture, underlying libraries and even problem size; because of this, the accurate prediction of their performance is a real challenge. In this article, we are not interested in creating accurate models for a given algorithm, but in correctly ranking a set of equivalent algorithms according to their performance. Aware of the hierarchical structure of dense linear algebra routines, we approach the problem by developing a framework for the automatic generation of statistical performance models for BLAS and LAPACK libraries. This allows us to obtain predictions through evaluating and combining such models. We demonstrate that our approach is successful in both single- and multi-core environments, not only in the ranking of algorithms but also in tuning their parameters.", "venue": "2012 SC Companion: High Performance Computing, Networking Storage and Analysis", "authors": ["Elmar  Peise", "Paolo  Bientinesi"], "year": 2012, "n_citations": 31}
{"id": 5025203, "s2_id": "6a6fb937dc6497e3ada99c6c100ddde1c8db4142", "title": "Pushing the Limits of Online Auto-Tuning: Machine Code Optimization in Short-Running Kernels", "abstract": "This paper proposes an online auto-tuning approach for computing kernels. Differently from existing online auto-tuners, which regenerate code with long compilation chains from the source to the binary code, our approach consists on deploying auto-tuning directly at the level of machine code generation. This allows auto-tuning to pay off in very short-running applications. As a proof of concept, our approach is demonstrated in two benchmarks, which execute during hundreds of milliseconds to a few seconds only. In a CPU-bound kernel, the average speedups achieved are 1.10 to 1.58 depending on the target micro-architecture, up to 2.53 in the most favourable conditions (all run-time overheads included). In a memory-bound kernel, less favourable to our runtime auto-tuning optimizations, the average speedups are 1.04 to 1.10, up to 1.30 in the best configuration. Despite the short execution times of our benchmarks, the overhead of our runtime auto-tuning is between 0.2 and 4.2 % only of the total application execution times. By simulating the CPU-bound application in 11 different CPUs, we showed that, despite the clear hardware disadvantage of In-Order (IO) cores vs. Out-of-Order (OOO) equivalent cores, online auto-tuning in IO CPUs obtained an average speedup of 1.03 and an energy efficiency improvement of 39 % over the SIMD reference in OOO CPUs.", "venue": "2016 IEEE 10th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSOC)", "authors": ["Fernando Akira Endo", "Damien  Courouss\u00e9", "Henri-Pierre  Charles"], "year": 2016, "n_citations": 1}
{"id": 5026993, "s2_id": "21630fbb8d5e1991892dd94f23c19f05641b9633", "title": "QoE Modelling, Measurement and Prediction: A Review", "abstract": "In mobile computing systems, users can access network services anywhere and anytime using mobile devices such as tablets and smart phones. These devices connect to the Internet via network or telecommunications operators. Users usually have some expectations about the services provided to them by dierent operators. Users\u2019 expectations along with additional factors such as cognitive and behavioural states, cost, and network quality of service (QoS) may determine their quality of experience (QoE). If users are not satisfied with their QoE, they may switch to dierent providers or may stop using a particular application or service. Thus, QoE measurement and prediction techniques may benefit users in availing personalized services from service providers. On the other hand, it can help service providers to achieve lower user-operator switchover. This paper presents an extensive review of the state-the-art research in the area of QoE modelling, measurement and prediction. In particular, we investigate and discuss the strengths and shortcomings of existing techniques. Finally, we present future research directions for developing novel QoE measurement and prediction techniques.", "venue": "ArXiv", "authors": ["Karan  Mitra", "Arkady B. Zaslavsky", "Christer  \u00c5hlund"], "year": 2014, "n_citations": 20}
{"id": 5027916, "s2_id": "dd8ccccb99aa711993e01cd0f6efe8f97bf8ccbd", "title": "The Locus Algorithm IV: Performance metrics of a grid computing system used to create catalogues of optimised pointings", "abstract": "Author(s): Creaner, Oisin; Walsh, John; Nolan, Kevin; Hickey, Eugene | Abstract: This paper discusses the requirements for and performance metrics of the the Grid Computing system used to implement the Locus Algorithm to identify optimum pointings for differential photometry of 61,662,376 stars and 23,779 quasars. Initial operational tests indicated a need for a software system to analyse the data and a High Performance Computing system to run that software in a scalable manner. Practical assessments of the performance of the software in a serial computing environment were used to provide a benchmark against which the performance metrics of the HPC solution could be compared, as well as to indicate any bottlenecks in performance. These performance metrics indicated a distinct split in the performance dictated more by differences in the input data than by differences in the design of the systems used. This indicates a need for experimental analysis of system performance, and suggests that algorithmic complexity analyses may lead to incorrect or naive conclusions, especially in systems with high data I/O overhead such as grid computing. Further, it implies that systems which reduce or eliminate this bottleneck such as in-memory processing could lead to a substantial increase in performance.", "venue": "ArXiv", "authors": ["Ois'in  Creaner", "John  Walsh", "Kevin  Nolan", "Eugene  Hickey"], "year": 2020, "n_citations": 3}
{"id": 5030154, "s2_id": "c2805b21413e75d342b33e627745dfb25cbe1cc1", "title": "On the stability of random multiple access with feedback exploitation and queue priority", "abstract": "In this paper, we study the stability of two interacting queues under random multiple access in which the queues leverage the feedback information. We derive the stability region under random multiple access where one of the two queues exploits the feedback information and backs off under negative acknowledgement (NACK) and the other, higher priority, queue will access the channel with probability one. We characterize the stability region of this feedback-based random access protocol and prove that this derived stability region encloses the stability region of the conventional random access (RA) scheme that does not exploit the feedback information.", "venue": "2014 IEEE International Symposium on Information Theory", "authors": ["Karim G. Seddik"], "year": 2014, "n_citations": 5}
{"id": 5032049, "s2_id": "1ad38534b6dd94f58d6f48c1e9dc223f6c304bfc", "title": "More bang for your buck: Improved use of GPU nodes for GROMACS 2018", "abstract": "We identify hardware that is optimal to produce molecular dynamics (MD) trajectories on Linux compute clusters with the GROMACS 2018 simulation package. Therefore, we benchmark the GROMACS performance on a diverse set of compute nodes and relate it to the costs of the nodes, which may include their lifetime costs for energy and cooling. In agreement with our earlier investigation using GROMACS 4.6 on hardware of 2014, the performance to price ratio of consumer GPU nodes is considerably higher than that of CPU nodes. However, with GROMACS 2018, the optimal CPU to GPU processing power balance has shifted even more toward the GPU. Hence, nodes optimized for GROMACS 2018 and later versions enable a significantly higher performance to price ratio than nodes optimized for older GROMACS versions. Moreover, the shift toward GPU processing allows to cheaply upgrade old nodes with recent GPUs, yielding essentially the same performance as comparable brand\u2010new hardware. \u00a9 2019 Wiley Periodicals, Inc.", "venue": "J. Comput. Chem.", "authors": ["Carsten  Kutzner", "Szil\u00e1rd  P\u00e1ll", "Martin  Fechner", "Ansgar  Esztermann", "Bert L. de Groot", "Helmut  Grubm\u00fcller"], "year": 2019, "n_citations": 102}
{"id": 5037878, "s2_id": "6b0f0cf82004435181eb6fa57df9d9fb04a12fec", "title": "System Level Performance Evaluation of LTE-V2X Network", "abstract": "Vehicles are among the fastest growing type of connected devices. Therefore, there is a need for Vehicle-toEverything (V2X) communication i.e. passing of information from a Vehicle-to-Vehicle (V2V) or Vehicle-to-Infrastructure (V2I) and vice versa. In this paper, the main focus is on the communication between vehicles and road side units (RSUs) commonly referred to as V2I communication in a multi-lane freeway scenario. Moreover, we analyze network related bottlenecks such as the maximum number of vehicles that can be supported when coverage is provided by the Long Term Evolution Advanced (LTE-A) network. The performance evaluation is assessed through extensive system-level simulations. Results show that new resource allocation and interference mitigation techniques are needed in order to achieve the required high reliability requirements, especially when network load is high.", "venue": "ArXiv", "authors": ["Petri  Luoto", "Mehdi  Bennis", "Pekka  Pirinen", "Sumudu  Samarakoon", "Kari  Horneman", "Matti  Latva-aho"], "year": 2016, "n_citations": 26}
{"id": 5044127, "s2_id": "947bff94d02ef74bca20bd7064009fbb7c2ecf77", "title": "Ginkgo - A Math Library designed for Platform Portability", "abstract": "The first associations to software sustainability might be the existence of a continuous integration (CI) framework; the existence of a testing framework composed of unit tests, integration tests, and end-to-end tests; and also the existence of software documentation. However, when asking what is a common deathblow for a scientific software product, it is often the lack of platform and performance portability. Against this background, we designed the Ginkgo library with the primary focus on platform portability and the ability to not only port to new hardware architectures, but also achieve good performance. In this paper we present the Ginkgo library design, radically separating algorithms from hardware-specific kernels forming the distinct hardware executors, and report our experience when adding execution backends for NVIDIA, AMD, and Intel GPUs. We also comment on the different levels of performance portability, and the performance we achieved on the distinct hardware backends.", "venue": "ArXiv", "authors": ["Terry  Cojean", "Yu-Hsiang Mike Tsai", "Hartwig  Anzt"], "year": 2020, "n_citations": 1}
{"id": 5048262, "s2_id": "dc6218303f092653e64eb3382928750b92e4e5bb", "title": "Graph-Based Approach for Buffer-Aware Timing Analysis of Heterogeneous Wormhole NoCs Under Bursty Traffic", "abstract": "This paper addresses the problem of worst-case timing analysis of heterogeneous wormhole NoCs, i.e., routers with different buffer sizes and transmission speeds, when consecutive-packet queuing (CPQ) occurs. The latter means that there are several consecutive packets of one flow queuing in the network. This scenario happens in the case of bursty traffic but also for non-schedulable traffic. Conducting such an analysis is known to be a challenging issue due to the sophisticated congestion patterns when enabling backpressure mechanisms. We tackle this problem through extending the applicability domain of our previous work for computing maximum delay bounds using Network Calculus, called Buffer-aware worst-case Timing Analysis (BATA). We propose a new Graph-based approach to improve the analysis of indirect blocking due to backpressure, while capturing the CPQ effect and keeping the information about dependencies between flows. Furthermore, the introduced approach improves the computation of indirect-blocking delay bounds in terms of complexity and ensures the safety of these bounds even for non-schedulable traffic. We provide further insights into the tightness and complexity issues of worst-case delay bounds yielded by the extended BATA with the Graph-based approach, denoted G-BATA. Our assessments show that the complexity has decreased by up to 100 times while offering an average tightness ratio of 71%, with reference to the basic BATA. Finally, we evaluate the yielded improvements with G-BATA for a realistic use case against a recent state-of-the-art approach. This evaluation shows the applicability of G-BATA under more general assumptions and the impact of such a feature on the tightness and computation time.", "venue": "IEEE Access", "authors": ["Fr\u00e9d\u00e9ric  Giroudot", "Ahlem  Mifdaoui"], "year": 2020, "n_citations": 2}
{"id": 5051164, "s2_id": "3b7e2038ec22cf637df70c833d473b0f3b43713a", "title": "Performance Evaluation of Container-Based Virtualization for High Performance Computing Environments", "abstract": "The use of virtualization technologies in high performance computing (HPC) environments has traditionally been avoided due to their inherent performance overhead. However, with the rise of container-based virtualization implementations, such as Linux VServer, OpenVZ and Linux Containers (LXC), it is possible to obtain a very low overhead leading to near-native performance. In this work, we conducted a number of experiments in order to perform an in-depth performance evaluation of container-based virtualization for HPC. We also evaluated the trade-off between performance and isolation in container-based virtualization systems and compared them with Xen, which is a representative of the traditional hypervisor-based virtualization systems used today.", "venue": "2013 21st Euromicro International Conference on Parallel, Distributed, and Network-Based Processing", "authors": ["Miguel G. Xavier", "Marcelo Veiga Neves", "Fabio D. Rossi", "Tiago C. Ferreto", "Timoteo  Lange", "C\u00e9sar A. F. De Rose"], "year": 2013, "n_citations": 260}
{"id": 5051518, "s2_id": "1712eccaded4b32364181d4f6db4a0f0b90d0836", "title": "Mixed-mode implementation of PETSc for scalable linear algebra on multi-core processors", "abstract": "With multi-core processors a ubiquitous building block of modern supercomputers, it is now past time to enable applications to embrace these developments in processor design. To achieve exascale performance, applications will need ways of exploiting the new levels of parallelism that are exposed in modern high-performance computers. A typical approach to this is to use shared-memory programming techniques to best exploit multi-core nodes along with inter-node message passing. In this paper, we describe the addition of OpenMP threaded functionality to the PETSc library. We highlight some issues that hinder good performance of threaded applications on modern processors and describe how to negate them. The OpenMP branch of PETSc was benchmarked using matrices extracted from Fluidity, a CFD application code, which uses the library as its linear solver engine. The overall performance of the mixed-mode implementation is shown to be superior to that of the pure-MPI version.", "venue": "ArXiv", "authors": ["Mich\u00e8le  Weiland", "Lawrence  Mitchell", "Gerard  Gorman", "Stephan  Kramer", "Mark  Parsons", "James  Southern"], "year": 2012, "n_citations": 4}
{"id": 5058860, "s2_id": "602465beb28d3f4ba5ea74da258424fbf09116c8", "title": "Performance of TCP/UDP under ad hoc IEEE802.11", "abstract": "TCP is the de facto standard for connection oriented transport layer protocol, while UDP is the de facto standard for transport layer protocol, which is used with real time traffic for audio and video. Although there have been many attempts to measure and analyze the performance of the TCP protocol in wireless networks, very few research was done on the UDP or the interaction between TCP and UDP traffic over the wireless link. We study the performance of TCP and UDP over IEEE802.11 ad hoc network. We used two topologies, a string and a mesh topology. Our work indicates that IEEE802.11 as a ad-hoc network is not very suitable for bulk transfer using TCP. It also indicates that it is much better for real-time audio. Although one has to be careful here since real-time audio does require much less bandwidth than the wireless link bandwidth. Careful and detailed studies are needed to further clarify that issue.", "venue": "10th International Conference on Telecommunications, 2003. ICT 2003.", "authors": ["Milenko  Petrovic", "Mokhtar  Aboelaze"], "year": 2003, "n_citations": 17}
{"id": 5059739, "s2_id": "41e970e1073da5ca032428b7c48a2870ed0dd7df", "title": "Comparing Spark vs MPI/OpenMP On Word Count MapReduce", "abstract": "Spark provides an in-memory implementation of MapReduce that is widely used in the big data industry. MPI/OpenMP is a popular framework for high performance parallel computing. This paper presents a high performance MapReduce design in MPI/OpenMP and uses that to compare with Spark on the classic word count MapReduce task. My result shows that the MPI/OpenMP MapReduce outperforms Apache Spark by about 300%.", "venue": "ArXiv", "authors": ["Junhao  Li"], "year": 2018, "n_citations": 2}
{"id": 5067125, "s2_id": "f203023c120dc64d345a74be36ebcf3d07804fab", "title": "On the accuracy and usefulness of analytic energy models for contemporary multicore processors", "abstract": "This paper presents refinements to the execution-cache-memory performance model and a previously published power model for multicore processors. The combination of both enables a very accurate prediction of performance and energy consumption of contemporary multicore processors as a function of relevant parameters such as number of active cores as well as core and Uncore frequencies. Model validation is performed on the Sandy Bridge-EP and Broadwell-EP microarchitectures. Production-related variations in chip quality are demonstrated through a statistical analysis of the fit parameters obtained on one hundred Broadwell-EP CPUs of the same model. Insights from the models are used to explain the performance- and energy-related behavior of the processors for scalable as well as saturating (i.e., memory-bound) codes. In the process we demonstrate the models' capability to identify optimal operating points with respect to highest performance, lowest energy-to-solution, and lowest energy-delay product and identify a set of best practices for energy-efficient execution.", "venue": "ISC", "authors": ["Johannes  Hofmann", "Georg  Hager", "Dietmar  Fey"], "year": 2018, "n_citations": 17}
{"id": 5067346, "s2_id": "647e5a136a8181afdf1d3de7aa25b905a7d67d92", "title": "Performance localisation", "abstract": "Profiling techniques highlight where performance issues manifest and provide a starting point for tracing cause back through a program. While people diagnose and understand the cause of performance to guide formulation of a performance improvement, we seek automated techniques for highlighting performance improvement opportunities to guide search algorithms. We investigate mutation-based approaches for highlighting where a performance improvement is likely to exist. For all modification locations in a program, we make all possible modifications and analyse how often modifications reduce execution count. We compare the resulting code location rankings against rankings derived using a profiler and find that mutation analysis provides the higher accuracy in highlighting performance improvement locations in a set of benchmark problems, though at a much higher execution cost. We see both approaches as complimentary and consider how they may be used to further guide Genetic Programming in finding performance improvements.", "venue": "GI@ICSE", "authors": ["Brendan  Cody-Kenny", "Stephen  Barrett"], "year": 2018, "n_citations": 2}
{"id": 5069137, "s2_id": "686c73e94e209913258583c6e916ff373698c2d2", "title": "Optimized routines for event generators in QED-PIC codes", "abstract": "In recent years, the prospects of performing fundamental and applied studies at the next-generation high-intensity laser facilities have greatly stimulated the interest in performing large-scale simulations of laser interaction with matter with the account for quantum electrodynamics (QED) processes such as emission of high energy photons and decay of such photons into electron-positron pairs. These processes can be modeled via probabilistic routines that include frequent computation of synchrotron functions and can constitute significant computational demands within accordingly extended Particle-in-Cell (QED-PIC) algorithms. In this regard, the optimization of these routines is of great interest. In this paper, we propose and describe two modifications. First, we derive a more accurate upper-bound estimate for the rate of QED events and use it to arrange local sub-stepping of the global time step in a significantly more efficient way than done previously. Second, we present a new high-performance implementation of synchrotron functions. Our optimizations made it possible to speed up the computations by a factor of up to 13.7 depending on the problem. Our implementation is integrated into the PICADOR and Hi-Chi codes, the latter of which is distributed publicly (this https URL).", "venue": "ArXiv", "authors": ["V.  Volokitin", "S.  Bastrakov", "A.  Bashinov", "E.  Efimenko", "A.  Muraviev", "A.  Gonoskov", "I.  Meyerov"], "year": 2020, "n_citations": 0}
{"id": 5069144, "s2_id": "f7b34170388da1f0d519a5b8ee0cfb7fe7d737fc", "title": "A polling model with reneging at polling instants", "abstract": "In this paper we consider a single-server, cyclic polling system with switch-over times and Poisson arrivals. The service disciplines that are discussed, are exhaustive and gated service. The novel contribution of the present paper is that we consider the reneging of customers at polling instants. In more detail, whenever the server starts or ends a visit to a queue, some of the customers waiting in each queue leave the system before having received service. The probability that a certain customer leaves the queue, depends on the queue in which the customer is waiting, and on the location of the server. We show that this system can be analysed by introducing customer subtypes, depending on their arrival periods, and keeping track of the moment when they abandon the system. In order to determine waiting time distributions, we regard the system as a polling model with varying arrival rates, and apply a generalised version of the distributional form of Little\u2019s law. The marginal queue length distribution can be found by conditioning on the state of the system (position of the server, and whether it is serving or switching).", "venue": "Ann. Oper. Res.", "authors": ["Marko A. A. Boon"], "year": 2012, "n_citations": 9}
{"id": 5069323, "s2_id": "e50eb222d7ae486ebd91ef03ddc565562e30b7a4", "title": "An infinite dimensional model for a many server priority queue", "abstract": "We consider a Markovian many server queueing system in which customers are preemptively scheduled according to exogenously assigned priority levels. The priority levels are randomly assigned from a continuous probability measure rather than a discrete one and hence, the queue is modeled by an infinite dimensional stochastic process. We analyze the equilibrium behavior of the system and provide several results. We derive the Radon-Nikodym derivative (with respect to Lebesgue measure) of the measure that describes the average distribution of customer priority levels in the system; we provide a formula for the expected sojourn time of a customer as a function of his priority level; and we provide a formula for the expected waiting time of a customer as a function of his priority level. We verify our theoretical analysis with discrete-event simulations. We discuss how each of our results generalizes previous work on infinite dimensional models for single server priority queues.", "venue": "2017 51st Annual Conference on Information Sciences and Systems (CISS)", "authors": ["Neal  Master", "Zhengyuan  Zhou", "Nicholas  Bambos"], "year": 2017, "n_citations": 3}
{"id": 5071967, "s2_id": "c8f256117469e8740a4aa3041eba01e7b2c7f6f9", "title": "Performance prediction of finite-difference solvers for different computer architectures", "abstract": "Abstract The life-cycle of a partial differential equation (PDE) solver is often characterized by three development phases: the development of a stable numerical discretization; development of a correct (verified) implementation; and the optimization of the implementation for different computer architectures. Often it is only after significant time and effort has been invested that the performance bottlenecks of a PDE solver are fully understood, and the precise details varies between different computer architectures. One way to mitigate this issue is to establish a reliable performance model that allows a numerical analyst to make reliable predictions of how well a numerical method would perform on a given computer architecture, before embarking upon potentially long and expensive implementation and optimization phases. The availability of a reliable performance model also saves developer effort as it both informs the developer on what kind of optimisations are beneficial, and when the maximum expected performance has been reached and optimisation work should stop. We show how discretization of a wave-equation can be theoretically studied to understand the performance limitations of the method on modern computer architectures. We focus on the roofline model, now broadly used in the high-performance computing community, which considers the achievable performance in terms of the peak memory bandwidth and peak floating point performance of a computer with respect to algorithmic choices. A first principles analysis of operational intensity for key time-stepping finite-difference algorithms is presented. With this information available at the time of algorithm design, the expected performance on target computer systems can be used as a driver for algorithm design.", "venue": "Comput. Geosci.", "authors": ["Mathias  Louboutin", "Michael  Lange", "Felix J. Herrmann", "Navjot  Kukreja", "Gerard  Gorman"], "year": 2017, "n_citations": 14}
{"id": 5073230, "s2_id": "1be95c75e09f293797831f1d6140f32e1b49e569", "title": "Boosting Memory Access Locality of the Spectral Element Method with Hilbert Space-Filling Curves", "abstract": "We propose an algorithm based on Hilbert spacefilling curves to reorder mesh elements in memory for use with the Spectral Element Method, aiming to attain fewer cache misses, better locality of data reference and faster execution. We present a technique to numerically simulate acoustic wave propagation in 2D domains using the Spectral Element Method, and discuss computational performance aspects of this procedure. We reorder mesh-related data via Hilbert curves to achieve sizable reductions in execution time under several mesh configurations in shared-memory systems. Our experiments show that the Hilbert curve approach works well with meshes of several granularities and also with small and large variations in element sizes, achieving reductions between 9% and 25% in execution time when compared to three other ordering schemes.", "venue": "Computers & Geosciences", "authors": ["Roger R. F. Ara\u00fajo", "Lutz  Gross", "Samuel Xavier de Souza"], "year": 2021, "n_citations": 1}
{"id": 5073701, "s2_id": "653a6a4979585cfab8cca6dd2673eca5c2d02c5e", "title": "Wireless vehicular networks in emergencies: A single frequency network approach", "abstract": "Obtaining high quality sensor information is critical in vehicular emergencies. However, existing standards such as IEEE 802.11p/DSRC and LTE-A cannot support either the required data rates or the latency requirements. One solution to this problem is for municipalities to invest in dedicated base stations to ensure that drivers have the information they need to make safe decisions in or near accidents. In this paper we further propose that these municipality-owned base stations form a Single Frequency Network (SFN). In order to ensure that transmissions are reliable, we derive tight bounds on the outage probability when the SFN is overlaid on an existing cellular network. Using our bounds, we propose a transmission power allocation algorithm. We show that our power allocation model can reduce the total instantaneous SFN transmission power up to 20 times compared to a static uniform power allocation solution, for the considered scenarios. The result is particularly important when base stations rely on an off-grid power source (i.e., batteries).", "venue": "2017 International Conference on Recent Advances in Signal Processing, Telecommunications & Computing (SigTelCom)", "authors": ["Andrea  Tassi", "Malcolm  Egan", "Robert J. Piechocki", "Andrew R. Nix"], "year": 2017, "n_citations": 7}
{"id": 5074009, "s2_id": "546cb52f87191d74f14beef2af36d8eeba7596ac", "title": "Jointly Optimal Channel and Power Assignment for Dual-Hop Multi-Channel Multi-User Relaying", "abstract": "We consider the problem of jointly optimizing channel pairing, channel-user assignment, and power allocation, to maximize the weighted sum-rate, in a single-relay cooperative system with multiple channels and multiple users. Common relaying strategies are considered, and transmission power constraints are imposed on both individual transmitters and the aggregate over all transmitters. The joint optimization problem naturally leads to a mixed-integer program. Despite the general expectation that such problems are intractable, we construct an efficient algorithm to find an optimal solution, which incurs computational complexity that is polynomial in the number of channels and the number of users. We further demonstrate through numerical experiments that the jointly optimal solution can significantly improve system performance over its suboptimal alternatives.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Mahdi  Hajiaghayi", "Min  Dong", "Ben  Liang"], "year": 2012, "n_citations": 36}
{"id": 5074203, "s2_id": "0bf6a156a1ef75a8e6977b20f4247e2a9b4bd382", "title": "A mechanism for balancing accuracy and scope in cross-machine black-box GPU performance modeling", "abstract": "The ability to model, analyze, and predict execution time of computations is an important building block that supports numerous efforts, such as load balancing, benchmarking, job scheduling, developer-guided performance optimization, and the automation of performance tuning for high performance, parallel applications. In today\u2019s increasingly heterogeneous computing environment, this task must be accomplished efficiently across multiple architectures, including massively parallel coprocessors like GPUs, which are increasingly prevalent in the world\u2019s fastest supercomputers. To address this challenge, we present an approach for constructing customizable, cross-machine performance models for GPU kernels, including a mechanism to automatically and symbolically gather performance-relevant kernel operation counts, a tool for formulating mathematical models using these counts, and a customizable parameterized collection of benchmark kernels used to calibrate models to GPUs in a black-box fashion. With this approach, we empower the user to manage trade-offs between model accuracy, evaluation speed, and generalizability. A user can define their own model and customize the calibration process, making it as simple or complex as desired, and as application-targeted or general as desired. As application examples of our approach, we demonstrate both linear and nonlinear models; these examples are designed to predict execution times for multiple variants of a particular computation: two matrix-matrix multiplication variants, four discontinuous Galerkin differentiation operation variants, and two 2D five-point finite difference stencil variants. For each variant, we present accuracy results on GPUs from multiple vendors and hardware generations. We view this highly user-customizable approach as a response to a central question arising in GPU performance modeling: how can we model GPU performance in a cost-explanatory fashion while maintaining accuracy, evaluation speed, portability, and ease of use, an attribute we believe precludes approaches requiring manual collection of kernel or hardware statistics.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["James D Stevens", "Andreas  Kl\u00f6ckner"], "year": 2020, "n_citations": 0}
{"id": 5076213, "s2_id": "38cb54b3480321e3a503dfede805f8808097779d", "title": "adPerf: Characterizing the Performance of Third-party Ads", "abstract": "Author(s): Pourghassemi, Behnam; Bonecutter, Jordan; Li, Zhou; Chandramowlishwaran, Aparna | Abstract: Monetizing websites and web apps through online advertising is widespread in the web ecosystem. The online advertising ecosystem nowadays forces publishers to integrate ads from these third-party domains. On the one hand, this raises several privacy and security concerns that are actively studied in recent years. On the other hand, given the ability of today's browsers to load dynamic web pages with complex animations and Javascript, online advertising has also transformed and can have a significant impact on webpage performance. The performance cost of online ads is critical since it eventually impacts user satisfaction as well as their Internet bill and device energy consumption. In this paper, we apply an in-depth and first-of-a-kind performance evaluation of web ads. Unlike prior efforts that rely primarily on adblockers, we perform a fine-grained analysis on the web browser's page loading process to demystify the performance cost of web ads. We aim to characterize the cost by every component of an ad, so the publisher, ad syndicate, and advertiser can improve the ad's performance with detailed guidance. For this purpose, we develop an infrastructure, adPerf, for the Chrome browser that classifies page loading workloads into ad-related and main-content at the granularity of browser activities (such as Javascript and Layout). Our evaluations show that online advertising entails more than 15% of browser page loading workload and approximately 88% of that is spent on JavaScript. We also track the sources and delivery chain of web ads and analyze performance considering the origin of the ad contents. We observe that 2 of the well-known third-party ad domains contribute to 35% of the ads performance cost and surprisingly, top news websites implicitly include unknown third-party ads which in some cases build up to more than 37% of the ads performance cost.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Behnam  Pourghassemi", "Jordan  Bonecutter", "Zhou  Li", "Aparna  Chandramowlishwaran"], "year": 2021, "n_citations": 2}
{"id": 5076535, "s2_id": "29cdd9991a19a89a778f384140a60cd176b8a231", "title": "An Empirical Study of UDP (CBR) Packet Performance over AODV Single & Multi-Channel Parallel Transmission in MANET", "abstract": "Mobile Ad-hoc Network is a temporary network which is the cooperative engagement of a collection of standalone mobile nodes that are not connected to any external network. It is a decentralized network where mobile nodes can be easily deployed in almost any environment without sophisticated infrastructure support. An empirical study has been done for AODV routing protocol under single channel and multi channel environment using the tool NS2. To compare the performance of AODV in the two environments, the simulation results have been analyzed by graphical manner and trace file based on QoS metrics such as throughput, packet drop, delay and jitter. The simulation result analysis verifies the AODV routing protocol performances for single channel and multi channel. After the analysis of the simulation scenario we suggest that use of Parallel MAC (P-MAC) may enhance the performance for multi channel.", "venue": "ArXiv", "authors": ["Md. Monzur Morshed", "Meftah Ur Rahman", "Md. Rafiqul Islam"], "year": 2011, "n_citations": 3}
{"id": 5078552, "s2_id": "e6c948ef4128b50a02865823b61c63d92c657acf", "title": "A Markov Chain based method for generating long-range dependence", "abstract": "This paper describes a model for generating time series which exhibit the statistical phenomenon known as long-range dependence (LRD). A Markov modulated process based on an infinite Markov chain is described. The work described is motivated by applications in telecommunications where LRD is a known property of time series measured on the Internet. The process can generate a time series exhibiting LRD with known parameters and is particularly suitable for modeling Internet traffic because the time series is in terms of ones and zeros, which can be interpreted as data packets and interpacket gaps. The method is extremely simple, both computationally and analytically, and could prove more tractable than other methods described in the literature.", "venue": "Physical review. E, Statistical, nonlinear, and soft matter physics", "authors": ["Richard G. Clegg", "Maurice  Dodson"], "year": 2005, "n_citations": 12}
{"id": 5085375, "s2_id": "3d6169b82ec0e3c2be97ef31aa59fcac10fad003", "title": "HPA: An opportunistic approach to embedded energy efficiency", "abstract": "Reducing energy consumption is a challenge that is faced on a daily basis by teams from the high-performance computing as well as the embedded domains. This issue is mostly approached from an hardware perspective by devising architectures that put energy efficiency as a primary target, often at the cost of processing power. Lately, computing platforms have become more and more heterogeneous, but the exploitation of these additional capabilities is so complex from the application developer's perspective that their optimization is often limited. In this paper we present a transparent, on-the-fly optimization scheme that allows a generic application to automatically and dynamically exploit the available computing units to partition its computational load. We have called our approach Heterogeneous Platform Accelerator (HPA). The idea is to use profiling to select a computing-intensive candidate for acceleration, and then distribute the computations to the different units by off-loading blocks of code to them. This is done automatically at run-time, thus requiring no effort from the developer and adapting to the current input data and load. Using an NVIDIA Jetson TK1 board, we validate our proposal on several benchmarks and on a real-world software package, the Unix text editor ed. The results we achieve substantiate our claim that not only HPA results in faster processing speed, but also in a considerable reduction in energy dissipation.", "venue": "2016 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Baptiste  Delporte", "Roberto  Rigamonti", "Alberto  Dassatti"], "year": 2016, "n_citations": 4}
{"id": 5085778, "s2_id": "5a66ce12deff0d52cdfab5ddc78542f62f6a358b", "title": "Hypernets - Good (G)news for Gnutella", "abstract": "Criticism of Gnutella network scalability has rested on the bandwidth attributes of the original interconnection topology: a Cayley tree. Trees, in general, are known to have lower aggregate bandwidth than higher dimensional topologies e.g., hypercubes, meshes and tori. Gnutella was intended to support thousands to millions of peers. Studies of interconnection topologies in the literature, however, have focused on hardware implementations which are limited by cost to a few thousand nodes. Since the Gnutella network is virtual, hyper-topologies are relatively unfettered by such constraints. We present performance models for several plausible hyper-topologies and compare their query throughput up to millions of peers. The virtual hypercube and the virtual hypertorus are shown to offer near linear scalability subject to the number of peer TCP/IP connections that can be simultaneously kept open.", "venue": "ArXiv", "authors": ["Neil J. Gunther"], "year": 2002, "n_citations": 3}
{"id": 5087182, "s2_id": "3718f1acb32ee1e176d37d5ee0a6fb278ee19461", "title": "The Impact of GPU DVFS on the Energy and Performance of Deep Learning: an Empirical Study", "abstract": "Over the past years, great progress has been made in improving the computing power of general-purpose graphics processing units (GPGPUs), which facilitates the prosperity of deep neural networks (DNNs) in multiple fields like computer vision and natural language processing. A typical DNN training process repeatedly updates tens of millions of parameters, which not only requires huge computing resources but also consumes significant energy. In order to train DNNs in a more energy-efficient way, we empirically investigate the impact of GPU Dynamic Voltage and Frequency Scaling (DVFS) on the energy consumption and performance of deep learning. Our experiments cover a wide range of GPU architectures, DVFS settings, and DNN configurations. We observe that, compared to the default core frequency settings of three tested GPUs, the optimal core frequency can help conserve 8.7%~23.1% energy consumption for different DNN training cases. Regarding the inference, the benefits vary from 19.6%~26.4%. Our findings suggest that GPU DVFS has great potentials to help develop energy efficient DNN training/inference schemes.", "venue": "e-Energy", "authors": ["Zhenheng  Tang", "Yuxin  Wang", "Qiang  Wang", "Xiaowen  Chu"], "year": 2019, "n_citations": 24}
{"id": 5087759, "s2_id": "9e8112adf85abd6b32104a2118df9c18847c18e9", "title": "Characterizing the Deep Neural Networks Inference Performance of Mobile Applications", "abstract": "Today's mobile applications are increasingly leveraging deep neural networks to provide novel features, such as image and speech recognitions. To use a pre-trained deep neural network, mobile developers can either host it in a cloud server, referred to as cloud-based inference, or ship it with their mobile application, referred to as on-device inference. In this work, we investigate the inference performance of these two common approaches on both mobile devices and public clouds, using popular convolutional neural networks. Our measurement study suggests the need for both on-device and cloud-based inferences for supporting mobile applications. In particular, newer mobile devices is able to run mobile-optimized CNN models in reasonable time. However, for older mobile devices or to use more complex CNN models, mobile applications should opt in for cloud-based inference. We further demonstrate that variable network conditions can lead to poor cloud-based inference end-to-end time. To support efficient cloud-based inference, we propose a CNN model selection algorithm called CNNSelect that dynamically selects the most appropriate CNN model for each inference request, and adapts its selection to match different SLAs and execution time budgets that are caused by variable mobile environments. The key idea of CNNSelect is to make inference speed and accuracy trade-offs at runtime using a set of CNN models. We demonstrated that CNNSelect smoothly improves inference accuracy while maintaining SLA attainment in 88.5% more cases than a greedy baseline.", "venue": "ArXiv", "authors": ["Samuel S. Ogden", "Tian  Guo"], "year": 2019, "n_citations": 9}
{"id": 5088571, "s2_id": "86c7f07deec5a2a52ade8be133c2f1e1a6c6c947", "title": "Architecture-aware configuration and scheduling of matrix multiplication on asymmetric multicore processors", "abstract": "Asymmetric multicore processors have recently emerged as an appealing technology for severely energy-constrained environments, especially in mobile appliances where heterogeneity in applications is mainstream. In addition, given the growing interest for low-power high performance computing, this type of architectures is also being investigated as a means to improve the throughput-per-Watt of complex scientific applications on clusters of commodity systems-on-chip. In this paper, we design and embed several architecture-aware optimizations into a multi-threaded general matrix multiplication (gemm), a key operation of the BLAS, in order to obtain a high performance implementation for ARM big.LITTLE AMPs. Our solution is based on the reference implementation of gemm in the BLIS library, and integrates a cache-aware configuration as well as asymmetric-static and dynamic scheduling strategies that carefully tune and distribute the operation\u2019s micro-kernels among the big and LITTLE cores of the target processor. The experimental results on a Samsung Exynos 5422, a system-on-chip with ARM Cortex-A15 and Cortex-A7 clusters that implements the big.LITTLE model, expose that our cache-aware versions of gemm with asymmetric scheduling attain important gains in performance with respect to its architecture-oblivious counterparts while exploiting all the resources of the AMP to deliver considerable energy efficiency.", "venue": "Cluster Computing", "authors": ["Sandra  Catal\u00e1n", "Francisco D. Igual", "Rafael  Mayo", "Rafael  Rodr\u00edguez-S\u00e1nchez", "Enrique S. Quintana-Ort\u00ed"], "year": 2016, "n_citations": 14}
{"id": 5091636, "s2_id": "54442bab5e5244195107901e23fc60857371d670", "title": "Quantum Monte Carlo for large chemical systems: Implementing efficient strategies for petascale platforms and beyond", "abstract": "Various strategies to implement efficiently quantum Monte Carlo (QMC) simulations for large chemical systems are presented. These include: (i) the introduction of an efficient algorithm to calculate the computationally expensive Slater matrices. This novel scheme is based on the use of the highly localized character of atomic Gaussian basis functions (not the molecular orbitals as usually done), (ii) the possibility of keeping the memory footprint minimal, (iii) the important enhancement of single\u2010core performance when efficient optimization tools are used, and (iv) the definition of a universal, dynamic, fault\u2010tolerant, and load\u2010balanced framework adapted to all kinds of computational platforms (massively parallel machines, clusters, or distributed grids). These strategies have been implemented in the QMC=Chem code developed at Toulouse and illustrated with numerical applications on small peptides of increasing sizes (158, 434, 1056, and 1731 electrons). Using 10\u201380 k computing cores of the Curie machine (GENCI\u2010TGCC\u2010CEA, France), QMC=Chem has been shown to be capable of running at the petascale level, thus demonstrating that for this machine a large part of the peak performance can be achieved. Implementation of large\u2010scale QMC simulations for future exascale platforms with a comparable level of efficiency is expected to be feasible. \u00a9 2013 Wiley Periodicals, Inc.", "venue": "J. Comput. Chem.", "authors": ["Anthony  Scemama", "Michel  Caffarel", "Emmanuel  Oseret", "William  Jalby"], "year": 2013, "n_citations": 28}
{"id": 5098331, "s2_id": "6d0879366beb1fc7defe32b2b8deabe44eae6ad3", "title": "TAOS-CI: Lightweight & Modular Continuous Integration System for Edge Computing", "abstract": "With the proliferation of IoT and edge devices, we are observing a lot of consumer electronics becoming yet another IoT and edge devices. Unlike traditional smart devices, such as smart phones, consumer electronics, in general, have significant diversities with fewer number of devices per product model. With such high diversities, the proliferation of edge devices requires frequent and seamless updates of consumer electronics, which makes the manufacturers prone to regressions because the manufacturers have less resource per an instance of software release; i.e., they need to repeat releases by the number of product models times the number of updates. Continuous Integration (CI) systems can help prevent regression bugs from actively developing software packages including the frequently updated device software platforms. The proposed CI system provides a portable and modular software platform automatically inspecting potential issues of incoming changes with the enabled modules: code format and style, performance regressions, static checks on the source code, build and packaging tests, and dynamic checks with the built binary before deploying a platform image on the IoT and edge devices. Besides, our proposed approach is lightweight enough to be hosted in normal desktop computers even for dozens of developers. As a result, it can be easily applied to a lot of various source code repositories. Evaluation results demonstrate that the proposed method drastically improves plugins execution time and memory consumption, compared with methods in previous studies.", "venue": "2019 IEEE International Conference on Consumer Electronics (ICCE)", "authors": ["Geunsik  Lim", "MyungJoo  Ham", "Ji Joong Moon", "Wook  Song", "Sangjung  Woo", "Sewon  Oh"], "year": 2019, "n_citations": 2}
{"id": 5100774, "s2_id": "1478ea2d56566cc7c19c6f0358d04033c52e6da5", "title": "BenchCouncil's View on Benchmarking AI and Other Emerging Workloads", "abstract": "This paper outlines BenchCouncil's view on the challenges, rules, and vision of benchmarking modern workloads like Big Data, AI or machine learning, and Internet Services. We conclude the challenges of benchmarking modern workloads as FIDSS (Fragmented, Isolated, Dynamic, Service-based, and Stochastic), and propose the PRDAERS benchmarking rules that the benchmarks should be specified in a paper-and-pencil manner, relevant, diverse, containing different levels of abstractions, specifying the evaluation metrics and methodology, repeatable, and scaleable. We believe proposing simple but elegant abstractions that help achieve both efficiency and general-purpose is the final target of benchmarking in future, which may be not pressing. In the light of this vision, we shortly discuss BenchCouncil's related projects.", "venue": "ArXiv", "authors": ["Jianfeng  Zhan", "Lei  Wang", "Wanling  Gao", "Rui  Ren"], "year": 2019, "n_citations": 10}
{"id": 5100778, "s2_id": "ddedef387a7c7fcb5751033d825e52feff15b272", "title": "A Data-Centric Optimization Framework for Machine Learning", "abstract": "ABSTRACT Rapid progress in deep learning is leading to a diverse set of quickly changing models, with a dramatically growing demand for compute. However, as frameworks specialize optimization to patterns in popular networks, they implicitly constrain novel and diverse models that drive progress in research. We empower deep learning researchers by defining a flexible and user-customizable pipeline for optimizing training of arbitrary deep neural networks, based on data movement minimization. The pipeline begins with standard networks in PyTorch or ONNX and transforms computation through progressive lowering. We define four levels of general-purpose transformations, from local intra-operator optimizations to global data movement reduction. These operate on a data-centric graph intermediate representation that expresses computation and data movement at all levels of abstraction, including expanding basic operators such as convolutions to their underlying computations. Central to the design is the interactive and introspectable nature of the pipeline. Every part is extensible through a Python API, and can be tuned interactively using a GUI. We demonstrate competitive performance or speedups on ten different networks, with interactive optimizations discovering new opportunities in EfficientNet.", "venue": "ArXiv", "authors": ["Oliver  Rausch", "Tal  Ben-Nun", "Nikoli  Dryden", "Andrei  Ivanov", "Shigang  Li", "Torsten  Hoefler"], "year": 2021, "n_citations": 0}
{"id": 5102250, "s2_id": "6756ead213eb4d102a025ecae06f5fdbf263fab4", "title": "SmartWatts: Self-Calibrating Software-Defined Power Meter for Containers", "abstract": "Fine-grained power monitoring of software activities becomes unavoidable to maximize the power usage efficiency of data centers. In particular, achieving an optimal scheduling of containers requires the deployment of software-defined power meters to go beyond the granularity of hardware power monitoring sensors, such as Power Distribution Units (PDU) or Intel\u2019s Running Average Power Limit (RAPL), to deliver power estimations of activities at the granularity of software containers. However, the definition of the underlying power models that estimate the power consumption remains a long and fragile process that is tightly coupled to the host machine.To overcome these limitations, this paper introduces SmartWatts: a lightweight power monitoring system that adopts online calibration to automatically adjust the CPU and DRAM power models in order to maximize the accuracy of runtime power estimations of containers. Unlike state-of-the-art techniques, SmartWatts does not require any a priori training phase or hardware equipment to configure the power models and can therefore be deployed on a wide range of machines including the latest power optimizations, at no cost.", "venue": "2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID)", "authors": ["Guillaume  Fieni", "Romain  Rouvoy", "Lionel  Seinturier"], "year": 2020, "n_citations": 6}
{"id": 5102375, "s2_id": "84f1f97b04345929536ad4cbe98087f6a77a3327", "title": "Wireless Network Coding via Modified 802.11 MAC/PHY: Design and Implementation on SDR", "abstract": "Network coding (NC), in principle, is a Layer-3 innovation that improves network throughput in wired networks for multicast/broadcast scenarios. Due to the fundamental differences between wired and wireless networks, extending NC to wireless networks generates several new and significant practical challenges. Two-way information exchange (both symmetric and asymmetric) between a pair of 802.11 sources/sinks using an intermediate relay node is a canonical scenario for evaluating the effectiveness of Wireless Network Coding (WNC) in a practical setting. Our primary objective in this work is to suggest pragmatic and novel modifications at the MAC and PHY layers of the 802.11 protocol stack on a Software Radio (SORA) platform to support WNC and obtain achievable throughput estimates via lab-scale experiments. Our results show that network coding (at the MAC or PHY layer) increases system throughput-typically by 20-30%.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Mohammad Hamed Firooz", "Zhiyong  Chen", "Sumit  Roy", "Hui  Liu"], "year": 2013, "n_citations": 33}
{"id": 5104570, "s2_id": "462ebaa9c9ecd16f74246d79b7dfd4f591204105", "title": "High Performance Depthwise and Pointwise Convolutions on Mobile Devices", "abstract": "Lightweight convolutional neural networks (e.g., MobileNets) are specifically designed to carry out inference directly on mobile devices. Among the various lightweight models, depthwise convolution (DWConv) and pointwise convolution (PWConv) are their key operations. In this paper, we observe that the existing implementations of DWConv and PWConv are not well utilizing the ARM processors in the mobile devices, and exhibit lots of cache misses under multi-core and poor data reuse at register level. We propose techniques to re-optimize the implementations of DWConv and PWConv based on ARM architecture. Experimental results show that our implementation can respectively achieve a speedup of up to 5.5\u00d7 and 2.1\u00d7 against TVM (Chen et al. 2018) on DWConv and PWConv.", "venue": "AAAI", "authors": ["Pengfei  Zhang", "Eric  Lo", "Baotong  Lu"], "year": 2020, "n_citations": 4}
{"id": 5104876, "s2_id": "60df6c7a01551991e08a700ff6ce052342c1a3ac", "title": "User Data Sharing Frameworks: A Blockchain-Based Incentive Solution", "abstract": "Currently, there is no universal method to track who shared what, with whom, when and for what purposes in a verifiable way to create an individual incentive for data owners. A platform that allows data owners to control, delete, and get rewards from sharing their data would be an important enabler of user data-sharing. We propose a usable blockchain- and smart contracts-based framework that allows users to store research data locally and share without losing control and ownership of it. We have created smart contracts for building automatic verification of the conditions for data access that also naturally supports building up a verifiable record of the provenance, incentives for users to share their data and accountability of access. The paper presents a review of the existing work of research data sharing, the proposed blockchain-based framework and an evaluation of the framework by measuring the transaction cost for smart contracts deployment. The results show that nodes responded quickly in all tested cases with a befitting transaction cost.", "venue": "2019 IEEE 10th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)", "authors": ["Ajay Kumar Shrestha", "Julita  Vassileva"], "year": 2019, "n_citations": 8}
{"id": 5108953, "s2_id": "071c576e69755a970835b55ac8cb7fc06d273117", "title": "Parallel In-Place Algorithms: Theory and Practice", "abstract": "Many parallel algorithms use at least linear auxiliary space in the size of the input to enable computations to be done independently without conflicts. Unfortunately, this extra space can be prohibitive for memory-limited machines, preventing large inputs from being processed. Therefore, it is desirable to design parallel in-place algorithms that use sublinear (or even polylogarithmic) auxiliary space. In this paper, we bridge the gap between theory and practice for parallel in-place (PIP) algorithms. We first define two computational models based on fork-join parallelism, which reflect modern parallel programming environments. We then introduce a variety of new parallel in-place algorithms that are simple and efficient, both in theory and in practice. Our algorithmic highlight is the Decomposable Property introduced in this paper, which enables existing non-in-place but highlyoptimized parallel algorithms to be converted into parallel in-place algorithms. Using this property, we obtain algorithms for random permutation, list contraction, tree contraction, and merging that take linear work, O(n1\u2212 ) auxiliary space, and O(n \u00b7 polylog(n)) span for 0 < < 1. We also present new parallel in-place algorithms for scan, filter, merge, connectivity, biconnectivity, and minimum spanning forest using other techniques. In addition to theoretical results, we present experimental results for implementations of many of our parallel in-place algorithms. We show that on a 72-core machine with twoway hyper-threading, the parallel in-place algorithms usually outperform existing parallel algorithms for the same problems that use linear auxiliary space, indicating that the theory developed in this paper indeed leads to practical benefits in terms of both space usage and running time.", "venue": "APOCS", "authors": ["Yan  Gu", "Omar  Obeya", "Julian  Shun"], "year": 2021, "n_citations": 3}
{"id": 5109775, "s2_id": "6ceff484a788a40a39911a3d6a0dd0842a73b436", "title": "An asymptotically optimal policy and state-space collapse for the multi-class shared queue", "abstract": "We consider a multi-class G/G/1 queue with a finite shared buffer. There is task admission and server scheduling control which aims to minimize the cost which consists of holding and rejection components. We construct a policy that is asymptotically optimal in the heavy traffic limit. The policy stems from solution to Harrison-Taksar (HT) free boundary problem and is expressed by a single free boundary point. We show that the HT problem solution translated into the queuelength processes follows a specific {\\it triangular} form. This form implies the queuelength control policy which is different from the known $c\\mu$ priority rule and has a novel structure. \nWe exemplify that the probabilistic methods we exploit can be successfully applied to solving scheduling and admission problems in cloud computing.", "venue": "ArXiv", "authors": ["Mark  Shifrin"], "year": 2015, "n_citations": 1}
{"id": 5112385, "s2_id": "a32c56acc0ad76266e3947880602385ee8624983", "title": "Comparison of Parallelisation Approaches, Languages, and Compilers for Unstructured Mesh Algorithms on GPUs", "abstract": "Efficiently exploiting GPUs is increasingly essential in scientific computing, as many current and upcoming supercomputers are built using them. To facilitate this, there are a number of programming approaches, such as CUDA, OpenACC and OpenMP 4, supporting different programming languages (mainly C/C++ and Fortran). There are also several compiler suites (clang, nvcc, PGI, XL) each supporting different combinations of languages. In this study, we take a detailed look at some of the currently available options, and carry out a comprehensive analysis and comparison using computational loops and applications from the domain of unstructured mesh computations. Beyond runtimes and performance metrics (GB/s), we explore factors that influence performance such as register counts, occupancy, usage of different memory types, instruction counts, and algorithmic differences. Results of this work show how clang\u2019s CUDA compiler frequently outperform NVIDIA\u2019s nvcc, performance issues with directive-based approaches on complex kernels, and OpenMP 4 support maturing in clang and XL; currently around 10% slower than CUDA.", "venue": "PMBS@SC", "authors": ["G\u00e1bor D\u00e1niel Balogh", "I. Z. Reguly", "Gihan R. Mudalige"], "year": 2017, "n_citations": 3}
{"id": 5120271, "s2_id": "e7ac932e1ee6357f369e88a9bdaedbf6f1c4d18d", "title": "Evaluation of Proactive, Reactive and Hybrid Ad hoc Routing Protocol for various Battery models in VANET using Qualnet", "abstract": "2 Abstract - In VANET high speed is the real characteristics which leads frequent breakdown, interference etc. In this paper we studied various Ad hoc routing protocols, Reactive, Proactive & Hybrid, taking in to consideration various VANET parameters like speed, altitude etc in real traffic scenario and evaluated them for various battery models for energy conservation.. The AODV and DYMO (Reactive), OLSR (Proactive) and ZRP (hybrid) protocols are compared for battery models Duracell AA(MX- 1500),Duracell AAA(MN-2400),Duracell AAA(MX-2400), Duracell C-MN(MN-1400),Panasonic AA standard using Qualnet as a Simulation tool. Since Energy conservation is main focus area now days. Hence performance of the protocols with various battery models counts and helps to make a right selection. Varying parameters of VANET shows that in the real traffic scenarios proactive protocol performs more efficiently for energy conservation.", "venue": "ArXiv", "authors": ["Manish  Sharma", "Gurpadam  Singh"], "year": 2012, "n_citations": 17}
{"id": 5122486, "s2_id": "6afe6686274fffafad4b94e112e9fd087a3971f1", "title": "Mechanism to Mitigate AVX-Induced Frequency Reduction", "abstract": "Modern Intel CPUs reduce their frequency when executing wide vector operations (AVX2 and AVX-512 instructions), as these instructions increase power consumption. The frequency is only increased again two milliseconds after the last code section containing such instructions has been executed in order to prevent excessive numbers of frequency changes. Due to this delay, intermittent use of wide vector operations can slow down the rest of the system significantly. For example, previous work has shown the performance of web servers to be reduced by up to 10% if the SSL library uses AVX-512 vector instructions. These performance variations are hard to predict during software development as the performance impact of vectorization depends on the specific workload. \nWe describe a mechanism to reduce the slowdown caused by wide vector instructions without requiring extensive changes to existing software. Our design allows the developer to mark problematic AVX code regions. The scheduler then restricts execution of this code to a subset of the cores so that only these cores' frequency is affected. Threads are automatically migrated to a suitable core whenever necessary. We identify a suitable load balancing policy to ensure good utilization of all available cores. Our approach is able to reduce the performance variability caused by AVX2 and AVX-512 instructions by over 70%.", "venue": "ArXiv", "authors": ["Mathias  Gottschlag", "Frank  Bellosa"], "year": 2019, "n_citations": 4}
{"id": 5123508, "s2_id": "33d0b24fb0fc3d985841b69c3081839de972971e", "title": "BlockSim: An Extensible Simulation Tool for Blockchain Systems", "abstract": "Both in the design and deployment of blockchain solutions many performance-impacting configuration choices need to be made. We introduce BlockSim, a framework and software tool to build and simulate discrete-event dynamic systems models for blockchain systems. BlockSim is designed to support the analysis of a large variety of blockchains and blockchain deployments as well as a wide set of analysis questions. At the core of BlockSim is a Base Model, which contains the main model constructs common across various blockchain systems organized in three abstraction layers (network, consensus, and incentives layer). The Base Model is usable for a wide variety of blockchain systems and can be extended easily to include system or deployment particulars. The BlockSim software tool provides a simulator that implements the Base Model in Python. The paper describes the Base Model, the simulator implementation, and the application of BlockSim to Bitcoin, Ethereum and other consensus algorithms. We validate BlockSim simulation results by comparison with performance results from actual systems and from other studies in the literature. We close the paper by a BlockSim simulation study of the impact of uncle blocks rewards on mining decentralization, for a variety of blockchain configurations.", "venue": "Frontiers in Blockchain", "authors": ["Maher  Alharby", "Aad van Moorsel"], "year": 2020, "n_citations": 16}
{"id": 5125546, "s2_id": "c6bb629a4a51d6feda5849514a400735763e8248", "title": "Memory-Efficient Deep Learning Inference in Trusted Execution Environments", "abstract": "This study identifies and proposes techniques to alleviate two key bottlenecks to executing deep neural networks in trusted execution environments (TEEs): page thrashing during the execution of convolutional layers and the decryption of large weight matrices in fully-connected layers. For the former, we propose a novel partitioning scheme, y-plane partitioning, designed to (i) provide consistent execution time when the layer output is large compared to the TEE secure memory; and (ii) significantly reduce the memory footprint of convolutional layers. For the latter, we leverage quantization and compression. In our evaluation, the proposed optimizations incurred latency overheads ranging from 1.09X to 2X baseline for a wide range of TEE sizes; in contrast, an unmodified implementation incurred latencies of up to 26X when running inside of the TEE.", "venue": "2021 IEEE International Conference on Cloud Engineering (IC2E)", "authors": ["Jean-Baptiste  Truong", "William  Gallagher", "Tian  Guo", "Robert J. Walls"], "year": 2021, "n_citations": 0}
{"id": 5129796, "s2_id": "89dc7d52ea3d12e77b117fc8fdd6f3be3860f76e", "title": "MDINFERENCE: Balancing Inference Accuracy and Latency for Mobile Applications", "abstract": "Deep Neural Networks are allowing mobile devices to incorporate a wide range of features into user applications. However, the computational complexity of these models makes it difficult to run them effectively on resource-constrained mobile devices. Prior work approached the problem of supporting deep learning in mobile applications by either decreasing model complexity or utilizing powerful cloud servers. These approaches each only focus on a single aspect of mobile inference and thus they often sacrifice overall performance.In this work we introduce a holistic approach to designing mobile deep inference frameworks. We first identify the key goals of accuracy and latency for mobile deep inference and the conditions that must be met to achieve them. We demonstrate our holistic approach through the design of a hypothetical framework called MDINFERENCE. This framework leverages two complementary techniques; a model selection algorithm that chooses from a set of cloud-based deep learning models to improve inference accuracy and an on-device request duplication mechanism to bound latency. Through empirically-driven simulations we show that MDINFERENCE improves aggregate accuracy over static approaches by over 40% without incurring SLA violations. Additionally, we show that with a target latency of 250ms, MDINFERENCE increased the aggregate accuracy in 99.74% cases on faster university networks and 96.84% cases on residential networks.", "venue": "2020 IEEE International Conference on Cloud Engineering (IC2E)", "authors": ["Samuel S. Ogden", "Tian  Guo"], "year": 2020, "n_citations": 5}
{"id": 5130024, "s2_id": "e74b5b873f14424834d9accdecdb73b027536e58", "title": "Determinism, Complexity, and Predictability in Computer Performance", "abstract": "Computers are deterministic dynamical systems (CHAOS 19:033124, 2009). Among other things, that implies that one should be able to use deterministic forecast rules to predict their behavior. That statement is sometimes-but not always-true. The memory and processor loads of some simple programs are easy to predict, for example, but those of more-complex programs like compilers are not. The goal of this paper is to determine why that is the case. We conjecture that, in practice, complexity can effectively overwhelm the predictive power of deterministic forecast models. To explore that, we build models of a number of performance traces from different programs running on different Intel-based computers. We then calculate the permutation entropy-a temporal entropy metric that uses ordinal analysis-of those traces and correlate those values against the prediction success", "venue": "ArXiv", "authors": ["Joshua  Garland", "Ryan G. James", "Elizabeth  Bradley"], "year": 2013, "n_citations": 2}
{"id": 5131464, "s2_id": "37bccfec0d01282ae6ba6cbbd5f12b7d95694159", "title": "Simulation of Resource Usage in Parallel Evolutionary Peptide Optimization using JavaSpaces Technology", "abstract": "Peptide Optimization is a highly complex problem and it takes very long time of computation. This optimization process uses many software applications in a cluster running GNU/Linux Operating System that perform special tasks. The application to organize the whole optimization process had been already developed, namely SEPP (System for Evolutionary Pareto Optimization of Peptides/Polymers). A single peptide optimization takes a lot of computation time to produce a certain number of individuals. However, it can be accelerated by increasing the degree of parallelism as well as the number of nodes (processors) in the cluster. In this master thesis, I build a model simulating the interplay of the programs so that the usage of each resource (processor) can be determined and also the approximated time needed for the overall optimization process. There are two Evolutionary Algorithms that could be used in the optimization, namely Generation-based and Steady-state Evolutionary Algorithm. The results of each Evolutionary Algorithm are shown based on the simulations. Moreover, the results are also compared by using different parameters (the degree of parallelism and the number of processors) in the simulation to give an overview of the advantages and the disadvantages of the algorithms in terms of computation time and resource usage. The model is built up using JavaSpaces Technology.", "venue": "ArXiv", "authors": ["Andias  Wira-Alam"], "year": 2009, "n_citations": 0}
{"id": 5131786, "s2_id": "abaf5ab58bbe89e5ca3bc179a0d92764f2bdd7aa", "title": "Breaking through the Full-Duplex Wi-Fi capacity gain", "abstract": "In this work we identify a seminal design guideline that prevents current Full-Duplex (FD) MAC protocols to scale the FD capacity gain (i.e. 2\u00d7 the half-duplex throughput) in single-cell Wi-Fi networks. Under such guideline (referred to as 1:1), a MAC protocol attempts to initiate up to two simultaneous transmissions in the FD bandwidth. Since in single-cell Wi-Fi networks MAC performance is bounded by the PHY layer capacity, this implies gains strictly less than 2\u00d7 over half-duplex at the MAC layer. To face this limitation, we argue for the 1:N design guideline. Under 1:N, FD MAC protocols `see' the FD bandwidth through N>1 orthogonal narrow-channel PHY layers. Based on theoretical results and software defined radio experiments, we show the 1:N design can leverage the Wi-Fi capacity gain more than 2\u00d7 at and below the MAC layer. This translates the denser modulation scheme incurred by channel narrowing and the increase in the spatial reuse factor enabled by channel orthogonality. With these results, we believe our design guideline can inspire a new generation of Wi-Fi MAC protocols that fully embody and scale the FD capacity gain.", "venue": "2016 7th International Conference on the Network of the Future (NOF)", "authors": ["Saulo  Queiroz", "Jo\u00e3o P. Vilela", "Roberto  Hexsel"], "year": 2016, "n_citations": 0}
{"id": 5133386, "s2_id": "0ef61421cea5cacc0d2b76e7e0e6a77172e77042", "title": "Separable projection integrals for higher-order correlators of the cosmic microwave sky: Acceleration by factors exceeding 100", "abstract": "We present a case study describing efforts to optimise and modernise \"Modal\", the simulation and analysis pipeline used by the Planck satellite experiment for constraining general non-Gaussian models of the early universe via the bispectrum (or three-point correlator) of the cosmic microwave background radiation. We focus on one particular element of the code: the projection of bispectra from the end of inflation to the spherical shell at decoupling, which defines the CMB we observe today. This code involves a three-dimensional inner product between two functions, one of which requires an integral, on a non-rectangular domain containing a sparse grid. We show that by employing separable methods this calculation can be reduced to a one-dimensional summation plus two integrations, reducing the overall dimensionality from four to three. The introduction of separable functions also solves the issue of the non-rectangular sparse grid. This separable method can become unstable in certain scenarios and so the slower non-separable integral must be calculated instead. We present a discussion of the optimisation of both approaches.We demonstrate significant speed-ups of ?100\u00d7, arising from a combination of algorithmic improvements and architecture-aware optimisations targeted at improving thread and vectorisation behaviour. The resulting MPI/OpenMP hybrid code is capable of executing on clusters containing processors and/or coprocessors, with strong-scaling efficiency of 98.6% on up to 16 nodes. We find that a single coprocessor outperforms two processor sockets by a factor of 1.3\u00d7 and that running the same code across a combination of both microarchitectures improves performance-per-node by a factor of 3.38\u00d7. By making bispectrum calculations competitive with those for the power spectrum (or two-point correlator) we are now able to consider joint analysis for cosmological science exploitation of new data.", "venue": "J. Comput. Phys.", "authors": ["James  Briggs", "Simon J. Pennycook", "J. R. Fergusson", "J.  J\u00e4ykk\u00e4", "E. P. S. Shellard"], "year": 2016, "n_citations": 4}
{"id": 5133879, "s2_id": "034b6211bc503deb3b6f5925ce099d33b0494822", "title": "KLARAPTOR: A Tool for Dynamically Finding Optimal Kernel Launch Parameters Targeting CUDA Programs", "abstract": "In this paper we present KLARAPTOR (Kernel LAunch parameters RAtional Program estimaTOR), a new tool built on top of the LLVM Pass Framework and NVIDIA CUPTI API to dynamically determine the optimal values of kernel launch parameters of a CUDA program P. To be precise, we describe a novel technique to statically build (at the compile time of P) a so-called rational program R. Using a performance prediction model, and knowing particular data and hardware parameters of P at runtime, the program R can automatically and dynamically determine the values of launch parameters of P that will yield optimal performance. Our technique can be applied to parallel programs in general, as well as to generic performance prediction models which account for program and hardware parameters. We are particularly interested in programs targeting manycore accelerators. We have implemented and successfully tested our technique in the context of GPU kernels written in CUDA using the MWP-CWP performance prediction model.", "venue": "ArXiv", "authors": ["Alexander  Brandt", "Davood  Mohajerani", "Marc Moreno Maza", "Jeeva  Paudel", "Linxiao  Wang"], "year": 2019, "n_citations": 0}
{"id": 5136872, "s2_id": "19fb68e3295c8ff4fb094279167d615ff12288a8", "title": "Iterative Variable Reordering: Taming Huge System Families", "abstract": "For the verification of systems using model-checking techniques, symbolic representations based on binary decision diagrams (BDDs) often help to tackle the well-known state-space explosion problem. Symbolic BDD-based representations have been also shown to be successful for the analysis of families of systems that arise, e.g., through configurable parameters or following the feature-oriented modeling approach. The state space of such system families face an additional exponential blowup in the number of parameters or features. It is well known that the order of variables in ordered BDDs is crucial for the size of the model representation. Especially for automatically generated models from real-world systems, family models might even be not constructible due to bad variable orders. In this paper we describe a technique, called iterative variable reordering, that can enable the construction of large-scale family models. We exemplify feasibility of our approach by means of an aircraft velocity control system with redundancy mechanisms modeled in the input language of the probabilistic model checker PRISM. We show that standard reordering and dynamic reordering techniques fail to construct the family model due to memory and time constraints, respectively, while the new iterative approach succeeds to generate a symbolic family model.", "venue": "MARS@ETAPS", "authors": ["Clemens  Dubslaff", "Andrey  Morozov", "Christel  Baier", "Klaus  Janschek"], "year": 2020, "n_citations": 1}
{"id": 5152345, "s2_id": "00de7982628b375b0c2ddf5acd28df1542706305", "title": "Modeling the Probability of Failure on LDAP Binding Operations in Iplanet Web Proxy 3.6 Server", "abstract": "This paper is devoted to the theoretical analysis of a problem derived from interaction between two Iplanet products: Web Proxy Server and the Directory Server. In particular, a probabilistic and stochastic-approximation model is proposed to minimize the occurrence of LDAP connection failures in Iplanet Web Proxy 3.6 Server. The proposed model serves not only to provide a parameterization of the aforementioned phenomena, but also to provide meaningful insights illustrating and supporting these theoretical results. In addition, we shall also address practical considerations when estimating the parameters of the proposed model from experimental data. Finally, we shall provide some interesting results from real-world data collected from our customers.", "venue": "ArXiv", "authors": ["Alejandro Chinea Manrique De Lara"], "year": 2010, "n_citations": 0}
{"id": 5158072, "s2_id": "61c4b219025511b64dd52e6f6597c7ba0d3c1a62", "title": "A Fast and Scalable Joint Estimator for Learning Multiple Related Sparse Gaussian Graphical Models", "abstract": "Estimating multiple sparse Gaussian Graphical Models (sGGMs) jointly for many related tasks (large $K$) under a high-dimensional (large $p$) situation is an important task. Most previous studies for the joint estimation of multiple sGGMs rely on penalized log-likelihood estimators that involve expensive and difficult non-smooth optimizations. We propose a novel approach, FASJEM for \\underline{fa}st and \\underline{s}calable \\underline{j}oint structure-\\underline{e}stimation of \\underline{m}ultiple sGGMs at a large scale. As the first study of joint sGGM using the Elementary Estimator framework, our work has three major contributions: (1) We solve FASJEM through an entry-wise manner which is parallelizable. (2) We choose a proximal algorithm to optimize FASJEM. This improves the computational efficiency from $O(Kp^3)$ to $O(Kp^2)$ and reduces the memory requirement from $O(Kp^2)$ to $O(K)$. (3) We theoretically prove that FASJEM achieves a consistent estimation with a convergence rate of $O(\\log(Kp)/n_{tot})$. On several synthetic and four real-world datasets, FASJEM shows significant improvements over baselines on accuracy, computational complexity, and memory costs.", "venue": "AISTATS", "authors": ["Beilun  Wang", "Ji  Gao", "Yanjun  Qi"], "year": 2017, "n_citations": 2}
{"id": 5161030, "s2_id": "92b3a08ea71aa7f8d2fec061a322ed7c8a7cca9a", "title": "Data Warehouse and Decision Support on Integrated Crop Big Data", "abstract": "In recent years, precision agriculture is becoming very popular. The introduction of modern information and communication technologies for collecting and processing Agricultural data revolutionise the agriculture practises. This has started a while ago (early 20th century) and it is driven by the low cost of collecting data about everything; from information on fields such as seed, soil, fertiliser, pest, to weather data, drones and satellites images. Specially, the agricultural data mining today is considered as Big Data application in terms of volume, variety, velocity and veracity. Hence it leads to challenges in processing vast amounts of complex and diverse information to extract useful knowledge for the farmer, agronomist, and other businesses. It is a key foundation to establishing a crop intelligence platform, which will enable efficient resource management and high quality agronomy decision making and recommendations. In this paper, we designed and implemented a continental level agricultural data warehouse (ADW). ADW is characterised by its (1) flexible schema; (2) data integration from real agricultural multi datasets; (3) data science and business intelligent support; (4) high performance; (5) high storage; (6) security; (7) governance and monitoring; (8) consistency, availability and partition tolerant; (9) cloud compatibility. We also evaluate the performance of ADW and present some complex queries to extract and return necessary knowledge about crop management.", "venue": "Int. J. Bus. Process. Integr. Manag.", "authors": ["V. M. Ngo", "N. A. Le-Khac", "M. T. Kechadi"], "year": 2020, "n_citations": 2}
{"id": 5162075, "s2_id": "0a378734bd24d90cdce707de6a0bf2d6c68c650a", "title": "Performance Modeling of Epidemic Routing in Mobile Social Networks with Emphasis on Scalability", "abstract": "This paper investigates the performance of epidemic routing in mobile social networks. It first analyzes the time taken for a node to meet the first node of a set of nodes restricted to move in a specific subarea. Afterwards, a monolithic Stochastic Reward Net (SRN) is proposed to evaluate the delivery delay and the average number of transmissions under epidemic routing by considering skewed location visiting preferences. This model is not scalable enough, in terms of the number of nodes and frequently visited locations. In order to achieve higher scalability, the folding technique is applied to the monolithic model, and an approximate folded SRN is proposed to evaluate performance of epidemic routing. Discrete-event simulation is used to validate the proposed models. Both SRN models show high accuracy in predicting the performance of epidemic routing. We also propose an Ordinary Differential Equation (ODE) model for epidemic routing and compare it with the folded model. Obtained results show that the folded model is more accurate than the ODE model. Moreover, it is proved that the number of transmissions by the time of delivery follows uniform distribution, in a general class of networks, where positions of nodes are always independent and identically distributed.", "venue": "ArXiv", "authors": ["Leila  Rashidi", "Amir  Dalili-Yazdi", "Reza  Entezari-Maleki", "Leonel  Sousa", "Ali  Movaghar"], "year": 2020, "n_citations": 0}
{"id": 5163856, "s2_id": "262c0e54370dfc03a7ad53d79930568d18dd448c", "title": "Speeding Up Distributed Machine Learning Using Codes", "abstract": "Codes are widely used in many engineering applications to offer <italic>robustness</italic> against <italic>noise</italic>. In large-scale systems, there are several types of noise that can affect the performance of distributed machine learning algorithms\u2014straggler nodes, system failures, or communication bottlenecks\u2014but there has been little interaction cutting across codes, machine learning, and distributed systems. In this paper, we provide theoretical insights on how <italic>coded</italic> solutions can achieve significant gains compared with uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: <italic>matrix multiplication</italic> and <italic>data shuffling</italic>. For matrix multiplication, we use codes to alleviate the effect of stragglers and show that if the number of homogeneous workers is <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$\\log n$ </tex-math></inline-formula>. For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction <inline-formula> <tex-math notation=\"LaTeX\">$\\alpha $ </tex-math></inline-formula> of the data matrix can be cached at each worker, and <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> is the number of workers, <italic>coded shuffling</italic> reduces the communication cost by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$\\left({\\alpha + \\frac {1}{n}}\\right)\\gamma (n)$ </tex-math></inline-formula> compared with uncoded shuffling, where <inline-formula> <tex-math notation=\"LaTeX\">$\\gamma (n)$ </tex-math></inline-formula> is the ratio of the cost of unicasting <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> messages to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users to multicasting a common message (of the same size) to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users. For instance, <inline-formula> <tex-math notation=\"LaTeX\">$\\gamma (n) \\simeq n$ </tex-math></inline-formula> if multicasting a message to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users is as cheap as unicasting a message to one user. We also provide experimental results, corroborating our theoretical gains of the coded algorithms.", "venue": "IEEE Transactions on Information Theory", "authors": ["Kangwook  Lee", "Maximilian  Lam", "Ramtin  Pedarsani", "Dimitris  Papailiopoulos", "Kannan  Ramchandran"], "year": 2018, "n_citations": 424}
{"id": 5164242, "s2_id": "fdf64fe4cc1ba9e9f16ae441d77c55197229f86f", "title": "RT-DAP: A Real-Time Data Analytics Platform for Large-Scale Industrial Process Monitoring and Control", "abstract": "In most process control systems nowadays, process measurements are periodically collected and archived in historians. Analytics applications process the data, and provide results offline or in a time period that is considerably slow in comparison to the performance of many manufacturing processes. Along with the proliferation of Internet-of-Things (IoT) and the introduction of \"pervasive sensors\" technology in process industries, increasing number of sensors and actuators are installed in process plants for pervasive sensing and control, and the volume of produced process data is growing exponentially. To digest these data and meet the ever-growing requirements to increase production efficiency and improve product quality, there needs a way to both improve the performance of the analytic system and scale the system to closely monitor a much larger set of plant resources. In this paper, we present a real-time data analytics platform, referred to as RT-DAP, to support large-scale continuous data analytics in process industries. RT-DAP is designed to be able to stream, store, process and visualize a large volume of real-time data flows collected from heterogeneous plant resources, and feedback to the control system and operators in a real-time manner. A prototype of the platform is implemented on Microsoft Azure. Our extensive experiments validate the design methodologies of RT-DAP and demonstrate its efficiency in both component and system levels.", "venue": "2018 IEEE International Conference on Industrial Internet (ICII)", "authors": ["Song  Han", "Tao  Gong", "Mark  Nixon", "Eric  Rotvold", "Kam-yiu  Lam", "Krithi  Ramamritham"], "year": 2018, "n_citations": 5}
{"id": 5164602, "s2_id": "6d118e71ff1df41e1184b944babad80762a44732", "title": "Mean Field for Markov Decision Processes: From Discrete to Continuous Optimization", "abstract": "We study the convergence of Markov decision processes, composed of a large number of objects, to optimization problems on ordinary differential equations. We show that the optimal reward of such a Markov decision process, which satisfies a Bellman equation, converges to the solution of a continuous Hamilton-Jacobi-Bellman (HJB) equation based on the mean field approximation of the Markov decision process. We give bounds on the difference of the rewards and an algorithm for deriving an approximating solution to the Markov decision process from a solution of the HJB equations. We illustrate the method on three examples pertaining, respectively, to investment strategies, population dynamics control and scheduling in queues. They are used to illustrate and justify the construction of the controlled ODE and to show the advantage of solving a continuous HJB equation rather than a large discrete Bellman equation.", "venue": "IEEE Transactions on Automatic Control", "authors": ["Nicolas  Gast", "Bruno  Gaujal", "Jean-Yves Le Boudec"], "year": 2012, "n_citations": 96}
{"id": 5166749, "s2_id": "9ea40b1e5a429cd31ec5f5dfedc948bd9caacd6e", "title": "Necessity of Future Information in Admission Control", "abstract": "We study the necessity of predictive information in a class of queueing admission control problems, where a system manager is allowed to divert incoming jobs up to a fixed rate, in order to minimize the queueing delay experienced by the admitted jobs. \nSpencer et al. (2014) show that the system's delay performance can be significantly improved by having access to future information in the form of a lookahead window, during which the times of future arrivals and services are revealed. They prove that, while delay under an optimal online policy diverges to infinity in the heavy-traffic regime, it can stay bounded by making use of future information. However, the diversion polices of Spencer et al. (2014) require the length of the lookahead window to grow to infinity at a non-trivial rate in the heavy-traffic regime, and it remained open whether substantial performance improvement could still be achieved with less future information. \nWe resolve this question to a large extent by establishing an asymptotically tight lower bound on how much future information is necessary to achieve superior performance, which matches the upper bound of Spencer et al. (2014) up to a constant multiplicative factor. Our result hence demonstrates that the system's heavy-traffic delay performance is highly sensitive to the amount of future information available. Our proof is based on analyzing certain excursion probabilities of the input sample paths, and exploiting a connection between a policy's diversion decisions and subsequent server idling, which may be of independent interest for related dynamic resource allocation problems.", "venue": "Oper. Res.", "authors": ["Kuang  Xu"], "year": 2015, "n_citations": 19}
{"id": 5174352, "s2_id": "44a7d769e34ebdd42ae12699cc6c9c11d8bf3b64", "title": "RegDem: Increasing GPU Performance via Shared Memory Register Spilling", "abstract": "GPU utilization, measured as occupancy, is limited by the parallel threads' combined usage of on-chip resources, such as registers and the programmer-managed shared memory. Higher resource demand means lower effective parallel thread count, and therefore lower program performance. Our investigation found that registers are often the occupancy limiters. \nThe de-facto nvcc compiler-based approach spills excessive registers to the off-chip memory, ignoring the shared memory and leaving the on-chip resources underutilized. To mitigate the register demand, this paper presents a binary translation technique, called RegDem, that spills excessive registers to the underutilized shared memory by transforming the GPU assembly code (SASS). Most GPU programs do not fully use shared memory, thus allowing RegDem to use it for register spilling. The higher occupancy achieved by RegDem outweighs the slightly higher cost of accessing shared memory instead of placing data in registers. The paper also presents a compile-time performance predictor that models instructions stalls to choose the best version from a set of program variants. Cumulatively, these techniques outperform the nvcc compiler with a 9% geometric mean, the highest observed being 18%.", "venue": "ArXiv", "authors": ["Putt  Sakdhnagool", "Amit  Sabne", "Rudolf  Eigenmann"], "year": 2019, "n_citations": 4}
{"id": 5174611, "s2_id": "047ae8cd14e0515cacf417d0c07e1311d07d0be9", "title": "Uncertainty Analysis of the Adequacy Assessment Model of a Distributed Generation System", "abstract": "Due to the inherent aleatory uncertainties in renewable generators, the reliability/adequacy assessments of distributed generation (DG) systems have been particularly focused on the probabilistic modeling of random behaviors, given sufficient informative data. However, another type of uncertainty (epistemic uncertainty) must be accounted for in the modeling, due to incomplete knowledge of the phenomena and imprecise evaluation of the related characteristic parameters. In circumstances of few informative data, this type of uncertainty calls for alternative methods of representation, propagation, analysis and interpretation. In this study, we make a first attempt to identify, model, and jointly propagate aleatory and epistemic uncertainties in the context of DG systems modeling for adequacy assessment. Probability and possibility distributions are used to model the aleatory and epistemic uncertainties, respectively. Evidence theory is used to incorporate the two uncertainties under a single framework. Based on the plausibility and belief functions of evidence theory, the hybrid propagation approach is introduced. A demonstration is given on a DG system adapted from the IEEE 34 nodes distribution test feeder. Compared to the pure probabilistic approach, it is shown that the hybrid propagation is capable of explicitly expressing the imprecision in the knowledge on the DG parameters into the final adequacy values assessed. It also effectively captures the growth of uncertainties with higher DG penetration levels.", "venue": "ArXiv", "authors": ["Yanfu  Li", "Enrico  Zio"], "year": 2012, "n_citations": 81}
{"id": 5177439, "s2_id": "8e1a667a1f4e61bc47e18649b4438b917f62e5df", "title": "A Stochastic Broadcast Pi-Calculus", "abstract": "In this paper we propose a stochastic broadcast \ufffd-calculus which can be used to model server-client based systems where synchronization is always governed by only one participant. Therefore, there is no need to determine the joint synchronization rates. We also take immediate transitions into account which is useful to model behaviors with no impact on the temporal properties of a system. Since immediate transitions may introduce non-determinism, we will show how these non-determinism can be resolved, and as result a valid CTMC will be obtained fin ally. Also some practical examples are given to show the application of this calculus.", "venue": "QAPL", "authors": ["Lei  Song", "Flemming  Nielson", "Bo Friis Nielsen"], "year": 2011, "n_citations": 1}
{"id": 5178897, "s2_id": "dba1ce223ee2b2a1952894b7913abb6e9204ae08", "title": "I/O Workload Management for All-Flash Datacenter Storage Systems Based on Total Cost of Ownership", "abstract": "Recently, the capital expenditure of flash-based Solid State Driver (SSDs) keeps declining and the storage capacity of SSDs keeps increasing. As a result, all-flash storage systems have started to become more economically viable for large shared storage installations in datacenters, where metrics like Total Cost of Ownership (TCO) are of paramount importance. On the other hand, flash devices suffer from write amplification, which, if unaccounted, can substantially increase the TCO of a storage system. In this paper, we first develop a TCO model for datacenter all-flash storage systems, and then plug a Write Amplification model (WAF) of NVMe SSDs we build based on empirical data into this TCO model. Our new WAF model accounts for workload characteristics like write rate and percentage of sequential writes. Furthermore, using both the TCO and WAF models as the optimization criterion, we design new flash resource management schemes (MINTCO) to guide datacenter managers to make workload allocation decisions under the consideration of TCO for SSDs. Based on that, we also develop MINTCO-RAID to support RAID SSDs and MINTCO-OFFLINE to optimize the offline workload-disk deployment problem during the initialization phase. Experimental results show that MINTCO can reduce the TCO and keep relatively high throughput and space utilization of the entire datacenter storage resources.", "venue": "ArXiv", "authors": ["Zhengyu  Yang", "Manu  Awasthi", "Mrinmoy  Ghosh", "Janki  Bhimani", "Ningfang  Mi"], "year": 2018, "n_citations": 2}
{"id": 5184986, "s2_id": "feb5a9009e19ba550a11a4a593f789c2b505dc02", "title": "Combinatorial Sleeping Bandits with Fairness Constraints", "abstract": "The multi-armed bandit (MAB) model has been widely adopted for studying many practical optimization problems (network resource allocation, ad placement, crowdsourcing, etc.) with unknown parameters. The goal of the player i.e., the decision maker) here is to maximize the cumulative reward in the face of uncertainty. However, the basic MAB model neglects several important factors of the system in many real-world applications, where multiple arms (i.e., actions) can be simultaneously played and an arm could sometimes be \u201csleeping\u201d (i.e., unavailable). Besides reward maximization, ensuring fairness is also a key design concern in practice. To that end, we propose a new Combinatorial Sleeping $MAB$ model with Fairness constraints, called CSMAB-F, aiming to address the aforementioned crucial modeling issues. The objective is now to maximize the reward while satisfying the fairness requirement of a minimum selection fraction for each individual arm. To tackle this new problem, we extend an online learning algorithm, called Upper Confidence Bound (UCB), to deal with a critical tradeoff between exploitation and exploration and employ the virtual queue technique to properly handle the fairness constraints. By carefully integrating these two techniques, we develop a new algorithm, called Learning with Fairness Guarantee (LFG), for the CSMAB-F problem. Further, we rigorously prove that not only LFG is feasibility-optimal but it also has a time-average regret upper bounded by $\\displaystyle \\frac {N}{2\\eta }+\\frac {\\beta _{1}\\sqrt {mNT\\log T}+\\beta _{2}N}{T}$, where $N$ is the total number of arms, $m$ is the maximum number of arms that can be simultaneously played, $T$ is the time horizon, $\\beta _{1}$ and $\\beta _{2}$ are constants, and $\\eta $ is a design parameter that we can tune. Finally, we perform extensive simulations to corroborate the effectiveness of the proposed algorithm. Interestingly, the simulation results reveal an important tradeoff between the regret and the speed of convergence to a point satisfying the fairness constraints.", "venue": "IEEE INFOCOM 2019 - IEEE Conference on Computer Communications", "authors": ["Fengjiao  Li", "Jia  Liu", "Bo  Ji"], "year": 2019, "n_citations": 60}
{"id": 5185099, "s2_id": "7e411c8b282a39aef2f99f3308480502532818d9", "title": "On Competitive Analysis for Polling Systems", "abstract": "Polling systems have been widely studied, however most of these studies focus on polling systems with renewal processes for arrivals and random variables for service times. There is a need driven by practical applications to study polling systems with arbitrary arrivals (not restricted to time-varying or in batches) and revealed service time upon a job's arrival. To address that need, our work considers a polling system with generic setting and for the first time provides the worst-case analysis for online scheduling policies in this system. We provide conditions for the existence of constant competitive ratios, and competitive lower bounds for general scheduling policies in polling systems. Our work also bridges the queueing and scheduling communities by proving the competitive ratios for several well-studied policies in the queueing literature, such as cyclic policies with exhaustive, gated or l-limited service disciplines for polling systems.", "venue": "ArXiv", "authors": ["Jin  Xu", "Natarajan  Gautam"], "year": 2020, "n_citations": 1}
{"id": 5186094, "s2_id": "790e3d062c27af02f461ded6bcbfa91203a69e5f", "title": "Multicore-aware parallel temporal blocking of stencil codes for shared and distributed memory", "abstract": "New algorithms and optimization techniques are needed to balance the accelerating trend towards bandwidth-starved multicore chips. It is well known that the performance of stencil codes can be improved by temporal blocking, lessening the pressure on the memory interface. We introduce a new pipelined approach that makes explicit use of shared caches in multicore environments and minimizes synchronization and boundary overhead. For clusters of shared-memory nodes we demonstrate how temporal blocking can be employed successfully in a hybrid shared/distributed-memory environment.", "venue": "2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)", "authors": ["Markus  Wittmann", "Georg  Hager", "Gerhard  Wellein"], "year": 2010, "n_citations": 44}
{"id": 5186319, "s2_id": "6f20d2360609b4b35bb5eca66f957a596e7d171f", "title": "Blind GB-PANDAS: A Blind Throughput-Optimal Load Balancing Algorithm for Affinity Scheduling", "abstract": "Dynamic affinity load balancing of multi-type tasks on multi-skilled servers, when the service rate of each task type on each of the servers is known and can possibly be different from each other, is an open problem for over three decades. The goal is to do task assignment on servers in a real time manner so that the system becomes stable, which means that the queue lengths do not diverge to infinity in steady state (throughput optimality), and the mean task completion time is minimized (delay optimality). The fluid model planning, Max-Weight, and c- $\\mu $ -rule algorithms have theoretical guarantees on optimality in some aspects for the affinity problem, but they consider a complicated queueing structure and either require the task arrival rates, the service rates of tasks on servers, or both. In many cases that are discussed in the introduction section, both task arrival rates and service rates of different task types on different servers are unknown. In this work, the Blind GB-PANDAS algorithm is proposed which is completely blind to task arrival rates and service rates. Blind GB-PANDAS uses an exploration-exploitation approach for load balancing. We prove that Blind GB-PANDAS is throughput optimal under arbitrary and unknown distributions for service times of different task types on different servers and unknown task arrival rates. Blind GB-PANDAS desires to route an incoming task to the server with the minimum weighted-workload, but since the service rates are unknown, such routing of incoming tasks is not guaranteed which makes the throughput optimality analysis more complicated than the case where service rates are known. Our extensive experimental results reveal that Blind GB-PANDAS significantly outperforms existing methods in terms of mean task completion time at high loads.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Ali  Yekkehkhany", "Rakesh  Nagi"], "year": 2020, "n_citations": 10}
{"id": 5190282, "s2_id": "5bcd3526add3f3eaf4f1d3091885477c72dcbf62", "title": "ScaleSimulator: A Fast and Cycle-Accurate Parallel Simulator for Architectural Exploration", "abstract": "Design of next generation computer systems should be supported by simulation infrastructure that must achieve a few contradictory goals such as fast execution time, high accuracy, and enough flexibility to allow comparison between large numbers of possible design points. Most existing architecture level simulators are designed to be flexible and to execute the code in parallel for greater efficiency, but at the cost of scarified accuracy. This paper presents the ScaleSimulator simulation environment, which is based on a new design methodology whose goal is to achieve near cycle accuracy while still being flexible enough to simulate many different future system architectures and efficient enough to run meaningful workloads. We achieve these goals by making the parallelism a first-class citizen in our methodology. Thus, this paper focuses mainly on the ScaleSimulator design points that enable better parallel execution while maintaining the scalability and cycle accuracy of a simulated architecture. The paper indicates that the new proposed ScaleSimulator tool can (1) efficiently parallelize the execution of a cycle-accurate architecture simulator, (2) efficiently simulate complex architectures (e.g., out-of-order CPU pipeline, cache coherency protocol, and network) and massive parallel systems, and (3) use meaningful workloads, such as full simulation of OLTP benchmarks, to examine future architectural choices.", "venue": "SimuTools", "authors": ["Chalak  Ori", "Weiguang  Cai", "Wei  Li", "Lei  Fang", "Libing  Zheng", "Jintang  Wang", "Zuguang  Wu", "Xiongli  Gu", "Haibin  Wang", "Avi  Mendelson"], "year": 2017, "n_citations": 0}
{"id": 5191627, "s2_id": "84bd344370d4bb24005dc6886860e711a6749ef1", "title": "A Comparative Evaluation of Log-Based Process Performance Analysis Techniques", "abstract": "Process mining has gained traction over the past decade and an impressive body of research has resulted in the introduction of a variety of process mining approaches measuring process performance. Having this set of techniques available, organizations might find it difficult to identify which approach is best suited considering context, performance indicator, and data availability. In light of this challenge, this paper aims at introducing a framework for categorizing and selecting performance analysis approaches based on existing research. We start from a systematic literature review for identifying the existing works discussing how to measure process performance based on information retrieved from event logs. Then, the proposed framework is built starting from the information retrieved from these studies taking into consideration different aspects of performance analysis.", "venue": "BIS", "authors": ["Fredrik  Milani", "Fabrizio Maria Maggi"], "year": 2018, "n_citations": 3}
{"id": 5193200, "s2_id": "a73b5eacaed60700cf11f67d299a99660b1e9c1c", "title": "The L-CSC cluster: Optimizing power efficiency to become the greenest supercomputer in the world in the Green500 list of November 2014", "abstract": "The L-CSC Lattice Computer for Scientific Computing is a general purpose compute cluster built with commodity hardware installed at GSI. Its main operational purpose is Lattice QCD LQCD calculations for physics simulations. Quantum Chromo Dynamics QCD is the physical theory describing the strong force, one of the four known fundamental interactions in the universe. L-CSC leverages a multi-GPU design accommodating the huge demand of LQCD for memory bandwidth. In recent years, heterogeneous clusters with accelerators such as GPUs have become more and more powerful while supercomputers in general have shown enormous increases in power consumption making electricity costs and cooling a significant factor in the total cost of ownership. Using mainly GPUs for processing, L-CSC is very power-efficient, and its architecture was optimized to provide the greatest possible power efficiency. This paper presents the cluster design as well as optimizations to improve the power efficiency. It examines the power measurements performed for the Green500 list of the most power-efficient supercomputers in the world which led to the number 1 position as the greenest supercomputer in November 2014.", "venue": "Supercomput. Front. Innov.", "authors": ["David  Rohr", "Gvozden  Neskovic", "Volker  Lindenstruth"], "year": 2015, "n_citations": 2}
{"id": 5194141, "s2_id": "d333e3da41e56199767dafaf36b8a7eb31c8ff3f", "title": "Parallel binary code analysis", "abstract": "Binary code analysis is widely used to help assess a program's correctness, performance, and provenance. Binary analysis applications often construct control flow graphs, analyze data flow, and use debugging information to understand how machine code relates to source lines, inlined functions, and data types. To date, binary analysis has been single-threaded, which is too slow for convenient use in performance tuning workflows where it is used to help attribute performance to complex applications with large binaries. This paper describes our design and implementation for accelerating the task of constructing control flow graphs (CFGs) from binaries by using multithreading. Prior research focuses on algorithms for analysis of challenging code constructs encountered while constructing CFGs, including functions sharing code, jump tables, non-returning functions, and tail calls. These algorithms are described from a program analysis perspective and are not suitable for direct parallel implementation. We abstract the task of constructing CFGs as repeated applications of several core CFG operations that include creating functions, basic blocks, and edges. We then derive CFG operation dependency, commutativity, and monotonicity. These operation properties guide our design of a new parallel analysis for constructing CFGs. Using 64 threads, we achieved as much as 25\u00d7 speedup for constructing CFGs and 8\u00d7 for a performance analysis tool that leverages our new analysis to recover program structure.", "venue": "PPoPP", "authors": ["Xiaozhu  Meng", "Jonathon M. Anderson", "John  Mellor-Crummey", "Mark W. Krentel", "Barton P. Miller", "Sr\u0111an  Milakovi\u0107"], "year": 2021, "n_citations": 1}
{"id": 5194388, "s2_id": "5258c7d160836ff47a53501f2019bc1532aeb630", "title": "Trade-off between accuracy and tractability of network calculus in FIFO networks", "abstract": "Computing accurate deterministic performance bounds is a strong need for communication technologies having strong requirements on latency and reliability. Beyond new scheduling protocols such as TSN, the FIFO policy remains at work within each class of communication. In this paper, we focus on computing deterministic performance bounds in FIFO networks in the network calculus framework. We propose a new algorithm based on linear programming that presents a trade-off between accuracy and tractability. This algorithm is first presented for tree networks. In a second time, we generalize our approach and present a linear program for computing performance bounds for arbitrary topologies, including cyclic dependencies. Finally, we provide numerical results, both of toy examples and real topologies, to assess the interest of our approach.", "venue": "Performance Evaluation", "authors": ["Anne  Bouillard"], "year": 2021, "n_citations": 4}
{"id": 5195815, "s2_id": "0d6f74bdb6baf0dd792287efcc774dcb58717f93", "title": "Performance analysis of Xen virtual machines in real-world scenarios", "abstract": "This paper presents results of the performance benchmarks of the Open Source hypervisor Xen. The study focuses on the network related performance as well as on the application related performance of multiple virtual machines that were running on the same Xen hypervisor. The comparison was carried out using a self-developed benchmark suite that consists of easily available Open Source tools. The goal is to measure the performance of the hypervisor in typical real-world application scenarios when used for \"mass virtual hosting\", such as hosting solutions of so called virtual private servers for small-to-medium sized businesses environments. The results of the benchmarks show, that the tested Xen setup offers good performance with respect to network traffic stress tests, but only 75% of the performance of the non-virtualized reference environment. This application performance score decreases as more virtual machines are running simultaneously.", "venue": "ArXiv", "authors": ["Adrian  Heissler"], "year": 2010, "n_citations": 1}
{"id": 5196500, "s2_id": "36c3a377ea91937c7f91ae8f822a65c1ca072e7e", "title": "Tearing Down the Memory Wall", "abstract": "We present a vision for the Erudite architecture that redefines the compute and memory abstractions such that memory bandwidth and capacity become first-class citizens along with compute throughput. In this architecture, we envision coupling a high-density, massively parallel memory technology like Flash with programmable near-data accelerators, like the streaming multiprocessors in modern GPUs. Each accelerator has a local pool of storage-class memory that it can access at high throughput by initiating very large numbers of overlapping requests that help to tolerate long access latency. The accelerators can also communicate with each other and remote memory through a high-throughput low-latency interconnect. As a result, systems based on the Erudite architecture scale compute and memory bandwidth at the same rate, tearing down the notorious memory wall that has plagued computer architecture for generations. In this paper, we present the motivation, rationale, design, benefit, and research challenges for Erudite.", "venue": "ArXiv", "authors": ["Zaid  Qureshi", "Vikram Sharma Mailthody", "Seung Won Min", "I-Hsin  Chung", "Jinjun  Xiong", "Wen-mei  Hwu"], "year": 2020, "n_citations": 2}
{"id": 5197081, "s2_id": "bc116a203871d59d37a5d07ce582fc0407771669", "title": "M/g/c/c state dependent queueing model for road traffic simulation", "abstract": "In this paper, we present a stochastic queuing model for the road traffic, which captures the stationary density-flow relationships in both uncongested and congestion conditions. The proposed model is based on the $M/g/c/c$ state dependent queuing model of Jain and Smith, and is inspired from the deterministic Godunov scheme for the road traffic simulation. We first propose a reformulation of the $M/g/c/c$ state dependent model that works with density-flow fundamental diagrams rather than density-speed relationships. We then extend this model in order to consider upstream traffic demand as well as downstream traffic supply. Finally, we calculate the speed and travel time distributions for the $M/g/c/c$ state dependent queuing model and for the proposed model, and derive stationary performance measures (expected number of cars, blocking probability, expected travel time, and throughput). A comparison with results predicted by the $M/g/c/c$ state dependent queuing model shows that the proposed model correctly represents the dynamics of traffic and gives good performances measures. The results illustrate the good accuracy of the proposed model.", "venue": "ArXiv", "authors": ["Nacira  Guerrouahane", "Djamil  A\u00efssani", "Louiza  Bouallouche-Medjkoune", "Nadir  Farhi"], "year": 2016, "n_citations": 5}
{"id": 5197848, "s2_id": "43125e8cfd90783146269a6fa378edb2b808693b", "title": "ORCA - a Benchmark for Data Web Crawlers", "abstract": "The number of RDF knowledge graphs available on the Web grows constantly. Gathering these graphs at large scale for downstream applications hence requires the use of crawlers. Although Data Web crawlers exist, and general Web crawlers could be adapted to focus on the Data Web, there is currently no benchmark to fairly evaluate their performance. Our work closes this gap by presenting the Orca benchmark. Orca generates a synthetic Data Web, which is decoupled from the original Web and enables a fair and repeatable comparison of Data Web crawlers. Our evaluations show that Orca can be used to reveal the different advantages and disadvantages of existing crawlers. The benchmark is open-source and available at https://w3id.org/dice-research/orca.", "venue": "2021 IEEE 15th International Conference on Semantic Computing (ICSC)", "authors": ["Michael  R\u00f6der", "Geraldo de Souza", "Denis  Kuchelev", "Abdelmoneim Amer Desouki", "Axel-Cyrille Ngonga Ngomo"], "year": 2021, "n_citations": 1}
{"id": 5204000, "s2_id": "c91d32c3d4d423a6069fafba303a1572dd8b885a", "title": "Optimizing Deep Learning Inference on Embedded Systems Through Adaptive Model Selection", "abstract": "Deep neural networks (DNNs) are becoming a key enabling technique for many application domains. However, on-device inference on battery-powered, resource-constrained embedding systems is often infeasible due to prohibitively long inferencing time and resource requirements of many DNNs. Offloading computation into the cloud is often unacceptable due to privacy concerns, high latency, or the lack of connectivity. Although compression algorithms often succeed in reducing inferencing times, they come at the cost of reduced accuracy. This article presents a new, alternative approach to enable efficient execution of DNNs on embedded devices. Our approach dynamically determines which DNN to use for a given input by considering the desired accuracy and inference time. It employs machine learning to develop a low-cost predictive model to quickly select a pre-trained DNN to use for a given input and the optimization constraint. We achieve this first by offline training a predictive model and then using the learned model to select a DNN model to use for new, unseen inputs. We apply our approach to two representative DNN domains: image classification and machine translation. We evaluate our approach on a Jetson TX2 embedded deep learning platform and consider a range of influential DNN models including convolutional and recurrent neural networks. For image classification, we achieve a 1.8x reduction in inference time with a 7.52% improvement in accuracy over the most capable single DNN model. For machine translation, we achieve a 1.34x reduction in inference time over the most capable single model with little impact on the quality of translation.", "venue": "ACM Trans. Embed. Comput. Syst.", "authors": ["Vicent Sanz Marco", "Ben  Taylor", "Zheng  Wang", "Yehia  Elkhatib"], "year": 2020, "n_citations": 19}
{"id": 5209996, "s2_id": "5327e71a52ab86d884623ae332e1ab40382d4aea", "title": "BUDaMaF - Data Management in Cloud Federations", "abstract": "Data management has always been a multi-domain problem even in the simplest cases. It involves, quality of service, security, resource management, cost management, incident identification, disaster avoidance and/or recovery, as well as many other concerns. In our case, this situation gets ever more complicated because of the divergent nature of a cloud federation like BASMATI. In this federation, the BASMATI Unified Data Management Framework (BUDaMaF), tries to create an automated uniform way of managing all the data transactions, as well as the data stores themselves, in a polyglot multi-cloud, consisting of a plethora of different machines and data store systems.", "venue": "CLOSER", "authors": ["Evangelos  Psomakelis", "Konstantinos  Tserpes", "Dimosthenis  Anagnostopoulos", "Theodora A. Varvarigou"], "year": 2018, "n_citations": 0}
{"id": 5210300, "s2_id": "5e0f4e989f36b65209e3efa353961a141848a437", "title": "A Preliminary Study of Neural Network-based Approximation for HPC Applications", "abstract": "Machine learning, as a tool to learn and model complicated (non)linear relationships between input and output data sets, has shown preliminary success in some HPC problems. Using machine learning, scientists are able to augment existing simulations by improving accuracy and significantly reducing latencies. Our ongoing research work is to create a general framework to apply neural network-based models to HPC applications. In particular, we want to use the neural network to approximate and replace code regions within the HPC application to improve performance (i.e., reducing the execution time) of the HPC application. In this paper, we present our preliminary study and results. Using two applications (the Newton-Raphson method and the Lennard-Jones (LJ) potential in LAMMP) for our case study, we achieve up to 2.7x and 2.46x speedup, respectively.", "venue": "ArXiv", "authors": ["Wenqian  Dong", "Anzheng  GuoLu", "Dong  Li"], "year": 2018, "n_citations": 0}
{"id": 5219049, "s2_id": "2d3d61b0c25fa298940bb7198f586d30bcf0c547", "title": "A unified approach to the performance analysis of caching systems", "abstract": "We propose a unified methodology to analyse the performance of caches (both isolated and interconnected), by extending and generalizing a decoupling technique originally known as Che's approximation, which provides very accurate results at low computational cost. We consider several caching policies, taking into account the effects of temporal locality. In the case of interconnected caches, our approach allows us to do better than the Poisson approximation commonly adopted in prior work. Our results, validated against simulations and trace-driven experiments, provide interesting insights into the performance of caching systems.", "venue": "IEEE INFOCOM 2014 - IEEE Conference on Computer Communications", "authors": ["Michele  Garetto", "Emilio  Leonardi", "Valentina  Martina"], "year": 2014, "n_citations": 197}
{"id": 5223500, "s2_id": "4b1d142e58f7fb18445bf651863076626167d98d", "title": "Stationary analysis of the shortest queue first service policy", "abstract": "We analyze the so-called shortest queue first (SQF) queueing discipline whereby a unique server addresses queues in parallel by serving at any time, the queue with the smallest workload. Considering a stationary system composed of two parallel queues and assuming Poisson arrivals and general service time distributions, we first establish the functional equations satisfied by the Laplace transforms of the workloads in each queue. We further specialize these equations to the so-called \u201csymmetric case,\u201d with same arrival rates and identical exponential service time distributions at each queue; we then obtain a functional equation $$\\begin{aligned} M(z) = q(z) \\cdot M \\circ h(z) + L(z) \\end{aligned}$$M(z)=q(z)\u00b7M\u2218h(z)+L(z)for unknown function M, where given functions $$q, L$$q,L, and $$h$$h are related to one branch of a cubic polynomial equation. We study the analyticity domain of function M and express it by a series expansion involving all iterates of function $$h$$h. This allows us to determine empty queue probabilities along with the tail of the workload distribution in each queue. This tail appears to be identical to that of the head-of-line preemptive priority system, which is the key feature desired for the SQF discipline.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Fabrice  Guillemin", "Alain  Simonian"], "year": 2014, "n_citations": 5}
{"id": 5228283, "s2_id": "f529d3007faa45f0d9dfb831b8260de34c084c4c", "title": "Dynamic scheduling in a partially fluid, partially lossy queueing system", "abstract": "We consider a single server queueing system with two classes of jobs: eager jobs with small sizes that require service to begin almost immediately upon arrival, and tolerant jobs with larger sizes that can wait for service. While blocking probability is the relevant performance metric for the eager class, the tolerant class seeks to minimize its mean sojourn time. In this paper, we discuss the performance of each class under dynamic scheduling policies, where the scheduling of both classes depends on the instantaneous state of the system. This analysis is carried out under a certain fluid limit, where the arrival rate and service rate of the eager class are scaled to infinity, holding the offered load constant. Our performance characterizations reveal a (dynamic) pseudo-conservation law that ties the performance of both the classes to the standalone blocking probabilities of the eager class. Further, the performance is robust to other specifics of the scheduling policies. We also characterize the Pareto frontier of the achievable region of performance vectors under the same fluid limit, and identify a (two-parameter) class of Pareto-complete scheduling policies.", "venue": "2019 International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOPT)", "authors": ["Kiran  Chaudhary", "Veeraruna  Kavitha", "Jayakrishnan  Nair"], "year": 2019, "n_citations": 0}
{"id": 5236221, "s2_id": "afc2c1fe33033faf96e969db4ef187bec12c97e7", "title": "The Finite-Skip Method for Multiserver Analysis", "abstract": "Multiserver queueing systems are found at the core of a wide variety of practical systems. Unfortunately, existing tools for analyzing multiserver models have major limitations: Techniques for exact analysis often struggle with high-dimensional models, while techniques for deriving bounds are often too specialized to handle realistic system features, such as variable service rates of jobs. New techniques are needed to handle these complex, important, high-dimensional models. In this paper we introduce the work-conserving finite-skip class of models. This class includes many important models, such as the heterogeneous M/G/k, the limited processor sharing policy for the M/G/1, the threshold parallelism model, and the multiserver-job model under a simple scheduling policy. We prove upper and lower bounds on mean response time for any model in the work-conserving finite-skip class. Our bounds are separated by an additive constant, giving a strong characterization of mean response time at all loads. When specialized to each of the models above, these bounds represent the first bounds on mean response time known for each setting.", "venue": "ArXiv", "authors": ["Isaac  Grosof", "Mor  Harchol-Balter", "Alan  Scheller-Wolf"], "year": 2021, "n_citations": 0}
{"id": 5238005, "s2_id": "50760f6372f0fc8a29b3070c055e882375125af8", "title": "An Efficient Thread Mapping Strategy for Multiprogramming on Manycore Processors", "abstract": "The emergence of multicore and manycore processors is set to change the parallel computing world. Applications are shifting towards increased parallelism in order to utilise these architectures efficiently. This leads to a situation where every application creates its desirable number of threads, based on its parallel nature and the system resources allowance. Task scheduling in such a multithreaded multiprogramming environment is a significant challenge. In task scheduling, not only the order of the execution, but also the mapping of threads to the execution resources is of a great importance. In this paper we state and discuss some fundamental rules based on results obtained from selected applications of the BOTS benchmarks on the 64-core TILEPro64 processor. We demonstrate how previously efficient mapping policies such as those of the SMP Linux scheduler become inefficient when the number of threads and cores grows. We propose a novel, low-overhead technique, a heuristic based on the amount of time spent by each CPU doing some useful work, to fairly distribute the workloads amongst the cores in a multiprogramming environment. Our novel approach could be implemented as a pragma similar to those in the new task-based OpenMP versions, or can be incorporated as a distributed thread mapping mechanism in future manycore programming frameworks. We show that our thread mapping scheme can outperform the native GNU/Linux thread scheduler in both single-programming and multiprogramming environments.", "venue": "PARCO", "authors": ["Ashkan  Tousimojarad", "Wim  Vanderbauwhede"], "year": 2013, "n_citations": 18}
{"id": 5239254, "s2_id": "5e2473e72c327a4c53206e7a90682bf4c942a4dd", "title": "Intermediate Data Caching Optimization for Multi-Stage and Parallel Big Data Frameworks", "abstract": "In the era of big data and cloud computing, large amounts of data are generated from user applications and need to be processed in the datacenter. Data-parallel computing frameworks, such as Apache Spark, are widely used to perform such data processing at scale. Specifically, Spark leverages distributed memory to cache the intermediate results, represented as Resilient Distributed Datasets (RDDs). This gives Spark an advantage over other parallel frameworks for implementations of iterative machine learning and data mining algorithms, by avoiding repeated computation or hard disk accesses to retrieve RDDs. By default, caching decisions are left at the programmer's discretion, and the LRU policy is used for evicting RDDs when the cache is full. However, when the objective is to minimize total work, LRU is woefully inadequate, leading to arbitrarily suboptimal caching decisions. In this paper, we design an algorithm for multi-stage big data processing platforms to adaptively determine and cache the most valuable intermediate datasets that can be reused in the future. Our solution automates the decision of which RDDs to cache: this amounts to identifying nodes in a direct acyclic graph (DAG) representing computations whose outputs should persist in the memory. Our experiment results show that our proposed cache optimization solution can improve the performance of machine learning applications on Spark decreasing the total work to recompute RDDs by 12%.", "venue": "2018 IEEE 11th International Conference on Cloud Computing (CLOUD)", "authors": ["Zhengyu  Yang", "Danlin  Jia", "Stratis  Ioannidis", "Ningfang  Mi", "Bo  Sheng"], "year": 2018, "n_citations": 26}
{"id": 5240024, "s2_id": "0a306c69baae965e1a595406547384ce60ad0e85", "title": "Stochastic Analysis on RAID Reliability for Solid-State Drives", "abstract": "Solid-state drives (SSDs) have been widely deployed in desktops and data centers. However, SSDs suffer from bit errors, and the bit error rate is time dependent since it increases as an SSD wears down. Traditional storage systems mainly use parity-based RAID to provide reliability guarantees by striping redundancy across multiple devices, but the effectiveness of RAID in SSDs remains debatable as parity updates aggravate the wearing and bit error rates of SSDs. In particular, an open problem is that how different parity distributions over multiple devices, such as the even distribution suggested by conventional wisdom, or uneven distributions proposed in recent RAID schemes for SSDs, may influence the reliability of an SSD RAID array. To address this fundamental problem, we propose the first analytical model to quantify the reliability dynamics of an SSD RAID array. Specifically, we develop a \"non-homogeneous\" continuous time Markov chain model, and derive the transient reliability solution. We validate our model via trace-driven simulations and conduct numerical analysis to provide insights into the reliability dynamics of SSD RAID arrays under different parity distributions and subject to different bit error rates and array configurations. Designers can use our model to decide the appropriate parity distribution based on their reliability requirements.", "venue": "2013 IEEE 32nd International Symposium on Reliable Distributed Systems", "authors": ["Yongkun  Li", "Patrick P. C. Lee", "John C. S. Lui"], "year": 2013, "n_citations": 23}
{"id": 5241235, "s2_id": "8dcb06353f03d314ca41fc3884d1a45df5f8186f", "title": "Retrial Queueing Models: A Survey on Theory and Applications", "abstract": "Retrial phenomenon naturally arises in various systems such as call centers, cellular networks and random access protocols in local area networks. This paper gives a comprehensive survey on theory and applications of retrial queues in these systems. We investigate the state of the art of the theoretical researches including exact solutions, stability, asymptotic analyses and multidimensional models. We present an overview on retrial models arising from real world applications. Some open problems and promising research directions are also discussed.", "venue": "ArXiv", "authors": ["Tuan  Phung-Duc"], "year": 2019, "n_citations": 17}
{"id": 5243721, "s2_id": "c7e3a7120b5347f4790e82fa2e28a0f6df57a8e8", "title": "Revisiting the D-iteration method: runtime comparison", "abstract": "In this paper, we revisit the D-iteration algorithm in order to better explain different performance results that were observed for the numerical computation of the eigenvector associated to the PageRank score. We revisit here the practical computation cost based on the execution runtime compared to the theoretical number of iterations.", "venue": "ArXiv", "authors": ["Dohy  Hong", "G\u00e9rard  Burnside", "Philippe  Raoult"], "year": 2012, "n_citations": 0}
{"id": 5245502, "s2_id": "e032d4685d4129a623730d901a9cebff0a0c6c6b", "title": "Exact Analytical Model of Age of Information in Multi-source Status Update Systems with Per-source Queueing", "abstract": "We consider an information update system consisting of N sources sending status packets at random instances according to a Poisson process to a remote monitor through a single server. We assume a heteregeneous server with exponentially distributed service times which is equipped with a waiting room holding the freshest packet from each source referred to as Single Buffer Per-Source Queueing (SBPSQ). The sources are assumed to be equally important, i.e., non-weighted average AoI is used as the information freshness metric, and subsequently two symmetric scheduling policies are studied in this paper, namely First Source First Serve (FSFS) and the Earliest Served First Serve (ESFS) policies, the latter policy being proposed the first time in the current paper to the best of our knowledge. By employing the theory of Markov Fluid Queues (MFQ), an analytical model is proposed to obtain the exact distribution of the Age of Information (AoI) for each source when the FSFS and ESFS policies are employed at the server. Subsequently, a benchmark scheduling-free scheme named as Single Buffer with Replacement (SBR) that uses a single one-packet buffer shared by all sources is also studied with a similar but less complex analytical model. We comparatively study the performance of the three schemes through numerical examples and show that the proposed ESFS policy outperforms the other two schemes in terms of the average AoI and the age violation probability averaged across all sources, in a scenario of sources possessing different traffic intensities but sharing a common service time.", "venue": "ArXiv", "authors": ["Ege Orkun Gamgam", "Nail  Akar"], "year": 2021, "n_citations": 0}
{"id": 5246815, "s2_id": "746824975d9dcd9b1c2f282f96a66f9e4def562c", "title": "Sync-Switch: Hybrid Parameter Synchronization for Distributed Deep Learning", "abstract": "Stochastic Gradient Descent (SGD) has become the de facto way to train deep neural networks in distributed clusters. A critical factor in determining the training throughput and model accuracy is the choice of the parameter synchronization protocol. For example, while Bulk Synchronous Parallel (BSP) often achieves better converged accuracy, the corresponding training throughput can be negatively impacted by stragglers. In contrast, Asynchronous Parallel (ASP) can have higher throughput, but its convergence and accuracy can be impacted by stale gradients. To improve the performance of synchronization protocol, recent work often focuses on designing new protocols with a heavy reliance on hard-to-tune hyper-parameters. In this paper, we design a hybrid synchronization approach that exploits the benefits of both BSP and ASP, i.e., reducing training time while simultaneously maintaining the converged accuracy. Based on extensive empirical profiling, we devise a collection of adaptive policies that determine how and when to switch between synchronization protocols. Our policies include both offline ones that target recurring jobs and online ones for handling transient stragglers. We implement the proposed policies in a prototype system, called Sync-Switch, on top of TensorFlow, and evaluate the training performance with popular deep learning models and datasets. Our experiments show that Sync-Switch can achieve ASP level training speedup while maintaining similar converged accuracy when comparing to BSP. Moreover, Sync-Switch's elastic-based policy can adequately mitigate the impact from transient stragglers.", "venue": "2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)", "authors": ["Shijian  Li", "Oren  Mangoubi", "Lijie  Xu", "Tian  Guo"], "year": 2021, "n_citations": 2}
{"id": 5248729, "s2_id": "6f11fea3eb6541627a1b478e9fa52d4063d672ec", "title": "Spatial multi-LRU: Distributed Caching for Wireless Networks with Coverage Overlaps", "abstract": "This article introduces a novel family of decen-tralised caching policies, applicable to wireless networks with finite storage at the edge-nodes (stations). These policies, that are based on the Least-Recently-Used replacement principle, are here referred to as spatial multi-LRU. They update cache inventories in a way that provides content diversity to users who are covered by, and thus have access to, more than one station. Two variations are proposed, the multi-LRU-One and-All, which differ in the number of replicas inserted in the involved caches. We analyse their performance under two types of traffic demand, the Independent Reference Model (IRM) and a model that exhibits temporal locality. For IRM, we propose a Che-like approximation to predict the hit probability, which gives very accurate results. Numerical evaluations show that the performance of multi-LRU increases the more the multi-coverage areas increase, and it is close to the performance of centralised policies, when multi-coverage is sufficient. For IRM traffic, multi-LRU-One is preferable to multi-LRU-All, whereas when the traffic exhibits temporal locality the-All variation can perform better. Both variations outperform the simple LRU. When popularity knowledge is not accurate, the new policies can perform better than centralised ones.", "venue": "ArXiv", "authors": ["Anastasios  Giovanidis", "Apostolos  Avranas"], "year": 2016, "n_citations": 7}
{"id": 5248854, "s2_id": "b931d52d7d72cac8cfc104f1945fb513f60ef3e3", "title": "Data Engineering for HPC with Python", "abstract": "Data engineering is becoming an increasingly important part of scientific discoveries with the adoption of deep learning and machine learning. Data engineering deals with a variety of data formats, storage, data extraction, transformation, and data movements. One goal of data engineering is to transform data from original data to vector/matrix/tensor formats accepted by deep learning and machine learning applications. There are many structures such as tables, graphs, and trees to represent data in these data engineering phases. Among them, tables are a versatile and commonly used format to load and process data. In this paper, we present a distributed Python API based on table abstraction for representing and processing data. Unlike existing state-of-the-art data engineering tools written purely in Python, our solution adopts high performance compute kernels in C++, with an in-memory table representation with Cython-based Python bindings. In the core system, we use MPI for distributed memory computations with a data-parallel approach for processing large datasets in HPC clusters.", "venue": "2020 IEEE/ACM 9th Workshop on Python for High-Performance and Scientific Computing (PyHPC)", "authors": ["Vibhatha  Abeykoon", "Niranda  Perera", "Chathura  Widanage", "Supun  Kamburugamuve", "Thejaka Amila Kanewala", "Hasara  Maithree", "Pulasthi  Wickramasinghe", "Ahmet  Uyar", "Geoffrey  Fox"], "year": 2020, "n_citations": 5}
{"id": 5256590, "s2_id": "9486ea387de082d5407c4b8a5b09874f4188ae54", "title": "Computational Performance of a Germline Variant Calling Pipeline for Next Generation Sequencing", "abstract": "With the booming of next generation sequencing technology and its implementation in clinical practice and life science research, the need for faster and more efficient data analysis methods becomes pressing in the field of sequencing. Here we report on the evaluation of an optimized germline mutation calling pipeline, HummingBird, by assessing its performance against the widely accepted BWA-GATK pipeline. We found that the HummingBird pipeline can significantly reduce the running time of the primary data analysis for whole genome sequencing and whole exome sequencing while without significantly sacrificing the variant calling accuracy. Thus, we conclude that expansion of such software usage will help to improve the primary data analysis efficiency for next generation sequencing.", "venue": "ArXiv", "authors": ["Jie  Liu", "Xiaotian  Wu", "Kai  Zhang", "Bing  Liu", "Renyi  Bao", "Xiao  Chen", "Yiran  Cai", "Yiming  Shen", "Xinjun  He", "Jun  Yan", "Weixing  Ji"], "year": 2020, "n_citations": 0}
{"id": 5258492, "s2_id": "355b3524ed3e8d3fd008e85e4dcddd25243a45be", "title": "Two Basic Queueing Models of Service Platforms in Digital Sharing Economy", "abstract": "This paper describes two basic queueing models of service platforms in digital sharing economy by means of two different policies of platform matching information. We show that the two queueing models of service platforms can be expressed as the level-independent quasi birth-and-death (QBD) processes. Using the proposed QBD processes, we provide a detailed analysis for the two queueing models of service platforms, including the system stability, the average stationary numbers of seekers and of idle owners, the expected sojourn time of an arriving seeker, and the expected profits for both the service platform and each owner. Finally, numerical examples are employed to verify our theoretical results, and demonstrate how the performance measures of service platforms are influenced by some key system parameters. We believe that the methodology and results developed in this paper not only can be applied to develop a broad class of queuing models of service platforms, but also will open a series of promising innovative research on performance evaluation, optimal control and queueing-game of service platforms and digital sharing economy.", "venue": "ArXiv", "authors": ["Heng-Li  Liu", "Quan-Lin  Li", "Xiaole  Wu", "Chi  Zhang"], "year": 2021, "n_citations": 0}
{"id": 5258896, "s2_id": "762df6cac8e32c66ef5c895dc39f3082305792a0", "title": "Reproducible Performance Optimization of Complex Applications on the Edge-to-Cloud Continuum", "abstract": "In more and more application areas, we are witnessing the emergence of complex workflows that combine computing, analytics and learning. They often require a hybrid execution infrastructure with IoT devices interconnected to cloud/HPC systems (aka Computing Continuum). Such workflows are subject to complex constraints and requirements in terms of performance, resource usage, energy consumption and financial costs. This makes it challenging to optimize their configuration and deployment. We propose a methodology to support the optimization of real-life applications on the Edge-to-Cloud Continuum. We implement it as an extension of E2Clab, a previously proposed framework supporting the complete experimental cycle across the Edge-to-Cloud Continuum. Our approach relies on a rigorous analysis of possible configurations in a controlled testbed environment to understand their behaviour and related performance tradeoffs. We illustrate our methodology by optimizing Pl@ntNet, a world-wide plant identification application. Our methodology can be generalized to other applications in the Edge-to-Cloud Continuum.", "venue": "2021 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Daniel  Rosendo", "Alexandru  Costan", "Gabriel  Antoniu", "Matthieu  Simonin", "Jean-Christophe  Lombardo", "Alexis  Joly", "Patrick  Valduriez"], "year": 2021, "n_citations": 1}
{"id": 5263562, "s2_id": "ddca83e3ee28d58055459a77e81de400404f7538", "title": "Performance Analysis of Open Source Machine Learning Frameworks for Various Parameters in Single-Threaded and Multi-Threaded Modes", "abstract": "The basic features of some of the most versatile and popular open source frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are considered and compared. Their comparative analysis was performed and conclusions were made as to the advantages and disadvantages of these platforms. The performance tests for the de facto standard MNIST data set were carried out on H2O framework for deep learning algorithms designed for CPU and GPU platforms for single-threaded and multithreaded modes of operation Also, we present the results of testing neural networks architectures on H2O platform for various activation functions, stopping metrics, and other parameters of machine learning algorithm. It was demonstrated for the use case of MNIST database of handwritten digits in single-threaded mode that blind selection of these parameters can hugely increase (by 2-3 orders) the runtime without the significant increase of precision. This result can have crucial influence for optimization of available and new machine learning methods, especially for image recognition problems.", "venue": "ArXiv", "authors": ["Yuriy  Kochura", "Sergii  Stirenko", "Oleg  Alienin", "Michail  Novotarskiy", "Yuri  Gordienko"], "year": 2017, "n_citations": 21}
{"id": 5263753, "s2_id": "bd39f7d651ac6f4f0a338a27c568b0a9b27344e3", "title": "Frame Replication and Elimination for Reliability in Time-Sensitive Networks", "abstract": "In modern applications such as in the prospective smart factory, timely and faultfree communication is one of the main concerns. Communication failures may lead to huge economic losses. Moreover, they can even endanger human life. Therefore, the TimeSensitive Networking (TSN) task group has introduced new standards for real-time capable Ethernet, which also include a fault tolerance mechanism called Frame Replication and Elimination for Reliability (FRER) as IEEE standard 802.1CB. This standard introduces procedures and protocols for bridges and end stations in time-sensitive networks. It also provides mechanisms for the identification and duplication of frames to enable redundant transmissions. In this paper, a simulation model is developed that implements the IEEE 802.1CB standard in OMNeT++. In addition, as supplement to the standard we propose a reliability mechanism for establishing redundant paths and an error model to model transient and permanent errors. As proof of concept, we evaluate the model with different topologies under various conditions.", "venue": "ArXiv", "authors": ["Peter  Danielis", "Helge  Parzyjegla", "Gero  M\u00fchl", "Eike  Schwei\u00dfguth", "Dirk  Timmermann"], "year": 2021, "n_citations": 2}
{"id": 5265612, "s2_id": "02b16785ca385fdfc25290d5b153ccaa1069ac95", "title": "Improving PARMA trailing", "abstract": "Taylor introduced a variable binding scheme for logic variables in his PARMA system, that uses cycles of bindings rather than the linear chains of bindings used in the standard WAM representation. Both the HAL and dProlog languages make use of the PARMA representation in their Herbrand constraint solvers. Unfortunately, PARMA's trailing scheme is considerably more expensive in both time and space consumption. The aim of this paper is to present several techniques that lower the cost. First, we introduce a trailing analysis for HAL using the classic PARMA trailing scheme that detects and eliminates unnecessary trailings. The analysis, whose accuracy comes from HAL's determinism and mode declarations, has been integrated in the HAL compiler and is shown to produce space improvements as well as speed improvements. Second, we explain how to modify the classic PARMA trailing scheme to halve its trailing cost. This technique is illustrated and evaluated both in the context of dProlog and HAL. Finally, we explain the modifications needed by the trailing analysis in order to be combined with our modified PARMA trailing scheme. Empirical evidence shows that the combination is more effective than any of the techniques when used in isolation.", "venue": "Theory and Practice of Logic Programming", "authors": ["Tom  Schrijvers", "Bart  Demoen", "Maria Garcia de la Banda", "Peter J. Stuckey"], "year": 2006, "n_citations": 0}
{"id": 5266802, "s2_id": "29ff3d19b0ee83c3d878ea9d41717e2d41c861c2", "title": "Achieving Zero Asymptotic Queueing Delay for Parallel Jobs", "abstract": "Zero queueing delay is highly desirable in large-scale computing systems. Existing work has shown that it can be asymptotically achieved by using the celebrated Power-of-d-choices (Pod) policy with a probe overhead d = \u03a9(log N/1-\u03bb), and it is impossible when d = O(1/1-\u03bb), where N is the number of servers and \u03bb is the load of the system. However, these results are based on the model where each job is an indivisible unit, which does not capture the parallel structure of jobs in today's predominant parallel computing paradigm. This paper considers a model where each job consists of a batch of parallel tasks. Under this model, we propose a new notion of zero (asymptotic) queueing delay that requires the job delay under a policy to approach the job delay given by the max of its tasks' service times, i.e., the job delay assuming its tasks entered service right upon arrival. This notion quantifies the effect of queueing on a job level for jobs consisting of multiple tasks, and thus deviates from the conventional zero queueing delay for single-task jobs in the literature. We show that zero queueing delay for parallel jobs can be achieved using the batch-filling policy (a variant of the celebrated Pod policy) with a probe overhead d = \u03a9(1/(1-\u03bb)log k) in the sub-Halfin-Whitt heavy-traffic regime, where k is the number of tasks in each job and k properly scales with N (the number of servers). This result demonstrates that for parallel jobs, zero queueing delay can be achieved with a smaller probe overhead. We also establish an impossibility result: we show that zero queueing delay cannot be achieved if d = (o(log N/log k)). Simulation results are provided to demonstrate the consistency between numerical results and theoretical results under reasonable settings, and to investigate gaps in the theoretical analysis.", "venue": "SIGMETRICS", "authors": ["Wentao  Weng", "Weina  Wang"], "year": 2021, "n_citations": 0}
{"id": 5267907, "s2_id": "834b94ae7a4eed1849302b5812673e9e7c16aac7", "title": "Google vs IBM: A Constraint Solving Challenge on the Job-Shop Scheduling Problem", "abstract": "The job-shop scheduling is one of the most studied optimization problems from the dawn of computer era to the present day. Its combinatorial nature makes it easily expressible as a constraint satisfaction problem. In this paper, we compare the performance of two constraint solvers on the job-shop scheduling problem. The solvers in question are: OR-Tools, an open-source solver developed by Google and winner of the last MiniZinc Challenge, and CP Optimizer, a proprietary IBM constraint solver targeted at industrial scheduling problems. The comparison is based on the goodness of the solutions found and the time required to solve the problem instances. First, we target the classic benchmarks from the literature, then we carry out the comparison on a benchmark that was created with known optimal solution, with size comparable to real-world industrial problems.", "venue": "ICLP Technical Communications", "authors": ["Giacomo Da Col", "Erich  Teppan"], "year": 2019, "n_citations": 3}
{"id": 5269299, "s2_id": "0e78413ad7394ba7cb68a344932cc9b9d1e409f8", "title": "Addressing Algorithmic Bottlenecks in Elastic Machine Learning with Chicle", "abstract": "Distributed machine learning training is one of the most common and important workloads running on data centers today, but it is rarely executed alone. Instead, to reduce costs, computing resources are consolidated and shared by different applications. In this scenario, elasticity and proper load balancing are vital to maximize efficiency, fairness, and utilization. Currently, most distributed training frameworks do not support the aforementioned properties. A few exceptions that do support elasticity, imitate generic distributed frameworks and use micro-tasks. In this paper we illustrate that micro-tasks are problematic for machine learning applications, because they require a high degree of parallelism which hinders the convergence of distributed training at a pure algorithmic level (i.e., ignoring overheads and scalability limitations). To address this, we propose Chicle, a new elastic distributed training framework which exploits the nature of machine learning algorithms to implement elasticity and load balancing without micro-tasks. We use Chicle to train deep neural network as well as generalized linear models, and show that Chicle achieves performance competitive with state of the art rigid frameworks, while efficiently enabling elastic execution and dynamic load balancing.", "venue": "ArXiv", "authors": ["Michael  Kaufmann", "Kornilios  Kourtis", "Celestine  Mendler-D\u00fcnner", "Adrian  Sch\u00fcpbach", "Thomas P. Parnell"], "year": 2019, "n_citations": 0}
{"id": 5271793, "s2_id": "6c432dc257978ef7890369194512b07dcb18e9e4", "title": "SP2Bench: A SPARQL Performance Benchmark", "abstract": "Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evaluation strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP^2Bench, a publicly available, language-specific SPARQL performance benchmark. SP^2Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellations and RDF access patterns. As a proof of concept, we apply SP^2Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results.", "venue": "Semantic Web Information Management", "authors": ["Michael  Schmidt", "Thomas  Hornung", "Georg  Lausen", "Christoph  Pinkel"], "year": 2009, "n_citations": 335}
{"id": 5271881, "s2_id": "7c8236420630f03f9967d25d6ace151f0e920597", "title": "Performance Evaluation of Wimax Physical Layer under Adaptive Modulation Techniques and Communication Channels", "abstract": "Wimax (Worldwide Interoperability for Microwave Access) is a promising technology which can offer high speed voice, video and data service up to the customer end. The aim of this paper is the performance evaluation of an Wimax system under different combinations of digital modulation (BPSK, QPSK, 4 QAM and 16 QAM) and different communication channels AWGN and fading channels (Rayleigh and Rician). And the Wimax system incorporates Reed Solomon (RS) encoder with Convolutional encoder with half and two third rated codes in FEC channel coding. The simulation results of estimated Bit Error Rate (BER) displays that the implementation of interleaved RS code (255, 239, 8) with two third rated Convolutional code under BPSK modulation technique is highly effective to combat in the Wimax communication system. To complete this performance analysis in Wimax based systems, a segment of audio signal is used for analysis. The transmitted audio message is found to have retrieved effectively under noisy situation.", "venue": "ArXiv", "authors": ["Md. Ashraful Islam", "Riaz  Mondal", "Md. Zahid Hasan"], "year": 2009, "n_citations": 47}
{"id": 5275562, "s2_id": "5c1b42ac3ef3aad6c84ce731f112ed9f6b8b016d", "title": "GoS Proposal to Improve Trust and Delay of MPLS Flows for MCN Services", "abstract": "Abstract \u2014In this article, Guarantee of Service (GoS) is defined as a proposal to improve the integration of Mission Critical Networking (MCN) services in the Internet, analyzing the congestion impact on those privileged flows with high requirements of trust and delay. Multiprotocol Label Switching (MPLS) is a technology that offers flow differentiation and QoS in the Internet. Therefore, in order to improve network performance in case of congested domains, GoS is proposed as a technique that allows the local recovering of lost packets of MPLS privileged flows. To fulfill the GoS requirements for integration of MCN in MPLS, a minimum set of extensions to RSVP-TE has been proposed to provide GoS capable routes. Moreover, we have carried out an analytical study of GoS scalability and a performance improvement analysis by means of simulations. Keywords-MPLS, congestion, trust, RSVP-TE, Guarantee of Service, local re-transmissions I. I NTRODUCTION The integration of Mission Critical Networking (MCN) with the Internet allows enhancing reachability and ubiquity and the cost reduction of deployment and maintenance. However, an efficient network operation for MCN services is always required, but the Internet is a heterogeneous network that typically includes numerous resource-constrained devices [1], which creates bottlenecks that affect the network performance. In this context, Multiprotocol Label Switching (MPLS) is currently used to provide policy management for heterogeneous networks and protocols with QoS integration purposes, combining traffic engineering capabilities with flexibility of IP and class-of-service differentiation [2], [3]. MPLS Label Switched Paths (LSP) let the head-end Label Edge Router (LER) to control the path that traffic takes to a particular destination [4]. This method is more flexible than forwarding traffic based on destination address only. LSP tunnels also allow the implementation of a variety of policies related to the optimization of network performance [5]. Moreover, resilience allows LSP tunnels being automatically r ou ted awy fr m nk f il scong po [6], [7]. Resource Reservation Protocol with Traffic Engineering (RSVP-TE) is the signalling protocol used to allocate resources for those LSP tunnels across the network [8]. Therefore, MPLS allocates bandwidth on the network when it uses RSVP-TE to build LSP [9]. When RSVP-TE is used to allocate bandwidth for a particular LSP, then the concept of", "venue": "ArXiv", "authors": ["Francisco J. Rodr\u00edguez-P\u00e9rez", "Jos\u00e9 Luis Gonz\u00e1lez S\u00e1nchez", "Alfonso  Gazo-Cervero"], "year": 2009, "n_citations": 0}
{"id": 5279490, "s2_id": "1d73eebd9d1d4ea457e3906f0635a72ec4f571aa", "title": "Optimisation of stochastic networks with blocking: a functional-form approach", "abstract": "This paper introduces a class of stochastic networks with blocking, motivated by applications arising in cellular network planning, mobile cloud computing, and spare parts supply chains. Blocking results in lost revenue due to customers or jobs being permanently removed from the system. We are interested in striking a balance between mitigating blocking by increasing service capacity, and maintaining low costs for service capacity. This problem is further complicated by the stochastic nature of the system. Owing to the complexity of the system there are no analytical results available that formulate and solve the relevant optimization problem in closed form. Traditional simulation-based methods may work well for small instances, but the associated computational costs are prohibitive for networks of realistic size. \nWe propose a hybrid functional-form based approach for finding the optimal resource allocation, combining the speed of an analytical approach with the accuracy of simulation-based optimisation. The key insight is to replace the computationally expensive gradient estimation in simulation optimisation with a closed-form analytical approximation that is calibrated using a single simulation run. We develop two implementations of this approach and conduct extensive computational experiments on complex examples to show that it is capable of substantially improving system performance. We also provide evidence that our approach has substantially lower computational costs compared to stochastic approximation.", "venue": "ArXiv", "authors": ["Brendan  Patch", "Mark S. Squillante", "Peter M. van de Ven"], "year": 2019, "n_citations": 0}
{"id": 5282548, "s2_id": "db0a903636ae1ff6fe34e2892a183791c679e205", "title": "PhoenixCloud: Provisioning Resources for Heterogeneous Cloud Workloads", "abstract": "As more and more service providers choose Cloud platforms, a resource provider needs to provision resources and supporting runtime environments (REs) for heterogeneous workloads in different scenarios. Previous work fails to resolve this issue in several ways: (1) it fails to pay attention to diverse RE requirements, and does not enable creating coordinated REs on demand; (2) few work investigates coordinated resource provisioning for heterogeneous workloads. In this paper, our contributions are three-fold: (1) we present an RE agreement that expresses diverse RE requirements, and build an innovative system PhoenixCloud that enables a resource provider to create REs on demand according to RE agreements; (2) we propose two coordinated resource provisioning solutions for heterogeneous workloads in two typical Cloud scenarios: first, a large organization operates a private Cloud for two heterogeneous workloads; second, a large organization or two service providers running heterogeneous workloads revert to a public Cloud; and (3) A comprehensive evaluation has been performed in experiments. For typical workload traces of parallel batch jobs and Web services, our experiments show that: a) In the first Cloud scenario, when the throughput is almost same like that of a dedicated cluster system, our solution decreases the configuration size of cluster by about 40%; b) in the second scenario, our solution decreases not only the total resource consumption, but also the peak resource consumption maximally to 31% with respect to that of EC2 + RightScale solution.", "venue": "ArXiv", "authors": ["Jianfeng  Zhan", "Lei  Wang", "Weisong  Shi", "Shimin  Gong", "Xiutao  Zang"], "year": 2010, "n_citations": 8}
{"id": 5287887, "s2_id": "adf051a597ab02f5289bce5a6f50b8330f2a9425", "title": "Modernizing Titan2D, a Parallel AMR Geophysical Flow Code to Support Multiple Rheologies and Extendability", "abstract": "In this work, we report on strategies and results of our initial approach for modernization of Titan2D code. Titan2D is a geophysical mass flow simulation code designed for modeling of volcanic flows, debris avalanches and landslides over a realistic terrain model. It solves an underlying hyperbolic system of partial differential equations using parallel adaptive mesh Godunov scheme. The following work was done during code refactoring and modernization. To facilitate user input two level python interface was developed. Such design permits large changes in C++ and Python low-level while maintaining stable high-level interface exposed to the end user. Multiple diverged forks implementing different material models were merged back together. Data storage layout was changed from a linked list of structures to a structure of arrays representation for better memory access and in preparation for further work on better utilization of vectorized instruction. Existing MPI parallelization was augmented with OpenMP parallelization. The performance of a hash table used to store mesh elements and nodes references was improved by switching from a linked list for overflow entries to dynamic arrays allowing the implementation of the binary search algorithm. The introduction of the new data layout made possible to reduce the number of hash table look-ups by replacing them with direct use of indexes from the storage class. The modifications lead to 8-9 times performance improvement for serial execution.", "venue": "ISC Workshops", "authors": ["Nikolay  Simakov", "Renette L. Jones-Ivey", "Ali  Akhavan-Safaei", "Hossein  Aghakhani", "Matthew D. Jones", "Abani K. Patra"], "year": 2019, "n_citations": 4}
{"id": 5292219, "s2_id": "b15d7f5b1c8ad6a920fa0a31365db83ddc00c845", "title": "A generic lazy evaluation scheme for exact geometric computations", "abstract": "We present a generic C++ design to perform exact geometric computations efficiently using lazy evaluations. Exact geometric computations are critical for the robustness of geometric algorithms. Their efficiency is also important for many applications, hence the need for delaying the costly exact computations at run time until they are actually needed, if at all. Our approach is generic and extensible in the sense that it is possible to make it a library that users can apply to their own geometric objects and primitives. It involves techniques such as generic functor-adaptors, static and dynamic polymorphism, reference counting for the management of directed acyclic graphs, and exception handling for triggering exact computations when needed. It also relies on multi-precision arithmetic as well as interval arithmetic. We apply our approach to the whole geometry kernel of Cgal.", "venue": "Sci. Comput. Program.", "authors": ["Sylvain  Pion", "Andreas  Fabri"], "year": 2011, "n_citations": 48}
{"id": 5304787, "s2_id": "770e1966cc517dffab8b5d86a970495feb8d120b", "title": "Inter-database validation of a deep learning approach for automatic sleep scoring", "abstract": "Study objectives Development of inter-database generalizable sleep staging algorithms represents a challenge due to increased data variability across different datasets. Sharing data between different centers is also a problem due to potential restrictions due to patient privacy protection. In this work, we describe a new deep learning approach for automatic sleep staging, and address its generalization capabilities on a wide range of public sleep staging databases. We also examine the suitability of a novel approach that uses an ensemble of individual local models and evaluate its impact on the resulting inter-database generalization performance. Methods A general deep learning network architecture for automatic sleep staging is presented. Different preprocessing and architectural variant options are tested. The resulting prediction capabilities are evaluated and compared on a heterogeneous collection of six public sleep staging datasets. Validation is carried out in the context of independent local and external dataset generalization scenarios. Results Best results were achieved using the CNN_LSTM_5 neural network variant. Average prediction capabilities on independent local testing sets achieved 0.80 kappa score. When individual local models predict data from external datasets, average kappa score decreases to 0.54. Using the proposed ensemble-based approach, average kappa performance on the external dataset prediction scenario increases to 0.62. To our knowledge this is the largest study by the number of datasets so far on validating the generalization capabilities of an automatic sleep staging algorithm using external databases. Conclusions Validation results show good general performance of our method, as compared with the expected levels of human agreement, as well as to state-of-the-art automatic sleep staging methods. The proposed ensemble-based approach enables flexible and scalable design, allowing dynamic integration of local models into the final ensemble, preserving data locality, and increasing generalization capabilities of the resulting system at the same time.", "venue": "PloS one", "authors": ["Diego  Alvarez-Estevez", "Roselyne M. Rijsman"], "year": 2021, "n_citations": 0}
{"id": 5304854, "s2_id": "5916880939f8cb1fd499c56eeb6b32bb214e795f", "title": "Towards Run Time Estimation of the Gaussian Chemistry Code for SEAGrid Science Gateway", "abstract": "Accurate estimation of the run time of computational codes has a number of significant advantages for scientific computing. It is required information for optimal resource allocation, improving turnaround times and utilization of science gateways. Furthermore, it allows users to better plan and schedule their research, streamlining workflows and improving the overall productivity of cyberinfrastructure. Predicting run time is challenging, however. The inputs to scientific codes can be complex and high dimensional. Their relationship to the run time may be highly non-linear, and, in the most general case is completely arbitrary and thus unpredictable (i.e., simply a random mapping from inputs to run time). Most codes are not so arbitrary, however, and there has been significant prior research on predicting the run time of applications and workloads. Such predictions are generally application-specific, however. In this paper, we focus on the Gaussian computational chemistry code. We characterize a data set of runs from the SEAGrid science gateway with a number of different studies. We also explore a number of different potential regression methods and present promising future directions.", "venue": "PEARC", "authors": ["Angel  Beltre", "Shehtab  Zaman", "Kenneth  Chiu", "Sudhakar  Pamidighantam", "Xingye  Qiao", "Madhusudhan  Govindaraju"], "year": 2019, "n_citations": 0}
{"id": 5306843, "s2_id": "6ee29797fab906302c58f79564404b386272db75", "title": "Leader Election in Arbitrarily Connected Networks with Process Crashes and Weak Channel Reliability", "abstract": "A channel from a process p to a process q satisfies the ADD property if there are constantsK andD, unknown to the processes, such that in any sequence of K consecutive messages sent by p to q, at least one of them is delivered to q at most D time units after it has been sent. This paper studies implementations of an eventual leader, namely, an \u03a9 failure detector, in an arbitrarily connected network of eventual ADD channels, where processes may fail by crashing. It first presents an algorithm that assumes that processes initially know n, the total number of processes, sending messages of size O( log n). Then, it presents a second algorithm that does not assume the processes know n. Eventually the size of the messages sent by this algorithm is alsoO( log n). These are the first implementations of leader election in the ADD model. In this model, only eventually perfect failure detectors were considered, sending messages of size O(n log n).", "venue": "Networked Systems", "authors": ["Carlos  L'opez", "Sergio  Rajsbaum", "Michel  Raynal", "Karla  Vargas"], "year": 2021, "n_citations": 1}
{"id": 5309088, "s2_id": "5d53c55fb96354671b7ca55d0d8c41756ced6032", "title": "A Nonlinear Solution to Closed Queueing Networks for Bike Sharing Systems with Markovian Arrival Processes and Under an Irreducible Path Graph", "abstract": "As a favorite urban public transport mode, the bike sharing system is a large-scale and complicated system, and there exists a key requirement that a user and a bike should be matched sufficiently in time. Such matched behavior makes analysis of the bike sharing systems more difficult and challenging. This paper considers a more general large-scale bike sharing system from two important views: (a) Bikes move in an irreducible path graph, which is related to geographical structure of the bike sharing system; and (b) Markovian arrival processes (MAPs) are applied to describe the non-Poisson and burst behavior of bike-user (abbreviated as user) arrivals, while the burstiness demonstrates that the user arrivals are time-inhomogeneous and space-heterogeneous in practice. For such a complicated bike sharing system, this paper establishes a multiclass closed queueing network by means of some virtual ideas, for example, bikes are abstracted as virtual customers; stations and roads are regarded as virtual nodes. Thus user arrivals are related to service times at station nodes; and users riding bikes on roads are viewed as service times at road nodes. Further, to deal with this multiclass closed queueing network, we provide a detailed observation practically on physical behavior of the bike sharing system in order to establish the routing matrix, which gives a nonlinear solution to compute the relative arrival rates in terms of the product-form solution to the steady-state probabilities of joint queue lengths at the virtual nodes. Based on this, we can compute the steady-state probability of problematic stations, and also deal with other interesting performance measures of the bike sharing system. We hope that the methodology and results of this paper can be applicable in the study of more general bike sharing systems through multiclass closed queueing networks.", "venue": "QTNA", "authors": ["Quan-Lin  Li", "Rui-Na  Fan", "Zhi-Yong  Qian"], "year": 2017, "n_citations": 6}
{"id": 5310660, "s2_id": "9ff4b3d808370b60cad04405e4376222653eafdd", "title": "Separation of timescales in a two-layered network", "abstract": "We investigate a computer network consisting of two layers occurring in, for example, application servers. The first layer incorporates the arrival of jobs at a network of multi-server nodes, which we model as a many-server Jackson network. At the second layer, active servers at these nodes act now as customers who are served by a common CPU. Our main result shows a separation of time scales in heavy traffic: the main source of randomness occurs at the (aggregate) CPU layer; the interactions between different types of nodes at the other layer is shown to converge to a fixed point at a faster time scale; this also yields a state-space collapse property. Apart from these fundamental insights, we also obtain an explicit approximation for the joint law of the number of jobs in the system, which is provably accurate for heavily loaded systems and performs numerically well for moderately loaded systems. The obtained results for the model under consideration can be applied to thread-pool dimensioning in application servers, while the technique seems applicable to other layered systems too.", "venue": "2012 24th International Teletraffic Congress (ITC 24)", "authors": ["Maria  Vlasiou", "Jiheng  Zhang", "Bert  Zwart", "Rob van der Mei"], "year": 2012, "n_citations": 5}
{"id": 5310880, "s2_id": "ba88bad7160fe24e0635870deb6de41fc6770d50", "title": "On the Asymptotic Validity of the Decoupling Assumption for Analyzing 802.11 MAC Protocol", "abstract": "Performance evaluation of the 802.11 MAC protocol is classically based on the decoupling assumption, which hypothesizes that the backoff processes at different nodes are independent. This decoupling assumption results from mean field convergence and is generally true in transient regime in the asymptotic sense (when the number of wireless nodes tends to infinity), but, contrary to widespread belief, may not necessarily hold in stationary regime. The issue is often related with the existence and uniqueness of a solution to a fixed point equation; however, it was also recently shown that this condition is not sufficient; in contrast, a sufficient condition is a global stability property of the associated ordinary differential equation. In this paper, we give a simple condition that establishes the asymptotic validity of the decoupling assumption for the homogeneous case (all nodes have the same parameters). We also discuss the heterogeneous and the differentiated service cases and formulate a new ordinary differential equation. We show that the uniqueness of a solution to the associated fixed point equation is not sufficient; we exhibit one case where the fixed point equation has a unique solution but the decoupling assumption is not valid in the asymptotic sense in stationary regime.", "venue": "IEEE Transactions on Information Theory", "authors": ["Jeong-woo  Cho", "Jean-Yves Le Boudec", "Yuming  Jiang"], "year": 2012, "n_citations": 39}
{"id": 5311596, "s2_id": "c004259d507f500ca7165634f63e5434c705bb33", "title": "Deploying large fixed file datasets with SquashFS and Singularity", "abstract": "Shared high-performance computing (HPC) platforms, such as those provided by XSEDE and Compute Canada, enable researchers to carry out large-scale computational experiments at a fraction of the cost of the cloud. Most systems require the use of distributed filesystems (e.g. Lustre) for providing a highly multi-user, large capacity storage environment. These suffer performance penalties as the number of files increases due to network contention and metadata performance. We demonstrate how a combination of two technologies, Singularity and SquashFS, can help developers, integrators, architects, and scientists deploy large datasets (O(10M) files) on these shared systems with minimal performance limitations. The proposed integration enables more efficient access and indexing than normal file-based dataset installations, while providing transparent file access to users and processes. Furthermore, the approach does not require administrative privileges on the target system. While the examples studied here have been taken from the field of neuroimaging, the technologies adopted are not specific to that field. Currently, this solution is limited to read-only datasets. We propose the adoption of this technology for the consumption and dissemination of community datasets across shared computing resources.", "venue": "PEARC", "authors": ["Pierre  Rioux", "Gregory  Kiar", "Alexandre  Hutton", "Alan C. Evans", "Shawn T. Brown"], "year": 2020, "n_citations": 0}
{"id": 5313503, "s2_id": "162ab99ff45dc25e97590aac35f08f10eeef6be1", "title": "Zero Queueing for Multi-Server Jobs", "abstract": "Cloud computing today is dominated by multi-server jobs. These are jobs that request multiple servers simultaneously and hold onto all of these servers for the duration of the job. Multi-server jobs add a lot of complexity to the traditional one-server-per-job model: an arrival might not \"fit\" into the available servers and might have to queue, blocking later arrivals and leaving servers idle. From a queueing perspective, almost nothing is understood about multi-server job queueing systems; even understanding the exact stability region is a very hard problem. In this paper, we investigate a multi-server job queueing model under scaling regimes where the number of servers in the system grows. Specifically, we consider a system with multiple classes of jobs, where jobs from different classes can request different numbers of servers and have different service time distributions, and jobs are served in first-come-first-served order. The multi-server job model opens up new scaling regimes where both the number of servers that a job needs and the system load scale with the total number of servers. Within these scaling regimes, we derive the first results on stability, queueing probability, and the transient analysis of the number of jobs in the system for each class. In particular we derive sufficient conditions for zero queueing. Our analysis introduces a novel way of extracting information from the Lyapunov drift, which can be applicable to a broader scope of problems in queueing systems.", "venue": "SIGMETRICS", "authors": ["Weina  Wang", "Qiaomin  Xie", "Mor  Harchol-Balter"], "year": 2021, "n_citations": 1}
{"id": 5319631, "s2_id": "801b65a2528a4b5047f98a1c955769c873931031", "title": "Undecidability of performance equivalence of Petri nets", "abstract": "We investigate bisimulation equivalence on Petri nets under durational semantics. Our motivation was to verify the conjecture that in durational setting, the bisimulation equivalence checking problem becomes more tractable than in ordinary setting (which is the case, e.g., over communication-free nets). We disprove this conjecture in three of four proposed variants of durational semantics. The fourth variant remains an intriguing open problem.", "venue": "Theor. Comput. Sci.", "authors": ["Slawomir  Lasota", "Marcin  Poturalski"], "year": 2016, "n_citations": 1}
{"id": 5320309, "s2_id": "c631fa8efeefb9c5c98e4e10da6c68da0fe20b4c", "title": "Age-of-information in the presence of error", "abstract": "We consider the peak age-of-information (PAoI) in an M/M/1 queueing system with packet delivery error, i.e., update packets can get lost during transmissions to their destination. We focus on two types of policies, one is to adopt Last-Come-First-Served (LCFS) scheduling, and the other is to utilize retransmissions, i.e., keep transmitting the most recent packet. Both policies can effectively avoid the queueing delay of a busy channel and ensure a small PAoI. Exact PAoI expressions under both policies with different error probabilities are derived, including First-Come-First-Served (FCFS), LCFS with preemptive priority, LCFS with non-preemptive priority, Retransmission with preemptive priority, and Retransmission with non-preemptive priority. Numerical results obtained from analysis and simulation are presented to validate our results.", "venue": "2016 IEEE International Symposium on Information Theory (ISIT)", "authors": ["Kun  Chen", "Longbo  Huang"], "year": 2016, "n_citations": 135}
{"id": 5321227, "s2_id": "ca877f64fdbe3641bec2d5b15dae5288f26b178f", "title": "Performance measurements of supercomputing and cloud storage solutions", "abstract": "Increasing amounts of data from varied sources, particularly in the fields of machine learning and graph analytics, are causing storage requirements to grow rapidly. A variety of technologies exist for storing and sharing these data, ranging from parallel file systems used by supercomputers to distributed block storage systems found in clouds. Relatively few comparative measurements exist to inform decisions about which storage systems are best suited for particular tasks. This work provides these measurements for two of the most popular storage technologies: Lustre and Amazon S3. Lustre is an open-source, high performance, parallel file system used by many of the largest supercomputers in the world. Amazon's Simple Storage Service, or S3, is part of the Amazon Web Services offering, and offers a scalable, distributed option to store and retrieve data from anywhere on the Internet. Parallel processing is essential for achieving high performance on modern storage systems. The performance tests used span the gamut of parallel I/O scenarios, ranging from single-client, single-node Amazon S3 and Lustre performance to a large-scale, multi-client test designed to demonstrate the capabilities of a modern storage appliance under heavy load. These results show that, when parallel I/O is used correctly (i.e., many simultaneous read or write processes), full network bandwidth performance is achievable and ranged from 10 gigabits/s over a 10 GigE S3 connection to 0.35 terabits/s using Lustre on a 1200 port 10 GigE switch. These results demonstrate that S3 is well-suited to sharing vast quantities of data over the Internet, while Lustre is well-suited to processing large quantities of data locally.", "venue": "2017 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Michael  Jones", "Jeremy  Kepner", "William  Arcand", "David  Bestor", "Bill  Bergeron", "Vijay  Gadepally", "Michael  Houle", "Matthew  Hubbell", "Peter  Michaleas", "Andrew  Prout", "Albert  Reuther", "Siddharth  Samsi", "Paul  Monticiollo"], "year": 2017, "n_citations": 3}
{"id": 5323195, "s2_id": "bd7166edeaf13446bd9d343df7e601e907d43e65", "title": "Node weighted scheduling", "abstract": "This paper proposes a new class of online policies for scheduling in input-buffered crossbar switches. Given an initial configuration of packets at the input buffers, these policies drain all packets in the system in the minimal amount of time provided that there are no further arrivals. These policies are also throughput optimal for a large class of arrival processes which satisfy strong-law of large numbers. We show that it is possible for policies in our class to be throughput optimal even if they are not constrained to be maximal in every time slot.\n Most algorithms for switch scheduling take an edge based approach; in contrast, we focus on scheduling (a large enough set of) the most congested ports. This alternate approach allows for lower-complexity algorithms, and also requires a non-standard technique to prove throughput-optimality. One algorithm in our class, Maximum Vertex-weighted Matching (MVM) has worst-case complexity similar to Max-size Matching, and in simulations shows slightly better delay performance than Max-(edge)weighted-Matching (MWM).", "venue": "SIGMETRICS '09", "authors": ["Gagan Raj Gupta", "Sujay  Sanghavi", "Ness B. Shroff"], "year": 2009, "n_citations": 32}
{"id": 5326646, "s2_id": "9ecbd59ed85d7b6ec34d785559439c7dad09c19f", "title": "Benchmarking TinyML Systems: Challenges and Direction", "abstract": "Recent advancements in ultra-low-power machine learning (TinyML) hardware promises to unlock an entirely new class of smart applications. However, continued progress is limited by the lack of a widely accepted benchmark for these systems. Benchmarking allows us to measure and thereby systematically compare, evaluate, and improve the performance of systems and is therefore fundamental to a field reaching maturity. In this position paper, we present the current landscape of TinyML and discuss the challenges and direction towards developing a fair and useful hardware benchmark for TinyML workloads. Furthermore, we present our three preliminary benchmarks and discuss our selection methodology. Our viewpoints reflect the collective thoughts of the TinyMLPerf working group that is comprised of 30 organizations.", "venue": "ArXiv", "authors": ["Colby R. Banbury", "Vijay Janapa Reddi", "Max  Lam", "William  Fu", "Amin  Fazel", "Jeremy  Holleman", "Xinyuan  Huang", "Robert  Hurtado", "David  Kanter", "Anton  Lokhmotov", "David  Patterson", "Danilo  Pau", "Jae-sun  Seo", "Jeff  Sieracki", "Urmish  Thakker", "Marian  Verhelst", "Poonam  Yadav"], "year": 2020, "n_citations": 54}
{"id": 5331706, "s2_id": "dc09bea2b13fb6ec97f0fc94017616dc2b960256", "title": "Performance Indicator for MIMO MMSE Receivers in the Presence of Channel Estimation Error", "abstract": "We present the derivation of post-processing SNR for minimum mean-squared error (MMSE) receivers with imperfect channel estimates, and show that it is an accurate indicator of the error rate performance of MIMO systems in the presence of channel estimation error. Simulation results show the tightness of the analysis.", "venue": "IEEE Wireless Communications Letters", "authors": ["Eren  Eraslan", "Babak  Daneshrad", "Chung-Yu  Lou"], "year": 2013, "n_citations": 58}
{"id": 5331855, "s2_id": "9a17a57fdd7bbeebdd0c5fe4a50d786034d49182", "title": "Efficient State-Based CRDTs by Delta-Mutation", "abstract": "CRDTs are distributed data types that make eventual consistency of a distributed object possible and non ad-hoc. Specifically, state-based CRDTs ensure convergence through disseminating the entire state, that may be large, and merging it to other replicas; whereas operation-based CRDTs disseminate operations (i.e., small states) assuming an exactly-once reliable dissemination layer. We introduce Delta State Conflict-Free Replicated Datatypes (\\(\\delta \\)-CRDT) that can achieve the best of both worlds: small messages with an incremental nature, disseminated over unreliable communication channels. This is achieved by defining \\(\\delta \\) -mutators to return a delta-state, typically with a much smaller size than the full state, that is joined to both: local and remote states. We introduce the \\(\\delta \\)-CRDT framework, and we explain it through establishing a correspondence to current state-based CRDTs. In addition, we present an anti-entropy algorithm that ensures causal consistency, and two \\(\\delta \\)-CRDT specifications of well-known replicated datatypes.", "venue": "NETYS", "authors": ["Paulo S\u00e9rgio Almeida", "Ali  Shoker", "Carlos  Baquero"], "year": 2015, "n_citations": 52}
{"id": 5332226, "s2_id": "2a9250a113ddb49b87ddfa4e11c77ace1f9cc7e5", "title": "PageRank Pipeline Benchmark: Proposal for a Holistic System Benchmark for Big-Data Platforms", "abstract": "The rise of big data systems has created a need for benchmarks to measure and compare the capabilities of these systems. Big data benchmarks present unique scalability challenges. The supercomputing community has wrestled with these challenges for decades and developed methodologies for creating rigorous scalable benchmarks (e.g., HPC Challenge). The proposed PageRank pipeline benchmark employs supercomputing benchmarking methodologies to create a scalable benchmark that is reflective of many real-world big data processing systems. The PageRank pipeline benchmark builds on existing prior scalable benchmarks (Graph500, Sort, and PageRank) to create a holistic benchmark with multiple integrated kernels that can be run together or independently. Each kernel is well defined mathematically and can be implemented in any programming environment. The linear algebraic nature of PageRank makes it well suited to being implemented using the GraphBLAS standard. The computations are simple enough that performance predictions can be made based on simple computing hardware models. The surrounding kernels provide the context for each kernel that allows rigorous definition of both the input and the output for each kernel. Furthermore, since the proposed PageRank pipeline benchmark is scalable in both problem size and hardware, it can be used to measure and quantitatively compare a wide range of present day and future systems. Serial implementations in C++, Python, Python with Pandas, Matlab, Octave, and Julia have been implemented and their single threaded performance has been measured.", "venue": "2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Patrick  Dreher", "Chansup  Byun", "Chris  Hill", "Vijay  Gadepally", "Bradley C. Kuszmaul", "Jeremy  Kepner"], "year": 2016, "n_citations": 14}
{"id": 5335444, "s2_id": "a634354eafca586c7c249403fd0b565110bd60f8", "title": "Optimized network-coded scalable video multicasting over eMBMS networks", "abstract": "Delivery of multicast video services over fourth generation (4G) networks such as 3GPP Long Term Evolution-Advanced (LTE-A) is gaining momentum. In this paper, we address the issue of efficiently multicasting layered video services by defining a novel resource allocation framework that aims to maximize the service coverage whilst keeping the radio resource footprint low. A key point in the proposed system mode is that the reliability of multicast video services is ensured by means of an Unequal Error Protection implementation of the Network Coding (UEP-NC) scheme. In addition, both the communication parameters and the UEP-NC scheme are jointly optimized by the proposed resource allocation framework. Numerical results show that the proposed allocation framework can significantly increase the service coverage when compared to a conventional Multi-rate Transmission (MrT) strategy.", "venue": "2015 IEEE International Conference on Communications (ICC)", "authors": ["Andrea  Tassi", "Ioannis  Chatzigeorgiou", "Dejan  Vukobratovic", "Andrew L.  Jones"], "year": 2015, "n_citations": 10}
{"id": 5338743, "s2_id": "48e9caa1349e7a2c51253d958b6e79874a455db5", "title": "Achievable Stability in Redundancy Systems", "abstract": "We investigate the achievable stability region for redundancy systems and a quite general workload model with different job types and heterogeneous servers, reflecting job-server affinity relations which may arise from data locality issues and soft compatibility constraints. Under the assumption that job types are known beforehand we establish for New-Better-than-Used (NBU) distributed speed variations that no replication gives a strictly larger stability region than replication. Strikingly, this does not depend on the underlying distribution of the intrinsic job sizes, but observing the job types is essential for this statement to hold. In case of non-observable job types we show that for New-Worse-than-Used (NWU) distributed speed variations full replication gives a larger stability region than no replication.", "venue": "SIGMETRICS", "authors": ["Youri  Raaijmakers", "Sem C. Borst"], "year": 2021, "n_citations": 0}
{"id": 5340622, "s2_id": "0de49f9778ecce677138cb3f902daebfbd70e287", "title": "The \"Hot Potato\" Case: Challenges in Multiplayer Pervasive Games Based on Ad hoc Mobile Sensor Networks and the Experimental Evaluation of a Prototype Game", "abstract": "In this work, we discuss multiplayer pervasive games that rely on the use of ad hoc mobile sensor networks. The unique feature in such games is that players interact with each other and their surrounding environment by using movement and presence as a means of performing game-related actions, utilizing sensor devices. We discuss the fundamental issues and challenges related to these type of games and the scenarios associated with them. We also present and evaluate an example of such a game, called the \"Hot Potato\", developed using the Sun SPOT hardware platform. We provide a set of experimental results, so as to both evaluate our implementation and also to identify issues that arise in pervasive games which utilize sensor network nodes, which show that there is great potential in this type of games.", "venue": "ArXiv", "authors": ["Ioannis  Chatzigiannakis", "Georgios  Mylonas", "Orestis  Akribopoulos", "Marios  Logaras", "Panagiotis C. Kokkinos", "Paul G. Spirakis"], "year": 2010, "n_citations": 14}
{"id": 5359148, "s2_id": "f1a5156af08ca7b86c102dd55cd85c6809813131", "title": "Efficient Checking of Individual Rewards Properties in Markov Population Models", "abstract": "In recent years fluid approaches to the analysis of Markov populations models have been demonstrated to have great pragmatic value. Initially developed to estimate the behaviour of the system in terms of the expected values of population counts, the fluid approach has subsequently been extended to more sophisticated interrogations of models through its embedding within model checking procedures. In this paper we extend recent work on checking CSL properties of individual agents within a Markovian population model, to consider the checking of properties which incorporate rewards.", "venue": "QAPL", "authors": ["Luca  Bortolussi", "Jane  Hillston"], "year": 2015, "n_citations": 8}
{"id": 5366373, "s2_id": "b8ae8d0e587b147e060cba62463dcd0c79cda41e", "title": "SCOPE: C3SR Systems Characterization and Benchmarking Framework", "abstract": "This report presents the design of the Scope infrastructure for extensible and portable benchmarking. Improvements in high- performance computing systems rely on coordination across different levels of system abstraction. Developing and defining accurate performance measurements is necessary at all levels of the system hierarchy, and should be as accessible as possible to developers with different backgrounds. The Scope project aims to lower the barrier to entry for developing performance benchmarks by providing a software architecture that allows benchmarks to be developed independently, by providing useful C/C++ abstractions and utilities, and by providing a Python package for generating publication-quality plots of resulting measurements.", "venue": "ArXiv", "authors": ["Carl  Pearson", "Abdul  Dakkak", "Cheng  Li", "Sarah  Hashash", "Jinjun  Xiong", "Wen-Mei W. Hwu"], "year": 2018, "n_citations": 0}
{"id": 5369043, "s2_id": "2587d39a79319dadb248e881e268bcd6a4663227", "title": "Inversion formula with hypergeometric polynomials and its application to an integral equation", "abstract": "For any complex parameters $x$ and $\\nu$, we provide a new class of linear inversion formulas $T = A(x,\\nu) \\cdot S \\Leftrightarrow S = B(x,\\nu) \\cdot T$ between sequences $S = (S_n)_{n \\in \\mathbb{N}^*}$ and $T = (T_n)_{n \\in \\mathbb{N}^*}$, where the infinite lower-triangular matrix $A(x,\\nu)$ and its inverse $B(x,\\nu)$ involve Hypergeometric polynomials $F(\\cdot)$, namely $$ \n\\left\\{ \n\\begin{array}{ll} \nA_{n,k}(x,\\nu) = \\displaystyle (-1)^k\\binom{n}{k}F(k-n,-n\\nu;-n;x), \n\\\\ \nB_{n,k}(x,\\nu) = \\displaystyle (-1)^k\\binom{n}{k}F(k-n,k\\nu;k;x) \n\\end{array} \\right. $$ for $1 \\leqslant k \\leqslant n$. Functional relations between the ordinary (resp. exponential) generating functions of the related sequences $S$ and $T$ are also given. \nThese new inversion formulas have been initially motivated by the resolution of an integral equation recently appeared in the field of Queuing Theory; we apply them to the full resolution of this integral equation. Finally, matrices involving generalized Laguerre polynomials polynomials are discussed as specific cases of our general inversion scheme.", "venue": "ArXiv", "authors": ["Ridha  Nasri", "Alain  Simonian", "Fabrice  Guillemin"], "year": 2019, "n_citations": 0}
{"id": 5369137, "s2_id": "1edf4d0acbba2ae54a13747a90a7b53b39b58590", "title": "An Empirical Study of Intel Xeon Phi", "abstract": "With at least 50 cores, Intel Xeon Phi is a true many-core architecture. Featuring fairly powerful cores, two cache levels, and very fast interconnections, the Xeon Phi can get a theoretical peak of 1000 GFLOPs and over 240 GB/s. These numbers, as well as its flexibility - it can be used both as a coprocessor or as a stand-alone processor - are very tempting for parallel applications looking for new performance records. \nIn this paper, we present an empirical study of Xeon Phi, stressing its performance limits and relevant performance factors, ultimately aiming to present a simplified view of the machine for regular programmers in search for performance. \nTo do so, we have micro-benchmarked the main hardware components of the processor - the cores, the memory hierarchies, the ring interconnect, and the PCIe connection. We show that, in ideal microbenchmarking conditions, the performance that can be achieved is very close to the theoretical peak, as given in the official programmer's guide. We have also identified and quantified several causes for significant performance penalties. Our findings have been captured in four optimization guidelines, and used to build a simplified programmer's view of Xeon Phi, eventually enable the design and prototyping of applications on a functionality-based model of the architecture.", "venue": "ArXiv", "authors": ["Jianbin  Fang", "Ana Lucia Varbanescu", "Henk J. Sips", "Lilun  Zhang", "Yonggang  Che", "Chuanfu  Xu"], "year": 2013, "n_citations": 42}
{"id": 5370529, "s2_id": "ed503975db8db02f00d1a5939c545cb7c598549e", "title": "On the cluster admission problem for cloud computing", "abstract": "Cloud computing providers must handle heterogeneous customer workloads for resources such as (virtual) CPU or GPU cores. This is particularly challenging if customers, who are already running a job on a cluster, scale their resource usage up and down over time. The provider therefore has to continuously decide whether she can add additional workloads to a given cluster or if doing so would impact existing workloads' ability to scale. Currently, this is often done using simple threshold policies to reserve large parts of each cluster, which leads to low average utilization of the cluster. In this paper, we propose more sophisticated policies for controlling admission to a cluster and demonstrate that they significantly increase cluster utilization. We first introduce the cluster admission problem and formalize it as a constrained Partially Observable Markov Decision Process (POMDP). As it is infeasible to solve the POMDP optimally, we then systematically design heuristic admission policies that estimate moments of each workload's distribution of future resource usage. Via simulations we show that our admission policies lead to a substantial improvement over the simple threshold policy. We then evaluate how much further this can be improved with learned or elicited prior information and how to incentivize users to provide this information.", "venue": "NetEcon@SIGMETRICS", "authors": ["Ludwig  Dierks", "Ian  Kash", "Sven  Seuken"], "year": 2019, "n_citations": 4}
{"id": 5375071, "s2_id": "30325b93bec5a98185cbfa18916441271d9061ba", "title": "Distributed multiscale computing with MUSCLE 2, the Multiscale Coupling Library and Environment", "abstract": "We present the Multiscale Coupling Library and Environment: MUSCLE 2. This multiscale component-based execution environment has a simple to use Java, C++, C, Python and Fortran API, compatible with MPI, OpenMP and threading codes. We demonstrate its local and distributed computing capabilities and compare its performance to MUSCLE 1, file copy, MPI, MPWide, and GridFTP. The local throughput of MPI is about two times higher, so very tightly coupled code should use MPI as a single submodel of MUSCLE 2; the distributed performance of GridFTP is lower, especially for small messages. We test the performance of a canal system model with MUSCLE 2, where it introduces an overhead as small as 5% compared to MPI.", "venue": "J. Comput. Sci.", "authors": ["Joris  Borgdorff", "Mariusz  Mamonski", "Bartosz  Bosak", "Krzysztof  Kurowski", "Mohamed Ben Belgacem", "Bastien  Chopard", "Derek  Groen", "Peter V. Coveney", "Alfons G. Hoekstra"], "year": 2014, "n_citations": 46}
{"id": 5375930, "s2_id": "9b71cca96a08a73d0480f57f4ff45281b49988bc", "title": "Fundamental Limits of Age-of-Information in Stationary and Non-stationary Environments", "abstract": "We study the multi-user scheduling problem for minimizing the Age of Information (AoI) in cellular wireless networks under stationary and non-stationary regimes. We derive fundamental lower bounds for the scheduling problem and design efficient online policies with provable performance guarantees. In the stationary setting, we consider the AoI optimization problem for a set of mobile users travelling around multiple cells. In this setting, we propose a scheduling policy and show that it is 2-optimal. Next, we propose a new adversarial channel model for studying the scheduling problem in non-stationary environments. For N users, we show that the competitive ratio of any online scheduling policy in this setting is at least \u03a9(N). We then propose an online policy and show that it achieves a competitive ratio of O(N2). Finally, we introduce a relaxed adversarial model with channel state estimations for the immediate future. We propose a heuristic model predictive control policy that exploits this feature and compare its performance through numerical simulations.", "venue": "2020 IEEE International Symposium on Information Theory (ISIT)", "authors": ["Subhankar  Banerjee", "Rajarshi  Bhattacharjee", "Abhishek  Sinha"], "year": 2020, "n_citations": 5}
{"id": 5377036, "s2_id": "faf8d67afd0ac8e174845883d9a0abd2c2dbb6fc", "title": "Performance Evaluation of Unicast and Broadcast Mobile Ad hoc Network Routing Protocols", "abstract": "Efficient routing mechanism is a challenging issue for group oriented computing in Mobile Ad Hoc Networks (MANETs). The ability of MANETs to support adequate Quality of Service (QoS) for group communication is limited by the ability of the underlying ad-hoc routing protocols to provide consistent behavior despite the dynamic properties of mobile computing devices. In MANET QoS requirements can be quantified in terms of Packet Delivery Ratio (PDR), Data Latency, Packet Loss Probability, Routing Overhead, Medium Access Control (MAC) Overhead and Data Throughput etc. This paper presents an in-depth study of one-to-many and many-to- many communications in MANETs and provides a comparative performance evaluation of unicast and broadcast routing protocols. Dynamic Source Routing protocol (DSR) is used as unicast protocol and BCAST is used to represent broadcast protocol. The performance differentials are analyzed using ns-2 network simulator varying multicast group size (number of data senders and data receivers). Both protocols are simulated with identical traffic loads and mobility models. Simulation result shows that BCAST performs better than DSR in most cases.", "venue": "ArXiv", "authors": ["Sumon Kumar Debnath", "Foez  Ahmed", "Nayeema  Islam"], "year": 2010, "n_citations": 8}
{"id": 5382726, "s2_id": "967c3f41c42594508931193ea26ade9f84818183", "title": "A Holistic Approach to Information Distribution in Ad Hoc Networks", "abstract": "We investigate the problem of spreading information contents in a wireless ad hoc network with mechanisms embracing the peer-to-peer paradigm. In our vision, information dissemination should satisfy the following requirements: (i) it conforms to a predefined distribution and (ii) it is evenly and fairly carried by all nodes in their turn. In this paper, we observe the dissemination effects when the information moves across nodes according to two well-known mobility models, namely random walk and random direction. Our approach is fully distributed and comes at a very low cost in terms of protocol overhead; in addition, simulation results show that the proposed solution can achieve the aforementioned goals under different network scenarios, provided that a sufficient number of information replicas are injected into the network. This observation calls for a further step: in the realistic case where the user content demand varies over time, we need a content replication/drop strategy to adapt the number of information replicas to the changes in the information query rate. We therefore devise a distributed, lightweight scheme that performs efficiently in a variety of scenarios.", "venue": "ArXiv", "authors": ["Claudio  Casetti", "Carla-Fabiana  Chiasserini", "Marco  Fiore", "Chi-Anh  La", "Pietro  Michiardi"], "year": 2009, "n_citations": 0}
{"id": 5383872, "s2_id": "3ae7f8b29faecca030b137ce84b7bbe4e75f5ee3", "title": "Classifying Application Phases in Asymmetric Chip Multiprocessors", "abstract": "In present study, in order to improve the performance and reduce the amount of power which is dissipated in heterogeneous multicore processors, the ability of detecting the program execution phases is investigated. The programs execution intervals have been classified in different phases based on their throughput and the utilization of the cores. The results of implementing the phase detection technique are investigated on a single core processor and also on a multicore processor. To minimize the profiling overhead, an algorithm for the dynamic adjustment of the profiling intervals is presented. It is based on the behavior of the program and reduces the profiling overhead more than three fold. The results are obtained from executing multiprocessor benchmarks on a given processor. In order to show the program phases clearly, throughput and utilization of execution intervals are presented on a scatter plot. The results are presented for both fixed and variable intervals.", "venue": "ArXiv", "authors": ["A. Z. Jooya", "M.  Analoui"], "year": 2010, "n_citations": 0}
{"id": 5385839, "s2_id": "eb42cd6668afedc3257b7dc0417866e8c00b8d4a", "title": "SRPT for Multiserver Systems", "abstract": "The Shortest Remaining Processing Time (SRPT) scheduling policy and variants thereof have been deployed in many computer systems, including web servers [5], networks [9], databases [3] and operating systems [1]. SRPT has also long been a topic of fascination for queueing theorists due to its optimality properties. In 1966, the mean response time for SRPT was first derived [11], and in 1968 SRPT was shown to minimize mean response time in both a stochastic sense and a worst-case sense [10]. However, these beautiful optimality results and the analysis of SRPT are only known for single-server systems. Almost nothing is known about SRPT in multiserver systems, such as the M/G/k, even for the case of just k = 2 servers.", "venue": "PERV", "authors": ["Isaac  Grosof", "Ziv  Scully", "Mor  Harchol-Balter"], "year": 2019, "n_citations": 5}
{"id": 5390342, "s2_id": "7a9799cfe2da4f3d54065abb03b453d7c38ec3cb", "title": "Multiple timescale dispatch and scheduling for stochastic reliability in smart grids with wind generation integration", "abstract": "Integrating volatile renewable energy resources into the bulk power grid is challenging, due to the reliability requirement that the load and generation in the system remain balanced all the time. In this study, we tackle this challenge for smart grid with integrated wind generation, by leveraging multi-timescale dispatch and scheduling. Specifically, we consider smart grids with two classes of energy users - traditional energy users and opportunistic energy users (e.g., smart meters or smart appliances), and investigate pricing and dispatch at two timescales, via day-ahead scheduling and real-time scheduling. In day-ahead scheduling, with the statistical information on wind generation and energy demands, we characterize the optimal procurement of the energy supply and the day-ahead retail price for the traditional energy users; in real-time scheduling, with the realization of wind generation and the load of traditional energy users, we optimize real-time prices to manage the opportunistic energy users so as to achieve system-wide reliability. More specifically, when the opportunistic users are non-persistent, we obtain closed-form solutions to the two-level scheduling problem. For the persistent case, we treat the scheduling problem as a multi-timescale Markov decision process. We show that it can be recast, explicitly, as a classic Markov decision process with continuous state and action spaces, the solution to which can be found via standard techniques.", "venue": "2011 Proceedings IEEE INFOCOM", "authors": ["Miao  He", "Sugumar  Murugesan", "Junshan  Zhang"], "year": 2011, "n_citations": 134}
{"id": 5390492, "s2_id": "1172a5be90b69b62ac029c9f2ef5e160ef0a70c2", "title": "Optimizing Inter-Datacenter Tail Flow Completion Times using Best Worst-case Routing", "abstract": "Flow routing over inter-datacenter networks is a well-known problem where the network assigns a path to a newly arriving flow potentially according to the network conditions and the properties of the new flow. An essential system-wide performance metric for a routing algorithm is the flow completion times, which affect the performance of applications running across multiple datacenters. Current static and dynamic routing approaches do not take advantage of flow size information in routing, which is practical in a controlled environment such as inter-datacenter networks that are managed by the datacenter operators. In this paper, we discuss Best Worst-case Routing (BWR), which aims at optimizing the tail completion times of long-running flows over inter-datacenter networks with non-uniform link capacities. Since finding the path with the best worst-case completion time for a new flow is NP-Hard, we investigate two heuristics, BWRH and BWRHF, which use two different upper bounds on the worst-case completion times for routing. We evaluate BWRH and BWRHF against several real WAN topologies and multiple traffic patterns. Although BWRH better models the BWR problem, BWRH and BWRHF show negligible difference across various system-wide performance metrics, while BWRHF being significantly faster. Furthermore, we show that compared to other popular routing heuristics, BWRHF can reduce the mean and tail flow completion times by over 1.5\u00d7 and 2\u00d7, respectively.", "venue": "2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "authors": ["Max  Noormohammadpour", "Ajitesh  Srivastava", "Cauligi S. Raghavendra"], "year": 2019, "n_citations": 0}
{"id": 5393480, "s2_id": "30a608351c7637b186f1c4ee35f04f2b79969a2e", "title": "Critical Utility Infrastructural Resilience", "abstract": "The paper refers to CRUTIAL, CRitical UTility InfrastructurAL Resilience, a European project within the research area of Critical Information Infrastructure Protection, with a specific focus on the infrastructures operated by power utilities, widely recognized as fundamental to national and international economy, security and quality of life. Such infrastructures faced with the recent market deregulations and the multiple interdependencies with other infrastructures are becoming more and more vulnerable to various threats, including accidental failures and deliberate sabotage and malicious attacks. The subject of CRUTIAL research are small scale networked ICT systems used to control and manage the electric power grid, in which artifacts controlling the physical process of electricity transportation need to be connected with corporate and societal applications performing management and maintenance functionality. The peculiarity of such ICT-supported systems is that they are related to the power system dynamics and its emergency conditions. Specific effort need to be devoted by the Electric Power community and by the Information Technology community to influence the technological progress in order to allow commercial intelligent electronic devices to be effectively deployed for the protection of citizens against cyber threats to electric power management and control systems. A well-founded know-how needs to be built inside the industrial power sector to allow all the involved stakeholders to achieve their service objectives without compromising the resilience properties of the logical and physical assets that support the electric power provision.", "venue": "ArXiv", "authors": ["Giovanna  Dondossola", "Geert  Deconinck", "Felicita Di Giandomenico", "Susanna  Donatelli", "Mohamed  Ka\u00e2niche", "Paulo  Ver\u00edssimo"], "year": 2012, "n_citations": 13}
{"id": 5393530, "s2_id": "d7ce5e1e16ff98ef0f42c117eddfde860d2a2b0d", "title": "Binary systematic network coding for progressive packet decoding", "abstract": "We consider binary systematic network codes and investigate their capability of decoding a source message either in full or in part. We carry out a probability analysis, derive closed-form expressions for the decoding probability and show that systematic network coding outperforms conventional network coding. We also develop an algorithm based on Gaussian elimination that allows progressive decoding of source packets. Simulation results show that the proposed decoding algorithm can achieve the theoretical optimal performance. Furthermore, we demonstrate that systematic network codes equipped with the proposed algorithm are good candidates for progressive packet recovery owing to their overall decoding delay characteristics.", "venue": "2015 IEEE International Conference on Communications (ICC)", "authors": ["Andrew L. Jones", "Ioannis  Chatzigeorgiou", "Andrea  Tassi"], "year": 2015, "n_citations": 27}
{"id": 5393880, "s2_id": "6800600e6451d0bf0a4e866a483cac8c8617da88", "title": "Automatic loop kernel analysis and performance modeling with Kerncraft", "abstract": "Analytic performance models are essential for understanding the performance characteristics of loop kernels, which consume a major part of CPU cycles in computational science. Starting from a validated performance model one can infer the relevant hardware bottlenecks and promising optimization opportunities. Unfortunately, analytic performance modeling is often tedious even for experienced developers since it requires in-depth knowledge about the hardware and how it interacts with the software. We present the \"Kerncraft\" tool, which eases the construction of analytic performance models for streaming kernels and stencil loop nests. Starting from the loop source code, the problem size, and a description of the underlying hardware, Kerncraft can ideally predict the single-core performance and scaling behavior of loops on multicore processors using the Roofline or the Execution-Cache-Memory (ECM) model. We describe the operating principles of Kerncraft with its capabilities and limitations, and we show how it may be used to quickly gain insights by accelerated analytic modeling.", "venue": "PMBS '15", "authors": ["Julian  Hammer", "Georg  Hager", "Jan  Eitzinger", "Gerhard  Wellein"], "year": 2015, "n_citations": 22}
{"id": 5394148, "s2_id": "635930d33fb548991841c4bb307943df54ce8060", "title": "Performance Measurement of Cloud Computing Services", "abstract": "Cloud computing today has now been growing as new technologies and new business models. In distributed technology perspective, cloud computing most like client-server services like web-based or web-service but it used virtual resources to execute. Currently, cloud computing relies on the use of an elastic virtual machine and the use of network for data exchange. We conduct an experimental setup to measure the quality of service received by cloud computing customers. Experimental setup done by creating a HTTP service that runs in the cloud computing infrastructure. We interest to know about the impact of increasing the number of users on the average quality received by users. The qualities received by user measured within two parameters consist of average response times and the number of requests time out. Experimental results of this study show that increasing the number of users has increased the average response time. Similarly, the number of request time out increasing with increasing number of users. It means that the qualities of service received by user are decreasing also. We found that the impact of the number of users on the quality of service is no longer in linear trend. The results of this study can be used as a reference model for the network operator in performing services in which a certain number of users in order to obtain optimal quality services.", "venue": "CloudCom 2012", "authors": ["Sinung  Suakanto", "Suhono H Supangkat", "Suhardi", "Roberd  Saragih"], "year": 2012, "n_citations": 28}
{"id": 5400331, "s2_id": "f3cbe245cf4f2ac81fdf7675efbe4e9568443d9e", "title": "Extreme Scaling of Lattice Quantum Chromodynamics", "abstract": "As the complexity and size of challenges in science and engineering are continually increasing, it is highly important that applications are able to scale strongly to very large numbers of cores (>100,000 cores) to enable HPC systems to be utilised efficiently. This paper presents results of strong scaling tests performed with an MPI only and a hybrid MPI + OpenMP version of the Lattice QCD application BQCD on the European Tier-0 system SuperMUC at LRZ.", "venue": "PARCO", "authors": ["David  Brayford", "Momme  Allalen", "Volker  Weinberg"], "year": 2013, "n_citations": 1}
{"id": 5406600, "s2_id": "0512a6a26efea1e91992b408e4ed3e0140cf3bc2", "title": "Duet Benchmarking: Improving Measurement Accuracy in the Cloud", "abstract": "We investigate the duet measurement procedure, which helps improve the accuracy of performance comparison experiments conducted on shared machines by executing the measured artifacts in parallel and evaluating their relative performance together, rather than individually. Specifically, we analyze the behavior of the procedure in multiple cloud environments and use experimental evidence to answer multiple research questions concerning the assumption underlying the procedure. We demonstrate improvements in accuracy ranging from 2.3x to 12.5x (5.03x on average) for the tested ScalaBench (and DaCapo) workloads, and from 23.8x to 82.4x (37.4x on average) for the SPEC CPU 2017 workloads.", "venue": "ICPE", "authors": ["Lubom\u00edr  Bulej", "Vojtech  Hork\u00fd", "Petr  Tuma", "Fran\u00e7ois  Farquet", "Aleksandar  Prokopec"], "year": 2020, "n_citations": 4}
{"id": 5408160, "s2_id": "e18a90afd8f9a74067e9a8a93c3c3294ac4919ea", "title": "Network traffic control for multi-homed end-hosts via SDN", "abstract": "Software Defined Networking (SDN) is an emerging technology of efficiently controlling and managing computer networks, such as in data centres, Wide Area Networks (WANs), as well as in ubiquitous communication. In this paper, we explore the idea of embedding the SDN components, represented by SDN controller and virtual switch, in end-hosts to improve network performance. In particular, we consider load balancing across multiple network interfaces on end-hosts with different link capacity scenarios. We have explored and implemented different SDN-based load balancing approaches based on OpenFlow software switches, and have demonstrated the feasibility and the potential of this approach. The proposed system has been evaluated with multipath transmission control protocol (MPTCP). Our results demonstrated the potential of applying the SDN concepts on multi-homed devices resulting in an increase in achieved throughput of 55\\% compared to the legacy single network approach and 10\\% compared to the MPTCP.", "venue": "IET Commun.", "authors": ["Anees  Al-Najjar", "Furqan Hameed Khan", "Marius  Portmann"], "year": 2020, "n_citations": 2}
{"id": 5413452, "s2_id": "159179f59230cd02ae9a19d49f8b0903a0f09b0f", "title": "RBioCloud: A Light-Weight Framework for Bioconductor and R-based Jobs on the Cloud", "abstract": "Large-scale ad hoc analytics of genomic data is popular using the R-programming language supported by over 700 software packages provided by Bioconductor. More recently, analytical jobs are benefitting from on-demand computing and storage, their scalability and their low maintenance cost, all of which are offered by the cloud. While biologists and bioinformaticists can take an analytical job and execute it on their personal workstations, it remains challenging to seamlessly execute the job on the cloud infrastructure without extensive knowledge of the cloud dashboard. How analytical jobs can not only with minimum effort be executed on the cloud, but also how both the resources and data required by the job can be managed is explored in this paper. An open-source light-weight framework for executing R-scripts using Bioconductor packages, referred to as `RBioCloud', is designed and developed. RBioCloud offers a set of simple command-line tools for managing the cloud resources, the data and the execution of the job. Three biological test cases validate the feasibility of RBioCloud.", "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics", "authors": ["Blesson  Varghese", "Ishan  Patel", "Adam  Barker"], "year": 2015, "n_citations": 1}
{"id": 5413853, "s2_id": "57eba27725f1a1e303f4342011cd7ad9383dd85a", "title": "Investigating Randomly Generated Adjacency Matrices For Their Use In Modeling Wireless Topologies", "abstract": "Generation of realistic topologies plays an important role in determining the accuracy and validity of simulation studies. This study presents a discussion to justify why, and how often randomly generated adjacency matrices may not not conform to wireless topologies in the physical world. Specifically, it shows through analysis and random trials that, more than 90% of times, a randomly generated adjacency matrix will not conform to a valid wireless topology, when it has more than 3 nodes. By showing that node triplets in the adjacency graph need to adhere to rules of a geometric vector space, the study shows that the number of randomly chosen node triplets failing consistency checks grow at the order of O(base^3), where base is the granularity of the distance metric. Further, the study models and presents a probability estimate with which any randomly generated adjacency matrix would fail realization. This information could be used to design simpler algorithms for generating k-connected wireless topologies.", "venue": "ArXiv", "authors": ["Gautam D. Bhanage", "Sanjit  Kaul"], "year": 2013, "n_citations": 0}
{"id": 5414076, "s2_id": "543ad5eca1bd5dc929144f1207e3ef729d70c444", "title": "Exploration of lane-changing duration for heavy vehicles and passenger cars: a survival analysis approach", "abstract": "Lane-changing (LC) behavior describes the lateral movement of the vehicle from the current-lane to the target-lane while proceeding forward. Among the many research directions, LC duration (LCD) measures the total time it takes for a vehicle to travel from the current lane to the target lane, which is an indispensable indicator to characterize the LC behavior. Although existing research has made some achievements, less attention has been paid to the research of heavy vehicles' LCD. Therefore, this paper aims to further explore the LCD between heavy vehicles and passenger cars. LC trajectories are extracted from the newly-released HighD dataset, which contains of 16.5 hours of measurement and over 11,000 vehicles. The survival function of LCD has been estimated, and the characteristic has been analyzed. Thereafter, the Accelerated Failure Time model is introduced to explore the influencing factors. Results demonstrate that the MST value of passenger cars and heavy vehicles is about 5.51s and 6.08s. The heavy vehicles would maintain a longer time-headway and distance-headway with preceding vehicle when performing LC. Nevertheless, these two factors do not significantly affect the LCD of heavy vehicles. Finally, the results and the modeling implications have been discussed. We hope this paper could contribute to our further understanding of the LC behaviors for heavy vehicles and passenger cars.", "venue": "ArXiv", "authors": ["Yang  Li", "Linbo  Li", "Daiheng  Ni"], "year": 2021, "n_citations": 2}
{"id": 5416151, "s2_id": "e7a3fb38c3a82039a42ce279f0954ce33b172d61", "title": "On benchmarking embedded Linux flash file systems", "abstract": "Due to its attractive characteristics in terms of performance, weight and power consumption, NAND flash memory became the main non volatile memory (NVM) in embedded systems. Those NVMs also present some specific characteristics/constraints: good but asymmetric I/O performance, limited lifetime, write/erase granularity asymmetry, etc.\n Those peculiarities are either managed in hardware for flash disks (SSDs, SD cards, USB sticks, etc.) or in software for raw embedded flash chips. When managed in software, flash algorithms and structures are implemented in a specific flash file system (FFS). In this paper, we present a performance study of the most widely used FFSs in embedded Linux: JFFS2, UBIFS, and YAFFS. We show some very particular behaviors and large performance disparities for tested FFS operations such as mounting, copying, and searching file trees, compression, etc.", "venue": "SIGBED", "authors": ["Pierre  Olivier", "Jalil  Boukhobza", "Eric  Senn"], "year": 2012, "n_citations": 11}
{"id": 5419387, "s2_id": "afb454f3446c0865ecc759a89a8cd2dcfd910975", "title": "Performance of CSMA in multi-channel wireless networks", "abstract": "We analyze the performance of CSMA in multi-channel wireless networks, accounting for the random nature of traffic. Specifically, we assess the ability of CSMA to fully utilize the radio resources and in turn to stabilize the network in a dynamic setting with flow arrivals and departures. We prove that CSMA is optimal in the ad-hoc mode, when each flow goes through a unique dedicated wireless link from a transmitter to a receiver. It is generally suboptimal in infrastructure mode, when all data flows originate from or are destined to the same set of access points, due to the inherent bias of CSMA against downlink traffic. We propose a slight modification of CSMA that we refer to as flow-aware CSMA, which corrects this bias and makes the algorithm optimal in all cases. The analysis is based on some time-scale separation assumption which is proved valid in the limit of large flow sizes.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Thomas  Bonald", "Mathieu  Feuillet"], "year": 2012, "n_citations": 12}
{"id": 5423478, "s2_id": "23274a30cfe21a829aa23ed068616f27ee79f0a9", "title": "Patch-based field-of-view matching in multi-modal images for electroporation-based ablations", "abstract": "Various multi-modal imaging sensors are currently involved at different steps of an interventional therapeutic work-flow. Cone beam computed tomography (CBCT), computed tomography (CT) or Magnetic Resonance (MR) images thereby provides complementary functional and/or structural information of the targeted region and organs at risk. Merging this information relies on a correct spatial alignment of the observed anatomy between the acquired images. This can be achieved by the means of multi-modal deformable image registration (DIR), demonstrated to be capable of estimating dense and elastic deformations between images acquired by multiple imaging devices. However, due to the typically different field-of-view (FOV) sampled across the various imaging modalities, such algorithms may severely fail in finding a satisfactory solution. In the current study we propose a new fast method to align the FOV in multi-modal 3D medical images. To this end, a patch-based approach is introduced and combined with a state-of-the-art multi-modal image similarity metric in order to cope with multi-modal medical images. The occurrence of estimated patch shifts is computed for each spatial direction and the shift value with maximum occurrence is selected and used to adjust the image field-of-view. The performance of the proposed method - in terms of both registration accuracy and computational needs - is analyzed in the practical case of on-line irreversible electroporation procedures. In total, 30 pairs of pre-/per-operative IRE images are considered to illustrate the efficiency of our algorithm. We show that a regional registration approach using voxel patches provides a good structural compromise between the voxel-wise and \"global shifts\" approaches. The method was thereby beneficial for CT to CBCT and MRI to CBCT registration tasks, especially when highly different image FOVs are involved. Besides, the benefit of the method for CT to CBCT and MRI to CBCT image registration is analyzed, including the impact of artifacts generated by percutaneous needle insertions. Additionally, the computational needs using commodity hardware are demonstrated to be compatible with clinical constraints in the practical case of on-line procedures. The proposed patch-based workflow thus represents an attractive asset for DIR at different stages of an interventional procedure.", "venue": "Comput. Medical Imaging Graph.", "authors": ["Luc  Lafitte", "R\u00e9mi  Giraud", "Cornel  Zachiu", "Mario  Ries", "Oliver  Sutter", "Arthur  Petit", "Oliver  Seror", "Clair  Poignard", "Baudouin Denis de Senneville"], "year": 2020, "n_citations": 0}
{"id": 5426994, "s2_id": "a50ffc56728b2b31cc1a914fe3d06b5c9f13df79", "title": "Symbolic Computation of the Worst-Case Execution Time of a Program", "abstract": "Parametric Worst-case execution time (WCET) analysis of a sequential program produces a formula that represents the worst-case execution time of the program, where parameters of the formula are user-defined parameters of the program (as loop bounds, values of inputs or internal variables, etc). \nIn this paper we propose a novel methodology to compute the parametric WCET of a program. Unlike other algorithms in the literature, our method is not based on Integer Linear Programming (ILP). Instead, we follow an approach based on the notion of symbolic computation of WCET formulae. After explaining our methodology and proving its correctness, we present a set of experiments to compare our method against the state of the art. We show that our approach dominates other parametric analyses, and produces results that are very close to those produced by non-parametric ILP-based approaches, while keeping very good computing time.", "venue": "ArXiv", "authors": ["Cl\u00e9ment  Ballabriga", "Julien  Forget", "Giuseppe  Lipari"], "year": 2017, "n_citations": 0}
{"id": 5434630, "s2_id": "4928348f82bec010cd807ad09a16468ce9d6c031", "title": "Exact two-terminal reliability of some directed networks", "abstract": "The calculation of network reliability in a probabilistic context has long been an issue of practical and academic importance. Conventional approaches (determination of bounds, sums of disjoint products algorithms, Monte Carlo evaluations, studies of the reliability polynomials, etc.) only provide approximations when the networkpsilas size increases, even when nodes do not fail and all edges have the same reliability p. We consider here a directed, generic graph of arbitrary size mimicking real-life long-haul communication networks, and give the exact, analytical solution for the two-terminal reliability. This solution involves a product of transfer matrices, in which individual reliabilities of edges and nodes are taken into account. The special case of identical edge and node reliabilities (p and p, respectively) is addressed. We consider a case study based on a commonly-used configuration, and assess the influence of the edges being directed (or not) on various measures of network performance. While the two-terminal reliability, the failure frequency and the failure rate of the connection are quite similar, the locations of complex zeros of the two-terminal reliability polynomials exhibit strong differences, and various structure transitions at specific values of p. The present work could be extended to provide a catalog of exactly solvable networks in terms of reliability, which could be useful as building blocks for new and improved bounds, as well as benchmarks, in the general case.", "venue": "2007 6th International Workshop on Design and Reliable Communication Networks", "authors": ["Christian  Tanguy"], "year": 2007, "n_citations": 16}
{"id": 5434705, "s2_id": "ca6ce6cd8e5f37cc5820712b53074662b38cfcaa", "title": "Sleep period optimization model for layered video service delivery over eMBMS networks", "abstract": "Long Term Evolution-Advanced (LTE-A) and the evolved Multimedia Broadcast Multicast System (eMBMS) are the most promising technologies for the delivery of highly bandwidth demanding applications. In this paper we propose a green resource allocation strategy for the delivery of layered video streams to users with different propagation conditions. The goal of the proposed model is to minimize the user energy consumption. That goal is achieved by minimizing the time required by each user to receive the broadcast data via an efficient power transmission allocation model. A key point in our system model is that the reliability of layered video communications is ensured by means of the Random Linear Network Coding (RLNC) approach. Analytical results show that the proposed resource allocation model ensures the desired quality of service constraints, while the user energy footprint is significantly reduced.", "venue": "2015 IEEE International Conference on Communications (ICC)", "authors": ["Lorenzo  Carla", "Francesco  Chiti", "Romano  Fantacci", "Andrea  Tassi"], "year": 2015, "n_citations": 1}
{"id": 5436537, "s2_id": "2e61aeb04a89d3956c8ba87f71982101255e8583", "title": "Low-Power Computer Vision: Status, Challenges, Opportunities", "abstract": "Computer vision has achieved impressive progress in recent years. Meanwhile, mobile phones have become the primary computing platforms for millions of people. In addition to mobile phones, many autonomous systems rely on visual data for making decisions and some of these systems have limited energy (such as unmanned aerial vehicles also called drones and mobile robots). These systems rely on batteries and energy efficiency is critical. This article serves two main purposes: (1) Examine the state-of-the-art for low-power solutions to detect objects in images. Since 2015, the IEEE Annual International Low-Power Image Recognition Challenge (LPIRC) has been held to identify the most energy-efficient computer vision solutions. This article summarizes 2018 winners' solutions. (2) Suggest directions for research as well as opportunities for low-power computer vision.", "venue": "ArXiv", "authors": ["Sergei  Alyamkin", "Matthew  Ardi", "Alexander C. Berg", "Achille  Brighton", "Bo  Chen", "Yiran  Chen", "Hsin-Pai  Cheng", "Zichen  Fan", "Chen  Feng", "Bo  Fu", "Kent  Gauen", "Abhinav  Goel", "Alexander  Goncharenko", "Xuyang  Guo", "Soonhoi  Ha", "Andrew  Howard", "Xiao  Hu", "Yuanjun  Huang", "Dong-Hyun  Kang", "Jaeyoun  Kim", "Jong Gook Ko", "Alexander  Kondratyev", "Jun-Hyeok  Lee", "Seung-Jae  Lee", "S. W. Lee", "Zichao  Li", "Zhiyu  Liang", "Juzheng  Liu", "Xin  Liu", "Yang  Lu", "Yung-Hsiang  Lu", "Deeptanshu  Malik", "Hong Hanh Nguyen", "Eunbyung  Park", "Denis  Repin", "Liang  Shen", "Tao  Sheng", "Fei  Sun", "David  Svitov", "George K. Thiruvathukal", "Baiwu  Zhang", "Jingchi  Zhang", "Xiaopeng  Zhang", "Shaojie  Zhuo"], "year": 2019, "n_citations": 4}
{"id": 5437964, "s2_id": "0ced1b8877cbbaa3dee9f18f0b1dfae6a615d1a0", "title": "DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks", "abstract": "Data movement between the CPU and main memory is a first-order obstacle against improv ing performance, scalability, and energy efficiency in modern systems. Computer systems employ a range of techniques to reduce overheads tied to data movement, spanning from traditional mechanisms (e.g., deep multi-level cache hierarch ies, aggressive hardware prefetcher s) to emerging techniques such as Near-Data Processing (NDP), where some computation is moved close to memory. Prior NDP works investigate the root causes of data movement bottlenecks using different profiling methodologies and tools. However, there is still a lack of understanding about the key metrics that can identify different data movement bottlenecks and their relation to traditional and emerging data movement mitigation mechanisms. Our goal is to methodically identify potential sources of data movement over a broad set of applications and to comprehensively compare traditional compute-centric data movement mitigation techniques (e.g., cach ing and prefetch ing) to more memory-centric techniques (e.g., NDP), thereby developing a rigorous understanding of the best techniques to mitigate each source of data movement. With this goal in mind, we perform the first large-scale characterization of a wide variety of applications, across a wide range of application domains, to identify fundamental program properties that lead to data movement to/from main memory. We develop the first systematic methodology to classify applications based on the sources contributing to data movement bottlenecks. From our large-scale characterization of 77K functions across 345 applications, we select 144 functions to form the first open-source benchmark suite (DAMOV) for main memory data movement studies. We select a diverse range of functions that (1) represent different types of data movement bottlenecks, and (2) come from a wide range of application domains. Using NDP as a case study, we identify new insights about the different data movement bottlenecks and use these insights to determine the most suitable data movement mitigation mechanism for a particular application. We open-source DAMOV and the complete source code for our new characterization methodology at https://github.com/CMU-SAFARI/DAMOV.", "venue": "IEEE Access", "authors": ["Geraldo F. Oliveira", "Juan  G\u00f3mez-Luna", "Lois  Orosa", "Saugata  Ghose", "Nandita  Vijaykumar", "Ivan  Fernandez", "Mohammad  Sadrosadati", "Onur  Mutlu"], "year": 2021, "n_citations": 10}
{"id": 5439687, "s2_id": "6d0f711394606b9bfefc82241794cfce81b8902c", "title": "A Fluid Reservoir Model for the Age of Information through Energy-Harvesting Transmitters", "abstract": "We apply a fluid-reservoir model to study the Age-of-Information (AoI) of update packets through energy-harvesting transmitters. The model is closer to how energy is stored and depleted in reality, and can reveal the system behavior for different settings of packet arrival rates, service rates, and energy charging and depletion rates. We present detailed results for both finite and infinite transmitter buffers and an infinite energy reservoir, and some indicative results for a finite reservoir. The results are derived for the mean AoI in the case of an infinite transmitter buffer and an infinite reservoir, and for the mean peak AoI for the remaining cases. The results show that, similar to a system without energy constraints, the transmitter buffer should be kept to a minimum in order to avoid queueing delays and maintain freshness of updates. Furthermore, a high update packet rate is only helpful in energy-rich regimes, whereas in energy-poor regimes more frequent updates deplete the energy reservoir and result in higher AoI values.", "venue": "2021 International Symposium on Performance Evaluation of Computer and Telecommunication Systems (SPECTS)", "authors": ["Ioannis Z. Koukoutsidis"], "year": 2021, "n_citations": 0}
{"id": 5439867, "s2_id": "f8c16ab76e75eaffb80df9643444bdf55a51028c", "title": "Best-by-Simulations: A Framework for Comparing Efficiency of Reconfigurable Multicore Architectures on Workloads with Deadlines", "abstract": "Energy consumption is a major concern in multicore systems. Perhaps the simplest strategy for reducing energy costs is to use only as many cores as necessary while still being able to deliver a desired quality of service. Motivated by earlier work on a dynamic (heterogeneous) core allocation scheme for H.264 video decoding that reduces energy costs while delivering desired frame rates, we formulate operationally the general problem of executing a sequence of actions on a reconfigurable machine while meeting a corresponding sequence of absolute deadlines, with the objective of reducing cost. Using a transition system framework that associates costs (e.g., time, energy) with executing an action on a particular resource configuration, we use the notion of amortised cost to formulate in terms of simulation relations appropriate notions for comparing deadline-conformant executions. We believe these notions can provide the basis for an operational theory of optimal cost executions and performance guarantees for approximate solutions, in particular relating the notion of simulation from transition systems to that of competitive analysis used for, e.g., online algorithms.", "venue": "PLACES@ETAPS", "authors": ["Sanjiva  Prasad"], "year": 2017, "n_citations": 0}
{"id": 5441476, "s2_id": "0024fd81ec4916fef0ff0d37fe4e7060824e8002", "title": "Heterogeneous download times in bandwidth-homogeneous BitTorrent swarms", "abstract": "Modeling and understanding BitTorrent (BT) dynamics is a recurrent research topic mainly due to its high complexity and tremendous practical efficiency. Over the years, different models have uncovered various phenomena exhibited by the system, many of which have direct impact on its performance. In this paper we identify and characterize a phenomenon that has not been previously observed: homogeneous peers (with respect to their upload capacities) experience heterogeneous download times. This behavior has direct impact on peer and system performance, such as high variability of download times, unfairness with respect to peer arrival order, bursty departures and content synchronization. Detailed packet-level simulations and prototype-based experiments on the Internet were performed to characterize this phenomenon. We also develop a mathematical model that accurately predicts the heterogeneous download rates of the homogeneous peers as a function of their content. In addition, we apply the model to calculate lower and upper bounds to the number of departures that occur in a burst. The heterogeneous download rates are more prevalent in unpopular swarms (very few peers). Although few works have addressed this kind of swarm, these by far represent the most common type of swarm in BT.", "venue": "ArXiv", "authors": ["Fabricio  Murai", "Antonio A. de A. Rocha", "Daniel R. Figueiredo", "Edmundo A. de Souza e Silva"], "year": 2021, "n_citations": 0}
{"id": 5444813, "s2_id": "8f1c7562645af237a939bb62cadf1d1cea196d41", "title": "Efficient Strategy Iteration for Mean Payoff in Markov Decision Processes", "abstract": "Markov decision processes (MDPs) are standard models for probabilistic systems with non-deterministic behaviours. Mean payoff (or long-run average reward) provides a mathematically elegant formalism to express performance related properties. Strategy iteration is one of the solution techniques applicable in this context. While in many other contexts it is the technique of choice due to advantages over e.g. value iteration, such as precision or possibility of domain-knowledge-aware initialization, it is rarely used for MDPs, since there it scales worse than value iteration. We provide several techniques that speed up strategy iteration by orders of magnitude for many MDPs, eliminating the performance disadvantage while preserving all its advantages.", "venue": "ATVA", "authors": ["Jan  Kret\u00ednsk\u00fd", "Tobias  Meggendorfer"], "year": 2017, "n_citations": 9}
{"id": 5447246, "s2_id": "6e82e62990e683282a8874145fd32cb2a96c666e", "title": "Bounds on series-parallel slowdown", "abstract": "We use activity networks (task graphs) to model parallel programs and consider series-parallel extensions of these networks. Our motivation is two-fold: the benefits of series-parallel activity networks and the modelling of programming constructs, such as those imposed by current parallel computing environments. Series-parallelisation adds precedence constraints to an activity network, usually increasing its makespan (execution time). The slowdown ratio describes how additional constraints affect the makespan. We disprove an existing conjecture positing a bound of two on the slowdown when workload is not considered. Where workload is known, we conjecture that 4/3 slowdown is always achievable, and prove our conjecture for small networks using max-plus algebra. We analyse a polynomial-time algorithm showing that achieving 4/3 slowdown is in exp-APX. Finally, we discuss the implications of our results.", "venue": "ArXiv", "authors": ["Andr\u00e1s Z. Salamon", "Vashti  Galpin"], "year": 2009, "n_citations": 1}
{"id": 5447375, "s2_id": "bc89d0e43781ede27a31e48b05d44592c17f3812", "title": "An infinite-server queueing model MMAPkGk in semi-Markov random environment with marked MAP arrival and subject to catastrophes", "abstract": "In the present paper the infinite-server MMAPkGk queueing model with random resource vector of customers, marked MAP arrival and semi-Markov (SM) arrival of catastrophes is considered. The joint generating functions (PGF) of transient and stationary distributions of number of busy servers and numbers of different types served customers, as well as Laplace transformations (LT) of joint distributions of total accumulated resources in the model at moment and total accumulated resources of served customers during time interval are found. The basic differential and renewal equations for transient and stationary PGF of queue sizes of customers are found.", "venue": "ArXiv", "authors": ["K.  Kerobyan", "R.  Covington", "R.  Kerobyan", "K.  Enakoutsa"], "year": 2018, "n_citations": 2}
{"id": 5448786, "s2_id": "70b1c2cc2a2c0e6c3f3de4bff6743f6dcfbd001c", "title": "Graph Computing based Distributed Fast Decoupled Power Flow Analysis", "abstract": "Power flow analysis plays a fundamental and critical role in the energy management system (EMS). It is required to well accommodate large and complex power system. To achieve a high performance and accurate power flow analysis, a graph computing based distributed power flow analysis approach is proposed in this paper. Firstly, a power system network is divided into multiple areas. Slack buses are selected for each area and, at each SCADA sampling period, the inter-area transmission line power flows are equivalently allocated as extra load injections to corresponding buses. Then, the system network is converted into multiple independent areas. In this way, the power flow analysis could be conducted in parallel for each area and the solved system states could be guaranteed without compromise of accuracy. Besides, for each area, graph computing based fast decoupled power flow (FDPF) is employed to quickly analyze system states. IEEE 118-bus system and MP 10790-bus system are employed to verify the results accuracy and present the promising computation performance of the proposed approach.", "venue": "2019 IEEE Power & Energy Society General Meeting (PESGM)", "authors": ["Chen  Yuan", "Yi  Lu", "Wei  Feng", "Guangyi  Liu", "Renchang  Dai", "Yachen  Tang", "Zhiwei  Wang"], "year": 2019, "n_citations": 4}
{"id": 5454855, "s2_id": "16fed54d28dcd21b5f2ff71b93ea4d9dc384bc48", "title": "Performance Evaluation of Realistic Vanet Using Traffic Light Scenario", "abstract": "Vehicular Ad-hoc Networks (VANETs) is attracting considerable attention from the research community and the automotive industry to improve the services of Intelligent Transportation System (ITS). As today\u2019s transportation system faces serious challenges in terms of road safety, efficiency, and environmental friendliness, the idea of so called \u201cITS\u201d has emerged. Due to the expensive cost of deployment and complexity of implementing such a system in real world, research in VANET relies on simulation. This paper attempts to evaluate the performance of VANET in a realistic environment. The paper contributes by generating a real world road Map of JNU using existing Google Earth and GIS tools. Traffic data from a limited region of road Map is collected to capture the realistic mobility. In this work, the entire region has been divided into various smaller routes. The realistic mobility model used here considers the driver\u2019s route choice at the run time. It also studies the clustering effect caused by traffic lights used at the intersection to regulate traffic movement at different directions. Finally, the performance of the VANET is evaluated in terms of average delivery ratio, packet loss, and router drop as statistical measures for driver route choice with traffic light scenario. This experiment has provided insight into the performance of vehicular traffic communication for a small realistic scenario.", "venue": "ArXiv", "authors": ["Nidhi", "D. K. Lobiyal"], "year": 2012, "n_citations": 31}
{"id": 5456796, "s2_id": "2cf6131c80ec580064135f78408398cf81915e22", "title": "Queueing networks with a single shared server: light and heavy traffic", "abstract": "We study a queueing network with a single shared server, that serves the queues in a cyclic order according to the gated service discipline. External customers arrive at the queues according to independent Poisson processes. After completing service, a customer either leaves the system or is routed to another queue. This model is very generic and finds many applications in computer systems, communication networks, manufacturing systems and robotics. Special cases of the introduced network include well-known polling models and tandem queues. We derive exact limits of the mean delays under both heavy-traffic and light-traffic conditions. By interpolating between these asymptotic regimes, we develop simple closed-form approximations for the mean delays for arbitrary loads.", "venue": "PERV", "authors": ["Marko A. A. Boon", "Robert D. van der Mei", "Erik M. M. Winands"], "year": 2011, "n_citations": 6}
{"id": 5460868, "s2_id": "7729be4a520e1b36b0a48048886d1021528fd4b7", "title": "A MARKOVIAN ANALYSIS OF IEEE 802.11 BROADCAST TRANSMISSION NETWORKS WITH BUFFERING", "abstract": "The purpose of this paper is to analyze the so-called back-off technique of the IEEE 802.11 protocol in broadcast mode with waiting queues. In contrast to existing models, packets arriving when a station (or node) is in back-off state are not discarded, but are stored in a buffer of infinite capacity. As in previous studies, the key point of our analysis hinges on the assumption that the time on the channel is viewed as a random succession of transmission slots (whose duration corresponds to the length of a packet) and mini-slots during which the back-off of the station is decremented. These events occur independently, with given probabilities. The state of a node is represented by a two-dimensional Markov chain in discrete-time, formed by the back-off counter and the number of packets at the station. Two models are proposed both of which are shown to cope reasonably well with the physical principles of the protocol. The stability (ergodicity) conditions are obtained and interpreted in terms of maximum throughput. Several approximations related to these models are also discussed.", "venue": "Probability in the Engineering and Informational Sciences", "authors": ["Guy  Fayolle", "Paul  M\u00fchlethaler"], "year": 2016, "n_citations": 0}
{"id": 5463350, "s2_id": "7ad2c9807d2df4bcaf5aec0bef875f076145c003", "title": "Delay Asymptotics With Retransmissions and Incremental Redundancy Codes Over Erasure Channels", "abstract": "Recent studies have shown that retransmissions can cause heavy-tailed transmission delays even when packet sizes are light tailed. In addition, the impact of heavy-tailed delays persists even when packets size are upper bounded. The key question we study in this paper is how the use of coding techniques to transmit information, together with different system configurations, would affect the distribution of delay. To investigate this problem, we model the underlying channel as a Markov modulated binary erasure channel, where transmitted bits are either received successfully or erased. Erasure codes are used to encode information prior to transmission, which ensures that a fixed fraction of the bits in the codeword can lead to successful decoding. We use incremental redundancy codes, where the codeword is divided into codeword trunks and these trunks are transmitted one at a time to provide incremental redundancies to the receiver until the information is recovered. We characterize the distribution of delay under two different scenarios: 1) decoder uses memory to cache all previously successfully received bits and 2) decoder does not use memory, where received bits are discarded if the corresponding information cannot be decoded. In both cases, we consider codeword length with infinite and finite support. From a theoretical perspective, our results provide a benchmark to quantify the tradeoff between system complexity and the distribution of delay.", "venue": "IEEE Transactions on Information Theory", "authors": ["Yang  Yang", "Jian  Tan", "Ness B. Shroff", "Hesham El Gamal"], "year": 2014, "n_citations": 1}
{"id": 5465267, "s2_id": "86d8b78d5e7fe872a60be9034967ad082e381011", "title": "An Analytical Solution for Probabilistic Guarantees of Reservation Based Soft Real-Time Systems", "abstract": "We show a methodology for the computation of the probability of deadline miss for a periodic real-time task scheduled by a resource reservation algorithm. We propose a modelling technique for the system that reduces the computation of such a probability to that of the steady state probability of an infinite state Discrete Time Markov Chain with a periodic structure. This structure is exploited to develop an efficient numeric solution where different accuracy/computation time trade-offs can be obtained by operating on the granularity of the model. More importantly we offer a closed form conservative bound for the probability of a deadline miss. Our experiments reveal that the bound remains reasonably close to the experimental probability in one real-time application of practical interest. When this bound is used for the optimisation of the overall Quality of Service for a set of tasks sharing the CPU, it produces a good sub-optimal solution in a small amount of time.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Luigi  Palopoli", "Daniele  Fontanelli", "Luca  Abeni", "Bernardo Villalba Frias"], "year": 2016, "n_citations": 15}
{"id": 5465433, "s2_id": "b811558eee33fe60b97e3d255ab5152423a4738d", "title": "AES Implementation and Performance Evaluation on 8-bit Microcontrollers", "abstract": "The sensor network is a network technique for the implementation of Ubiquitous computing environment. It is wireless network environment that consists of the many sensors of lightweight and low power. Though sensor network provides various capabilities, it is unable to ensure the secure authentication between nodes. Eventually it causes the losing reliability of the entire network and many secure problems. Therefore, encryption algorithm for the implementation of reliable sensor network environments is required to the applicable sensor network. In this paper, we proposed the solution of reliable sensor network to analyze the communication efficiency through measuring performance of AES encryption algorithm by plaintext size, and cost of operation per hop according to the network scale.", "venue": "ArXiv", "authors": ["Hyubgun  Lee", "Kyounghwa  Lee", "Yongtae  Shin"], "year": 2009, "n_citations": 30}
{"id": 5466196, "s2_id": "4c64632ca2ea27ab259f16df140836cf8917231c", "title": "A Prompt Report on the Performance of Intel Optane DC Persistent Memory Module", "abstract": "In this prompt report, we present the basic performance evaluation of Intel Optane Data Center Persistent Memory Module (Optane DCPMM), which is the first commercially-available, byte-addressable non-volatile memory modules released in April 2019. Since at the moment of writing only a few reports on its performance were published, this letter is intended to complement other performance studies. Through experiments using our own measurement tools, we obtained that the latency of random read-only access was approximately 374 ns. That of random writeback-involving access was 391 ns. The bandwidths of read-only and writeback-involving access for interleaved memory modules were approximately 38 GB/s and 3 GB/s, respectively.", "venue": "IEICE Trans. Inf. Syst.", "authors": ["Takahiro  Hirofuchi", "Ryousei  Takano"], "year": 2020, "n_citations": 5}
{"id": 5467290, "s2_id": "2a90ed57ce976fcb057d5a8c8f33478e2c3733f6", "title": "Preparing for Performance Analysis at Exascale", "abstract": "Performance tools for forthcoming heterogeneous exascale platforms must address two principal challenges when analyzing execution measurements. First, measurement of extreme-scale executions generates large volumes of performance data. Second, performance metrics for heterogeneous applications are significantly sparse across code regions. To address these challenges, we developed a novel \u201cstreaming aggregation\u201d approach to post-mortem analysis that employs both shared and distributed memory parallelism to aggregate sparse performance measurements from every rank, thread and GPU stream of a large-scale application execution. Analysis results are stored in a pair of sparse formats designed for efficient access to related data elements, supporting responsive interactive presentation and scalable data analytics. Empirical analysis shows that our implementation of this approach in HPCToolkit effectively processes measurement data from thousands of threads using a fraction of the compute resources employed by the application itself. Our approach is able to perform analysis up to 9.4 times faster and store analysis results 23 times smaller than HPCToolkit, providing a key building block for scalable exascale performance tools.", "venue": "ArXiv", "authors": ["Jonathon  Anderson", "Yumeng  Liu", "John  Mellor-Crummey"], "year": 2021, "n_citations": 0}
{"id": 5469119, "s2_id": "13233d772dcb581d9566b55ba605566708e153e4", "title": "Sentinel: Runtime Data Management on Heterogeneous Main MemorySystems for Deep Learning", "abstract": "Software-managed heterogeneous memory (HM) provides a promising solution to increase memory capacity and cost efficiency. However, to release the performance potential of HM, we face a problem of data management. Given an application with various execution phases and each with possibly distinct working sets, we must move data between memory components of HM to optimize performance. The deep neural network (DNN), as a common workload on data centers, imposes great challenges on data management on HM. This workload often employs a task dataflow execution model, and is featured with a large amount of small data objects and fine-grained operations (tasks). This execution model imposes challenges on memory profiling and efficient data migration. \nWe present Sentinel, a runtime system that automatically optimizes data migration (i.e., data management) on HM to achieve performance similar to that on the fast memory-only system with a much smaller capacity of fast memory. To achieve this,Sentinel exploits domain knowledge about deep learning to adopt a custom approach for data management. Sentinel leverages workload repeatability to break the dilemma between profiling accuracy and overhead; It enables profiling and data migration at the granularity of data objects (not pages), by controlling memory allocation. This method bridges the semantic gap between operating system and applications. By associating data objects with the DNN topology, Sentinel avoids unnecessary data movement and proactively triggers data movement. Using only 20% of peak memory consumption of DNN models as fast memory size, Sentinel achieves the same or comparable performance (at most 8% performance difference) to that of the fast memory-only system on common DNN models; Sentinel also consistently outperforms a state-of-the-art solution by 18%.", "venue": "ArXiv", "authors": ["Jie  Ren", "Jiaolin  Luo", "Kai  Wu", "Minjia  Zhang", "Dong  Li"], "year": 2019, "n_citations": 3}
{"id": 5469994, "s2_id": "c9b1fdbfa7ac20f0696219d1e334f902ebece641", "title": "A measurement-based analysis of the energy consumption of data center servers", "abstract": "Energy consumption is a growing issue in data centers, impacting their economic viability and their public image. In this work we empirically characterize the power and energy consumed by different types of servers. In particular, in order to understand the behavior of their energy and power consumption, we perform measurements in different servers. In each of them, we exhaustively measure the power consumed by the CPU, the disk, and the network interface under different configurations, identifying the optimal operational levels. One interesting conclusion of our study is that the curve that defines the minimal CPU power as a function of the load is neither linear nor purely convex as has been previously assumed. Moreover, we find that the efficiency of the various server components can be maximized by tuning the CPU frequency and the number of active cores as a function of the system and network load, while the block size of I/O operations should be always maximized by applications. We also show how to estimate the energy consumed by an application as a function of some simple parameters, like the CPU load, and the disk and network activity. We validate the proposed approach by accurately estimating the energy of a map-reduce computation in a Hadoop platform.", "venue": "e-Energy", "authors": ["Jordi Arjona Aroca", "Angelos  Chatzipapas", "Antonio  Fern\u00e1ndez", "Vincenzo  Mancuso"], "year": 2014, "n_citations": 55}
{"id": 5475773, "s2_id": "1e698584b91f0de792f95b10db407f6d87315c84", "title": "Investigating Applications on the A64FX", "abstract": "The A64FX processor from Fujitsu, being designed for computational simulation and machine learning applications, has the potential for unprecedented performance in HPC systems. In this paper, we evaluate the A64FX by benchmarking against a range of production HPC platforms that cover a number of processor technologies. We investigate the performance of complex scientific applications across multiple nodes, as well as single node and mini-kernel benchmarks. This paper finds that the performance of the A64FX processor across our chosen benchmarks often significantly exceeds other platforms, even without specific application optimisations for the processor instruction set or hardware. However, this is not true for all the benchmarks we have undertaken. Furthermore, the specific configuration of applications can have an impact on the runtime and performance experienced.", "venue": "2020 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Adrian  Jackson", "Michele  Weiland", "Nick  Brown", "Andrew  Turner", "Mark  Parsons"], "year": 2020, "n_citations": 6}
{"id": 5476787, "s2_id": "af0d32e2e87cfb78f2e28b6333ea953f4b239d8a", "title": "A New Parallel Algorithm for Sinkhorn Word-Movers Distance and Its Performance on PIUMA and Xeon CPU", "abstract": "The Word Movers Distance (WMD) measures the semantic dissimilarity between two text documents by computing the cost of optimally moving all words of a source/query document to the most similar words of a target document. Computing WMD between two documents is costly because it requires solving an optimization problem that costs O (V 3log(V )) where V is the number of unique words in the document. Fortunately, WMD can be framed as an Earth Mover\u2019s Distance (EMD) for which the algorithmic complexity can be reduced to O (V 2) by adding an entropy penalty to the optimization problem and solving it using the Sinkhorn-Knopp algorithm. Additionally, the computation can be made highly parallel by computing the WMD of a single query document against multiple target documents at once, for example by finding whether a given tweet is similar to any other tweets of a given day. In this paper, we first present a shared-memory parallel SinkhornKnopp algorithm to compute the WMD of one document against many other documents by adopting theO (V 2) EMD algorithm. We then algorithmically transform the original O (V 2) dense computeheavy version into an equivalent sparse one which is mapped onto the new Intel Programmable Integrated Unified Memory Architecture (PIUMA) system. The WMD parallel implementation achieves 67\u00d7 speedup on 96 cores across 4 NUMA sockets of an Intel Cascade Lake system. We also show that PIUMA cores are around 1.2 \u2014 2.6\u00d7 faster than Xeon cores on Sinkhorn-WMD and also provide better strong scaling.", "venue": "ArXiv", "authors": ["Jesmin Jahan Tithi", "Fabrizio  Petrini"], "year": 2021, "n_citations": 0}
{"id": 5477434, "s2_id": "9e4d7d9bf96e8ca95119d36433ac7c5a039edd2d", "title": "Extension of PRISM by Synthesis of Optimal Timeouts in Fixed-Delay CTMC", "abstract": "We present a practically appealing extension of the probabilistic model checker PRISM rendering it to handle fixed-delay continuous-time Markov chains fdCTMCs with rewards, the equivalent formalism to the deterministic and stochastic Petri nets DSPNs. fdCTMCs allow transitions with fixed-delays or timeouts on top of the traditional transitions with exponential rates. Our extension supports an evaluation of expected reward until reaching a given set of target states. The main contribution is that, considering the fixed-delays as parameters, we implemented ai\u00be?synthesis algorithm that computes the epsilon-optimal values of the fixed-delays minimizing the expected reward. We provide a performance evaluation of the synthesis on practical examples.", "venue": "IFM", "authors": ["Lubos  Korenciak", "Vojtech  Reh\u00e1k", "Adrian  Farmadin"], "year": 2016, "n_citations": 3}
{"id": 5479204, "s2_id": "65983b4ce42dcbe8a08c1158bcf46a5da0337b45", "title": "Performance Models for Data Transfers: A Case Study with Molecular Chemistry Kernels", "abstract": "In distributed memory systems, it is paramount to develop strategies to overlap the data transfers between memory nodes with the computations in order to exploit their full potential. In this paper, we consider the problem of determining the order of data transfers between two memory nodes for a set of independent tasks with the objective of minimizing the makespan. We prove that, with limited memory capacity, the problem of obtaining the optimal data transfer order is NP-complete. We propose several heuristics to determine this order and discuss the conditions that might be favorable to different heuristics. We analyze our heuristics on traces obtained by running two molecular chemistry kernels, namely, Hartree--Fock (HF) and Coupled Cluster Singles Doubles (CCSD), on 10 nodes of an HPC system. Our results show that some of our heuristics achieve significant overlap for moderate memory capacities and resulting in makespans that are very close to the lower bound.", "venue": "ICPP", "authors": ["Suraj  Kumar", "Lionel  Eyraud-Dubois", "Sriram  Krishnamoorthy"], "year": 2019, "n_citations": 0}
{"id": 5480919, "s2_id": "bbbc815f6329798fcc72c6c620499eb0d1bda073", "title": "CAVBench: A Benchmark Suite for Connected and Autonomous Vehicles", "abstract": "Connected and autonomous vehicles (CAVs) have recently attracted a significant amount of attention both from researchers and industry. Numerous studies targeting algorithms, software frameworks, and applications on the CAVs scenario have emerged. Meanwhile, several pioneer efforts have focused on the edge computing system and architecture design for the CAVs scenario and provided various heterogeneous platform prototypes for CAVs. However, a standard and comprehensive application benchmark for CAVs is missing, hindering the study of these emerging computing systems. To address this challenging problem, we present CAVBench, the first benchmark suite for the edge computing system in the CAVs scenario. CAVBench is comprised of six typical applications covering four dominate CAVs scenarios and takes four datasets as standard input. CAVBench provides quantitative evaluation results via application and system perspective output metrics. We perform a series of experiments and acquire three systemic characteristics of the applications in CAVBench. First, the operation intensity of the applications is polarized, which explains why heterogeneous hardware is important for a CAVs computing system. Second, all applications in CAVBench consume high memory bandwidth, so the system should be equipped with high bandwidth memory or leverage good memory bandwidth management to avoid the performance degradation caused by memory bandwidth competition. Third, some applications have worse data/instruction locality based on the cache miss observation, so the computing system targeting these applications should optimize the cache architecture. Last, we use the CAVBench to evaluate a typical edge computing platform and present the quantitative and qualitative analysis of the benchmarking results.", "venue": "2018 IEEE/ACM Symposium on Edge Computing (SEC)", "authors": ["Yifan  Wang", "Shaoshan  Liu", "Xiaopei  Wu", "Weisong  Shi"], "year": 2018, "n_citations": 34}
{"id": 5484046, "s2_id": "a1c7f7646d6a874cb76fe87c567e8ca4dc462368", "title": "On the energy efficiency of rate and transmission power control in 802.11", "abstract": "Rate adaptation and transmission power control in 802.11 WLANs have received a lot of attention from the research community, with most of the proposals aiming at maximising throughput based on network conditions. Considering energy consumption, an implicit assumption is that optimality in throughput implies optimality in energy efficiency, but this assumption has been recently put into question. In this paper, we address via analysis, simulation and experimentation the relation between throughput performance and energy efficiency in multi-rate 802.11 scenarios. We demonstrate the trade-off between these performance figures, confirming that they may not be simultaneously optimised, and analyse their sensitivity towards the energy consumption parameters of the device. We analyse this trade-off in existing rate adaptation with transmission power control algorithms, and discuss how to design novel schemes taking energy consumption into account.", "venue": "Comput. Commun.", "authors": ["I\u00f1aki  Ucar", "Carlos  Donato", "Pablo  Serrano", "Andres  Garcia-Saavedra", "Arturo  Azcorra", "Albert  Banchs"], "year": 2018, "n_citations": 4}
{"id": 5495498, "s2_id": "cc46dc05a3d8608f5a0c3a857983f70834d3aee8", "title": "Optimising energy and overhead for large parameter space simulations", "abstract": "Many systems require optimisation over multiple objectives, where objectives are characteristics of the system such as energy consumed or increase in time to perform the work. Optimisation is performed by selecting the \u2018best\u2019 set of input parameters to elicit the desired objectives. However, the parameter search space can often be far larger than can be searched in a reasonable time. Additionally, the objectives are often mutually exclusive \u2013 leading to a decision being made as to which objective is more important or optimising over a combination of the objectives. This work is an application of a Genetic Algorithm to identify the Pareto frontier for finding the optimal parameter sets for all combinations of objectives. A Pareto frontier can be used to identify the sets of optimal parameters for which each is the \u2018best\u2019 for a given combination of objectives \u2013 thus allowing decisions to be made with full knowledge. We demonstrate this approach for the HTC-Sim simulation system in the case where a Reinforcement Learning scheduler is tuned for the two objectives of energy consumption and task overhead. Demonstrating that this approach can reduce the energy consumed by $\\sim 36$ % over previously published work without significantly increasing the overhead.", "venue": "2019 Tenth International Green and Sustainable Computing Conference (IGSC)", "authors": ["Alexander J. M. Kell", "Matthew  Forshaw", "A. Stephen McGough"], "year": 2019, "n_citations": 0}
{"id": 5495982, "s2_id": "a9496089892f58fd95925bd80a47a3fc158365dc", "title": "Solving dense generalized eigenproblems on multi-threaded architectures", "abstract": "We compare two approaches to compute a fraction of the spectrum of dense symmetric definite generalized eigenproblems: one is based on the reduction to tridiagonal form, and the other on the Krylov-subspace iteration. Two large-scale applications, arising in molecular dynamics and material science, are employed to investigate the contributions of the application, architecture, and parallelism of the method to the performance of the solvers. The experimental results on a state-of-the-art 8-core platform, equipped with a graphics processing unit (GPU), reveal that in realistic applications, iterative Krylov-subspace methods can be a competitive approach also for the solution of dense problems.", "venue": "Appl. Math. Comput.", "authors": ["Jos\u00e9 Ignacio Aliaga", "Paolo  Bientinesi", "Davor  Davidovic", "Edoardo Di Napoli", "Francisco D. Igual", "Enrique S. Quintana-Ort\u00ed"], "year": 2012, "n_citations": 10}
{"id": 5496021, "s2_id": "2b323852b3d35a240267c39e6216243a1356e7f4", "title": "Exponential Convergence Rate for the Asymptotic Optimality of Whittle Index Policy", "abstract": "We evaluate the performance of Whittle index policy for restless Markovian bandits, when the number of bandits grows. It is proven in [30] that this performance is asymptotically optimal if the bandits are indexable and the associated deterministic system has a global attractor fixed point. In this paper we show that, under the same conditions, the convergence rate is exponential in the number of bandits, unless the fixed point is singular (to be defined later). Our proof is based on the nature of the deterministic equation governing the stochastic system: We show that it is a piecewise affine continuous dynamical system inside the simplex of the empirical measure of the bandits. Using simulations and numerical solvers, we also investigate the cases where the conditions for the exponential rate theorem are violated, notably when attracting limit cycles appear, or when the fixed point is singular. We illustrate our theorem on a Markovian fading channel model, which has been well studied in the literature. Finally, we extend our synchronous model results to the asynchronous model.", "venue": "ArXiv", "authors": ["Nicolas  Gast", "Bruno  Gaujal", "Chen  Yan"], "year": 2020, "n_citations": 1}
{"id": 5496751, "s2_id": "ce175e9e7fd299ebd39dd59462e1bcc27b3bbb6b", "title": "Estimating the Potential Speedup of Computer Vision Applications on Embedded Multiprocessors", "abstract": "Computer vision applications constitute one of the key drivers for embedded multicore architectures. Although the number of available cores is increasing in new architectures, designing an application to maximize the utilization of the platform is still a challenge. In this sense, parallel performance prediction tools can aid developers in understanding the characteristics of an application and finding the most adequate parallelization strategy. In this work, we present a method for early parallel performance estimation on embedded multiprocessors from sequential application traces. We describe its implementation in Parana, a fast trace-driven simulator targeting OpenMP applications on the STMicroelectronics' STxP70 Application-Specific Multiprocessor (ASMP). Results for the FAST key point detector application show an error margin of less than 10% compared to the reference cycle-approximate simulator, with lower modeling effort and up to 20x faster execution time.", "venue": "ArXiv", "authors": ["V\u00edtor  Schwambach", "S\u00e9bastien  Cleyet-Merle", "Alain  Issard", "St\u00e9phane  Mancini"], "year": 2015, "n_citations": 4}
{"id": 5497171, "s2_id": "ca4e7a50e4ca6d4bf815d7a5ad4d3c65741a561d", "title": "Automated Instruction Stream Throughput Prediction for Intel and AMD Microarchitectures", "abstract": "An accurate prediction of scheduling and execution of instruction streams is a necessary prerequisite for predicting the in-core performance behavior of throughput-bound loop kernels on out-of-order processor architectures. Such predictions are an indispensable component of analytical performance models, such as the Roofline and the Execution-Cache-Memory (ECM) model, and allow a deep understanding of the performance-relevant interactions between hardware architecture and loop code. We present the Open Source Architecture Code Analyzer (OSACA), a static analysis tool for predicting the execution time of sequential loops comprising x86 instructions under the assumption of an infinite first-level cache and perfect out-of-order scheduling. We show the process of building a machine model from available documentation and semi-automatic benchmarking, and carry it out for the latest Intel Skylake and AMD Zen micro-architectures. To validate the constructed models, we apply them to several assembly kernels and compare runtime predictions with actual measurements. Finally we give an outlook on how the method may be generalized to new architectures.", "venue": "2018 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)", "authors": ["Jan  Laukemann", "Julian  Hammer", "Johannes  Hofmann", "Georg  Hager", "Gerhard  Wellein"], "year": 2018, "n_citations": 19}
{"id": 5497295, "s2_id": "e4130b4edecd876f57d979999aee3526d6f64470", "title": "Security Analysis of Distributed Ledgers and Blockchains through Agent-based Simulation", "abstract": "In this paper, we describe LUNES-Blockchain, an agent-based simulator of blockchains that relies on Parallel and Distributed Simulation (PADS) techniques to obtain high scalability. The software is organized as a multi-level simulator that permits to simulate a virtual environment, made of many nodes running the protocol of a specific Distributed Ledger Technology (DLT), such as the Bitcoin or the Ethereum blockchains. This virtual environment is executed on top of a lower-level Peer-to-Peer (P2P) network overlay, which can be structured based on different topologies and with a given number of nodes and edges. Functionalities at different levels of abstraction are managed separately, by different software modules and with different time granularity. This allows for accurate simulations, where (and when) it is needed, and enhances the simulation performance. Using LUNES-Blockchain, it is possible to simulate different types of attacks on the DLT. In this paper, we specifically focus on the P2P layer, considering the selfish mining, the 51% attack and the Sybil attack. For which concerns selfish mining and the 51% attack, our aim is to understand how much the hash-rate (i.e. a general measure of the processing power in the blockchain network) of the attacker can influence the outcome of the misbehaviour. On the other hand, in the filtering denial of service (i.e. Sybil Attack), we investigate which G. D\u2019Angelo University of Bologna (Italy) E-mail: g.dangelo@unibo.it 0 This is the authors\u2019 version of the following article: \u201cLuca Serena, Gabriele D\u2019Angelo, Stefano Ferretti. Security Analysis of Distributed Ledgers and Blockchains through Agent-based Simulation. To appear in Simulation Modelling Practice and Theory (Elsevier)\u201d. 1 An early version of this work appeared in [26]. This paper is an extensively revised and extended version where more than 50% is new material. dissemination protocol in the underlying P2P network makes the system more resilient to a varying number of nodes that drop the messages. The results confirm the viability of the simulation-based techniques for the investigation of security aspects of DLTs.", "venue": "Simul. Model. Pract. Theory", "authors": ["Luca  Serena", "Gabriele  D'Angelo", "Stefano  Ferretti"], "year": 2022, "n_citations": 0}
{"id": 5498223, "s2_id": "b4b2fe2240cc766dc93772cdadfc835af5d1ed12", "title": "Asymmetry-aware Scalable Locking", "abstract": "The pursuit of power-efficiency is popularizing asymmetric multicore processors (AMP) such as ARM big.LITTLE, Apple M1 and recent Intel Alder Lake with big and little cores. However, we find that existing scalable locks fail to scale on AMP and cause collapses in either throughput or latency, or both, because their implicit assumption of symmetric cores no longer holds. To address this issue, we propose the first asymmetry-aware scalable lock named LibASL. LibASL provides a new lock ordering guided by applications\u2019 latency requirements, which allows big cores to reorder with little cores for higher throughput under the condition of preserving applications\u2019 latency requirements. Using LibASL only requires linking the applications with it and, if latency-critical, inserting few lines of code to annotate the coarse-grained latency requirement. We evaluate LibASL in various benchmarks including five popular databases on Apple M1. Evaluation results show that LibASL can improve the throughput by up to 5 times while precisely preserving the tail latency designated by applications. CCS Concepts: \u2022 Software and its engineering \u2192 Mutual exclusion; Software performance; Multithreading.", "venue": "ArXiv", "authors": ["Nian  Liu", "Jinyu  Gu", "Dahai  Tang", "Kenli  Li", "Binyu  Zang", "Haibo  Chen"], "year": 2021, "n_citations": 0}
{"id": 5498772, "s2_id": "b6c92fc88094dbf909bfd46309b16e1d7f04c1c7", "title": "START: Straggler Prediction and Mitigation for Cloud Computing Environments using Encoder LSTM Networks", "abstract": "Modern large-scale computing systems distribute jobs into multiple smaller tasks which execute in parallel to accelerate job completion rates and reduce energy consumption. However, a common performance problem in such systems is dealing with straggler tasks that are slow running instances that increase the overall response time. Such tasks can significantly impact the system\u2019s Quality of Service (QoS) and the Service Level Agreements (SLA). To combat this issue, there is a need for automatic straggler detection and mitigation mechanisms that execute jobs without violating the SLA. Prior work typically builds reactive models that focus first on detection and then mitigation of straggler tasks, which leads to delays. Other works use prediction based proactive mechanisms, but ignore heterogeneous host or volatile task characteristics. In this paper, we propose a Straggler Prediction and Mitigation Technique (START) that is able to predict which tasks might be stragglers and dynamically adapt scheduling to achieve lower response times. Our technique analyzes all tasks and hosts based on compute and network resource consumption using an Encoder Long-Short-Term-Memory (LSTM) network. The output of this network is then used to predict and mitigate expected straggler tasks. This reduces the SLA violation rate and execution time without compromising QoS. Specifically, we use the CloudSim toolkit to simulate START in a cloud environment and compare it with state-of-the-art techniques (IGRU-SD, SGC, Dolly, GRASS, NearestFit and Wrangler) in terms of QoS parameters such as energy consumption, execution time, resource contention, CPU utilization and SLA violation rate. Experiments show that START reduces execution time, resource contention, energy and SLA violations by 13%, 11%, 16% and 19%, respectively, compared to the state-of-the-art approaches.", "venue": "IEEE Transactions on Services Computing", "authors": ["Shreshth  Tuli", "Sukhpal Singh Gill", "Peter  Garraghan", "Rajkumar  Buyya", "Giuliano  Casale", "Nicholas R. Jennings"], "year": 2021, "n_citations": 1}
{"id": 5503340, "s2_id": "0cc032ee980b0c6ed0d587a7c0dbecdfe91e7053", "title": "A foundation for stochastic bandwidth estimation of networks with random service", "abstract": "We develop a stochastic foundation for bandwidth estimation of networks with random service, where bandwidth availability is expressed in terms of bounding functions with a defined violation probability. Exploiting properties of a stochastic max-plus algebra and system theory, the task of bandwidth estimation is formulated as inferring an unknown bounding function from measurements of probing traffic. We derive an estimation methodology that is based on iterative constant rate probes. Our solution provides evidence for the utility of packet trains for bandwidth estimation in the presence of variable cross traffic. Taking advantage of statistical methods, we show how our estimation method can be realized in practice, with adaptive train lengths of probe packets, probing rates, and replicated measurements required to achieve both high accuracy and confidence levels. We evaluate our method in a controlled testbed network, where we show the impact of cross traffic variability on the time-scales of service availability, and provide a comparison with existing bandwidth estimation tools.", "venue": "2011 Proceedings IEEE INFOCOM", "authors": ["Ralf  L\u00fcbben", "Markus  Fidler", "J\u00f6rg  Liebeherr"], "year": 2011, "n_citations": 12}
{"id": 5505496, "s2_id": "3a3eb55128d4aef3d7ac699aad687dd9973a8e9b", "title": "Light traffic behavior under the power-of-two load balancing strategy: The case of heterogeneous servers", "abstract": "Abstract We consider a multi-server queueing system under the power-of-two policy with Poisson job arrivals, heterogeneous servers and a general job requirement distribution; each server operates under the first-come first-serve policy and there are no buffer constraints. We analyze the performance of this system in light traffic by evaluating the first two light traffic derivatives of the average job response time. These expressions point to several interesting structural features associated with server heterogeneity in light traffic: For unequal capacities, the average job response time is seen to decrease for small values of the arrival rate, and the more diverse the server capacities, the greater the gain in performance. These theoretical findings are assessed through limited simulations.", "venue": "Perform. Evaluation", "authors": ["Ane  Izagirre", "Armand M. Makowski"], "year": 2017, "n_citations": 1}
{"id": 5506602, "s2_id": "07627629d44161d7e0f172e036fea30cd5d5b032", "title": "Robust Safety for Autonomous Vehicles through Reconfigurable Networking", "abstract": "Autonomous vehicles bring the promise of enhancing the consumer experience in terms of comfort and convenience and, in particular, the safety of the autonomous vehicle. Safety functions in autonomous vehicles such as Automatic Emergency Braking and Lane Centering Assist rely on computation, information sharing, and the timely actuation of the safety functions. One opportunity to achieve robust autonomous vehicle safety is by enhancing the robustness of in-vehicle networking architectures that support built-in resiliency mechanisms. Software Defined Networking (SDN) is an advanced networking paradigm that allows fine-grained manipulation of routing tables and routing engines and the implementation of complex features such as failover, which is a mechanism of protecting in-vehicle networks from failure, and in which a standby link automatically takes over once the main link fails. In this paper, we leverage SDN network programmability features to enable resiliency in the autonomous vehicle realm. We demonstrate that a Software Defined In-Vehicle Networking (SDIVN) does not add overhead compared to Legacy In-Vehicle Networks (LIVNs) under non-failure conditions and we highlight its superiority in the case of a link failure and its timely delivery of messages. We verify the proposed architectures benefits using a simulation environment that we have developed and we validate our design choices through testing and simulations", "venue": "SCAV@CPSWeek", "authors": ["Khalid  Halba", "Charif  Mahmoudi", "Edward  Griffor"], "year": 2018, "n_citations": 10}
{"id": 5511755, "s2_id": "c8beecb8549be3b83c7f03386e8d6fd8aba8531a", "title": "A Choreographed Outline Instrumentation Algorithm for Asynchronous Components", "abstract": "The runtime analysis of decentralised software requires instrumentation methods that are scalable, but also minimally invasive. This paper presents a new algorithm that instruments choreographed outline monitors. Our instrumentation algorithm scales and reorganises monitors dynamically as the system executes. We demonstrate the implementability of choreographed outline instrumentation and compare it to inline instrumentation, subject to rigorous and comprehensive benchmarking. Our results debunk the general notion that outline monitoring is necessarily infeasible, and show that our implementation induces runtime overhead comparable to that of its inline counterpart for many practical cases.", "venue": "ArXiv", "authors": ["Luca  Aceto", "Duncan Paul Attard", "Adrian  Francalanza", "Anna  Ing'olfsd'ottir"], "year": 2021, "n_citations": 2}
{"id": 5514861, "s2_id": "28d4b8f61cd7b9aa7d4e7c0c669f8c4cb105c3ba", "title": "Performance heuristics for GR(1) synthesis and related algorithms", "abstract": "Reactive synthesis for the GR(1) fragment of LTL has been implemented and studied in many works. In this work we present and evaluate a list of heuristics to potentially reduce running times for GR(1) synthesis and related algorithms. The list includes several heuristics for controlled predecessor computation and BDDs, early detection of fixed-points and unrealizability, fixed-point recycling, and several heuristics for unrealizable core computations. We have implemented the heuristics and integrated them in our synthesis environment Spectra Tools, a set of tools for writing specifications and running synthesis and related analyses. We evaluate the presented heuristics on SYNTECH15, a total of 78 specifications of 6 autonomous Lego robots, on SYNTECH17, a total of 149 specifications of 5 autonomous Lego robots, all written by 3rd year undergraduate computer science students in two project classes we have taught, as well as on benchmarks from the literature. The evaluation investigates not only the potential of the suggested heuristics to improve computation times, but also the difference between existing benchmarks and the robot\u2019s specifications in terms of the effectiveness of the heuristics. Our evaluation shows positive results for the application of all the heuristics together, which get more significant for specifications with slower original running times. It also shows differences in effectiveness when applied to different sets of specifications. Furthermore, a comparison between Spectra, with all the presented heuristics, and two existing tools, RATSY and Slugs, over two well-known benchmarks, shows that Spectra outperforms both on most of the specifications; the larger the specification, the faster Spectra becomes relative to the two other tools.", "venue": "Acta Informatica", "authors": ["Elizabeth  Firman", "Shahar  Maoz", "Jan Oliver Ringert"], "year": 2019, "n_citations": 5}
{"id": 5515395, "s2_id": "e3895624b6640b1a1b05807f7d2eac237e5dab4f", "title": "PERSEUS: Characterizing Performance and Cost of Multi-Tenant Serving for CNN Models", "abstract": "Deep learning models are increasingly used for end-user applications, supporting both novel features such as facial recognition, and traditional features, e.g. web search. To accommodate high inference throughput, it is common to host a single pre-trained Convolutional Neural Network (CNN) in dedicated cloud-based servers with hardware accelerators such as Graphics Processing Units (GPUs). However, GPUs can be orders of magnitude more expensive than traditional Central Processing Unit (CPU) servers. These resources could also be under-utilized facing dynamic workloads, which may result in inflated serving costs. One potential way to alleviate this problem is by allowing hosted models to share the underlying resources, which we refer to as multi-tenant inference serving. One of the key challenges is maximizing the resource efficiency for multi-tenant serving given hardware with diverse characteristics, models with unique response time Service Level Agreement (SLA), and dynamic inference workloads. In this paper, we present PERSEUS, a measurement framework that provides the basis for understanding the performance and cost trade-offs of multi-tenant model serving. We implemented PERSEUS in Python atop a popular cloud inference server called Nvidia TensorRT Inference Server. Leveraging PERSEUS, we evaluated the inference throughput and cost for serving various models and demonstrated that multi-tenant model serving led to up to 12% cost reduction.", "venue": "2020 IEEE International Conference on Cloud Engineering (IC2E)", "authors": ["Matthew  LeMay", "Shijian  Li", "Tian  Guo"], "year": 2020, "n_citations": 6}
{"id": 5522189, "s2_id": "237c8f32eed1c6c76d5307052e4f905087d5d403", "title": "Delay asymptotics and bounds for multitask parallel jobs", "abstract": "We study delay of jobs that consist of multiple parallel tasks, which is a critical performance metric in a wide range of applications such as data file retrieval in coded storage systems and parallel computing. In this problem, each job is completed only when all of its tasks are completed, so the delay of a job is the maximum of the delays of its tasks. Despite the wide attention this problem has received, tight analysis is still largely unknown since analyzing job delay requires characterizing the complicated correlation among task delays, which is hard to do. We first consider an asymptotic regime where the number of servers, n, goes to infinity, and the number of tasks in a job, $$k^{(n)}$$k(n), is allowed to increase with\u00a0n. We establish the asymptotic independence of any $$k^{(n)}$$k(n) queues under the condition $$k^{(n)}= o(n^{1/4})$$k(n)=o(n1/4). This greatly generalizes the asymptotic independence type of results in the literature, where asymptotic independence is shown only for a fixed constant number of queues. As a consequence of our independence result, the job delay converges to the maximum of independent task delays. We next consider the non-asymptotic regime. Here, we prove that independence yields a stochastic upper bound on job delay for any n and any $$k^{(n)}$$k(n) with $$k^{(n)}\\le n$$k(n)\u2264n. The key component of our proof is a new technique we develop, called \u201cPoisson oversampling.\u201d Our approach converts the job delay problem into a corresponding balls-and-bins problem. However, in contrast with typical balls-and-bins problems where there is a negative correlation among bins, we prove that our variant exhibits positive correlation.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Weina  Wang", "Mor  Harchol-Balter", "Haotian  Jiang", "Alan  Scheller-Wolf", "R.  Srikant"], "year": 2019, "n_citations": 12}
{"id": 5522760, "s2_id": "72abb774f783c36807c9b7b32be9a5c7571c9487", "title": "On the Quantum Performance Evaluation of Two Distributed Quantum Architectures", "abstract": "Distributed quantum applications impose requirements on the quality of the quantum states that they consume. When analyzing architecture implementations of quantum hardware, characterizing this quality forms an important factor in understanding their performance. Fundamental characteristics of quantum hardware lead to inherent tradeoffs between the quality of states and traditional performance metrics such as throughput. Furthermore, any real-world implementation of quantum hardware exhibits time-dependent noise that degrades the quality of quantum states over time. Here, we study the performance of two possible architectures for interfacing a quantum processor with a quantum network. The first corresponds to the current experimental state of the art in which the same device functions both as a processor and a network device. The second corresponds to a future architecture that separates these two functions over two distinct devices. We model these architectures as continuous-time Markov chains and compare their quality of executing quantum operations and producing entangled quantum states as functions of their memory lifetimes, as well as the time that it takes to perform various operations within each architecture. As an illustrative example, we apply our analysis to architectures based on Nitrogen-Vacancy centers in diamond, where we find that for present-day device parameters one architecture is more suited to computation-heavy applications, and the other for network-heavy ones. We validate our analysis with the quantum network simulator NetSquid. Besides the detailed study of these architectures, a novel contribution of our work are several formulas that connect an understanding of waiting time distributions to the decay of quantum quality over time for the most common noise models employed in quantum technologies. This provides a valuable new tool for performance evaluation experts, and its applications extend beyond the two architectures studied in this work.", "venue": "Performance Evaluation", "authors": ["Gayane  Vardoyan", "Matthew  Skrzypczyk", "Stephanie  Wehner"], "year": 2021, "n_citations": 1}
{"id": 5523019, "s2_id": "9a0f2accd0cab4298d84c732f0a1639578b15ad8", "title": "Model for Predicting End User Web Page Response Time", "abstract": "Perceived responsiveness of a web page is one of the most important and least understood metrics of web page design, and is critical for attracting and maintaining a large audience. Web pages can be designed to meet performance SLAs early in the product lifecycle if there is a way to predict the apparent responsiveness of a particular page layout. Response time of a web page is largely influenced by page layout and various network characteristics. Since the network characteristics vary widely from country to country, accurately modeling and predicting the perceived responsiveness of a web page from the end user's perspective has traditionally proven very difficult. We propose a model for predicting end user web page response time based on web page, network, browser download and browser rendering characteristics. We start by understanding the key parameters that affect perceived response time. We then model each of these parameters individually using experimental tests and statistical techniques. Finally, we demonstrate the effectiveness of this model by conducting an experimental study with Yahoo! web pages in two countries and compare it with 3rd party measurement application.", "venue": "ArXiv", "authors": ["Sathya Narayanan Nagarajan", "Srijith  Ravikumar"], "year": 2012, "n_citations": 3}
{"id": 5526856, "s2_id": "4fde55259b61f640cbdec5d9095759483fb4c7dd", "title": "Benchmark Problems for Constraint Solving", "abstract": "Constraint Programming is roughly a new software technology introduced by Jaffar and Lassez in 1987 for description and effective solving of large, particularly combinatorial, problems especially in areas of planning and scheduling. In the following we define three problems for constraint solving from the domain of electrical networks; based on them we define 43 related problems. For the defined set of problems we benchmarked five systems: ILOG OPL, AMPL, GAMS, Mathematica and UniCalc. As expected some of the systems performed very well for some problems while others performed very well on others.", "venue": "ArXiv", "authors": ["Alin  Suciu", "Rodica  Potolea", "Tudor  Muresan"], "year": 2006, "n_citations": 0}
{"id": 5527996, "s2_id": "ce52fde4ffe8832e1445213c980d568c91ef7b84", "title": "Solving the Klein-Gordon equation using fourier spectral methods: a benchmark test for computer performance", "abstract": "The cubic Klein-Gordon equation is a simple but non-trivial partial differential equation whose numerical solution has the main building blocks required for the solution of many other partial differential equations. In this study, the library 2DECOMP&FFT is used in a Fourier spectral scheme to solve the Klein-Gordon equation and strong scaling of the code is examined on thirteen different machines for a problem size of 5123. The results are useful in assessing likely performance of other parallel fast Fourier transform based programs for solving partial differential equations. The problem is chosen to be large enough to solve on a workstation, yet also of interest to solve quickly on a supercomputer, in particular for parametric studies. Unlike the Linpack benchmark, a high ranking will not be obtained by simply building a bigger computer.", "venue": "SpringSim", "authors": ["Samar  Aseeri", "Oleg  Batrasev", "Matteo  Icardi", "Brian  Leu", "Albert  Liu", "Ning  Li", "Benson K. Muite", "Eike  M\u00fcller", "Brock  Palen", "Michael  Quell", "Harald  Servat", "Parth  Sheth", "Robert  Speck", "Mark Van Moer", "Jerome  Vienne"], "year": 2015, "n_citations": 6}
{"id": 5530723, "s2_id": "d3b847468f590a81abfac41ef80163163cd9f5d1", "title": "Real-Time Prediction of Delay Distribution in Service Systems using Mixture Density Networks", "abstract": "Motivated by interest in providing more efficient services in customer service systems, we use statistical learning methods and delay history information to predict the conditional distribution of the customers' waiting times in queueing systems. From the predicted distributions, descriptive statistics of the system such as the mean, variance and percentiles of the waiting times can be obtained, which can be used for delay announcements, SLA conformance and better system management. We model the conditional distributions by mixtures of Gaussians, parameters of which can be estimated using Mixture Density Networks. The evaluations show that exploiting more delay history information can result in much more accurate predictions under realistic time-varying arrival assumptions.", "venue": "ArXiv", "authors": ["Majid  Raeis", "Ali  Tizghadam", "Alberto  Leon-Garcia"], "year": 2019, "n_citations": 0}
{"id": 5531170, "s2_id": "8c17c972bef686d4e593942e4f2e21dd0ef93e6a", "title": "Scaling Turbo Boost to a 1000 cores", "abstract": "The Intel Core i7 processor code named Nehalem provides a feature named Turbo Boost which opportunistically varies the frequencies of the processor's cores. The frequency of a core is determined by core temperature, the number of active cores, the estimated power consumption, the estimated current consumption, and operating system frequency scaling requests. For a chip multi-processor(CMP) that has a small number of physical cores and a small set of performance states, deciding the Turbo Boost frequency to use on a given core might not be difficult. However, we do not know the complexity of this decision making process in the context of a large number of cores, scaling to the 100s, as predicted by researchers in the field.", "venue": "ArXiv", "authors": ["Narayan S. Ananth", "Somsubhra  Sharangi", "Alexandra  Fedorova"], "year": 2010, "n_citations": 0}
{"id": 5544807, "s2_id": "ba3c0f5ba5b8b08610ab4709e5d64a0441429e61", "title": "A Compositional Semantics for Stochastic Reo Connectors", "abstract": "In this paper we present a compositional semantics for the channel-based coordination language Reo which enables the analysis of quality of service (QoS) properties of service compositions. For this purpose, we annotate Reo channels with stochastic delay rates and explicitly model data-arrival rates at the boundary of a connector, to capture its interaction with the services that comprise its environment. We propose Stochastic Reo automata as an extension of Reo automata, in order to compositionally derive a QoS-aware semantics for Reo. We further present a translation of Stochastic Reo automata to Continuous-Time Markov Chains (CTMCs). This translation enables us to use third-party CTMC verification tools to do an end-to-end performance analysis of service compositions.", "venue": "FOCLASA", "authors": ["Young-Joo  Moon", "Alexandra  Silva", "Christian  Krause", "Farhad  Arbab"], "year": 2010, "n_citations": 18}
{"id": 5546091, "s2_id": "3bc171afb57097d71aa0226833b24a1e3122a9d8", "title": "Analysis of a reputation system for mobile ad-hoc networks with liars", "abstract": "Using decentralized reputation systems is a promising approach to ensuring cooperation and fairness in mobile ad-hoc networks. However, they are vulnerable to liars and robustness has not been analyzed in detail. With our work, we provide a first step to the analysis of a reputation system based on a deviation test. Nodes accept second hand information only if this does not differ too much from their reputation values. Whereas our earlier paper [J. Mundinger and J.-Y. Le Boudec, 2005] dealt with a simplified one-dimensional model, we now consider the original two-dimensional system. We show that the system exhibits a phase transition. In the subcritical regime, it is robust and lying has no effect. In the supercritical regime, lying does have an impact. We compute the critical values via a mean-field approach and use simulations to verify our results. Thus, we obtain conditions for the deviation test to make the reputation system robust and provide guidelines for a good choice of parameters.", "venue": "Third International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt'05)", "authors": ["Jochen  Mundinger", "Jean-Yves Le Boudec"], "year": 2005, "n_citations": 123}
{"id": 5549599, "s2_id": "539edce648edacbc486703701045edd7c188688e", "title": "SRPT for Multiserver Systems", "abstract": "The Shortest Remaining Processing Time (SRPT) scheduling policy and its variants have been extensively studied in both theoretical and practical settings. While beautiful results are known for single-server SRPT, much less is known for multiserver SRPT. In particular, stochastic analysis of the M/G/k under multiserver SRPT is entirely open. Intuition suggests that multiserver SRPT should be optimal or near-optimal for minimizing mean response time. However, the only known analysis of multiserver SRPT is in the worst-case adversarial setting, where SRPT can be far from optimal. In this paper, we give the first stochastic analysis bounding mean response time of the M/G/k under multiserver SRPT. Using our response time bound, we show that multiserver SRPT has asymptotically optimal mean response time in the heavy-traffic limit. The key to our bounds is a strategic combination of stochastic and worst-case techniques. Beyond SRPT, we prove similar response time bounds and optimality results for several other multiserver scheduling policies.", "venue": "Perform. Evaluation", "authors": ["Isaac  Grosof", "Ziv  Scully", "Mor  Harchol-Balter"], "year": 2018, "n_citations": 30}
{"id": 5552748, "s2_id": "c67992498694812ae401623f30c9a3f9f9159082", "title": "Renewable Energy-Aware Information-Centric Networking", "abstract": "The ICT industry today is placed as one of the major consumers of energy, where recent reports have also shown that the industry is a major contributor to global carbon emissions. While renewable energy-aware data centers have been proposed, these solutions have certain limitations. The primary limitation is due to the design of data centers which focus on large-size facilities located in selected locations. This paper addresses this problem, by utilizing in-network caching with each router having storage and being powered by renewable energy sources (wind and solar). Besides placing contents closer to end users, utilizing in-network caching could potentially increase probability of capturing renewable energy in diverse geographical locations. Our proposed solution is dual- layered: on the first layer a distributed gradient-based routing protocol is used to discover the paths along routers that are powered by the highest renewable energy, and on the second layer, a caching mechanism will pull the contents from the data centre and place them on routers of the paths that are discovered by our routing protocol. Through our experiments on a testbed utilizing real meteorological data, our proposed solution has demonstrated increased quantity of renewable energy consumption, while reducing the workload on the data centers.", "venue": "ArXiv", "authors": ["Julien  Mineraud", "Liang  Wang", "Sasitharan  Balasubramaniam", "Jussi  Kangasharju"], "year": 2014, "n_citations": 0}
{"id": 5554187, "s2_id": "0379e528fa88eb3cdae60d95177e8486e0e38307", "title": "Acceleration of a Full-Scale Industrial CFD Application with OP2", "abstract": "Hydra is a full-scale industrial CFD application used for the design of turbomachinery at Rolls Royce plc., capable of performing complex simulations over highly detailed unstructured mesh geometries. Hydra presents major challenges in data organization and movement that need to be overcome for continued high performance on emerging platforms. We present research in achieving this goal through the OP2 domain-specific high-level framework, demonstrating the viability of such a high-level programming approach. OP2 targets the domain of unstructured mesh problems and enables execution on a range of back-end hardware platforms. We chart the conversion of Hydra to OP2, and map out the key difficulties encountered in the process. Specifically we show how different parallel implementations can be achieved with an active library framework, even for a highly complicated industrial application and how different optimizations targeting contrasting parallel architectures can be applied to the whole application, seamlessly, reducing developer effort and increasing code longevity. Performance results demonstrate that not only the same runtime performance as that of the hand-tuned original code could be achieved, but it can be significantly improved on conventional processor systems, and many-core systems. Our results provide evidence of how high-level frameworks such as OP2 enable portability across a wide range of contrasting platforms and their significant utility in achieving high performance without the intervention of the application programmer.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["I. Z. Reguly", "Gihan R. Mudalige", "Carlo  Bertolli", "Michael B. Giles", "Adam  Betts", "Paul H. J. Kelly", "David  Radford"], "year": 2016, "n_citations": 47}
{"id": 5559208, "s2_id": "33c2ef0e86c82bbe5ebf87cec9e380cf33a00294", "title": "Accuracy vs. Computational Cost Tradeoff in Distributed Computer System Simulation", "abstract": "Simulation is a fundamental research tool in the computer architecture field. These kinds of tools enable the exploration and evaluation of architectural proposals capturing the most relevant aspects of the highly complex systems under study. Many state-of-the-art simulation tools focus on single-system scenarios, but the scalability required by trending applications has shifted towards distributed computing systems integrated via complex software stacks. Web services with client-server architectures or distributed storage and processing of scale-out data analytics (Big Data) are among the main exponents. The complete simulation of a distributed computer system is the appropriate methodology to conduct accurate evaluations. Unfortunately, this methodology could have a significant impact on the already large computational effort derived from detailed simulation. In this work, we conduct a set of experiments to evaluate this accuracy/cost tradeoff. We measure the error made if client-server applications are evaluated in a single-node environment, as well as the overhead induced by the methodology and simulation tool employed for multi-node simulations. We quantify this error for different micro-architecture components, such as last-level cache and instruction/data TLB. Our findings show that accuracy loss can lead to completely wrong conclusions about the effects of proposed hardware optimizations. Fortunately, our results also demonstrate that the computational overhead of a multi-node simulation framework is affordable, suggesting multi-node simulation as the most appropriate methodology.", "venue": "ArXiv", "authors": ["Adrian  Colaso", "Pablo  Prieto", "Jose Angel Herrero", "Pablo Abad Fidalgo", "Valentin  Puente", "Jos\u00e9-\u00c1ngel  Gregorio"], "year": 2019, "n_citations": 0}
{"id": 5559435, "s2_id": "97cbf93b7060ac79eb7944cd4c83f1593795b950", "title": "Delay and Price Differentiation in Cloud Computing: A Service Model, Supporting Architectures, and Performance", "abstract": "Many cloud service providers (CSPs) provide on-demand service at a price with a small delay. We propose a QoS-differentiated model where multiple SLAs deliver both on-demand service for latency-critical users and delayed services for delay-tolerant users at lower prices. Two architectures are considered to fulfill SLAs. The first is based on priority queues. The second simply separates servers into multiple modules, each for one SLA. As an ecosystem, we show that the proposed framework is dominant-strategy incentive compatible. Although the first architecture appears more prevalent in the literature, we prove the superiority of the second architecture, under which we further leverage queueing theory to determine the optimal SLA delays and prices. Finally, the viability of the proposed framework is validated through numerical comparison with the on-demand service and it exhibits a revenue improvement in excess of 200%. Our results can help CSPs design optimal delay-differentiated services and choose appropriate serving architectures.", "venue": "ArXiv", "authors": ["Xiaohu  Wu", "Francesco De Pellegrini", "Giuliano  Casale"], "year": 2020, "n_citations": 1}
{"id": 5566131, "s2_id": "9436c0e123507250526a12ccde29cd9213e50954", "title": "Automatic Detection of Performance Anomalies in Task-Parallel Programs", "abstract": "To efficiently exploit the resources of new many-core architectures, integrating dozens or even hundreds of cores per chip, parallel programming models have evolved to expose massive amounts of parallelism, often in the form of fine-grained tasks. Task-parallel languages, such as OpenStream, X10, Habanero Java and C or StarSs, simplify the development of applications for new architectures, but tuning task-parallel applications remains a major challenge. Performance bottlenecks can occur at any level of the implementation, from the algorithmic level (e.g., lack of parallelism or over-synchronization), to interactions with the operating and runtime systems (e.g., data placement on NUMA architectures), to inefficient use of the hardware (e.g., frequent cache misses or misaligned memory accesses); detecting such issues and determining the exact cause is a difficult task. \nIn previous work, we developed Aftermath, an interactive tool for trace-based performance analysis and debugging of task-parallel programs and run-time systems. In contrast to other trace-based analysis tools, such as Paraver or Vampir, Aftermath offers native support for tasks, i.e., visualization, statistics and analysis tools adapted for performance debugging at task granularity. However, the tool currently does not provide support for the automatic detection of performance bottlenecks and it is up to the user to investigate the relevant aspects of program execution by focusing the inspection on specific slices of a trace file. In this paper, we present ongoing work on two extensions that guide the user through this process.", "venue": "ArXiv", "authors": ["Andi  Drebes", "Karine  Heydemann", "Antoniu  Pop", "Albert  Cohen", "Nathalie  Drach-Temam"], "year": 2014, "n_citations": 4}
{"id": 5566144, "s2_id": "34b68807aa1d5de5aa1e1a670260a303cc21ce02", "title": "Strategies for Big Data Analytics through Lambda Architectures in Volatile Environments", "abstract": "Expectations regarding the future growth of Internet of Things (IoT)-related technologies are high. These expectations require the realization of a sustainable general purpose application framework that is capable to handle these kinds of environments with their complexity in terms of heterogeneity and volatility. The paradigm of the Lambda architecture features key characteristics (such as robustness, fault tolerance, scalability, generalization, extensibility, ad-hoc queries, minimal maintenance, and low-latency reads and updates) to cope with this complexity. The paper at hand suggest a basic set of strategies to handle the arising challenges regarding the volatility, heterogeneity, and desired low latency execution by reducing the overall system timing (scheduling, execution, monitoring, and faults recovery) as well as possible faults (churn, no answers to executions). The proposed strategies make use of services such as migration, replication, MapReduce simulation, and combined processing methods (batch- and streaming-based). Via these services, a distribution of tasks for the best balance of computational resources is achieved, while monitoring and management can be performed asynchronously in the background. %An application of batch and stream-based methods are proposed to reduce the latency.", "venue": "ArXiv", "authors": ["Alexandre Da Silva Veith", "Julio C. S. dos Anjos", "Edison Pignaton de Freitas", "Thomas J. Lampoltshammer", "Cl\u00e1udio Fernando Resin Geyer"], "year": 2017, "n_citations": 9}
{"id": 5570445, "s2_id": "7b60873d124b3e92afd54f698b663cd08e2f00b8", "title": "ScALPEL: A Scalable Adaptive Lightweight Performance Evaluation Library for application performance monitoring", "abstract": "As supercomputers continue to grow in scale and capabilities, it is becoming increasingly difficult to isolate processor and system level causes of performance degradation. Over the last several years, a significant number of performance analysis and monitoring tools have been built/proposed. However, these tools suffer from several important shortcomings, particularly in distributed environments. In this paper we present ScALPEL, a Scalable Adaptive Lightweight Performance Evaluation Library for application performance monitoring at the functional level. Our approach provides several distinct advantages. First, ScALPEL is portable across a wide variety of architectures, and its ability to selectively monitor functions presents low run-time overhead, enabling its use for large-scale production applications. Second, it is run-time configurable, enabling both dynamic selection of functions to profile as well as events of interest on a per function basis. Third, our approach is transparent in that it requires no source code modifications. Finally, ScALPEL is implemented as a pluggable unit by reusing existing performance monitoring frameworks such as Perfmon and PAPI and extending them to support both sequential and MPI applications.", "venue": "ArXiv", "authors": ["Hari K. Pyla", "Bharath  Ramesh", "Calvin J. Ribbens", "Srinidhi  Varadarajan"], "year": 2009, "n_citations": 0}
{"id": 5570532, "s2_id": "23fb7e1abf4d4d04d6b7e8b0432722c07b6d3c8a", "title": "Performance Engineering for a Medical Imaging Application on the Intel Xeon Phi Accelerator", "abstract": "We examine the Xeon Phi, which is based on Intel's Many Integrated Cores architecture, for its suitability to run the FDK algorithm--the most commonly used algorithm to perform the 3D image reconstruction in cone-beam computed tomography. We study the challenges of efficiently parallelizing the application and means to enable sensible data sharing between threads despite the lack of a shared last level cache. Apart from parallelization, SIMD vectorization is critical for good performance on the Xeon Phi; we perform various micro-benchmarks to investigate the platform's new set of vector instructions and put a special emphasis on the newly introduced vector gather capability. We refine a previous performance model for the application and adapt it for the Xeon Phi to validate the performance of our optimized hand-written assembly implementation, as well as the performance of several different auto-vectorization approaches.", "venue": "ARCS Workshops", "authors": ["Johannes  Hofmann", "Jan  Treibig", "Georg  Hager", "Gerhard  Wellein"], "year": 2014, "n_citations": 14}
{"id": 5572626, "s2_id": "fd86587484ced86deaffbc285be98fbf61391ff1", "title": "On Degree-Based Decentralized Search in Complex Networks", "abstract": "Decentralized search aims to find the target node in a large network by using only local information. The applications of it include peer-to-peer file sharing, web search and anything else that requires locating a specific target in a complex system. In this paper, we examine the degree-based decentralized search method. Specifically, we evaluate the efficiency of the method in different cases with different amounts of available local information. In addition, we propose a simple refinement algorithm for significantly shortening the length of the route that has been found. Some insights useful for the future developments of efficient decentralized search schemes have been achieved.", "venue": "ArXiv", "authors": ["Shi  Xiao", "Gaoxi  Xiao"], "year": 2006, "n_citations": 8}
{"id": 5573918, "s2_id": "c147bb71e502eb19e3f92de0d32100cb7c2133df", "title": "MiXiM, PAWiS, and STEAM-Sim Integration - Combining Channel Models, Energy Awareness, and Real-life Application Code", "abstract": "After a decade of research in the field of wireless sensor networks (WSNs) there are still open issues. WSNs impose several severe requirements regarding energy consumption, processing capabilities, mobility, and robustness of wireless transmissions. Simulation has shown to be the most cost-efficient approach for evaluation of WSNs, thus a number of simulators are available. Unfortunately, these simulation environments typically consider WSNs from a special point of view. In this work we present the integration of three such specialized frameworks, namely MiXiM, PAWiS, and STEAM-Sim. This integration combines the strengths of the single frameworks such as realistic channel models, mobility patterns, accurate energy models, and inclusion of real-life application code. The result is a new simulation environment which enables a more general consideration of WSNs. We implemented and verified our proposed concept by means of static and mobile scenarios. As the presented results show, the combined framework gives the same results regarding the functionality and energy consumption as our \"golden model\". Therefore the system integration was successful and the framework is ready to be used by the community.", "venue": "ArXiv", "authors": ["Georg  M\u00f6stl", "Andreas  Springer"], "year": 2015, "n_citations": 0}
{"id": 5576490, "s2_id": "246d5069ebdf457ebed914484e90b6303047d592", "title": "Deterministic contention management for low latency Cloud RAN over an optical ring", "abstract": "The N-GREEN project has for goal to design a low cost optical ring technology with good performances (throughput, latency...) without using expensive end-to-end connections. We study the compatibility of such a technology with the development of the Cloud RAN, a latency critical application which is a major aspect of 5G deployment. We show that deterministically managing Cloud RAN traffic minimizes its latency while also improving the latency of the other traffics.", "venue": "ONDM", "authors": ["Dominique  Barth", "Ma\u00ebl  Guiraud", "Yann  Strozecki"], "year": 2019, "n_citations": 2}
{"id": 5585380, "s2_id": "d2bc0cbee0ad3dc8220a6c80328a143f4e1356f7", "title": "SDN Flow Entry Management Using Reinforcement Learning", "abstract": "Modern information technology services largely depend on cloud infrastructures to provide their services. These cloud infrastructures are built on top of Datacenter Networks (DCNs) constructed with high-speed links, fast switching gear, and redundancy to offer better flexibility and resiliency. In this environment, network traffic includes long-lived (elephant) and short-lived (mice) flows with partitioned/aggregated traffic patterns. Although SDN-based approaches can efficiently allocate networking resources for such flows, the overhead due to network reconfiguration can be significant. With limited capacity of Ternary Content-Addressable Memory (TCAM) deployed in an OpenFlow enabled switch, it is crucial to determine which forwarding rules should remain in the flow table and which rules should be processed by the SDN controller in case of a table-miss on the SDN switch. This is needed in order to obtain the flow entries that satisfy the goal of reducing the long-term control plane overhead introduced between the controller and the switches. To achieve this goal, we propose a machine learning technique that utilizes two variations of Reinforcement Learning (RL) algorithms\u2014the first of which is a traditional RL-based algorithm, while the other is deep reinforcement learning-based. Emulation results using the RL algorithm show around 60% improvement in reducing the long-term control plane overhead and around 14% improvement in the table-hit ratio compared to the Multiple Bloom Filters (MBF) method, given a fixed size flow table of 4KB.", "venue": "ACM Trans. Auton. Adapt. Syst.", "authors": ["Ting-Yu  Mu", "Ala I. Al-Fuqaha", "Khaled  Shuaib", "Farag  Sallabi", "Junaid  Qadir"], "year": 2018, "n_citations": 26}
{"id": 5585480, "s2_id": "8864b38dbaa06222f21fc8b0b52e04c0f374bfb3", "title": "Worst-case Bounds and Optimized Cache on Mth Request Cache Insertion Policies under Elastic Conditions", "abstract": "Cloud services and other shared third-party infrastructures allow individual content providers to easily scale their services based on current resource demands. In this paper, we consider an individual content provider that wants to minimize its delivery costs under the assumptions that the storage and bandwidth resources it requires are elastic, the content provider only pays for the resources that it consumes, and costs are proportional to the resource usage. Within this context, we (i) derive worst-case bounds for the optimal cost and competitive cost ratios of different classes of \"cache on $M^{th}$ request\" cache insertion policies, (ii) derive explicit average cost expressions and bounds under arbitrary inter-request distributions, (iii) derive explicit average cost expressions and bounds for short-tailed (deterministic, Erlang, and exponential) and heavy-tailed (Pareto) inter-request distributions, and (iv) present numeric and trace-based evaluations that reveal insights into the relative cost performance of the policies. Our results show that a window-based \"cache on $2^{nd}$ request\" policy using a single threshold optimized to minimize worst-case costs provides good average performance across the different distributions and the full parameter ranges of each considered distribution, making it an attractive choice for a wide range of practical conditions where request rates of individual file objects typically are not known and can change quickly.", "venue": "Perform. Evaluation", "authors": ["Niklas  Carlsson", "Derek L. Eager"], "year": 2018, "n_citations": 9}
{"id": 5586933, "s2_id": "30e3d8f9299148b8f5e5a53e348ae7c6d98792be", "title": "Optimisation of job scheduling for supercomputers with burst buffers", "abstract": "The ever-increasing gap between compute and I/O performance in HPC platforms, together with the development of novel NVMe storage devices (NVRAM), led to the emergence of the burst buffer concept - an intermediate persistent storage layer logically positioned between random-access main memory and a parallel file system. Since the appearance of this technology, numerous supercomputers have been equipped with burst buffers exploring various architectures. Despite the development of real-world architectures as well as research concepts, Resource and Job Management Systems, such as Slurm, provide only marginal support for scheduling jobs with burst buffer requirements. This research is primarily motivated by the alerting observation that burst buffers are omitted from reservations in the procedure of backfilling in existing job schedulers. In this dissertation, we forge a detailed supercomputer simulator based on Batsim and SimGrid, which is capable of simulating I/O contention and I/O congestion effects. Due to the lack of publicly available workloads with burst buffer requests, we create a burst buffer request distribution model derived from Parallel Workload Archive logs. We investigate the impact of burst buffer reservations on the overall efficiency of online job scheduling for canonical algorithms: First-Come-First-Served (FCFS) and Shortest-Job-First (SJF) EASY-backfilling. Our results indicate that the lack of burst buffer reservations in backfilling may significantly deteriorate the performance of scheduling. [...] Furthermore, this lack of reservations may cause the starvation of medium-size and wide jobs. Finally, we propose a burst-buffer-aware plan-based scheduling algorithm with simulated annealing optimisation, which improves the mean waiting time by over 20% and mean bounded slowdown by 27% compared to the SJF EASY-backfilling.", "venue": "ArXiv", "authors": ["Jan  Kopanski"], "year": 2021, "n_citations": 0}
{"id": 5589977, "s2_id": "312d048f0e012e69db4b7d60d7e440f7f68b367f", "title": "Performance Analysis of SPAD-based OFDM", "abstract": "In this paper, an analytical approach for the nonlinear distorted bit error rate performance of optical orthogonal frequency division multiplexing (O-OFDM) with single photon avalanche diode (SPAD) receivers is presented. Major distortion effects of passive quenching (PQ) and active quenching (AQ) SPAD receivers are analysed in this study. The performance analysis of DC-biased O-OFDM and asymmetrically clipped O-OFDM with PQ and AQ SPAD are derived. The comparison results show the maximum optical irradiance caused by the nonlinear distortion, which limits the transmission power and bit rate. The theoretical maximum bit rate of SPAD-based OFDM is found which is up to 1~Gbits/s. This approach supplies a closed-form analytical solution for designing an optimal SPAD-based system.", "venue": "ArXiv", "authors": ["Yichen  Li", "Majid  Safari", "Robert  Henderson", "Harald  Haas"], "year": 2019, "n_citations": 2}
{"id": 5592543, "s2_id": "ec1f582446aa24f3f0920123ee6f05feea0b5e0a", "title": "Online normalizer calculation for softmax", "abstract": "The Softmax function is ubiquitous in machine learning, multiple previous works suggested faster alternatives for it. In this paper we propose a way to compute classical Softmax with fewer memory accesses and hypothesize that this reduction in memory accesses should improve Softmax performance on actual hardware. The benchmarks confirm this hypothesis: Softmax accelerates by up to 1.3x and Softmax+TopK combined and fused by up to 5x.", "venue": "ArXiv", "authors": ["Maxim  Milakov", "Natalia  Gimelshein"], "year": 2018, "n_citations": 6}
{"id": 5592925, "s2_id": "cbef2b2c62ca4d8b29d9270a430ef4855a7dee5f", "title": "Profit-Aware Server Allocation for Green Internet Services", "abstract": "A server farm is examined, where a number of servers are used to offer a service to impatient customers. Every completed request generates a certain amount of profit, running servers consume electricity for power and cooling, while waiting customers might leave the system before receiving service if they experience excessive delays. A dynamic allocation policy aiming at satisfying the conflicting goals of maximizing the quality of users' experience while minimizing the cost for the provider is introduced and evaluated. The results of several experiments are described, showing that the proposed scheme performs well under different traffic conditions.", "venue": "2010 IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems", "authors": ["Michele  Mazzucco", "Dmytro  Dyachuk", "Marios D. Dikaiakos"], "year": 2010, "n_citations": 27}
{"id": 5593444, "s2_id": "d80b11e2d5e7eaede3dc50db6cc9a06ed337f1cb", "title": "Constrained Linear Movement Model (CALM): Simulation of passenger movement in airplanes", "abstract": "Pedestrian dynamics models the walking movement of individuals in a crowd. It has recently been used in the analysis of procedures to reduce the risk of disease spread in airplanes, relying on the SPED model. This is a social force model inspired by molecular dynamics; pedestrians are treated as point particles, and their trajectories are determined in a simulation. A parameter sweep is performed to address uncertainties in human behavior, which requires a large number of simulations. The SPED model\u2019s slow speed is a bottleneck to performing a large parameter sweep. This is a severe impediment to delivering real-time results, which are often required in the course of decision meetings, especially during emergencies. We propose a new model, called CALM, to remove this limitation. It is designed to simulate a crowd\u2019s movement in constrained linear passageways, such as inside an aircraft. We show that CALM yields realistic results while improving performance by two orders of magnitude over the SPED model.", "venue": "PloS one", "authors": ["Mehran Sadeghi Lahijani", "Tasvirul  Islam", "Ashok  Srinivasan", "Sirish  Namilae"], "year": 2020, "n_citations": 8}
{"id": 5598287, "s2_id": "4b22981df2612756b67ae7838cc34fa110341476", "title": "Of Kernels and Queues: When Network Calculus Meets Analytic Combinatorics", "abstract": "Stochastic network calculus is a tool for computing error bounds on the performance of queueing systems. However, deriving accurate bounds for networks consisting of several queues or subject to non-independent traffic inputs is challenging. In this paper, we investigate the relevance of the tools from analytic combinatorics, especially the kernel method, to tackle this problem. Applying the kernel method allows us to compute the generating functions of the queue state distributions in the stationary regime of the network. As a consequence, error bounds with an arbitrary precision can be computed. In this preliminary work, we focus on simple examples which are representative of the difficulties that the kernel method allows us to overcome.", "venue": "2018 30th International Teletraffic Congress (ITC 30)", "authors": ["Anne  Bouillard", "C\u00e9line  Comte", "Elie de Panafieu", "Fabien  Mathieu"], "year": 2018, "n_citations": 4}
{"id": 5599469, "s2_id": "18e938443132f7e822fe408b577b42f03cd83bd6", "title": "Analysis of Buffer Starvation With Application to Objective QoE Optimization of Streaming Services", "abstract": "Our purpose in this paper is to characterize buffer starvations for streaming services. The buffer is modeled as a FIFO queue with exponential service time and Poisson arrivals. When the buffer is empty, the service restarts after a certain amount of packets are prefetched. With this goal, we propose two approaches to obtain exact distribution of the number of buffer starvations, one of which is based on Ballot theorem, and the other uses recursive equations. The Ballot theorem approach gives an explicit result. We extend this approach to the scenario with a constant playback rate using Ta\u0300kacs Ballot theorem. The recursive approach, though not offering an explicit result, allows us to obtain the distribution of starvations with non-independent and identically distributed (i.i.d.) arrival process in which an ON/OFF bursty arrival process is considered. We further compute the starvation probability as a function of the amount of prefetched packets for a large number of files via a fluid analysis. Among many potential applications of starvation analysis, we show how to apply it to optimize objective quality of experience (QoE) of media streaming, by exploiting the tradeoff between startup/rebuffering delay and starvations.", "venue": "IEEE Transactions on Multimedia", "authors": ["Yuedong  Xu", "Eitan  Altman", "Rachid El Azouzi", "Majed  Haddad", "Salah-Eddine  Elayoubi", "Tania  Jim\u00e9nez"], "year": 2014, "n_citations": 48}
{"id": 5600335, "s2_id": "829dd54bb0a768949441d43e35b5d02011901f44", "title": "A flexible framework for accurate simulation of cloud in-memory data stores", "abstract": "Abstract In-memory (transactional) data stores, also referred to as data grids, are recognized as a first-class data management technology for cloud platforms, thanks to their ability to match the elasticity requirements imposed by the pay-as-you-go cost model. On the other hand, determining how performance and reliability/availability of these systems vary as a function of configuration parameters, such as the amount of cache servers to be deployed, and the degree of in-memory replication of slices of data, is far from being a trivial task. Yet, it is an essential aspect of the provisioning process of cloud platforms, given that it has an impact on the amount of cloud resources that are planned for usage. To cope with the issue of predicting/analysing the behavior of different configurations of cloud in-memory data stores, in this article we present a flexible simulation framework offering skeleton simulation models that can be easily specialized in order to capture the dynamics of diverse data grid systems, such as those related to the specific (distributed) protocol used to provide data consistency and/or transactional guarantees. Besides its flexibility, another peculiar aspect of the framework lies in that it integrates simulation and machine-learning (black-box) techniques, the latter being used to capture the dynamics of the data-exchange layer (e.g. the message passing layer) across the cache servers. This is a relevant aspect when considering that the actual data-transport/networking infrastructure on top of which the data grid is deployed might be unknown, hence being not feasible to be modeled via white-box (namely purely simulative) approaches. We also provide an extended experimental study aimed at validating instances of simulation models supported by our framework against execution dynamics of real data grid systems deployed on top of either private or public cloud infrastructures. Particularly, our validation test-bed has been based on an industrial-grade open-source data grid, namely Infinispan by JBoss/Red-Hat, and a de-facto standard benchmark for NoSQL platforms, namely YCSB by Yahoo. The validation study has been conducted by relying on both public and private cloud systems, scaling the underlying infrastructure up to 100 (resp. 140) Virtual Machines for the public (resp. private) cloud case. Further, we provide some experimental data related to a scenario where our framework is used for on-line capacity planning and reconfiguration of the data grid system.", "venue": "Simul. Model. Pract. Theory", "authors": ["Pierangelo di Sanzo", "Francesco  Quaglia", "Bruno  Ciciani", "Alessandro  Pellegrini", "Diego  Didona", "Paolo  Romano", "Roberto  Palmieri", "Sebastiano  Peluso"], "year": 2015, "n_citations": 14}
{"id": 5601937, "s2_id": "618df334f68b26bcdc482f536e9008a77fbf4485", "title": "Proceedings of the 5th International Workshop on Adaptive Self-tuning Computing Systems 2015 (ADAPT'15)", "abstract": "This is the proceedings of the 5th International Workshop on Adaptive Self-tuning Computing Systems 2015 (ADAPT'15).", "venue": "ArXiv", "authors": ["Christophe  Dubach", "Grigori  Fursin"], "year": 2014, "n_citations": 0}
{"id": 5603503, "s2_id": "cbc4ad96926ad0c6853ef34a6eaf19b10e446874", "title": "Scheduling in Parallel Finite Buffer Systems: Optimal Decisions under Delayed Feedback", "abstract": "Scheduling decisions in parallel queuing systems arise as a fundamental problem, underlying the dimensioning and operation of many computing and communication systems, such as job routing in data center clusters, multipath communication, and Big Data systems. In essence, the scheduler maps each arriving job to one of the possibly heterogeneous servers while aiming at an optimization goal such as load balancing, low average delay or low loss rate. One main difficulty in finding optimal scheduling decisions here is that the scheduler only partially observes the impact of its decisions, e.g., through the delayed acknowledgements of the served jobs. In this paper, we provide a partially observable (PO) model that captures the scheduling decisions in parallel queuing systems under limited information of delayed acknowledgements. We present a simulation model for this PO system to find a near-optimal scheduling policy in real-time using a scalable Monte Carlo tree search algorithm. We numerically show that the resulting policy outperforms other limited information scheduling strategies such as variants of Join-the-Most-Observations and has comparable performance to full information strategies like: Join-the-Shortest-Queue, Join-theShortest-Queue(d) and Shortest-Expected-Delay. Finally, we show how our approach can optimise the real-time parallel processing by using network data provided by Kaggle.", "venue": "ArXiv", "authors": ["Anam  Tahir", "Bastian  Alt", "Amr  Rizk", "Heinz  Koeppl"], "year": 2021, "n_citations": 0}
{"id": 5606143, "s2_id": "261df280e2062477c806d5e51857927b4b109661", "title": "Performance evaluation of a foot-controlled human-robot interface", "abstract": "Robotic minimally invasive interventions typically require using more than two instruments. We thus developed a foot pedal interface which allows the user to control a robotic arm (simultaneously to working with the hands) with four degrees of freedom in continuous directions and speeds. This paper evaluates and compares the performances of ten naive operators in using this new pedal interface and a traditional button interface in completing tasks. These tasks are geometrically complex path-following tasks similar to those in laparoscopic training, and the traditional button interface allows axis-by-axis control with constant speeds. Precision, time, and smoothness of the subjects\u2019 control movements for these tasks are analysed. The results demonstrate that the pedal interface can be used to control a robot for complex motion tasks. The subjects kept the average error rate at a low level of around 2.6% with both interfaces, but the pedal interface resulted in about 30% faster operation speed and 60% smoother movement, which indicates improved efficiency and user experience as compared with the button interface. The results of a questionnaire show that the operators found that controlling the robot with the pedal interface was more intuitive, comfortable, and less tiring than using the button interface.", "venue": "ArXiv", "authors": ["Yanpei  Huang", "Etienne  Burdet", "Lin  Cao", "Phuoc Thien Phan", "Anthony Meng Huat Tiong", "Pai  Zheng", "Soo Jay Phee"], "year": 2019, "n_citations": 1}
{"id": 5608636, "s2_id": "7d307d4c37624f84545d98e7c498b677793b266c", "title": "Efficient local unfolding with ancestor stacks*", "abstract": "Abstract The most successful unfolding rules used nowadays in the partial evaluation of logic programs are based on well quasi orders (wqo) applied over (covering) ancestors, i.e., a subsequence of the atoms selected during a derivation. Ancestor (sub)sequences are used to increase the specialization power of unfolding while still guaranteeing termination and also to reduce the number of atoms for which the wqo has to be checked. Unfortunately, maintaining the structure of the ancestor relation during unfolding introduces significant overhead. We propose an efficient, practical local unfolding rule based on the notion of covering ancestors which can be used in combination with a wqo and allows a stack-based implementation without losing any opportunities for specialization. Using our technique, certain nonleftmost unfoldings are allowed as long as local unfolding is performed, i.e., we cover depth-first strategies. To deal with practical programs, we propose assertion-based techniques which allow our approach to treat programs that include (Prolog) built-ins and external predicates in a very extensible manner, for the case of leftmost unfolding. Finally, we report on our implementation of these techniques embedded in a practical partial evaluator, which shows that our techniques, in addition to dealing with practical programs, are also significantly more efficient in time and somewhat more efficient in memory than traditional tree-based implementations.", "venue": "Theory and Practice of Logic Programming", "authors": ["Germ\u00e1n  Puebla", "Elvira  Albert", "Manuel V. Hermenegildo"], "year": 2010, "n_citations": 3}
{"id": 5608814, "s2_id": "468745c2e2fc6f8b562650fa02a9af379dfc4e25", "title": "Stability for Two-class Multiserver-job Systems", "abstract": "Multiserver-job systems, where jobs require concurrent service at many servers, occur widely in practice. Much is known in the dropping setting, where jobs are immediately discarded if they require more servers than are currently available. However, very little is known in the more practical setting where jobs queue instead. \nIn this paper, we derive a closed-form analytical expression for the stability region of a two-class (non-dropping) multiserver-job system where each class of jobs requires a distinct number of servers and requires a distinct exponential distribution of service time, and jobs are served in first-come-first-served (FCFS) order. This is the first result of any kind for an FCFS multiserver-job system where the classes have distinct service distributions. Our work is based on a technique that leverages the idea of a \"saturated\" system, in which an unlimited number of jobs are always available. \nOur analytical formula provides insight into the behavior of FCFS multiserver-job systems, highlighting the huge wastage (idle servers while jobs are in the queue) that can occur, as well as the nonmonotonic effects of the service rates on wastage.", "venue": "ArXiv", "authors": ["Isaac  Grosof", "Mor  Harchol-Balter", "Alan  Scheller-Wolf"], "year": 2020, "n_citations": 3}
{"id": 5611424, "s2_id": "fc46e8567eae292c0834adf48a6708dd3b26f308", "title": "Analysis of Petri Net Models through Stochastic Differential Equations", "abstract": "It is well known, mainly because of the work of Kurtz, that density dependent Markov chains can be approximated by sets of ordinary differential equations (ODEs) when their indexing parameter grows very large. This approximation cannot capture the stochastic nature of the process and, consequently, it can provide an erroneous view of the behavior of the Markov chain if the indexing parameter is not sufficiently high. Important phenomena that cannot be revealed include non-negligible variance and bi-modal population distributions. A less-known approximation proposed by Kurtz applies stochastic differential equations (SDEs) and provides information about the stochastic nature of the process.", "venue": "Petri Nets", "authors": ["Marco  Beccuti", "Enrico  Bibbona", "Andr\u00e1s  Horv\u00e1th", "Roberta  Sirovich", "Alessio  Angius", "Gianfranco  Balbo"], "year": 2014, "n_citations": 20}
{"id": 5613963, "s2_id": "94d8b3ab5a51f7567b04cfd8dae3ac012af15274", "title": "Generalized Analysis of a Distributed Energy Efficient Algorithm for Change Detection", "abstract": "We propose an energy efficient distributed cooperative Change Detection scheme called DualCUSUM based on Page's CUSUM algorithm. In the algorithm, each sensor runs a CUSUM and transmits only when the CUSUM is above some threshold. The transmissions from the sensors are fused at the physical layer. The channel is modeled as a Multiple Access Channel (MAC) corrupted with noise. The fusion center performs another CUSUM to detect the change. The algorithm performs better than several existing schemes when energy is at a premium. We generalize the algorithm to also include nonparametric CUSUM and provide a unified analysis. Our results show that while the false alarm probability is smaller for observation distribution with a lighter tail, the detection delay is asymptotically the same for any distribution. Consequently, we provide a new viewpoint on why parametric CUSUM performs better than nonparametric CUSUM. In the process, we also develop new results on a reflected random walk which can be of independent interest.", "venue": "IEEE Trans. Wirel. Commun.", "authors": ["Taposh  Banerjee", "Vinod  Sharma", "Veeraruna  Kavitha", "ArunKumar  Jayaprakasam"], "year": 2011, "n_citations": 19}
{"id": 5614665, "s2_id": "bc3b928ef13ebf32d56c35115115da7967882c96", "title": "Updating the Theory of Buffer Sizing", "abstract": "Routers have packet buffers to reduce packet drops during times of congestion. It is important to correctly size the buffer: make it too small, and packets are dropped unnecessarily and the link may be underutilized; make it too big, and packets may wait for a long time, and the router itself may be more expensive to build. Despite its importance, there are few guidelines for picking the buffer size. The two most well-known rules only apply to long-lived TCP Reno flows; either for a network carrying a single TCP Reno flow (the buffer size should equal the bandwidth-delay product, or BDP) or for a network carrying n TCP Reno flows (the buffer size should equal BDP/ \u221a n). Since these rules were introduced, TCP Reno has been replaced by newer algorithms as the default congestion control algorithm in all major operating systems, yet little has been written about how the rules need to change. This paper revisits both rules. For the single flow case, we generalize the BDP rule to account for changes to TCP, such as Proportional Rate Reduction (PRR), and the introduction of new algorithms including Cubic and BBR. We find that buffers can be made 60-75% smaller for newer algorithms. For the multiple flow case, we show that the square root of n rule holds under a broader set of assumptions than previously known, including for these new congestion control algorithms. We also demonstrate situations where the square root of n rule does not hold, including for unfair flows and certain settings with ECN. We validate our results by precisely measuring the time series of buffer occupancy in a real network, and comparing it to the per-packet window size.", "venue": "Perform. Evaluation", "authors": ["Bruce  Spang", "Serhat  Arslan", "Nick  McKeown"], "year": 2021, "n_citations": 0}
{"id": 5617090, "s2_id": "26cebf41c5f82708c781164ee825d0402b7f93ac", "title": "Characterizing Deep Learning Training Workloads on Alibaba-PAI", "abstract": "Modern deep learning models have been exploited in various domains, including computer vision (CV), natural language processing (NLP), search and recommendation. In practical AI clusters, workloads training these models are run using software frameworks such as TensorFlow, Caffe, PyTorch and CNTK. One critical issue for efficiently operating practical AI clouds, is to characterize the computing and data transfer demands of these workloads, and more importantly, the training performance given the underlying software framework and hardware configurations. In this paper, we characterize deep learning training workloads from Platform of Artificial Intelligence (PAI) in Alibaba. We establish an analytical framework to investigate detailed execution time breakdown of various workloads using different training architectures, to identify performance bottleneck. Results show that weight/gradient communication during training takes almost 62% of the total execution time among all our workloads on average. The computation part, involving both GPU computing and memory access, are not the biggest bottleneck based on collective behavior of the workloads. We further evaluate attainable performance of the workloads on various potential software/hardware mappings, and explore implications on software architecture selection and hardware configurations. We identify that 60% of PS/Worker workloads can be potentially sped up when ported to the AllReduce architecture exploiting the high-speed NVLink for GPU interconnect, and on average 1.7X speedup can be achieved when Ethernet bandwidth is upgraded from 25 Gbps to 100 Gbps.", "venue": "2019 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Mengdi  Wang", "Chen  Meng", "Guoping  Long", "Chuan  Wu", "Jun  Yang", "Wei  Lin", "Yangqing  Jia"], "year": 2019, "n_citations": 19}
{"id": 5617611, "s2_id": "330220fc1feb5d224af2c0ffcefe0f497185cc17", "title": "FedLess: Secure and Scalable Federated Learning Using Serverless Computing", "abstract": "The traditional cloud-centric approach for Deep Learning (DL) requires training data to be collected and processed at a central server which is often challenging in privacysensitive domains like healthcare. Towards this, a new learning paradigm called Federated Learning (FL) has been proposed that brings the potential of DL to these domains while addressing privacy and data ownership issues. FL enables remote clients to learn a shared ML model while keeping the data local. However, conventional FL systems face several challenges such as scalability, complex infrastructure management, and wasted compute and incurred costs due to idle clients. These challenges of FL systems closely align with the core problems that serverless computing and Function-as-a-Service (FaaS) platforms aim to solve. These include rapid scalability, no infrastructure management, automatic scaling to zero for idle clients, and a payper-use billing model. To this end, we present a novel system and framework for serverless FL, called FedLess. Our system supports multiple commercial and self-hosted FaaS providers and can be deployed in the cloud, on-premise in institutional data centers, and on edge devices. To the best of our knowledge, we are the first to enable FL across a large fabric of heterogeneous FaaS providers while providing important features like security and Differential Privacy. We demonstrate with comprehensive experiments that the successful training of DNNs for different tasks across up to 200 client functions and more is easily possible using our system. Furthermore, we demonstrate the practical viability of our methodology by comparing it against a traditional FL system and show that it can be cheaper and more resourceefficient.", "venue": "ArXiv", "authors": ["Andreas  Grafberger", "Mohak  Chadha", "Anshul  Jindal", "Jianfeng  Gu", "Michael  Gerndt"], "year": 2021, "n_citations": 0}
{"id": 5619937, "s2_id": "daf9f6601a0ad81ce22cdfce0cb0e0db00971949", "title": "Random Walks with Anti-Correlated Steps", "abstract": "We conjecture the expected value of random walks with anti-correlated steps to be exactly 1. We support this conjecture with 2 plausibility arguments and experimental data. The experimental analysis includes the computation of the expected values of random walks for steps up to 22. The result shows the expected value asymptotically converging to 1.", "venue": "ArXiv", "authors": ["Dirk  Wagner", "John  Noga"], "year": 2005, "n_citations": 0}
{"id": 5620411, "s2_id": "fd2be7693ec76a293916037634b0115364a43a2e", "title": "Offloading Execution from Edge to Cloud: A Dynamic Node-RED Based Approach", "abstract": "Fog computing enables use cases where data produced in end devices are stored, processed, and acted on directly at the edges of the network, yet computation can be offloaded to more powerful instances through the edge to cloud continuum. Such offloading mechanism is especially needed in case of modern multi-purpose IoT gateways, where both demand and operation conditions can vary largely between deployments. To facilitate the development and operations of gateways, we implement offloading directly as part of the IoT rapid prototyping process embedded in the software stack, based on Node-RED. We evaluate the implemented method using an image processing example, and compare various offloading strategies based on resource consumption and other system metrics, highlighting the differences in handling demand and service levels reached.", "venue": "2018 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)", "authors": ["Rom\u00e1n  Sosa", "Csaba  Kir\u00e1ly", "Juan D. Parra Rodriguez"], "year": 2018, "n_citations": 5}
{"id": 5622835, "s2_id": "a4f1aa0d44200982776186deacb39347dbc51995", "title": "TTC: A high-performance Compiler for Tensor Transpositions", "abstract": "We present TTC, an open-source parallel compiler for multidimensional tensor transpositions. In order to generate high-performance C++ code, TTC explores a number of optimizations, including software prefetching, blocking, loop-reordering, and explicit vectorization. To evaluate the performance of multidimensional transpositions across a range of possible use-cases, we also release a benchmark covering arbitrary transpositions of up to six dimensions. Performance results show that the routines generated by TTC achieve close to peak memory bandwidth on both the Intel Haswell and the AMD Steamroller architectures, and yield significant performance gains over modern compilers. By implementing a set of pruning heuristics, TTC allows users to limit the number of potential solutions; this option is especially useful when dealing with high-dimensional tensors, as the search space might become prohibitively large. Experiments indicate that when only 100 potential solutions are considered, the resulting performance is about 99% of that achieved with exhaustive search.", "venue": "ACM Trans. Math. Softw.", "authors": ["Paul  Springer", "Jeff R. Hammond", "Paolo  Bientinesi"], "year": 2017, "n_citations": 21}
{"id": 5623839, "s2_id": "61dea99d49979962044b6b4eac1ff2ffeb352354", "title": "Testing performance with and without Block Low Rank Compression in MUMPS and the new PaStiX 6.0 for JOREK nonlinear MHD simulations", "abstract": "The interface to the MUMPS solver was updated in the JOREK MHD code to support Block Low Rank (BLR) compression and an interface to the new PaStiX solver version 6 has been implemented supporting BLR as well. First tests were carried out with JOREK, which solves a large sparse matrix system iteratively in each time step. For the preconditioning, a direct solver is applied in the code to sub-matrices, and at this point BLR was applied with the results being summarized in this report. For a simple case with a linearly growing mode, results with both solvers look promising with a considerable reduction of the memory consumption by several ten percent was obtained. A direct increase in performance was seen in particular configurations already. \nThe choice of the BLR accuracy parameter $\\epsilon$ proves to be critical in this simple test and also in more realistic simulations, which were carried out only with MUMPS due to the limited time available. The more realistic test showed an increase in run time when using BLR, which was mitigated when using larger values of $\\epsilon$. However, the GMRes iterative solver does not reach convergence anymore when $\\epsilon$ is too large, since the preconditioner becomes too inaccurate in that case. It is thus critical to use an $\\epsilon$ as large as possible, while still reaching convergence. More tests regarding this optimum will be necessary in the future. BLR can also lead to an indirect speed-up in particular cases, when the simulation can be run on a smaller number of compute nodes due to the reduced memory consumption.", "venue": "ArXiv", "authors": ["Richard  Nies", "Matthias  Hoelzl"], "year": 2019, "n_citations": 3}
{"id": 5623880, "s2_id": "1aab408be1de456d119a426c05dea386bbd40dcb", "title": "A First Look at Commercial 5G Performance on Smartphones", "abstract": "We conduct to our knowledge a first measurement study of commercial 5G performance on smartphones by closely examining 5G networks of three carriers (two mmWave carriers, one mid-band carrier) in three U.S. cities. We conduct extensive field tests on 5G performance in diverse urban environments. We systematically analyze the handoff mechanisms in 5G and their impact on network performance. We explore the feasibility of using location and possibly other environmental information to predict the network performance. We also study the app performance (web browsing and HTTP download) over 5G. Our study consumes more than 15 TB of cellular data. Conducted when 5G just made its debut, it provides a \u201cbaseline\u201d for studying how 5G performance evolves, and identifies key research directions on improving 5G users\u2019 experience in a cross-layer manner. We have released the data collected from our study (referred to as 5Gophers) at https://fivegophers.umn.edu/www20.", "venue": "WWW", "authors": ["Arvind  Narayanan", "Eman  Ramadan", "Jason  Carpenter", "Qingxu  Liu", "Yu  Liu", "Feng  Qian", "Zhi-Li  Zhang"], "year": 2020, "n_citations": 61}
{"id": 5625751, "s2_id": "bf1350c9ebc23212b3427b41a7642e8a7c37c933", "title": "Revec: program rejuvenation through revectorization", "abstract": "Modern microprocessors are equipped with Single Instruction Multiple Data (SIMD) or vector instructions which expose data level parallelism at a fine granularity. Programmers exploit this parallelism by using low-level vector intrinsics in their code. However, once programs are written using vector intrinsics of a specific instruction set, the code becomes non-portable. Modern compilers are unable to analyze and retarget the code to newer vector instruction sets. Hence, programmers have to manually rewrite the same code using vector intrinsics of a newer generation to exploit higher data widths and capabilities of new instruction sets. This process is tedious, error-prone and requires maintaining multiple code bases. We propose Revec, a compiler optimization pass which revectorizes already vectorized code, by retargeting it to use vector instructions of newer generations. The transformation is transparent, happening at the compiler intermediate representation level, and enables performance portability of hand-vectorized code. Revec can achieve performance improvements in real-world performance critical kernels. In particular, Revec achieves geometric mean speedups of 1.160\u00d7 and 1.430\u00d7 on fast integer unpacking kernels, and speedups of 1.145\u00d7 and 1.195\u00d7 on hand-vectorized x265 media codec kernels when retargeting their SSE-series implementations to use AVX2 and AVX-512 vector instructions respectively. We also extensively test Revec\u2019s impact on 216 intrinsic-rich implementations of image processing and stencil kernels relative to hand-retargeting.", "venue": "CC", "authors": ["Charith  Mendis", "Ajay  Jain", "Paras  Jain", "Saman P. Amarasinghe"], "year": 2019, "n_citations": 2}
{"id": 5625822, "s2_id": "70355c736e5169fa69b1f7d788451b7c16653de5", "title": "Feature-Specific Profiling", "abstract": "High-level languages come with significant readability and maintainability benefits. Their performance costs, however, are usually not predictable, at least not easily. Programmers may accidentally use high-level features in ways that compiler writers could not anticipate, and they may thus produce underperforming programs as a result.", "venue": "CC", "authors": ["Vincent  St-Amour", "Leif  Andersen", "Matthias  Felleisen"], "year": 2015, "n_citations": 13}
{"id": 5627410, "s2_id": "f86c0dbfbb8932f8becc82a9c0dc55114728844a", "title": "Dynamic Weighted Fairness with Minimal Disruptions", "abstract": "In this paper, we consider the following dynamic fair allocation problem: Given a sequence of job arrivals and departures, the goal is to maintain an approximately fair allocation of the resource against a target fair allocation policy, while minimizing the total number of \\em disruptions, which is the number of times the allocation of any job is changed. We consider a rich class of fair allocation policies that significantly generalize those considered in previous work. We first consider the models where jobs only arrive, or jobs only depart. We present tight upper and lower bounds for the number of disruptions required to maintain a constant approximate fair allocation every time step. In particular, for the canonical case where jobs have weights and the resource allocation is proportional to the job's weight, we show that maintaining a constant approximate fair allocation requires \u0398(\u0142og^* n) disruptions per job, almost matching the bounds in prior work for the unit weight case. For the more general setting where the allocation policy only decreases the allocation to a job when new jobs arrive, we show that maintaining a constant approximate fair allocation requires \u0398(\u0142og n) disruptions per job. We then consider the model where jobs can both arrive and depart. We first show strong lower bounds on the number of disruptions required to maintain constant approximate fairness for arbitrary instances. In contrast we then show that there there is an algorithm that can maintain constant approximate fairness with $O(1)$ expected disruptions per job if the weights of the jobs are independent of the jobs arrival and departure order. We finally show how our results can be extended to the setting with multiple resources.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Sungjin  Im", "Benjamin  Moseley", "Kamesh  Munagala", "Kirk  Pruhs"], "year": 2020, "n_citations": 2}
{"id": 5630671, "s2_id": "bfd6de179949b03865aceba03893709e36d094c7", "title": "Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration", "abstract": "DNN accelerators are often developed and evaluated in isolation without considering the cross-stack, system-level effects in real-world environments. This makes it difficult to appreciate the impact of Systemon-Chip (SoC) resource contention, OS overheads, and programming-stack inefficiencies on overall performance/energy-efficiency. To address this challenge, we present Gemmini, an open-source, full-stack DNN accelerator generator. Gemmini generates a wide design-space of efficient ASIC accelerators from a flexible architectural template, together with flexible programming stacks and full SoCs with shared resources that capture system-level effects. Gemmini-generated accelerators have also been fabricated, delivering up to three orders-of-magnitude speedups over high-performance CPUs on various DNN benchmarks.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Hasan  Genc", "Seah  Kim", "Alon  Amid", "Ameer  Haj-Ali", "Vighnesh  Iyer", "Pranav  Prakash", "Jerry  Zhao", "Daniel  Grubb", "Harrison  Liew", "Howard  Mao", "Albert J. Ou", "Colin  Schmidt", "Samuel  Steffl", "John  Wright", "Ion  Stoica", "Jonathan  Ragan-Kelley", "Krste  Asanovic", "Borivoje  Nikolic", "Yakun Sophia Shao"], "year": 2021, "n_citations": 5}
{"id": 5634853, "s2_id": "d13683706ad118420e6c869c085d316e3dce7908", "title": "DCRoute: Speeding up Inter-Datacenter Traffic Allocation while Guaranteeing Deadlines", "abstract": "Datacenters provide the infrastructure for cloud computing services used by millions of users everyday. Many such services are distributed over multiple datacenters at geographically distant locations possibly in different continents. These datacenters are then connected through high speed WAN links over private or public networks. To perform data backups or data synchronization operations, many transfers take place over these networks that have to be completed before a deadline in order to provide necessary service guarantees to end users. Upon arrival of a transfer request, we would like the system to be able to decide whether such a request can be guaranteed successful delivery. If yes, it should provide us with transmission schedule in the shortest time possible. In addition, we would like to avoid packet reordering at the destination as it affects TCP performance. Previous work in this area either cannot guarantee that admitted transfers actually finish before the specified deadlines or use techniques that can result in packet reordering. In this paper, we propose DCRoute, a fast and efficient routing and traffic allocation technique that guarantees transfer completion before deadlines for admitted requests. It assigns each transfer a single path to avoid packet reordering. Through simulations, we show that DCRoute is at least 200 times faster than other traffic allocation techniques based on linear programming (LP) while admitting almost the same amount of traffic to the system.", "venue": "HiPC", "authors": ["Mohammad  Noormohammadpour", "Cauligi S. Raghavendra", "Sriram  Rao"], "year": 2016, "n_citations": 4}
{"id": 5638629, "s2_id": "7027767f9015010966c79756fae09e978338549f", "title": "A Beaconless Asymmetric Energy-Efficient Time Synchronization Scheme for Resource-Constrained Multi-Hop Wireless Sensor Networks", "abstract": "The ever-increasing number of WSN deployments based on a large number of battery-powered, low-cost sensor nodes, which are limited in their computing and power resources, puts the focus of WSN time synchronization research on three major aspects of accuracy, energy consumption, and computational complexity. In the literature, the latter two aspects haven\u2019t received much attention compared to the accuracy of WSN time synchronization. Especially in multi-hop WSNs, intermediate gateway nodes are overloaded with tasks for not only relaying messages but also a variety of computations for their offspring nodes as well as themselves. Therefore, not only minimizing the energy consumption but also lowering the computational complexity while maintaining the synchronization accuracy is crucial to the design of time synchronization schemes for resource-constrained sensor nodes. In this paper, focusing on the three aspects of WSN time synchronization, we introduce a framework of reverse asymmetric time synchronization for resource-constrained multi-hop WSNs and propose a beaconless energy-efficient time synchronization scheme based on reverse one-way message dissemination. Experimental results with a WSN testbed based on TelosB motes running TinyOS demonstrate that the proposed scheme conserves up to 95% energy consumption compared to the flooding time synchronization protocol while achieving microsecond-level synchronization accuracy.", "venue": "IEEE Transactions on Communications", "authors": ["Xintao  Huan", "Kyeong Soo Kim", "Sanghyuk  Lee", "Eng Gee Lim", "Alan  Marshall"], "year": 2020, "n_citations": 5}
{"id": 5639153, "s2_id": "b15bfea83b710c3439db8e6f9647494251d66a20", "title": "Cache-aware Parallel Programming for Manycore Processors", "abstract": "With rapidly evolving technology, multicore and manycore processors have emerged as promising architectures to benefit from increasing transistor numbers. The transition towards these parallel architectures makes today an exciting time to investigate challenges in parallel computing. The TILEPro64 is a manycore accelerator, composed of 64 tiles interconnected via multiple 8x8 mesh networks. It contains per-tile caches and supports cache-coherent shared memory by default. In this paper we present a programming technique to take advantages of distributed caching facilities in manycore processors. However, unlike other work in this area, our approach does not use architecture-specific libraries. Instead, we provide the programmer with a novel technique on how to program future Non-Uniform Cache Architecture (NUCA) manycore systems, bearing in mind their caching organisation. We show that our localised programming approach can result in a significant improvement of the parallelisation efficiency (speed-up).", "venue": "ArXiv", "authors": ["Ashkan  Tousimojarad", "Wim  Vanderbauwhede"], "year": 2014, "n_citations": 3}
{"id": 5639294, "s2_id": "6ee1181b36af21e226e4c7c6fcb82bda63c1363a", "title": "pCAMP: Performance Comparison of Machine Learning Packages on the Edges", "abstract": "Machine learning has changed the computing paradigm. Products today are built with machine intelligence as a central attribute, and consumers are beginning to expect near-human interaction with the appliances they use. However, much of the deep learning revolution has been limited to the cloud. Recently, several machine learning packages based on edge devices have been announced which aim to offload the computing to the edges. However, little research has been done to evaluate these packages on the edges, making it difficult for end users to select an appropriate pair of software and hardware. In this paper, we make a performance comparison of several state-of-the-art machine learning packages on the edges, including TensorFlow, Caffe2, MXNet, PyTorch, and TensorFlow Lite. We focus on evaluating the latency, memory footprint, and energy of these tools with two popular types of neural networks on different edge devices. This evaluation not only provides a reference to select appropriate combinations of hardware and software packages for end users but also points out possible future directions to optimize packages for developers.", "venue": "HotEdge", "authors": ["Xingzhou  Zhang", "Yifan  Wang", "Weisong  Shi"], "year": 2018, "n_citations": 52}
{"id": 5642964, "s2_id": "eb9ed76ed6954e1e0d4b0b5c9a4be3c56124e6e3", "title": "An LLVM Instrumentation Plug-in for Score-P", "abstract": "Reducing application runtime, scaling parallel applications to higher numbers of processes/threads, and porting applications to new hardware architectures are tasks necessary in the software development process. Therefore, developers have to investigate and understand application runtime behavior. Tools such as monitoring infrastructures that capture performance relevant data during application execution assist in this task. The measured data forms the basis for identifying bottlenecks and optimizing the code. Monitoring infrastructures need mechanisms to record application activities in order to conduct measurements. Automatic instrumentation of the source code is the preferred method in most application scenarios. We introduce a plug-in for the LLVM infrastructure that enables automatic source code instrumentation at compile-time. In contrast to available instrumentation mechanisms in LLVM/Clang, our plug-in can selectively include/exclude individual application functions. This enables developers to fine-tune the measurement to the required level of detail while avoiding large runtime overheads due to excessive instrumentation.", "venue": "LLVM-HPC@SC", "authors": ["Ronny  Tsch\u00fcter", "Johannes  Ziegenbalg", "Bert  Wesarg", "Matthias  Weber", "Christian  Herold", "Sebastian  D\u00f6bel", "Ronny  Brendel"], "year": 2017, "n_citations": 1}
{"id": 5646672, "s2_id": "0b2790c2c1e23e5535421696e1ec763bbc651bbd", "title": "To Index or Not to Index: Optimizing Exact Maximum Inner Product Search", "abstract": "Exact Maximum Inner Product Search (MIPS) is an important task that is widely pertinent to recommender systems and high-dimensional similarity search. The brute-force approach to solving exact MIPS is computationally expensive, thus spurring recent development of novel indexes and pruning techniques for this task. In this paper, we show that a hardware-efficient brute-force approach, blocked matrix multiply (BMM), can outperform the state-of-the-art MIPS solvers by over an order of magnitude, for some\u2014but not all\u2014inputs. In this paper we also present a novel MIPS solution, MAX-IMUS, that takes advantage of hardware efficiency and pruning of the search space. Like BMM, MAXIMUS is faster than other solvers by up to an order of magnitude, but again only for some inputs. Since no single solution offers the best runtime performance for all inputs, we introduce a new data-dependent optimizer, OPTIMUS, that selects online with minimal overhead the best MIPS solver for a given input. Together, OPTIMUS and MAXIMUS outperform state-of-the-art MIPS solvers by 3.2\u00d7 on average, and up to 10.9\u00d7, on widely studied MIPS datasets.", "venue": "2019 IEEE 35th International Conference on Data Engineering (ICDE)", "authors": ["Firas  Abuzaid", "Geet  Sethi", "Peter  Bailis", "Matei  Zaharia"], "year": 2019, "n_citations": 12}
{"id": 5650847, "s2_id": "18f4143d9740153c86799598762c08e8a89d43ce", "title": "Benchmarking and Performance Modelling of MapReduce Communication Pattern", "abstract": "Understanding and predicting the performance of big data applications running in the cloud or on-premises could help minimise the overall cost of operations and provide opportunities in efforts to identify performance bottlenecks. The complexity of the low-level internals of big data frameworks and the ubiquity of application and workload configuration parameters makes it challenging and expensive to come up with comprehensive performance modelling solutions. In this paper, instead of focusing on a wide range of configurable parameters, we studied the low-level internals of the MapReduce communication pattern and used a minimal set of performance drivers to develop a set of phase level parametric models for approximating the execution time of a given application on a given cluster. Models can be used to infer the performance of unseen applications and approximate their performance when an arbitrary dataset is used as input. Our approach is validated by running empirical experiments in two setups. On average, the error rate in both setups is \u00b110% from the measured values.", "venue": "2019 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)", "authors": ["Sheriffo  Ceesay", "Adam  Barker", "Yuhui  Lin"], "year": 2019, "n_citations": 2}
{"id": 5654215, "s2_id": "cc468533b564c6086bc517693f88cd83e98c3b2a", "title": "Estimating the Spatial Reuse with Configuration Models", "abstract": "We propose a new methodology to estimate the spatial reuse of CSMA-like scheduling. Instead of focusing on spatial congurations of users, we model the interferences between users as a random graph. Using conguration models for random graphs, we show how the properties of the medium access mechanism are captured by some deterministic dierential equations, when the size of the graph gets large. Performance indicators such as the probability of connection of a given node can then be eciently computed from these equations. We also perform simulations to illustrate the results on dierent types of random graphs. Even on spatial structures, these estimates get very accurate as soon as the variance of the interference is not negligible.", "venue": "ArXiv", "authors": ["Paola  Bermolen", "Matthieu  Jonckheere", "Federico  Larroca", "Pascal  Moyal"], "year": 2014, "n_citations": 10}
{"id": 5661978, "s2_id": "c33a8d9ab5b9cf1f4243e4f7f454d306bb91d43d", "title": "Optimizing stochastic scheduling in fork-join queueing models: Bounds and applications", "abstract": "Fork-Join (FJ) queueing models capture the dynamics of system parallelization under synchronization constraints, for example, for applications such as MapReduce, multipath transmission and RAID systems. Arriving jobs are first split into tasks and mapped to servers for execution, such that a job can only leave the system when all of its tasks are executed. In this paper, we provide computable stochastic bounds for the waiting and response time distributions for heterogeneous FJ systems under general parallelization benefit. Our main contribution is a generalized mathematical framework for probabilistic server scheduling strategies that are essentially characterized by a probability distribution over the number of utilized servers, and the optimization thereof. We highlight the trade-off between the scaling benefit due to parallelization and the FJ inherent synchronization penalty. Further, we provide optimal scheduling strategies for arbitrary scaling regimes that map to different levels of parallelization benefit. One notable insight obtained from our results is that different applications with varying parallelization benefits result in different optimal strategies. Finally, we complement our analytical results by applying them to various applications showing the optimality of the proposed scheduling strategies.", "venue": "IEEE INFOCOM 2017 - IEEE Conference on Computer Communications", "authors": ["Wasiur R. KhudaBukhsh", "Amr  Rizk", "Alexander  Fr\u00f6mmgen", "Heinz  Koeppl"], "year": 2017, "n_citations": 7}
{"id": 5665293, "s2_id": "4c099aaa76a9f6c76c85e3d1086a0ffd9abfa0e9", "title": "Analysis on 60 GHz Wireless Communications with Beamwidth-Dependent Misalignment", "abstract": "High speed wireless access on 60 GHz spectrum relies on high-gain directional antennas to overcome the severe signal attenuation. However, perfect alignment between transmitting and receiving antenna beams is rare in practice and overheard signals from concurrent transmissions may cause significant interference. In this paper we analyze the impact of antenna beam misalignment on the system performance of 60 GHz wireless access. We quantify the signal power loss caused by beam misalignment and the interference power accumulated from neighboring concurrent transmissions whose signals are leaked either via the main-beam pointing in the similar direction or via side-lobe emission, and derive the probability distribution of the signal to interference plus noise power ratio (SINR). For scenarios where interfering transmitters are distributed uniformly at random, we derive upper and lower bounds on the cumulative distribution function (abbreviated as CDF or c.d.f.) of SINR, which can be easily applied to evaluate system performance. We validate our analytical results by simulations where random nodes are uniformly distributed within a circular hall, and evaluate the sensitivity of average throughput and outage probability against two parameters: the half-power (3 dB) beamwidth to main-lobe beamwidth ratio and the beam misalignment deviation to main-lobe beamwidth ratio. Our results indicate that the derived lower bound performs well when the half-power beamwidth to main-lobe beamwidth ratio or the number of concurrent transmission links is small. When the number of active links is high, it is desirable in antenna design to balance the degradation caused by beam misalignment (wider beam is better) and the interference from concurrent transmission (narrower beam is better).", "venue": "ArXiv", "authors": ["Guang  Yang", "Jinfeng  Du", "Ming  Xiao"], "year": 2016, "n_citations": 10}
{"id": 5669011, "s2_id": "f99bdae226e2b868815ef1faf7985b32427de55b", "title": "Analytic performance modeling and analysis of detailed neuron simulations", "abstract": "Big science initiatives are trying to reconstruct and model the brain by attempting to simulate brain tissue at larger scales and with increasingly more biological detail than previously thought possible. The exponential growth of parallel computer performance has been supporting these developments, and at the same time maintainers of neuroscientific simulation code have strived to optimally and efficiently exploit new hardware features. Current state-of-the-art software for the simulation of biological networks has so far been developed using performance engineering practices, but a thorough analysis and modeling of the computational and performance characteristics, especially in the case of morphologically detailed neuron simulations, is lacking. Other computational sciences have successfully used analytic performance engineering, which is based on \u201cwhite-box,\u201d that is, first-principles performance models, to gain insight on the computational properties of simulation kernels, aid developers in performance optimizations and eventually drive codesign efforts, but to our knowledge a model-based performance analysis of neuron simulations has not yet been conducted. We present a detailed study of the shared-memory performance of morphologically detailed neuron simulations based on the Execution-Cache-Memory performance model. We demonstrate that this model can deliver accurate predictions of the runtime of almost all the kernels that constitute the neuron models under investigation. The gained insight is used to identify the main governing mechanisms underlying performance bottlenecks in the simulation. The implications of this analysis on the optimization of neural simulation software and eventually codesign of future hardware architectures are discussed. In this sense, our work represents a valuable conceptual and quantitative contribution to understanding the performance properties of biological networks simulations.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Francesco  Cremonesi", "Georg  Hager", "Gerhard  Wellein", "Felix  Sch\u00fcrmann"], "year": 2020, "n_citations": 9}
{"id": 5672176, "s2_id": "60c0cca28c92f75cad56f2ba156be4d8bf237374", "title": "SLO beyond the Hardware Isolation Limits", "abstract": "Performance isolation is a keystone for SLO guarantees with shared resources in cloud and datacenter environments. To meet SLO requirements, the state of the art relies on hardware QoS support (e.g., Intel RDT) to allocate shared resources such as last-level caches and memory bandwidth for co-located latency-critical applications. As a result, the number of latency-critical applications that can be deployed on a physical machine is bounded by the hardware allocation capability. Unfortunately, such hardware capability is very limited. For example, Intel Xeon E5 v3 processors support at most four partitions for last-level caches, i.e., at most four applications can have dedicated resource allocation. This paper discusses the feasibility and unexplored challenges of providing SLO guarantees beyond the limits of hardware capability. We present CoCo to show the feasibility and the benefits. CoCo schedules applications to time-share interferencefree partitions as a transparent software layer. Our evaluation shows that CoCo outperforms non-partitioned and the round-robin approaches by up to 9\u00d7 and 1.2\u00d7.", "venue": "ArXiv", "authors": ["Haoran  Qiu", "Yongzhou  Chen", "Tianyin  Xu", "Zbigniew T. Kalbarczyk", "Ravishankar K. Iyer"], "year": 2021, "n_citations": 1}
{"id": 5674124, "s2_id": "08daf8dfa01b31d8c486c31eeb39416f7731c68c", "title": "The Accuracy and Efficiency of Posit Arithmetic", "abstract": "Motivated by the increasing interest in the posit numeric format, in this paper we evaluate the accuracy and efficiency of posit arithmetic in contrast to the traditional IEEE 754 32-bit floating-point (FP32) arithmetic. We first design and implement a Posit Arithmetic Unit (PAU), called POSAR, with flexible bit-sized arithmetic suitable for applications that can trade accuracy for savings in chip area. Next, we analyze the accuracy and efficiency of POSAR with a series of benchmarks including mathematical computations, ML kernels, NAS Parallel Benchmarks (NPB), and Cifar-10 CNN. This analysis is done on our implementation of POSAR integrated into a RISC-V Rocket Chip core in comparison with the IEEE 754-based Floting Point Unit (FPU) of Rocket Chip. Our analysis shows that POSAR can outperform the FPU, but the results are not spectacular. For NPB, 32-bit posit achieves better accuracy than FP32 and improves the execution by up to 2%. However, POSAR with 32-bit posit needs 30% more FPGA resources compared to the FPU. For classic ML algorithms, we find that 8-bit posits are not suitable to replace FP32 because they exhibit low accuracy leading to wrong results. Instead, 16-bit posit offers the best option in terms of accuracy and efficiency. For example, 16-bit posit achieves the same Top-1 accuracy as FP32 on a Cifar-10 CNN with a speedup of 18%.", "venue": "2021 IEEE 39th International Conference on Computer Design (ICCD)", "authors": ["Stefan Dan Ciocirlan", "Dumitrel  Loghin", "Lavanya  Ramapantulu", "Nicolae  Tapus", "Yong Meng Teo"], "year": 2021, "n_citations": 2}
{"id": 5680515, "s2_id": "171ebd9da0d9dce27c6faaabb6c7c6b290dfb752", "title": "Serial Concatenation of RS Codes with Kite Codes: Performance Analysis, Iterative Decoding and Design", "abstract": "In this paper, we propose a new ensemble of rateless forward error correction (FEC) codes. The proposed codes are serially concatenated codes with Reed-Solomon (RS) codes as outer codes and Kite codes as inner codes. The inner Kite codes are a special class of prefix rateless low-density parity-check (PRLDPC) codes, which can generate potentially infinite (or as many as required) random-like parity-check bits. The employment of RS codes as outer codes not only lowers down error-floors but also ensures (with high probability) the correctness of successfully decoded codewords. In addition to the conventional two-stage decoding, iterative decoding between the inner code and the outer code are also implemented to improve the performance further. The performance of the Kite codes under maximum likelihood (ML) decoding is analyzed by applying a refined Divsalar bound to the ensemble weight enumerating functions (WEF). We propose a simulation-based optimization method as well as density evolution (DE) using Gaussian approximations (GA) to design the Kite codes. Numerical results along with semi-analytic bounds show that the proposed codes can approach Shannon limits with extremely low error-floors. It is also shown by simulation that the proposed codes performs well within a wide range of signal-to-noise-ratios (SNRs).", "venue": "ArXiv", "authors": ["Xiao  Ma", "Kai  Zhang", "Baoming  Bai", "Xiaoyi  Zhang"], "year": 2011, "n_citations": 14}
{"id": 5681483, "s2_id": "221d1294f4e56f8442b899af81429168916d7643", "title": "Accurate Closed-Form Approximations to Channel Distributions of RIS-Aided Wireless Systems", "abstract": "This letter proposes highly accurate closed-form approximations to channel distributions of two different reconfigurable intelligent surface (RIS)-based wireless system setups, namely, dual-hop RIS-aided (RIS-DH) scheme and RIS-aided transmit (RIS-T) scheme. Differently from previous works, the proposed approximations reveal to be very tight for arbitrary number <inline-formula> <tex-math notation=\"LaTeX\">${N}$ </tex-math></inline-formula> of reflecting metasurface\u2019s elements. Our findings are then applied to the performance analysis of the considered systems, in which the outage probability, bit error rate, and average channel capacity are derived. Results show that the achievable diversity orders <inline-formula> <tex-math notation=\"LaTeX\">$G_{d}$ </tex-math></inline-formula> for RIS-DH and RIS-T schemes are <inline-formula> <tex-math notation=\"LaTeX\">$N-1< G_{d}< N$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">${N}$ </tex-math></inline-formula>, respectively. Furthermore, it is revealed that both schemes can not provide the multiplexing gain and only diversity gains are achieved. For the RIS-DH scheme, the channels are similar to the keyhole multiple-input multiple-output (MIMO) channels with only one degree of freedom, while the RIS-T scheme is like the transmit diversity structure.", "venue": "IEEE Wireless Communications Letters", "authors": ["Liang  Yang", "Fanxu  Meng", "Qingqing  Wu", "Daniel Benevides da Costa", "Mohamed-Slim  Alouini"], "year": 2020, "n_citations": 36}
{"id": 5682826, "s2_id": "b49aeb7da8194c1c94d2cc08990e4732d35c96ac", "title": "Galactos: computing the anisotropic 3-point correlation function for 2 billion galaxies", "abstract": "The nature of dark energy and the complete theory of gravity are two central questions currently facing cosmology. A vital tool for addressing them is the 3-point correlation function (3PCF), which probes deviations from a spatially random distribution of galaxies. However, the 3PCF's formidable computational expense has prevented its application to astronomical surveys comprising millions to billions of galaxies. We present Galactos, a high-performance implementation of a novel, O(N2) algorithm that uses a load-balanced k-d tree and spherical harmonic expansions to compute the anisotropic 3PCF. Our implementation is optimized for the Intel Xeon Phi architecture, exploiting SIMD parallelism, instruction and thread concurrency, and significant L1 and L2 cache reuse, reaching 39% of peak performance on a single node. Galactos scales to the full Cori system, achieving 9.8 PF (peak) and 5.06 PF (sustained) across 9636 nodes, making the 3PCF easily computable for all galaxies in the observable universe.", "venue": "SC", "authors": ["Brian  Friesen", "Md. Mostofa Ali Patwary", "Brian  Austin", "Nadathur  Satish", "Zachary  Slepian", "Narayanan  Sundaram", "Deborah  Bard", "Daniel J. Eisenstein", "Jack  Deslippe", "Pradeep  Dubey", "Prabhat"], "year": 2017, "n_citations": 6}
{"id": 5682940, "s2_id": "892ecb80be483c15604fa25646a96962e0ba0e00", "title": "Survey of prognostics methods for condition-based maintenance in engineering systems", "abstract": "It is not surprising that the idea of efficient maintenance algorithms (originally motivated by strict emission regulations, and now driven by safety issues, logistics and customer satisfaction) has culminated in the so-called condition-based maintenance program. Condition-based program/monitoring consists of two major tasks, i.e., \\textit{diagnostics} and \\textit{prognostics} each of which has provided the impetus and technical challenges to the scientists and engineers in various fields of engineering. Prognostics deals with the prediction of the remaining useful life, future condition, or probability of reliable operation of an equipment based on the acquired condition monitoring data. This approach to modern maintenance practice promises to reduce the downtime, spares inventory, maintenance costs, and safety hazards. Given the significance of prognostics capabilities and the maturity of condition monitoring technology, there have been an increasing number of publications on machinery prognostics in the past few years. These publications cover a wide range of issues important to prognostics. Fortunately, improvement in computational resources technology has come to the aid of engineers by presenting more powerful onboard computational resources to make some aspects of these new problems tractable. In addition, it is possible to even leverage connected vehicle information through cloud-computing. Our goal is to review the state of the art and to summarize some of the recent advances in prognostics with the emphasis on models, algorithms and technologies used for data processing and decision making.", "venue": "ArXiv", "authors": ["Ehsan  Taheri", "Ilya  Kolmanovsky", "Oleg  Gusikhin"], "year": 2019, "n_citations": 3}
{"id": 5686905, "s2_id": "f7ce0c0f41cfb28535f50148265bc7dd7225cf14", "title": "An Analysis into the Performance and Memory Usage of MATLAB Strings", "abstract": "MATLAB is a mathematical computing environment used by many engineers, mathematicians, and students to process and understand their data. Important to all data science is the managing of textual data. MATLAB supports two textual data containers: (1) cell arrays of characters and (2) string arrays. This research showcases the strengths of string arrays over cell arrays by quantifying their performance, memory contiguity, syntax readability, interface fluidity, and autocomplete capabilities. These results demonstrate that string arrays often run 2x to 40x faster than cell arrays for common string benchmarks, are optimized for data locality by reducing metadata overhead, and offer a more expressive syntax due to their automatic data type conversions and vectorized methods.", "venue": "ArXiv", "authors": ["Travis  Near"], "year": 2021, "n_citations": 0}
{"id": 5695036, "s2_id": "5f6f73faacc7b4b4373fb2d44bb2f149ffd6f090", "title": "Understanding Model Drift in a Large Cellular Network", "abstract": "Operational networks are increasingly using machine learning models for a variety of tasks, including detecting anomalies, inferring application performance, and forecasting demand. Accurate models are important, yet accuracy can degrade over time due to concept drift, whereby either the characteristics of the data change over time (data drift) or the relationship between the features and the target predictor change over time (model drift). Drift is important to detect because changes in properties of the underlying data or relationships to the target prediction can require model retraining, which can be time-consuming and expensive. Concept drift occurs in operational networks for a variety of reasons, ranging from software upgrades to seasonality to changes in user behavior. Yet, despite the prevalence of drift in networks, its extent and effects on prediction accuracy have not been extensively studied. This paper presents an initial exploration into concept drift in a large cellular network in the United States for a major metropolitan area in the context of demand forecasting. We find that concept drift arises largely due to data drift, and it appears across different key performance indicators (KPIs), models, training set sizes, and time intervals. We identify the sources of concept drift for the particular problem of forecasting downlink volume. Weekly and seasonal patterns introduce both high and lowfrequency model drift, while disasters and upgrades result in sudden drift due to exogenous shocks. Regions with high population density, lower traffic volumes, and higher speeds also tend to correlate with more concept drift. The features that contribute most significantly to concept drift are User Equipment (UE) downlink packets, UE uplink packets, and Real-time Transport Protocol (RTP) total received packets.", "venue": "ArXiv", "authors": ["Shinan  Liu", "Francesco  Bronzino", "Paul  Schmitt", "Nick  Feamster", "Ricardo  Borges", "Hector Garcia Crespo", "Brian  Ward"], "year": 2021, "n_citations": 0}
{"id": 5695627, "s2_id": "74564272dc241fc785d74f25385f777f51c5bb51", "title": "Empirically Analyzing Ethereum's Gas Mechanism", "abstract": "Ethereum's Gas mechanism attempts to set transaction fees in accordance with the computational cost of transaction execution: a cost borne by default by every node on the network to ensure correct smart contract execution. Gas encourages users to author transactions that are efficient to execute and in so doing encourages node diversity, allowing modestly resourced nodes to join and contribute to the security of the network. However, the effectiveness of this scheme relies on Gas costs being correctly aligned with observed computational costs in reality. In this work, we performed the first large scale empirical study to understand to what degree this alignment exists in practice, by collecting and analyzing Tera-bytes worth of nanosecond-precision transaction execution traces. Besides confirming potential denial-of-service vectors, our results also shed light on the role of I/O in transaction costs which remains poorly captured by the current Gas cost model. Finally, our results suggest that under the current Gas cost model, nodes with modest computational resources are disadvantaged compared to their better resourced peers, which we identify as an ongoing threat to node diversity and network decentralization.", "venue": "2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)", "authors": ["Renlord  Yang", "Toby  Murray", "Paul  Rimba", "Parampalli  Udaya"], "year": 2019, "n_citations": 19}
{"id": 5696705, "s2_id": "ecca159df69776dd8dfc10ebd49d78e30b04f1f6", "title": "Acquisition probability of multi-user UWB systems in the presence of a novel synchronization approach", "abstract": "In this paper, to synchronize Ultra Wideband (UWB) systems in ad-hoc multi-user environments, we propose a new timing acquisition approach for achieving a good performance despite the difficulties to get there. Synchronization constraints are caused by the ultra-short emitted waveforms nature of UWB signals. Used in [1, 2] for single-user environments, our timing acquisition approach is based on two successive stages or floors. Extended for multi-user environments, the used algorithm is a combination between coarse synchronization based on timing with dirty templates (TDT) acquisition scheme and a new fine synchronization scheme developed in [3-6] which conduct to an improved estimate of timing offset. In this work, we develop and test this method in both data-aided (DA) and non-data-aided (NDA) modes. Simulation results and comparisons are also given to confirm performance improvement of our approach (in terms of mean square error and acquisition probability) compared to the original TDT algorithm in multi-user environments, especially in the NDA mode.", "venue": "ArXiv", "authors": ["Moez  Hizem", "Ridha  Bouallegue"], "year": 2012, "n_citations": 0}
{"id": 5697194, "s2_id": "f38b1a7e859a5942fdb6ed643f86f4f4d6de0fe3", "title": "Performance analysis of an unreliable M/G/1 retrial queue with two-way communication", "abstract": "Efficient use of call center operators through technological innovations more often come at the expense of added operation management issues. In this paper, the stationary characteristics of an M/G/1 retrial queue is investigated where the single server, subject to active failures, primarily attends incoming calls and directs outgoing calls only when idle. The incoming calls arriving at the server follow a Poisson arrival process, while outgoing calls are made in an exponentially distributed time. On finding the server unavailable (either busy or temporarily broken down), incoming calls intrinsically join the virtual orbit from which they re-attempt for service at exponentially distributed time intervals. The system stability condition along with probability generating functions for the joint queue length distribution of the number of calls in the orbit and the state of the server are derived and evaluated numerically in the context of mean system size, server availability, failure frequency and orbit waiting time.", "venue": "Oper. Res.", "authors": ["Muthukrishnan Senthil Kumar", "Aresh  Dadlani", "Kiseon  Kim"], "year": 2020, "n_citations": 5}
{"id": 5700413, "s2_id": "94282ce0807fe80217bfc69581fd2457a883fc0a", "title": "Steady state availability general equations of decision and sequential processes in Continuous Time Markov Chain models", "abstract": "Continuous Time Markov Chain (CMTC) is widely used to describe and analyze systems in several knowledge areas. Steady state availability is one important analysis that can be made through Markov chain formalism that allows researchers generate equations for several purposes, such as channel capacity estimation in wireless networks as well as system performance estimations. The problem with this kind of analysis is the complex process to generating these equations. In this letter, we have developed general equations for decision and sequential processes of CMTC Models, aiming to help researchers to develop steady state availability equations. We also have developed the general equation here termed as Closed Decision Process.", "venue": "ArXiv", "authors": ["Eduardo M. Vasconcelos"], "year": 2017, "n_citations": 0}
{"id": 5702572, "s2_id": "2191e8013a7d056b833923f6b37e5f3f44ca0afd", "title": "Towards a Taxonomy of Performance Evaluation of Commercial Cloud Services", "abstract": "Cloud Computing, as one of the most promising computing paradigms, has become increasingly accepted in industry. Numerous commercial providers have started to supply public Cloud services, and corresponding performance evaluation is then inevitably required for Cloud provider selection or cost-benefit analysis. Unfortunately, inaccurate and confusing evaluation implementations can be often seen in the context of commercial Cloud Computing, which could severely interfere and spoil evaluation-related comprehension and communication. This paper introduces a taxonomy to help profile and standardize the details of performance evaluation of commercial Cloud services. Through a systematic literature review, we constructed the taxonomy along two dimensions by arranging the atomic elements of Cloud-related performance evaluation. As such, this proposed taxonomy can be employed both to analyze existing evaluation practices through decomposition into elements and to design new experiments through composing elements for evaluating performance of commercial Cloud services. Moreover, through smooth expansion, we can continually adapt this taxonomy to the more general area of evaluation of Cloud Computing.", "venue": "2012 IEEE Fifth International Conference on Cloud Computing", "authors": ["Zheng  Li", "Liam  O'Brien", "Rainbow  Cai", "He  Zhang"], "year": 2012, "n_citations": 54}
{"id": 5703127, "s2_id": "a5e9395fcc8ae09105c8e309dbcc491bcf13916b", "title": "Cross-layer Visualization and Profiling of Network and I/O Communication for HPC Clusters", "abstract": "Understanding and visualizing the full-stack performance trade-offs and interplay between HPC applications, MPI libraries, the communication fabric, and the file system is a challenging endeavor. Designing a holistic profiling and visualization method for HPC communication networks is challenging since different levels of communication coexist and interact with each other on the communication fabric. A breakdown of traffic is essential to understand the interplay of different layers along with the application\u2019s communication behavior without losing a general view of network traffic. Unfortunately, existing profiling tools are disjoint and either focus on only profiling and visualizing a few levels of the HPC stack, which limits the insights they can provide, or they provide extremely detailed information which necessitates a steep learning curve to understand. We target our profiling tool visualization to provide holistic and real-time insights into HPC communication stacks. In this paper, we propose and implement our visualization methods to enable holistic insight for representing the cross-stack metrics. Moreover, we propose and implement a low-overhead I/O profiling inside the communication library, collect and store the profiling information, and then study the correlation and evaluation of I/O traffic with MPI communication using a crossstack approach by INAM. Through experimental evaluations and use cases, we demonstrate novel benefits of our cross-stack communication analysis in real-time to detect bottlenecks and understand communication performance.", "venue": "ArXiv", "authors": ["Pouya  Kousha", "Quentin  Anthony", "Hari  Subramoni", "Dhabaleswar K. Panda"], "year": 2021, "n_citations": 0}
{"id": 5703813, "s2_id": "34cb4c7c44d1c8137fd67f8f43fe03859aff147a", "title": "Q-ESP: A QoS-Compliant Security Protocol to Enrich IPSec Framework", "abstract": "IPSec is a protocol that allows to make secure connections between branch offices and allows secure VPN accesses. However, the efforts to improve IPSec are still under way; one aspect of this improvement is to take Quality of Service (QoS) requirements into account. QoS is the ability of the network to provide a service at an assured service level while optimizing the global usage of network resources. The QoS level that a flow receives depends on a six-bit identifier in the IP header; the so-called Differentiated Services code point (DSCP). Basically, Multi-Field classifiers classify a packet by inspecting IP/TCP headers, to decide how the packet should be processed. The current IPSec standard does hardly offer any guidance to do this, because the existing IPSec ESP security protocol hides much of this information in its encrypted payloads, preventing network control devices such as routers and switches from utilizing this information in performing classification appropriately. To solve this problem, we propose a QoS-friendly Encapsulated Security Payload (Q-ESP) as a new IPSec security protocol that provides both security and QoS supports. We also present our NetBSD kernel-based implementation as well as our evaluation results of Q-ESP.", "venue": "2009 3rd International Conference on New Technologies, Mobility and Security", "authors": ["Mahmoud  Mostafa", "Anas Abou El Kalam", "Christian  Fraboul"], "year": 2009, "n_citations": 4}
{"id": 5705025, "s2_id": "d7de4b03f2705f44fd638494940c87aaa14ccfb7", "title": "Coupling Microscopic Mobility and Mobile Network Emulation for Pedestrian Communication Applications", "abstract": "Network emulation is a well-established method for demonstrating and testing real devices and mobile apps in a controlled scenario. This paper reports preliminary results for an open-source extension of the CrowNet pedestrian communication framework. It enables the interaction between simulated and real devices using the emulation feature of OMNeT++. The interaction is handled by several OMNeT++ modules that can be combined to match different use-cases. Initial timing measurements have been conducted for an example application which creates decentralized pedestrian density maps based on pedestrian communication. The results indicate that the approach is feasible for scenarios with a limited number of pedestrians. This limitation is mainly due to the real-time simulation requirements in coupled emulation.", "venue": "ArXiv", "authors": ["Matthias  Rupp", "Stefan  Schuhb\u00e4ck", "Lars  Wischhof"], "year": 2021, "n_citations": 0}
{"id": 5707677, "s2_id": "7730396a063b11bfec77de848c850de6f847400c", "title": "Can We Spot Energy Regressions using Developers Tests?", "abstract": "Background: Software Energy Consumption (SEC) is gaining more and more attention. In this paper, we tackle the problem of hinting developers about the SEC of their programs in the context of software developments based on Continuous Integration (CI). Objective: In this study, we investigate if the CI can leverage developers\u2019 tests to perform a new class of test: the energy regression testing. Energy regression is similar to performance regression, but focused on the energy consumption of the program instead of standard performance indicators, like execution time or memory consumption. Method: We propose to perform an exploratory study of the usage of developers\u2019 tests for energy regression testing. We propose to first investigate if developers\u2019 tests can be used to obtain stable SEC indicators. Then, to consider if comparing the SEC of developers\u2019 tests between two versions can accurately spot energy regressions introduced by automated program mutations. Finally, to assess if it can successfully pinpoint the source code lines guilty of energy regressions. Impact: Our study will pave the way for automated SEC regression tools that can be readily deployed inside an existing CI infrastructure to raise awareness of SEC issues among practitioners.", "venue": "ArXiv", "authors": ["Benjamin  Danglot", "Jean-R'emy  Falleri", "Romain  Rouvoy"], "year": 2021, "n_citations": 1}
{"id": 5708757, "s2_id": "12ba42ba94b19908e367dcf5cf3593b5f76b4f37", "title": "Numerical simulation of skin transport using Parareal", "abstract": "In silico investigation of skin permeation is an important but also computationally demanding problem. To resolve all scales involved in full detail will not only require exascale computing capacities but also suitable parallel algorithms. This article investigates the applicability of the time-parallel Parareal algorithm to a brick and mortar setup, a precursory problem to skin permeation. The C++ library Lib4PrM implementing Parareal is combined with the UG4 simulation framework, which provides the spatial discretization and parallelization. The combination\u2019s performance is studied with respect to convergence and speedup. It is confirmed that anisotropies in the domain and jumps in diffusion coefficients only have a minor impact on Parareal\u2019s convergence. The influence of load imbalances in time due to differences in number of iterations required by the spatial solver as well as spatio-temporal weak scaling is discussed.", "venue": "Comput. Vis. Sci.", "authors": ["Andreas  Kreienbuehl", "Arne  N\u00e4gel", "Daniel  Ruprecht", "Robert  Speck", "Gabriel  Wittum", "Rolf  Krause"], "year": 2015, "n_citations": 19}
{"id": 5710437, "s2_id": "182595cd7b57b0b9e083f3d9ae018c1c56133710", "title": "On Formal Methods for Collective Adaptive System Engineering. {Scalable Approximated, Spatial} Analysis Techniques. Extended Abstract", "abstract": "In this extended abstract a view on the role of Formal Methods in System Engineering is briefly presented. Then two examples of useful analysis techniques based on solid mathematical theories are discussed as well as the software tools which have been built for supporting such techniques. The first technique is Scalable Approximated Population DTMC Model-checking. The second one is Spatial Model-checking for Closure Spaces. Both techniques have been developed in the context of the EU funded project QUANTICOL.", "venue": "FORECAST@STAF", "authors": ["Diego  Latella"], "year": 2016, "n_citations": 0}
{"id": 5719235, "s2_id": "23001e0d175e952fbb753712dfa7bf5ee1e32b1a", "title": "Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory Machines", "abstract": "Multi-socket machines with 1-100 TBs of physical memory are becoming prevalent. Applications running on such multi-socket machines suffer non-uniform bandwidth and latency when accessing physical memory. Decades of research have focused on data allocation and placement policies in NUMA settings, but there have been no studies on the question of how to place page-tables amongst sockets. We make the case for explicit page-table allocation policies and show that page-table placement is becoming crucial to overall performance. We propose Mitosis to mitigate NUMA effects on page-table walks by transparently replicating and migrating page-tables across sockets without application changes. This reduces the frequency of accesses to remote NUMA nodes when performing page-table walks. Mitosis uses two components: (i) a mechanism to efficiently enable and (ii) policies to effectively control -- page-table replication and migration. We implement Mitosis in Linux and evaluate its benefits on real hardware. Mitosis improves performance for large-scale multi-socket workloads by up to 1.34x by replicating page-tables across sockets. Moreover, it improves performance by up to 3.24x in cases when the OS migrates a process across sockets by enabling cross-socket page-table migration.", "venue": "ASPLOS", "authors": ["Reto  Achermann", "Ashish  Panwar", "Abhishek  Bhattacharjee", "Timothy  Roscoe", "Jayneel  Gandhi"], "year": 2020, "n_citations": 14}
{"id": 5720853, "s2_id": "b83834ae75c7812b70b544a0e35ebb9870938c74", "title": "Flashmon V2: monitoring raw NAND flash memory I/O requests on embedded Linux", "abstract": "This paper presents Flashmon version 2, a tool for monitoring embedded Linux NAND flash memory I/O requests. It is designed for embedded boards based devices containing raw flash chips. Flashmon is a kernel module and stands for \"flash monitor\". It traces flash I/O by placing kernel probes at the NAND driver level. It allows tracing at runtime the 3 main flash operations: page reads/writes and block erasures. Flashmon is (1) generic as it was successfully tested on the three most widely used flash file systems that are JFFS2, UBIFS and YAFFS, and several NAND chip models. Moreover, it is (2) non intrusive, (3) has a controllable memory footprint, and (4) exhibits a low overhead (< 6%) on the traced system. Finally, it is (5) simple to integrate and used as a standalone module or as a built-in function/module in existing kernel sources. Monitoring flash memory operations allows a better understanding of existing flash management systems by studying and analyzing their behavior. Moreover it is useful in development phase for prototyping and validating new solutions.", "venue": "SIGBED", "authors": ["Pierre  Olivier", "Jalil  Boukhobza", "Eric  Senn"], "year": 2014, "n_citations": 16}
{"id": 5728776, "s2_id": "fd0edae78e4071c032d00abe1f04f7f3746721ce", "title": "Cramer-Rao Bounds for Joint RSS/DoA-Based Primary-User Localization in Cognitive Radio Networks", "abstract": "Knowledge about the location of licensed primary-users (PU) could enable several key features in cognitive radio (CR) networks including improved spatio-temporal sensing, intelligent location-aware routing, as well as aiding spectrum policy enforcement. In this paper we consider the achievable accuracy of PU localization algorithms that jointly utilize received-signal-strength (RSS) and direction-of-arrival (DoA) measurements by evaluating the Cramer-Rao Bound (CRB). Previous works evaluate the CRB for RSS-only and DoA-only localization algorithms separately and assume DoA estimation error variance is a fixed constant or rather independent of RSS. We derive the CRB for joint RSS/DoA-based PU localization algorithms based on the mathematical model of DoA estimation error variance as a function of RSS, for a given CR placement. The bound is compared with practical localization algorithms and the impact of several key parameters, such as number of nodes, number of antennas and samples, channel shadowing variance and correlation distance, on the achievable accuracy are thoroughly analyzed and discussed. We also derive the closed-form asymptotic CRB for uniform random CR placement, and perform theoretical and numerical studies on the required number of CRs such that the asymptotic CRB tightly approximates the numerical integration of the CRB for a given placement.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Jun  Wang", "Jianshu  Chen", "Danijela  Cabric"], "year": 2013, "n_citations": 54}
{"id": 5732902, "s2_id": "a2a54474d5ecbfc46a8e3bd972d227a07ebd4ea4", "title": "Three Other Models of Computer System Performance", "abstract": "This note argues for more use of simple models beyond Amdahl's Law: Bottleneck Analysis, Little's Law, and a M/M/1 Queue.", "venue": "ArXiv", "authors": ["Mark D. Hill"], "year": 2019, "n_citations": 2}
{"id": 5733576, "s2_id": "f9340c148f2f6642666f42872895802c42586b2d", "title": "Intermittent Opportunistic Routing Components for the INET Framework", "abstract": "Intermittently-powered wireless sensor networks (WSNs) use energy harvesting and small energy storage to remove the need for battery replacement and to extend the operational lifetime. However, an intermittently-powered forwarder regularly turns on or off, which requires alternative networking solutions. Opportunistic routing (OR) is a potential cross-layer solution for this novel application, but due to the interaction with the energy storage, the operation of these protocols is highly dynamic. To compare protocols and components in like-for-like scenarios we propose module interfaces for MAC, routing and discovery protocols, that enable clear separation of concerns and good interchangeability. We also suggest some candidates for each of the protocols based on our own implementation and research.", "venue": "ArXiv", "authors": ["Edward  Longman", "Mohammed  El-Hajjar", "Geoff V. Merrett"], "year": 2021, "n_citations": 0}
{"id": 5736543, "s2_id": "f6ea0ffa1db834c60f37c04274f58dfe12f4818f", "title": "Proceedings of the 8th OMNeT++ Community Summit, Virtual Summit, September 8-10, 2021", "abstract": "These are the Proceedings of the 8th OMNeT++ Community Summit, which was held virtually on September 8-10, 2021.", "venue": "ArXiv", "authors": ["Marcel  Marek", "Giovanni  Nardini", "Vladim'ir  Vesel'y"], "year": 2021, "n_citations": 0}
{"id": 5746624, "s2_id": "46315b10904e1c74de4737c2d7115d74d6e73ff1", "title": "Compression Scheme for Faster and Secure Data Transmission Over Internet", "abstract": "Compression algorithms reduce the redundancy in data representation to decrease the storage required for that data. Data compression offers an attractive approach to reducing communication costs by using available bandwidth effectively. Over the last decade there has been an unprecedented explosion in the amount of digital data transmitted via the Internet, representing text, images, video, sound, computer programs, etc. With this trend expected to continue, it makes sense to pursue research on developing algorithms that can most effectively use available network bandwidth by maximally compressing data. It is also important to consider the security aspects of the data being transmitted while compressing it, as most of the text data transmitted over the Internet is very much vulnerable to a multitude of attacks. This paper is focused on addressing this problem of lossless compression of text files with an added security.", "venue": "ArXiv", "authors": ["B. S. Shajeemohan", "V. K. Govindan"], "year": 2006, "n_citations": 1}
{"id": 5748463, "s2_id": "190674a9ff30915d03c8c513c592db864fee288e", "title": "Proactive bottleneck performance analysis in parallel computing using openMP", "abstract": "The aim of parallel computing is to increase an application performance by executing the application on multiple processors. OpenMP is an API that supports multi platform shared memory programming model and shared-memory programs are typically executed by multiple threads. The use of multi threading can enhance the performance of application but its excessive use can degrade the performance. This paper describes a novel approach to avoid bottlenecks in application and provide some techniques to improve performance in OpenMP application. This paper analyzes bottleneck performance as bottleneck inhibits performance. Performance of multi threaded applications is limited by a variety of bottlenecks, e.g. critical sections, barriers and so on. This paper provides some tips how to avoid performance bottleneck problems. This paper focuses on how to reduce overheads and overall execution time to get better performance of application.", "venue": "ArXiv", "authors": ["Vibha  Rajput", "Alok  Katiyar"], "year": 2013, "n_citations": 4}
{"id": 5752190, "s2_id": "4b7b50a118bffed8974950bf93bac6de52b24404", "title": "Dynamic Allocation Problems in Loss Network Systems with Advanced Reservation", "abstract": "We consider a class of well-known dynamic resource allocation models in loss network systems with advanced reservation. The most important performance measure in any loss network system is to compute its blocking probability, i.e., the probability of an arriving customer in equilibrium finds a fully utilized system (thereby getting rejected by the system). In this paper, we derive upper bounds on the asymptotic blocking probabilities for such systems in high-volume regimes. There have been relatively few results on loss network systems with advanced reservation due to its inherent complexity. The theoretical results find applications in a wide class of revenue management problems in systems with reusable resources and advanced reservation, e.g., hotel room, car rental and workforce management. We propose a simple control policy called the improved class selection policy (ICSP) based on solving a continuous knapsack problem, similar in spirit to the one proposed in Levi and Radovanovic (2010). Using our results derived for loss network systems with advanced reservation, we show the ICSP performs asymptotically near- optimal in high-volume regimes.", "venue": "ArXiv", "authors": ["Retsef  Levi", "Cong  Shi"], "year": 2015, "n_citations": 3}
{"id": 5756067, "s2_id": "9014686ae37f68056c9b30b7ae41acdbe83f8728", "title": "Degradation Analysis of Probabilistic Parallel Choice Systems", "abstract": "Degradation analysis is used to analyze the useful lifetimes of systems, their failure rates, and various other system parameters like mean time to failure (MTTF), mean time between failures (MTBF), and the system failure rate (SFR). In many systems, certain possible parallel paths of execution that have greater chances of success are preferred over others. Thus we introduce here the concept of probabilistic parallel choice. We use binary and n-ary probabilistic choice operators in describing the selections of parallel paths. These binary and n-ary probabilistic choice operators are considered so as to represent the complete system (described as a series-parallel system) in terms of the probabilities of selection of parallel paths and their relevant parameters. Our approach allows us to derive new and generalized formulae for system parameters like MTTF, MTBF, and SFR. We use a generalized exponential distribution, allowing distinct installation times for individual components, and use this model to derive expressions for such system parameters.", "venue": "ArXiv", "authors": ["Avinash  Saxena", "Shrisha  Rao"], "year": 2014, "n_citations": 0}
{"id": 5756557, "s2_id": "10ea8a2083d6af2733f0e60992a85b3a620c662f", "title": "Asymptotically Optimal Load Balancing in Large-scale Heterogeneous Systems with Multiple Dispatchers", "abstract": "We consider the load balancing problem in large-scale heterogeneous systems with multiple dispatchers. We introduce a general framework called Local-Estimation-Driven (LED). Under this framework, each dispatcher keeps local (possibly outdated) estimates of queue lengths for all the servers, and the dispatching decision is made purely based on these local estimates. The local estimates are updated via infrequent communications between dispatchers and servers. We derive sufficient conditions for LED policies to achieve throughput optimality and delay optimality in heavy-traffic, respectively. These conditions directly imply delay optimality for many previous local-memory based policies in heavy traffic. Moreover, the results enable us to design new delay optimal policies for heterogeneous systems with multiple dispatchers. Finally, the heavy-traffic delay optimality of the LED framework directly resolves a recent open problem on how to design optimal load balancing schemes using delayed information.", "venue": "Perform. Evaluation", "authors": ["Xingyu  Zhou", "Ness  Shroff", "Adam  Wierman"], "year": 2021, "n_citations": 6}
{"id": 5757196, "s2_id": "bf924614988b0c8843164689c57e6845467083c5", "title": "Understanding Open Source Serverless Platforms: Design Considerations and Performance", "abstract": "Serverless computing is increasingly popular because of the promise of lower cost and the convenience it provides to users who do not need to focus on server management. This has resulted in the availability of a number of proprietary and open-source serverless solutions. We seek to understand how the performance of serverless computing depends on a number of design issues using several popular open-source serverless platforms. We identify the idiosyncrasies affecting performance (throughput and latency) for different open-source serverless platforms. Further, we observe that just having either resource-based (CPU and memory) or workload-based (request per second (RPS) or concurrent requests) auto-scaling is inadequate to address the needs of the serverless platforms.", "venue": "WOSC@Middleware", "authors": ["Junfeng  Li", "Sameer G. Kulkarni", "K. K. Ramakrishnan", "Dan  Li"], "year": 2019, "n_citations": 22}
{"id": 5759740, "s2_id": "063a58e0a5deffa5ab603f65b52d3112afdad5e0", "title": "CompilerGym: Robust, Performant Compiler Optimization Environments for AI Research", "abstract": "Interest in applying Artificial Intelligence (AI) techniques to compiler optimizations is increasing rapidly, but compiler research has a high entry barrier. Unlike in other domains, compiler and AI researchers do not have access to the datasets and frameworks that enable fast iteration and development of ideas, and getting started requires a significant engineering investment. What is needed is an easy, reusable experimental infrastructure for real world compiler optimization tasks that can serve as a common benchmark for comparing techniques, and as a platform to accelerate progress in the field. We introduce CompilerGym, a set of environments for real world compiler optimization tasks, and a toolkit for exposing new optimization tasks to compiler researchers. CompilerGym enables anyone to experiment on production compiler optimization problems through an easy-to-use package, regardless of their experience with compilers. We build upon the popular OpenAI Gym interface enabling researchers to interact with compilers using Python and a familiar API. We describe the CompilerGym architecture and implementation, characterize the optimization spaces and computational efficiencies of three included compiler environments, and provide extensive empirical evaluations. Compared to prior works, CompilerGym offers larger datasets and optimization spaces, is 27\u00d7 more computationally efficient, is fault-tolerant, and capable of detecting reproducibility bugs in the underlying compilers. In making it easy for anyone to experiment with compilers \u2013 irrespective of their background \u2013 we aim to accelerate progress in the AI and compiler research domains.", "venue": "ArXiv", "authors": ["Chris  Cummins", "Bram  Wasti", "Jiadong  Guo", "Brandon  Cui", "Jason  Ansel", "Sahir  Gomez", "Somya  Jain", "Jia  Liu", "Olivier  Teytaud", "Benoit  Steiner", "Yuandong  Tian", "Hugh  Leather"], "year": 2021, "n_citations": 2}
{"id": 5760444, "s2_id": "b771fb3a80329c1004f5107575cc599e1892c0e6", "title": "A Tutorial on Trace-based Simulations of Mobile Ad-hoc Networks on the Example of Aeronautical Communications", "abstract": "The OMNeT++ simulator is well-suited for the simulation of randomized user behavior in communication networks. However, there are scenarios, where such a random model is unsuited to evaluate a communication system, and this paper attempts to highlight such a case. Using this example of ad-hoc communication between aircraft mid-flight, a tutorialstyle description is attempted that shall show how the OMNeT++ simulator can be used when a wealth of real-world trace data is available. In particular, it is described how mobility trace files can be directly used within OMNeT++, and how to link the generation of data messages to this mobility data. This is explained via an example simulation that evaluates a communication network in which an aircraft notifies the ground control when it enters or leaves a specific geographic region. Additionally, a novel trace-based application has been developed to achieve this link between mobility and message generation. Furthermore, a new TDMA-based medium access protocol for decentralized communication networks is presented, which is oracle-based and thus allows a TDMA-like behavior of medium access without causing any overhead; it can be useful when upper-layer protocols should be evaluated under the assumption of TDMA-like behavior, but isolated from the effects of a full-fledged TDMA protocol. Finally, physical layer behavior is often either overly simplistic or overly computationally expensive. For the latter case, when a detailed channel model is available but its evaluation requires prohibitive computational effort, then averaging its behavior into trace data can find a middle ground between efficient evaluation and realistic representation. Hence, a novel trace-based radio model has been developed that makes use of an SNR to PER mapping. In the spirit of open science, all implementations have been made available under open licenses \u2013 please see the conclusion.", "venue": "ArXiv", "authors": ["Musab Ahmed Eltayeb Ahmed", "Konrad  Fuger", "Sebastian  Lindner", "Fatema  Khan", "Andreas  Timm-Giel"], "year": 2021, "n_citations": 0}
{"id": 5767218, "s2_id": "fcd1c8ad5f8f6aee053f8ad7a5e915101cead6c0", "title": "Self-Healing Dilemmas in Distributed Systems: Fault Correction vs. Fault Tolerance", "abstract": "Large-scale decentralized systems of autonomous agents interacting via asynchronous communication often experience the following self-healing dilemma: fault detection inherits network uncertainties making a remote faulty process indistinguishable from a slow process. In the case of a slow process without fault, fault correction is undesirable as it can trigger new faults that could be prevented with fault tolerance that is a more proactive system maintenance. But in the case of an actual faulty process, fault tolerance alone without eventually correcting persistent faults can make systems underperforming. Measuring, understanding and resolving such self-healing dilemmas is a timely challenge and critical requirement given the rise of distributed ledgers, edge computing, the Internet of Things in several energy, transport and health applications. This paper contributes a novel and general-purpose modeling of fault scenarios during system runtime. They are used to accurately measure and predict inconsistencies generated by the undesirable outcomes of fault correction and fault tolerance as the means to improve self-healing of large-scale decentralized systems at the design phase. A rigorous experimental methodology is designed that evaluates 696 experimental settings of different fault scales, fault profiles and fault detection thresholds in a prototyped decentralized network of 3000 nodes. Almost 9 million measurements of inconsistencies were collected in a network, where each node monitors the health status of another node, while both can defect. The prediction performance of the modeled fault scenarios is validated in a challenging application scenario of decentralized and dynamic in-network data aggregation using real-world data from a Smart Grid pilot project. Findings confirm the origin of inconsistencies at design phase and provide new insights how to tune self-healing at an early stage. Strikingly, the aggregation accuracy is well predicted as shown by high correlations and low root mean square errors.", "venue": "IEEE Transactions on Network and Service Management", "authors": ["Jovan  Nikoli\u0107", "Nursultan  Jubatyrov", "Evangelos  Pournaras"], "year": 2021, "n_citations": 0}
{"id": 5767569, "s2_id": "4675b42900b6dc797fd839fd9b452ceb78f59bcb", "title": "P = FS: Parallel is Just Fast Serial", "abstract": "We prove that parallel processing with homogeneous processors is logically equivalent to fast serial processing. The reverse proposition can also be used to identify obscure opportunities for applying parallelism. To our knowledge, this theorem has not been previously reported in the queueing theory literature. A plausible explanation is offered for why this might be. The basic homogeneous theorem is also extended to optimizing the latency of heterogenous parallel arrays.", "venue": "ArXiv", "authors": ["Neil J. Gunther"], "year": 2020, "n_citations": 0}
{"id": 5770827, "s2_id": "0e44988f0b55689bbebf8d8c499f269b8b13fabc", "title": "On Expert Behaviors and Question Types for Efficient Query-Based Ontology Fault Localization", "abstract": "We challenge existing query-based ontology fault localization methods wrt. assumptions they make, criteria they optimize, and interaction means they use. We find that their efficiency depends largely on the behavior of the interacting expert, that performed calculations can be inefficient or imprecise, and that used optimization criteria are often not fully realistic. As a remedy, we suggest a novel (and simpler) interaction approach which overcomes all identified problems and, in comprehensive experiments on faulty real-world ontologies, enables a successful fault localization while requiring fewer expert interactions in 66 % of the cases, and always at least 80 % less expert waiting time, compared to existing methods.", "venue": "ArXiv", "authors": ["Patrick  Rodler"], "year": 2020, "n_citations": 0}
{"id": 5779319, "s2_id": "abf7aac7d9f69d2e08fbb885ed3af8fe3c3ce6e5", "title": "Generic approach for hierarchical modulation performance analysis: Application to DVB-SH", "abstract": "Broadcasting systems have to deal with channel diversity in order to offer the best rate to the users. Hierarchical modulation is a practical solution to provide several rates in function of the channel quality. Unfortunately the performance evaluation of such modulations requires time consuming simulations. We propose in this paper a novel approach based on the channel capacity to avoid these simulations. The method allows to study the performance in terms of spectrum efficiency of hierarchical and also classical modulations combined with error correcting codes. Our method will be applied to the DVB-SH standard which considers hierarchical modulation as an optional feature.", "venue": "2011 Wireless Telecommunications Symposium (WTS)", "authors": ["Hugo  M\u00e9ric", "J\u00e9r\u00f4me  Lacan", "Caroline  Amiot-Bazile", "Fabrice  Arnal", "Marie-Laure  Boucheret"], "year": 2011, "n_citations": 18}
{"id": 5780689, "s2_id": "0e3edae681d94b425b331c86bb867c42b0681df1", "title": "Performance study and simulation of an anycast protocol for wireless mobile ad hoc networks", "abstract": "This paper conducts a detailed simulation study of stateless anycast routing in a mobile wireless ad hoc network. The model covers all the fundamental aspects of such networks with a routing mechanism using a scheme of orientation-dependent inter-node communication links. The simulation system Winsim is used which explicitly represents parallelism of events and processes in the network. The purpose of these simulations is to investigate the effect of node\u2019s maximum speed, and different TTL over the network performance under two different scenarios. Simulation study investigates five practically important performance metrics of a wireless mobile ad hoc network and shows the dependence of this metrics on the transmission radius, link availability, and maximal possible node speed.", "venue": "ArXiv", "authors": ["Reza  Azizi"], "year": 2013, "n_citations": 4}
{"id": 5782748, "s2_id": "e0baec860e1150fa82c959ca77d23b44241dc255", "title": "Optimal Scheduling and Exact Response Time Analysis for Multistage Jobs", "abstract": "Scheduling to minimize mean response time in an M/G/1 queue is a classic problem. The problem is usually addressed in one of two scenarios. In the perfect-information scenario, the scheduler knows each job's exact size, or service requirement. In the zero-information scenario, the scheduler knows only each job's size distribution. The well-known shortest remaining processing time (SRPT) policy is optimal in the perfect-information scenario, and the more complex Gittins policy is optimal in the zero-information scenario. \nIn real systems the scheduler often has partial but incomplete information about each job's size. We introduce a new job model, that of multistage jobs, to capture this partial-information scenario. A multistage job consists of a sequence of stages, where both the sequence of stages and stage sizes are unknown, but the scheduler always knows which stage of a job is in progress. We give an optimal algorithm for scheduling multistage jobs in an M/G/1 queue and an exact response time analysis of our algorithm.", "venue": "ArXiv", "authors": ["Ziv  Scully", "Mor  Harchol-Balter", "Alan  Scheller-Wolf"], "year": 2018, "n_citations": 6}
{"id": 5782835, "s2_id": "0ba6cbb7c7a9ae4589dd45b70bb6316ee61bdb0d", "title": "A two-queue polling model with two priority levels in the first queue", "abstract": "In this paper we consider a single-server cyclic polling system consisting of two queues. Between visits to successive queues, the server is delayed by a random switch-over time. Two types of customers arrive at the first queue: high and low priority customers. For this situation the following service disciplines are considered: gated, globally gated, and exhaustive. We study the cycle time distribution, the waiting times for each customer type, the joint queue length distribution at polling epochs, and the steady-state marginal queue length distributions for each customer type.", "venue": "Valuetools 2008", "authors": ["Marko  Boon", "Ivo  Adan", "Onno  Boxma"], "year": 2008, "n_citations": 8}
{"id": 5783270, "s2_id": "dce3d2362fa245e0dd39bc54b237bac25b28523a", "title": "State-of-the-Art on Query & Transaction Processing Acceleration", "abstract": "The vast amount of processing power and memory bandwidth provided by modern Graphics Processing Units (GPUs) make them a platform for data-intensive applications. The database community identified GPUs as effective co-processors for data processing. In the past years, there were many approaches to make use of GPUs at different levels of a database system. In this Internal Technical Report, based on the [1] and some other research papers, we identify possible research areas at LIP6 for GPU-accelerated database management systems. We describe some key properties, typical challenges of GPU-aware database architectures, and identify major open challenges.", "venue": "ArXiv", "authors": ["Bernd  Amann", "Youry  Khmelevsky", "Gaetan  Hains"], "year": 2019, "n_citations": 0}
{"id": 5783277, "s2_id": "d3d23b78366fa0bb3d566acd7f810851e640752f", "title": "D-SPACE4Cloud: A Design Tool for Big Data Applications", "abstract": "The last years have seen a steep rise in data generation worldwide, with the development and widespread adoption of several software projects targeting the Big Data paradigm. Many companies currently engage in Big Data analytics as part of their core business activities, nonetheless there are no tools and techniques to support the design of the underlying hardware configuration backing such systems. In particular, the focus in this report is set on Cloud deployed clusters, which represent a cost-effective alternative to on premises installations. We propose a novel tool implementing a battery of optimization and prediction techniques integrated so as to efficiently assess several alternative resource configurations, in order to determine the minimum cost cluster deployment satisfying QoS constraints. Further, the experimental campaign conducted on real systems shows the validity and relevance of the proposed method.", "venue": "ICA3PP", "authors": ["Michele  Ciavotta", "Eugenio  Gianniti", "Danilo  Ardagna"], "year": 2016, "n_citations": 8}
{"id": 5783325, "s2_id": "3243455831d2d27a95dc074ebaf0bdef68b217c7", "title": "An Effective Early Multi-core System Shared Cache Design Method Based on Reuse-distance Analysis", "abstract": "In this paper, we proposed an effective and efficient multi-core shared-cache design optimization approach based on reuse-distance analysis of the data traces of target applications. Since data traces are independent of system hardware architectures, a designer can easily compute the best cache design at early system design phase using our approach. We devise a very efficient and yet accurate method to derive the aggregated reuse-distance histograms of concurrent applications for accurate cache performance analysis and optimization. Essentially, the actual shared-cache contention results of concurrent applications are embedded in the aggregated reuse-distance histograms and therefore the approach is very effective. The experimental results show that the average error rate of shared-cache miss-count estimations of our approach is less than 2.4%. Using a simple scanning search method, one can easily determine the true optimal cache configurations at early system design phase.", "venue": "ArXiv", "authors": ["Hsin-Yu  Ho", "Ren-Song  Tsay"], "year": 2021, "n_citations": 0}
{"id": 5784500, "s2_id": "50035aabb7101a3eb00ac009c1b633472e73f386", "title": "On the Representation of Correlated Exponential Distributions by Phase Type Distributions", "abstract": "In this paper we present results for bivariate exponential distributions which are represented by phase type distributions. The paper extends results from previous publications [5, 14] on this topic by introducing new representations that require a smaller number of phases to reach some correlation coefficient and introduces different ways to describe correlation between exponentially distributed random variables. Furthermore, it is shown how Markovian Arrival Processes (MAPs) with exponential marginal distribution can be generated from the phase type representations of exponential distributions and how the results for exponential distributions can be applied to define correlated hyperexponential or Erlang distributions. As application examples we analyze two queueing models with correlated inter-arrival and service times.", "venue": "ArXiv", "authors": ["Peter  Buchholz"], "year": 2021, "n_citations": 0}
{"id": 5786108, "s2_id": "14d79838b4068d6c9a0804e766d406a8081eb58c", "title": "Analytical Process Scheduling Optimization for Heterogeneous Multi-core Systems", "abstract": "In this paper, we propose the first optimum process scheduling algorithm for an increasingly prevalent type of heterogeneous multicore (HEMC) system that combines high-performance big cores and energy-efficient small cores with the same instruction-set architecture (ISA). Existing algorithms are all heuristics-based, and the well-known IPC-driven approach essentially tries to schedule high scaling factor processes on big cores. Our analysis shows that, for optimum solutions, it is also critical to consider placing long running processes on big cores. Tests of SPEC 2006 cases on various big-small core combinations show that our proposed optimum approach is up to 34% faster than the IPC-driven heuristic approach in terms of total workload completion time. The complexity of our algorithm is O(NlogN) where N is the number of processes. Therefore, the proposed optimum algorithm is practical for use.", "venue": "ArXiv", "authors": ["Chien-Hao  Chen", "Ren-Song  Tsay"], "year": 2021, "n_citations": 0}
{"id": 5786180, "s2_id": "4d049b4f815695b19bfcf2d1561200d20ae72310", "title": "Guidelines for benchmarking of optimization approaches for fitting mathematical models", "abstract": "Insufficient performance of optimization approaches for fitting of mathematical models is still a major bottleneck in systems biology. In this manuscript, the reasons and methodological challenges are summarized as well as their impact in benchmark studies. Important aspects for increasing evidence of outcomes of benchmark analyses are discussed. Based on general guidelines for benchmarking in computational biology, a collection of tailored guidelines is presented for performing informative and unbiased benchmarking of optimization-based fitting approaches. Comprehensive benchmark studies based on these recommendations are urgently required for establishing of a robust and reliable methodology for the systems biology community.", "venue": "ArXiv", "authors": ["Clemens  Kreutz"], "year": 2019, "n_citations": 1}
{"id": 5797789, "s2_id": "6962e48dc674a9f46e80d31028d8b40022557ca8", "title": "Delay-Based Back-Pressure Scheduling in Multihop Wireless Networks", "abstract": "Scheduling is a critical and challenging resource allocation mechanism for multihop wireless networks. It is well known that scheduling schemes that favor links with larger queue length can achieve high throughput performance. However, these queue-length-based schemes could potentially suffer from large (even infinite) packet delays due to the well-known last packet problem, whereby packets belonging to some flows may be excessively delayed due to lack of subsequent packet arrivals. Delay-based schemes have the potential to resolve this last packet problem by scheduling the link based on the delay the packet has encountered. However, characterizing throughput optimality of these delay-based schemes has largely been an open problem in multihop wireless networks (except in limited cases where the traffic is single-hop.) In this paper, we investigate delay-based scheduling schemes for multihop traffic scenarios with fixed routes. We develop a scheduling scheme based on a new delay metric and show that the proposed scheme achieves optimal throughput performance. Furthermore, we conduct simulations to support our analytical results and show that the delay-based scheduler successfully removes excessive packet delays, while it achieves the same throughput region as the queue-length-based scheme.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Bo  Ji", "Changhee  Joo", "Ness B. Shroff"], "year": 2011, "n_citations": 92}
{"id": 5800189, "s2_id": "54868145f7387a8d3d06f1cec0dc4cb0b7219cbe", "title": "Global Stability Analysis for an Internet Congestion Control Model with a Time-Varying Link Capacity", "abstract": "In this paper, a global stability analysis is given for a rate-based congestion control system modeled by a nonlinear delayed differential equation. The model determines the dynamics of a single-source single-link network, with a time-varying capacity of link and a fixed communication delay. We obtain a sufficient delay-independent conditions on system parameters under which global asymptotic stability of the system is guarantied. The proof is based on an extension of Lyapunov-Krasovskii theorem for a class of nonlinear time-delay systems. The numerical simulations for a typical scenario justify the theoretical results.", "venue": "ArXiv", "authors": ["Behrooz  Rezaie", "Mohammad Reza Jahed-Motlagh", "Morteza  Analoui", "Siavash  Khorsandi"], "year": 2009, "n_citations": 0}
{"id": 5804480, "s2_id": "269bdedb58606a391a550ef6bb11d560110b5f49", "title": "Proof-of-Concept Examples of Performance-Transparent Programming Models", "abstract": "Machine-specific optimizations command the machine to behave in a specific way. As current programming models largely leave machine details unexposed, they cannot accommodate direct encoding of such commands. In previous work we have proposed the design of performance-transparent programming models to facilitate this use-case; this report contains proof-of-concept examples of such programming models. We demonstrate how programming model abstractions may reveal the memory footprint, vector unit utilization and data reuse of an application, with prediction accuracy ranging from 0 to 25 \\%.", "venue": "ArXiv", "authors": ["Benjamin Andreassen Bj\u00f8rnseth", "Jan Christian Meyer", "Lasse  Natvig"], "year": 2018, "n_citations": 1}
{"id": 5805934, "s2_id": "60250e60fe668e2839f06f38a5bd6f641c434e47", "title": "Lossy Bulk Synchronous Parallel Processing Model for Very Large Scale Grids", "abstract": "The performance of a parallel algorithm in a very large scale grid is significantly influenced by the underlying Internet protocols and inter-connectivity. Many grid programming platforms use TCP due to its reliability, usually with some optimizations to reduce its costs. However, TCP does not perform well in a high bandwidth and high delay network environment. On the other hand, UDP is the fastest protocol available because it omits connection setup process, acknowledgments and retransmissions sacrificing reliable transfer. Many new bulk data transfer schemes using UDP for data transmission such as RBUDP, Tsunami, and SABUL have been introduced and shown to have better performance compared to TCP. In this paper, we consider the use of UDP and examine the relationship between packet loss and speedup with respect to the number of grid nodes. Our measurement suggests that packet loss rates between 5%-15% on average are not uncommon between PlanetLab nodes that are widely distributed over the Internet. We show that transmitting multiple copies of same packet produces higher speedup. We show the minimum number of packet duplication required to maximize the possible speedup for a given number of nodes using a BSP based model. Our work demonstrates that by using an appropriate number of packet copies, we can increase performance of parallel program.", "venue": "ArXiv", "authors": ["Elankovan  Sundararajan", "Aaron  Harwood", "Kotagiri  Ramamohanarao"], "year": 2006, "n_citations": 4}
{"id": 5809521, "s2_id": "1b2bdbae8451cbb56be2649aba3c818b6d0eb16a", "title": "Best practices for HPM-assisted performance engineering on modern multicore processors", "abstract": "Many tools and libraries employ hardware performance monitoring (HPM) on modern processors, and using this data for performance assessment and as a starting point for code optimizations is very popular. However, such data is only useful if it is interpreted with care, and if the right metrics are chosen for the right purpose. We demonstrate the sensible use of hardware performance counters in the context of a structured performance engineering approach for applications in computational science. Typical performance patterns and their respective metric signatures are defined, and some of them are illustrated using case studies. Although these generic concepts do not depend on specific tools or environments, we restrict ourselves to modern x86-based multicore processors and use the likwid-perfctr tool under the Linux OS.", "venue": "ArXiv", "authors": ["Jan  Treibig", "Georg  Hager", "Gerhard  Wellein"], "year": 2012, "n_citations": 11}
{"id": 5811637, "s2_id": "a76e9d92d64c3d24ae9ce8571f83dac31d59f088", "title": "Optimal Hyper-Scalable Load Balancing with a Strict Queue Limit", "abstract": "Load balancing plays a critical role in efficiently dispatching jobs in parallel-server systems such as cloud networks and data centers. A fundamental challenge in the design of load balancing algorithms is to achieve an optimal trade-off between delay performance and implementation overhead (e.g. communication or memory usage). This trade-off has primarily been studied so far from the angle of the amount of overhead required to achieve asymptotically optimal performance, particularly vanishing delay in large-scale systems. In contrast, in the present paper, we focus on an arbitrarily sparse communication budget, possibly well below the minimum requirement for vanishing delay, referred to as the hyper-scalable operating region. Furthermore, jobs may only be admitted when a specific limit on the queue position of the job can be guaranteed. \nThe centerpiece of our analysis is a universal upper bound for the achievable throughput of any dispatcher-driven algorithm for a given communication budget and queue limit. We also propose a specific hyper-scalable scheme which can operate at any given message rate and enforce any given queue limit, while allowing the server states to be captured via a closed product-form network, in which servers act as customers traversing various nodes. The product-form distribution is leveraged to prove that the bound is tight and that the proposed hyper-scalable scheme is throughput-optimal in a many-server regime given the communication and queue limit constraints. Extensive simulation experiments are conducted to illustrate the results.", "venue": "Perform. Evaluation", "authors": ["Mark van der Boor", "Sem  Borst", "Johan van Leeuwaarden"], "year": 2021, "n_citations": 0}
{"id": 5813063, "s2_id": "320188a9ee55b220bd4866dd823ca2caf5830ac7", "title": "An optimal scheduling architecture for accelerating batch algorithms on Neural Network processor architectures", "abstract": "In neural network topologies, algorithms are running on batches of data tensors. The batches of data are typically scheduled onto the computing cores which execute in parallel. For the algorithms running on batches of data, an optimal batch scheduling architecture is very much needed by suitably utilizing hardware resources - thereby resulting in significant reduction training and inference time. In this paper, we propose to accelerate the batch algorithms for neural networks through a scheduling architecture enabling optimal compute power utilization. The proposed optimal scheduling architecture can be built into HW or can be implemented in SW alone which can be leveraged for accelerating batch algorithms. The results demonstrate that the proposed architecture speeds up the batch algorithms compared to the previous solutions. The proposed idea applies to any HPC architecture meant for neural networks.", "venue": "ArXiv", "authors": ["Phani Kumar Nyshadham", "Mohit  Sinha", "Biswajit  Mishra", "H S Vijay"], "year": 2020, "n_citations": 0}
{"id": 5818945, "s2_id": "8502763c39f77354d2045d91b5e0865d0fd3a35c", "title": "Performance Analysis of Non-DC-Biased OFDM", "abstract": "The performance analysis of a novel optical modulation scheme is presented in this paper. The basic concept is to transmit signs of modulated optical orthogonal frequency division multiplexing (O-OFDM) symbols and absolute values of the symbols separately by two information carrying units: 1) indices of two light emitting diodes (LED) transmitters that represent positive and negative signs separately; and 2) optical intensity symbols that carry the absolute values of signals. The new approach, referred as to non-DC-biased OFDM (NDC-OFDM), uses the optical spatial modulation (OSM) technique to eliminate the effect of the clipping distortion in DC-biased optical OFDM (DCO-OFDM). In addition, it can achieve similar advantages as the conventional unipolar modulation scheme, asymmetrically clipped optical OFDM (ACO-OFDM), without using additional subcarriers. In this paper, the analytical BER performance is compared with the Monte Carlo result in order to prove the reliability of the new method. Moreover, the practical BER performance of NDC-OFDM with DCO-OFDM and ACO-OFDM is compared for different constellation sizes to verify the improvement of NDC-OFDM on the spectral and power efficiencies.", "venue": "ArXiv", "authors": ["Yichen  Li", "Dobroslav  Tsonev", "Harald  Haas"], "year": 2019, "n_citations": 2}
{"id": 5822535, "s2_id": "8c51cd04be72721166c44f65279f9b9bebf423d1", "title": "Parallel Sparse Matrix-Matrix Multiplication and Indexing: Implementation and Experiments", "abstract": "Generalized sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many high performance graph algorithms as well as for some linear solvers, such as algebraic multigrid. Here we show that SpGEMM also yields efficient algorithms for general sparse-matrix indexing in distributed memory, provided that the underlying SpGEMM implementation is sufficiently flexible and scalable. We demonstrate that our parallel SpGEMM methods, which use two-dimensional block data distributions with serial hypersparse kernels, are indeed highly flexible, scalable, and memory-efficient in the general case. This algorithm is the first to yield increasing speedup on an unbounded number of processors; our experiments show scaling up to thousands of processors in a variety of test scenarios.", "venue": "SIAM J. Sci. Comput.", "authors": ["Aydin  Bulu\u00e7", "John R. Gilbert"], "year": 2012, "n_citations": 164}
{"id": 5828655, "s2_id": "f0158f329babc053f17b75a79626b7df7ee84f6d", "title": "PerfVis: Pervasive Visualization in Immersive Augmented Reality for Performance Awareness", "abstract": "Developers are usually unaware of the impact of code changes to the performance of software systems. Although developers can analyze the performance of a system by executing, for instance, a performance test to compare the performance of two consecutive versions of the system, changing from a programming task to a testing task would disrupt the development flow. In this paper, we propose the use of a city visualization that dynamically provides developers with a pervasive view of the continuous performance of a system. We use an immersive augmented reality device (Microsoft HoloLens) to display our visualization and extend the integrated development environment on a computer screen to use the physical space. We report on technical details of the design and implementation of our visualization tool, and discuss early feedback that we collected of its usability. Our investigation explores a new visual metaphor to support the exploration and analysis of possibly very large and multidimensional performance data. Our initial result indicates that the city metaphor can be adequate to analyze dynamic performance data on a large and non-trivial software system.", "venue": "ICPE Companion", "authors": ["Leonel  Merino", "Mario  Hess", "Alexandre  Bergel", "Oscar  Nierstrasz", "Daniel  Weiskopf"], "year": 2019, "n_citations": 10}
{"id": 5844516, "s2_id": "fddf6d55501bc37bea19f5358831e3d2b8ad72f0", "title": "The Theoretical Limit of Radar Target Detection", "abstract": "In the field of radar target detection, the false alarm and detection probabilities are used as the universal indicator for detection performance evaluation so far, such as Neyman-Person detector. In this paper, inspired by the thoughts of Shannon\u2019s information theory, the new system model introducing the target existent state variable v into a general radar system model is established for target detection in the presence of complex white Gaussian noise. We make several main contributions: (1) the equivalent detection channel and the posterior probability distribution are derived based on the priori statistical characteristic of the noise, target scattering and existent state; (2) the detection performance is measured by the false alarm and detection probabilities and the detection information that is defined as the mutual information between received signal and existent state; (3) the false alarm theorem is proved that false alarm probability is equal to the prior probability of the target existence if the observation interval is large enough and the theorem is the basis for the performance comparison proposed detector with Neyman-Person detector; (4) the sampling a posterior probability detector is proposed, and its performance is measured by the empirical detection information that is dependent on the posterior probability distribution; (5) the target detection theorem is proved that the detection information is the limit of the detection performance, that is, the detection information is achievable and the empirical detection information of any detector is no greater than the detection information. Simulation results verify the correctness of the false alarm and the target detection theorems, and show that the performance of the sampling a posterior probability detector is asymptotically optimal and outperforms Dazhuan Xu is the Corresponding Author, e-mail: xudazhuan@nuaa.edu.cn. ar X iv :2 10 9. 09 94 1v 1 [ cs .I T ] 2 1 Se p 20 21 2 JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, SEPTEMBER 2021 other detectors. In addition, the detector is more favorable to detect the dim targets under the detection information than other detectors.", "venue": "ArXiv", "authors": ["Dazhuan  Xu", "Nan  Wang"], "year": 2021, "n_citations": 0}
{"id": 5844607, "s2_id": "d41ffc2a65032b1fe079521f0762c5b43268c8b1", "title": "Exact solutions for the two- and all-terminal reliabilities of a simple ladder network", "abstract": "The exact calculation of network reliability in a probabilistic context has been a long-standing issue of practical importance, but a difficult one, even for planar graphs, with perfect nodes and with edges of identical reliability p. Many approaches (determination of bounds, sums of disjoint products algorithms, Monte Carlo evaluations, studies of the reliability polynomials, etc.) can only provide approximations when the network's size increases. We consider here a ladder graph of arbitrary size corresponding to real-life network configurations, and give the exact, analytical solutions for the all- and two-terminal reliabilities. These solutions use transfer matrices, in which individual reliabilities of edges and nodes are taken into account. The special case of identical edge and node reliabilities -- p and rho, respectively -- is solved. We show that the zeros of the two-terminal reliability polynomial exhibit structures which differ substantially for seemingly similar networks, and we compare the sensitivity of various edges. We discuss how the present work may be further extended to lead to a catalog of exactly solvable networks in terms of reliability, which could be useful as elementary bricks for a new and improved set of bounds or benchmarks in the general case.", "venue": "ArXiv", "authors": ["Christian  Tanguy"], "year": 2006, "n_citations": 9}
{"id": 5847981, "s2_id": "3bb9962b83934b6d213d4f0f784ec951f78e615a", "title": "Towards Adaptive Resilience in High Performance Computing", "abstract": "Failure rates in high performance computers rapidly increase due to the growth in system size and complexity. Hence, failures became the norm rather than the exception. Different approaches on high performance computing (HPC) systems have been introduced, to prevent failures (e. g., redundancy) or at least minimize their impacts (e. g., checkpoint and restart). In most cases, when these approaches are employed to increase the resilience of certain parts of a system, energy consumption rapidly increases, or performance significantly degrades. To address this challenge, we propose on-demand resilience as an approach to achieve adaptive resilience in HPC systems. In this work, the HPC system is considered in its entirety and resilience mechanisms such as checkpointing, isolation, and migration, are activated on-demand. Using the proposed approach, the unavoidable increase in total energy consumption and system performance degradation is decreased compared to the typical checkpoint/restart and redundant resilience mechanisms. Our work aims to mitigate a large number of failures occurring at various layers in the system, to prevent their propagation, and to minimize their impact, all of this in an energy-saving manner. In the case of failures that are estimated to occur but cannot be mitigated using the proposed on-demand resilience approach, the system administrators will be notified in view of performing further investigations into the causes of these failures and their impacts.", "venue": "ArXiv", "authors": ["Siavash  Ghiasvand", "Florina M. Ciorba"], "year": 2017, "n_citations": 1}
{"id": 5848131, "s2_id": "c07211ecd6cbae3c69660c3d0478d30f130ea6f8", "title": "DisCo: Physics-Based Unsupervised Discovery of Coherent Structures in Spatiotemporal Systems", "abstract": "Extracting actionable insight from complex unlabeled scientific data is an open challenge and key to unlocking data-driven discovery in science. Complementary and alternative to supervised machine learning approaches, unsupervised physics-based methods based on behavior-driven theories hold great promise. Due to computational limitations, practical application on real-world domain science problems has lagged far behind theoretical development. However, powerful modern supercomputers provide the opportunity to narrow the gap between theory and practical application. We present our first step towards bridging this divide - DisCo - a high-performance distributed workflow for the behavior-driven local causal state theory. DisCo provides a scalable unsupervised physics-based representation learning method that decomposes spatiotemporal systems into their structurally relevant components, which are captured by the latent local causal state variables. In several firsts we demonstrate the efficacy of DisCo in capturing physically meaningful coherent structures from observational and simulated scientific data. To the best of our knowledge, DisCo is also the first application software developed entirely in Python to scale to over 1000 machine nodes, providing good performance along with ensuring domain scientists' productivity. Our capstone experiment, using newly developed and optimized DisCo workflow and libraries, performs unsupervised spacetime segmentation analysis of CAM5.1 climate simulation data, processing an unprecedented 89.5 TB in 6.6 minutes end-to-end using 1024 Intel Haswell nodes on the Cori supercomputer obtaining 91% weak-scaling and 64% strong-scaling efficiency. This enables us to achieve state-of-the-art unsupervised segmentation of coherent spatiotemporal structures in complex fluid flows.", "venue": "2019 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC)", "authors": ["Adam  Rupe", "Nalini  Kumar", "Vladislav  Epifanov", "Karthik  Kashinath", "Oleksandr  Pavlyk", "Frank  Schlimbach", "Mostofa  Patwary", "Sergey  Maidanov", "Victor  Lee", "Prabhat", "James P. Crutchfield"], "year": 2019, "n_citations": 8}
{"id": 5850538, "s2_id": "7b0586d723b2c902df6195a2d79df7df1365a233", "title": "Sorting Data on Ultra-Large Scale with RADULS - New Incarnation of Radix Sort", "abstract": "The paper introduces RADULS, a new parallel sorter based on radix sort algorithm, intended to organize ultra-large data sets efficiently. For example 4 G 16-byte records can be sorted with 16 threads in less than 15 s on Intel Xeon-based workstation. The implementation of RADULS is not only highly optimized to gain such an excellent performance, but also parallelized in a cache friendly manner to make the most of modern multicore architectures. Besides, our parallel scheduler launches a few different procedures at runtime, according to the current parameters of the execution, for proper workload management. All experiments show RADULS to be superior to competing algorithms.", "venue": "BDAS", "authors": ["Marek  Kokot", "Sebastian  Deorowicz", "Agnieszka  Debudaj-Grabysz"], "year": 2017, "n_citations": 6}
{"id": 5852743, "s2_id": "dd491887e18f4f4cb2d9e45c506a781987cd688e", "title": "Optimal Power Allocation for Outage Minimization in Fading Channels with Energy Harvesting Constraints", "abstract": "This paper studies the optimal power allocation for outage minimization in point-to-point fading channels with the energy-harvesting constraints and channel distribution information (CDI) at the transmitter. Both the cases with non-causal and causal energy state information (ESI) are considered, which correspond to the energy harvesting rates being known and unknown prior to the transmissions, respectively. For the non-causal ESI case, the average outage probability minimization problem over a finite horizon is shown to be non-convex for a large class of practical fading channels. However, the globally optimal \"offline\" power allocation is obtained by a forward search algorithm with at most $N$ one-dimensional searches, and the optimal power profile is shown to be non-decreasing over time and have an interesting \"save-then-transmit\" structure. In particular, for the special case of N=1, our result revisits the classic outage capacity for fading channels with uniform power allocation. Moreover, for the case with causal ESI, we propose both the optimal and suboptimal \"online\" power allocation algorithms, by applying the technique of dynamic programming and exploring the structure of optimal offline solutions, respectively.", "venue": "ArXiv", "authors": ["Chuan  Huang", "Rui  Zhang", "Shuguang  Cui"], "year": 2012, "n_citations": 8}
{"id": 5856583, "s2_id": "48751b5182b0e16851bbd1effe04fa5dba5f95f5", "title": "A Distributed Algorithm for Training Augmented Complex Adaptive IIR Filters", "abstract": "In this paper we consider the problem of decentralized (distributed) adaptive learning, where the aim of the network is to train the coefficients of a widely linear autoregressive moving average (ARMA) model by measurements collected by the nodes. Such a problem arises in many sensor network-based applications such as target tracking, fast rerouting, data reduction and data aggregation. We assume that each node of the network uses the augmented complex adaptive infinite impulse response (ACAIIR) filter as the learning rule, and nodes interact with each other under an incremental mode of cooperation. Since the proposed algorithm (incremental augmented complex IIR (IACA-IIR) algorithm) relies on the augmented complex statistics, it can be used to model both types of complex-valued signals (proper and improper signals). To evaluate the performance of the proposed algorithm, we use both synthetic and real-world complex signals in our simulations. The results exhibit superior performance of the proposed algorithm over the non-cooperative ACAIIR algorithm.", "venue": "ArXiv", "authors": ["Azam  Khalili", "Reza G. Rahmati", "Amir  Rastegarnia", "Wael  Bazzi"], "year": 2016, "n_citations": 0}
{"id": 5858772, "s2_id": "bf5243429b1a8a566760c6d38ac5eddb808764c0", "title": "Tiling for Performance Tuning on Different Models of GPUs", "abstract": "The strategy of using CUDA-compatible GPUs as a parallel computation solution to improve the performance of programs has been more and more widely approved during the last two years since the CUDA platform was released. Its benefit extends from the graphic domain to many other computationally intensive domains. Tiling, as the most general and important technique, is widely used for optimization in CUDA programs. New models of GPUs with better compute capabilities have, however, been released, new versions of CUDA SDKs were also released. These updated compute capabilities must to be considered when optimizing using the tiling technique. In this paper, we implement image interpolation algorithms as a test case to discuss how different tiling strategies affect the program\u2019s performance. We especially focus on how the different models of GPUs affect the tiling\u2019s effectiveness by executing the same program on two different models of GPUs equipped testing platforms. The results demonstrate that an optimized tiling strategy on one GPU model is not always a good solution when execute on other GPU models, especially when some external conditions were changed.", "venue": "2009 Second International Symposium on Information Science and Engineering", "authors": ["Chang  Xu", "Steven R. Kirk", "Samantha  Jenkins"], "year": 2009, "n_citations": 14}
{"id": 5858896, "s2_id": "dec1bd5b0ba86846a94544e97f4a861f201d7f96", "title": "On the Reproducibility of Experiments of Indexing Repetitive Document Collections", "abstract": "Abstract This work introduces a companion reproducible paper with the aim of allowing the exact replication of the methods, experiments, and results discussed in a previous work Claude et\u00a0al., (2016). In that parent paper, we proposed many and varied techniques for compressing indexes which exploit that highly repetitive collections are formed mostly of documents that are near-copies of others. More concretely, we describe a replication framework, called uiHRDC \u00a0(universal indexes for Highly Repetitive Document Collections), that allows our original experimental setup to be easily replicated using various document collections. The corresponding experimentation is carefully explained, providing precise details about the parameters that can be tuned for each indexing solution. Finally, note that we also provide uiHRDC \u00a0as reproducibility package.", "venue": "CIRCLE", "authors": ["Antonio  Fari\u00f1a", "Miguel A. Mart\u00ednez-Prieto", "Francisco  Claude", "Gonzalo  Navarro", "Juan J. Lastra-D\u00edaz", "Nicola  Prezza", "Diego  Seco"], "year": 2020, "n_citations": 3}
{"id": 5859719, "s2_id": "e608ff1f04afd21a34de9a2948b01bf130aa8f45", "title": "Designing installations for verification of the model of active queue management discipline RED in the GNS3", "abstract": "The problem of RED-module mathematical model results verification, based on GNS3 experimental stand, is discussed in this article. The experimental stand consists of virtual Cisco router, traffic generator D-ITG and traffic receiver. The process of construction of such stand is presented. Also, the interaction between experimental stand and a computer of investigation in order to obtain and analyze data from stand is revised.", "venue": "2014 6th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT)", "authors": ["Tatiana R. Velieva", "Anna V. Korolkova", "Dmitry S. Kulyabov"], "year": 2014, "n_citations": 18}
{"id": 5864991, "s2_id": "abc34f44973dff4df997d38b033c1e8e6d826e58", "title": "Looking into Hardware-in-the-Loop Coupling of OMNeT++ and RoSeNet", "abstract": "Network emulation using real sensor node hardware is used to increase the accuracy of pure network simulations. Coupling OMNeT++ with network emulation platforms and tools introduces new application possibilities for both sides. This work-in-progress report covers our experiences of using OMNeT++ as a test driver for RoSeNet, a network emulation and test platform for low-power wireless technologies like IEEE 802.15.4. OMNeT++ and RoSeNet were interconnected to enable a co-simulation of real sensor networks with a MAC layer simulation model. Experiences and insights on this Hardware-in-the-Loop (HIL) simulation together with ideas to extend OMNeT++ and to provide a generic interconnection API complete the report.", "venue": "ArXiv", "authors": ["Sebastian  B\u00f6hm", "Michael  Kirsche"], "year": 2015, "n_citations": 7}
{"id": 5865345, "s2_id": "02224a35d8328c993e7b949b9275643974374249", "title": "Narses: A Scalable Flow-Based Network Simulator", "abstract": "Most popular, modern network simulators, such as ns, are targeted towards simulating low-level protocol details. These existing simulators are not intended for simulating large distributed applications with many hosts and many concurrent connections over long periods of simulated time. We introduce a new simulator, Narses, targeted towards large distributed applications. The goal of Narses is to simulate and validate large applications efficiently using network models of varying levels of detail. We introduce several simplifying assumptions that allow our simulator to scale to the needs of large distributed applications while maintaining a reasonable degree of accuracy. Initial results show up to a 45 times speedup while consuming 28% of the memory used by ns. Narses maintains a reasonable degree of accuracy -- within 8% on average.", "venue": "ArXiv", "authors": ["Thomas J. Giuli", "Mary  Baker"], "year": 2002, "n_citations": 65}
{"id": 5867099, "s2_id": "52730b88942494736d72c50e3049b05f9e7cefb4", "title": "Bandwidth-Aware Scheduling With SDN in Hadoop: A New Trend for Big Data", "abstract": "Software-defined networking (SDN)\u00a0is a revolutionary network architecture that separates out network control functions from the underlying equipment and is an increasing trend to help enterprises build more manageable data centers where big data processing emerges as an important part of applications. To concurrently process large-scale data, MapReduce with an open-source implementation named Hadoop is proposed. In practical Hadoop systems, one kind of issue that vitally impacts the overall performance is known as the NP-complete minimum make span11Make span means the time between job\u2019s start time and job\u2019s finish time. problem. One main solution is to assign tasks on data local nodes to avoid link occupation since network bandwidth is a scarce resource. Many methodologies for enhancing data locality are proposed such as the Hadoop default scheduler (HDS)\u00a0and state-of-the-art scheduler balance-reduce scheduler (BAR). However, all of them either ignore allocating tasks in a global view or disregard available bandwidth as the basis for scheduling. In this paper, we propose a heuristic bandwidth-aware task scheduler bandwidth-aware scheduling with SDN in Hadoop (BASS)\u00a0to combine Hadoop with SDN. It is not only able to guarantee data locality in a global view but also can efficiently assign tasks in an optimized way. Both examples and experiments demonstrate that BASS has the best performance in terms of job completion time. To our knowledge, BASS is the first to exploit talent of SDN for job scheduling of big data processing and we believe that it points out a new trend for large-scale data processing.", "venue": "IEEE Systems Journal", "authors": ["Peng  Qin", "Bin  Dai", "Benxiong  Huang", "Guan  Xu"], "year": 2017, "n_citations": 86}
{"id": 5868664, "s2_id": "11c3d3b9cf697910903e342719904a2c45684ac9", "title": "Evaluating kernels on Xeon Phi to accelerate Gysela application", "abstract": "This work describes the challenges presented by porting parts ofthe Gysela code to the Intel Xeon Phi coprocessor, as well as techniques used for optimization, vectorization and tuning that can be applied to other applications. We evaluate the performance of somegeneric micro-benchmark on Phi versus Intel Sandy Bridge. Several interpolation kernels useful for the Gysela application are analyzed and the performance are shown. Some memory-bound and compute-bound kernels are accelerated by a factor 2 on the Phi device compared to Sandy architecture. Nevertheless, it is hard, if not impossible, to reach a large fraction of the peek performance on the Phi device,especially for real-life applications as Gysela. A collateral benefit of this optimization and tuning work is that the execution time of Gysela (using 4D advections) has decreased on a standard architecture such as Intel Sandy Bridge.", "venue": "ArXiv", "authors": ["Guillaume  Latu", "Matthieu  Haefele", "Julien  Bigot", "Virginie  Grandgirard", "Thomas  Cartier-Michaud", "Fabien  Rozar"], "year": 2015, "n_citations": 4}
{"id": 5869033, "s2_id": "ec2f24485e4f3dc0541f1d8f9086704fe6cc9f3e", "title": "DINAMITE: A modern approach to memory performance profiling", "abstract": "Diagnosing and fixing performance problems on multicore machines with deep memory hierarchies is extremely challenging. Certain problems are best addressed when we can analyze the entire trace of program execution, e.g., every memory access. Unfortunately such detailed execution logs are very large and cannot be analyzed by direct inspection. We present DINAMITE: a toolkit for Dynamic INstrumentation and Analysis for MassIve Trace Exploration. DINAMITE is a collection of tools for end-to-end performance analysis: from the LLVM compiler pass that instruments the program to plug-and-play tools that use a modern data analytics engine Spark Streaming for trace introspection. Using DINAMITE we found opportunities to improve data layout in several applications that resulted in 15-20% performance improvements and found a shared-variable bottleneck in a popular key-value store, whose elimination improved performance by 20x.", "venue": "ArXiv", "authors": ["Svetozar  Miucin", "Conor  Brady", "Alexandra  Fedorova"], "year": 2016, "n_citations": 4}
{"id": 5869357, "s2_id": "e47c046a1837fb25e4da091e4d36a3ed5cd45604", "title": "A Tool for Automatically Suggesting Source-Code Optimizations for Complex GPU Kernels", "abstract": "Future computing systems, from handhelds to supercomputers, will undoubtedly be more parallel and heterogeneous than todays systems to provide more performance and energy efficiency. Thus, GPUs are increasingly being used to accelerate general purpose applications, including applications with data dependent, irregular control flow and memory access patterns. However, the growing complexity, exposed memory hierarchy, incoherence, heterogeneity, and parallelism will make accelerator based systems progressively more difficult to program. In the foreseeable future, the vast majority of programmers will no longer be able to extract additional performance or energy savings from next generation systems be-cause the programming will be too difficult. Automatic performance analysis and optimization recommendation tools have the potential to avert this situation. They embody expert knowledge and make it available to software developers when needed. In this paper, we describe and evaluate such a tool.", "venue": "ArXiv", "authors": ["Saeed  Taheri", "Apan  Qasem", "Martin  Burtscher"], "year": 2019, "n_citations": 0}
{"id": 5869645, "s2_id": "cf2c88b428e60542e014a146f8babffdfae7c0d5", "title": "Model-guided performance analysis of the sparse matrix-matrix multiplication", "abstract": "Achieving high efficiency with numerical kernels for sparse matrices is of utmost importance, since they are part of many simulation codes and tend to use most of the available compute time and resources. In addition, especially in large scale simulation frameworks the readability and ease of use of mathematical expressions are essential components for the continuous maintenance, modification, and extension of software. In this context, the sparse matrix-matrix multiplication is of special interest. In this paper we thoroughly analyze the single-core performance of sparse matrix-matrix multiplication kernels in the Blaze Smart Expression Template (SET) framework. We develop simple models for estimating the achievable maximum performance, and use them to assess the efficiency of our implementations. Additionally, we compare these kernels with several commonly used SET-based C++ libraries, which, just as Blaze, aim at combining the requirements of high performance with an elegant user interface. For the different sparse matrix structures considered here, we show that our implementations are competitive or faster than those of the other SET libraries for most problem sizes on a current Intel multicore processor.", "venue": "2013 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Tobias  Scharpff", "Klaus  Iglberger", "Georg  Hager", "Ulrich  R\u00fcde"], "year": 2013, "n_citations": 1}
{"id": 5879316, "s2_id": "da1dc97fc37e6f5db7aef7b9199c7bc588ce68c5", "title": "A Reinforcement Learning Environment for Polyhedral Optimizations", "abstract": "The polyhedral model allows a structured way of defining semantics-preserving transformations to improve the performance of a large class of loops. Finding profitable points in this space is a hard problem which is usually approached by heuristics that generalize from domain-expert knowledge. Existing problem formulations in state-of-the-art heuristics depend on the shape of particular loops, making it hard to leverage generic and more powerful optimization techniques from the machine learning domain. In this paper, we propose PolyGym, a shape-agnostic formulation for the space of legal transformations in the polyhedral model as a Markov Decision Process (MDP). Instead of using transformations, the formulation is based on an abstract space of possible schedules. In this formulation, states model partial schedules, which are constructed by actions that are reusable across different loops. With a simple heuristic to traverse the space, we demonstrate that our formulation is powerful enough to match and outperform state-of-the-art heuristics. On the Polybench benchmark suite, we found transformations that led to a speedup of 3.39x over LLVM O3, which is 1.83x better than the speedup achieved by ISL. Our generic MDP formulation enables using reinforcement learning to learn optimization policies over a wide range of loops. This also contributes to the emerging field of machine learning in compilers, as it exposes a novel problem formulation that can push the limits of existing methods.", "venue": "ArXiv", "authors": ["Alexander  Brauckmann", "Andr'es  Goens", "Jeronimo  Castrillon"], "year": 2021, "n_citations": 1}
{"id": 5880278, "s2_id": "b4ad46ad93a369d1edbd36549ea496cce00b54f2", "title": "On a Caching System with Object Sharing", "abstract": "We consider a data/content-caching system located in an edge-cloud that is shared (to reduce costs) by a number of proxies each serving, e.g., a group of data-driven IoT devices. Each proxy operates its own LRU-list of a certain capacity in the shared cache. The length of objects simultaneously appearing in plural LRU-lists is equally divided among them, i.e., object sharing among the LRUs. We give numerical results for our MemCacheD with Object Sharing (MCD-OS) prototype. Also, a working-set approximation for the shared cache is described and how to reduce ripple evictions is discussed.", "venue": "ArXiv", "authors": ["George  Kesidis", "Bhuvan  Urgaonkar", "Mahmut  Kandemir", "Takis  Konstantopoulos"], "year": 2019, "n_citations": 0}
{"id": 5880311, "s2_id": "80220e68f3f3ebbc6aac2ff7753aff7d42d6f35d", "title": "Constrained Multi-user Multi-server Max-Min Fair Queuing", "abstract": "In this paper, a multi-user multi-server queuing system is studied in which each user is constrained to get service from a subset of servers. In the studied system, rate allocation in the sense of max-min fairness results in multi-level fair rates. To achieve such fair rates, we propose $CM^4FQ$ algorithm. In this algorithm users are chosen for service on a packet by packet basis. The priority of each user $i$ to be chosen at time $t$ is determined based on a parameter known as service tag (representing the amount of work counted for user $i$ till time $t$). Hence, a free server will choose to serve an eligible user with the minimum service tag. Based on such simple selection criterion, $CM^4FQ$ aims at guaranteed fair throughput for each demanding user without explicit knowledge of each server service rate. We argue that $CM^4FQ$ can be applied in a variety of practical queuing systems specially in mobile cloud computing architecture.", "venue": "ArXiv", "authors": ["Jalal  Khamse-Ashari", "Ioannis  Lambadaris", "Yiqiang Q. Zhao"], "year": 2016, "n_citations": 7}
{"id": 5882837, "s2_id": "4eb48590950972a7c1c21b7d42d7a83a518cfc6e", "title": "Stochastic Analysis of Satellite Broadband by Mega-Constellations with Inclined LEOs", "abstract": "As emerging massive constellations are intended to provide seamless connectivity for remote areas using hundreds of small low Earth orbit (LEO) satellites, new methodologies have great importance to study the performance of these networks. In this paper, we derive both downlink and uplink analytical expressions for coverage probability and data rate of an inclined LEO constellation under general fading, regardless of exact satellites\u2019 positions. Our solution involves two phases as we, first, abstract the network into a uniformly distributed network. Secondly, we obtain a new parameter, effective number of satellites, for every user\u2019s latitude which compensates for the performance mismatch between the actual and uniform constellations. In addition to exact derivation of the network performance metrics, this study provides insight into selecting the constellation parameters, e.g., the total number of satellites, altitude, and inclination angle.", "venue": "2020 IEEE 31st Annual International Symposium on Personal, Indoor and Mobile Radio Communications", "authors": ["Niloofar  Okati", "Taneli  Riihonen"], "year": 2020, "n_citations": 4}
{"id": 5886060, "s2_id": "484cb6f755630e10c6637f001542d91dbe4831ae", "title": "Towards Co-execution on Commodity Heterogeneous Systems: Optimizations for Time-Constrained Scenarios", "abstract": "Heterogeneous systems are present from powerful supercomputers, to mobile devices, including desktop computers, thanks to their excellent performance and energy consumption. The ubiquity of these architectures in both desktop systems and medium-sized service servers allow enough variability to exploit a wide range of problems, such as multimedia workloads, video encoding, image filtering and inference in machine learning. Due to the heterogeneity, some efforts have been done to reduce the programming effort and preserve performance portability, but these systems include a set of challenges. The context in which applications offload the workload along with the management overheads introduced when doing co-execution, penalize the performance gains under time-constrained scenarios. Therefore, this paper proposes optimizations for the EngineCL runtime to reduce the penalization when co-executing in commodity systems, as well as algorithmic improvements when load balancing. An exhaustive experimental evaluation is performed, showing optimization improvements of 7.5% and 17.4% for binary and ROI-based offloading modes, respectively. Thanks to all the optimizations, the new load balancing algorithm is always the most efficient scheduling configuration, achieving an average efficiency of 0.84 under a pessimistic scenario.", "venue": "2019 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Ra\u00fal  Nozal", "Jos\u00e9 Luis Bosque", "Ram\u00f3n  Beivide"], "year": 2019, "n_citations": 2}
{"id": 5890123, "s2_id": "7f7e520d851262ae2b063f19159f8d189287e6d1", "title": "Implications of dissemination strategies on the security of distributed ledgers", "abstract": "This paper describes a simulation study on security attacks over Distributed Ledger Technologies (DLTs). We specifically focus on attacks at the underlying peer-to-peer layer of these systems, that is in charge of disseminating messages containing data and transaction to be spread among all participants. In particular, we consider the Sybil attack, according to which a malicious node creates many Sybils that drop messages coming from a specific attacked node, or even all messages from honest nodes. Our study shows that the selection of the specific dissemination protocol, as well as the amount of connections each peer has, have an influence on the resistance to this attack.", "venue": "CryBlock@MOBICOM", "authors": ["Luca  Serena", "Gabriele  D'Angelo", "Stefano  Ferretti"], "year": 2020, "n_citations": 3}
{"id": 5892197, "s2_id": "bf40c0d84d6de5b3988688a68ef81391cccffc2a", "title": "On-Disk Data Processing: Issues and Future Directions", "abstract": "In this paper, we present a survey of \"on-disk\" data processing (ODDP). ODDP, which is a form of near-data processing, refers to the computing arrangement where the secondary storage drives have the data processing capability. Proposed ODDP schemes vary widely in terms of the data processing capability, target applications, architecture and the kind of storage drive employed. Some ODDP schemes provide only a specific but heavily used operation like sort whereas some provide a full range of operations. Recently, with the advent of Solid State Drives, powerful and extensive ODDP solutions have been proposed. In this paper, we present a thorough review of architectures developed for different on-disk processing approaches along with current and future challenges and also identify the future directions which ODDP can take.", "venue": "ArXiv", "authors": ["Mayank  Mishra", "Arun K. Somani"], "year": 2017, "n_citations": 1}
{"id": 5893602, "s2_id": "96d58acfbf28a47de206cb504fcd2d27153a7397", "title": "Middleware-based database replication: the gaps between theory and practice", "abstract": "The need for high availability and performance in data management systems has been fueling a long running interest in database replication from both academia and industry. However, academic groups often attack replication problems in isolation, overlooking the need for completeness in their solutions, while commercial teams take a holistic approach that often misses opportunities for fundamental innovation. This has created over time a gap between academic research and industrial practice.\n This paper aims to characterize the gap along three axes: performance, availability, and administration. We build on our own experience developing and deploying replication systems in commercial and academic settings, as well as on a large body of prior related work. We sift through representative examples from the last decade of open-source, academic, and commercial database replication systems and combine this material with case studies from real systems deployed at Fortune 500 customers. We propose two agendas, one for academic research and one for industrial R&D, which we believe can bridge the gap within 5-10 years. This way, we hope to both motivate and help researchers in making the theory and practice of middleware-based database replication more relevant to each other.", "venue": "SIGMOD Conference", "authors": ["Emmanuel  Cecchet", "George  Candea", "Anastasia  Ailamaki"], "year": 2008, "n_citations": 118}
{"id": 5894263, "s2_id": "0ae8ed7e07dc79c57a2a372eda77310af44c58e1", "title": "\"Superluminal\" FITS File Processing on Multiprocessors: Zero Time Endian Conversion Technique", "abstract": "ABSTRACT.The FITS is the standard file format in astronomy, and it has been extended to meet the astronomical needs of the day. However, astronomical datasets have been inflating year by year. In the case of the ALMA telescope, a \u223cTB\u223cTB-scale four-dimensional data cube may be produced for one target. Considering that typical Internet bandwidth is tens of MB/sMB/s at most, the original data cubes in FITS format are hosted on a VO server, and the region which a user is interested in should be cut out and transferred to the user (Eguchi et\u00a0al. 2012). The system will equip a very high-speed disk array to process a TB-scale data cube in 10\u00a0s, and disk I/O speed, endian conversion, and data processing speeds will be comparable. Hence, reducing the endian conversion time is one of issues to solve in our system. In this article, I introduce a technique named \u201cjust-in-time endian conversion\u201d, which delays the endian conversion for each pixel just before it is really needed, to sweep out the endian conversion time;...", "venue": "ArXiv", "authors": ["Satoshi  Eguchi"], "year": 2013, "n_citations": 1}
{"id": 5907760, "s2_id": "8e6771d39ad6289bdefa9dbefb311c94ed69f309", "title": "A C Code Generator for Fast Inference and Simple Deployment of Convolutional Neural Networks on Resource Constrained Systems", "abstract": "Inference of Convolutional Neural Networks in time critical applications usually requires a GPU. In robotics or embedded devices these are often not available due to energy, space and cost constraints. Furthermore, installation of a deep learning framework or even a native compiler on the target platform is not possible. This paper presents a neural network code generator (NNCG) that generates from a trained CNN a plain ANSI C code file that encapsulates the inference in single a function. It can easily be included in existing projects and due to lack of dependencies, cross compilation is usually possible. Additionally, the code generation is optimized based on the known trained CNN and target platform following four design principles. The system is evaluated utilizing small CNN designed for this application. Compared to TensorFlow XLA and Glow speed-ups of up to 11.81 can be shown and even GPUs are outperformed regarding latency.", "venue": "2020 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS)", "authors": ["Oliver  Urbann", "Simon  Camphausen", "Arne  Moos", "Ingmar  Schwarz", "S\u00f6ren  Kerner", "Maximilian  Otten"], "year": 2020, "n_citations": 0}
{"id": 5910106, "s2_id": "52dd62aaca7bc4bcefc2129dab3979c3347db98f", "title": "Worldwide Fast File Replication on Grid Datafarm", "abstract": "The Grid Datafarm architecture is designed for global petascale data-intensive computing. It provides a global parallel filesystem with online petascale storage, scalable I/O bandwidth, and scalable parallel processing, and it can exploit local I/O in a grid of clusters with tens of thousands of nodes. One of features is that it manages file replicas in filesystem metadata for fault tolerance and load balancing. \nThis paper discusses and evaluates several techniques to support long-distance fast file replication. The Grid Datafarm manages a ranked group of files as a Gfarm file, each file, called a Gfarm file fragment, being stored on a filesystem node, or replicated on several filesystem nodes. Each Gfarm file fragment is replicated independently and in parallel using rate-controlled HighSpeed TCP with network striping. On a US-Japan testbed with 10,000 km distance, we achieve 419 Mbps using 2 nodes on each side, and 741 Mbps using 4 nodes out of 893 Mbps with two transpacific networks.", "venue": "ArXiv", "authors": ["Osamu  Tatebe", "Satoshi  Sekiguchi", "Youhei  Morita", "Satoshi  Matsuoka", "Noriyuki  Soda"], "year": 2003, "n_citations": 17}
{"id": 5912935, "s2_id": "731b0db115338ec0c4d89a785320a7e1b4a60f24", "title": "Search in Power-Law Networks", "abstract": "Many communication and social networks have power-law link distributions, containing a few nodes that have a very high degree and many with low degree. The high connectivity nodes play the important role of hubs in communication and networking, a fact that can be exploited when designing efficient search algorithms. We introduce a number of local search strategies that utilize high degree nodes in power-law graphs and that have costs scaling sublinearly with the size of the graph. We also demonstrate the utility of these strategies on the GNUTELLA peer-to-peer network.", "venue": "Physical review. E, Statistical, nonlinear, and soft matter physics", "authors": ["Lada A. Adamic", "Rajan M. Lukose", "Amit R. Puniyani", "Bernardo A. Huberman"], "year": 2001, "n_citations": 1232}
{"id": 5914195, "s2_id": "d744e64e1b191fb1945aec89bd05e024e818e7af", "title": "Improving DNN Fault Tolerance using Weight Pruning and Differential Crossbar Mapping for ReRAM-based Edge AI", "abstract": "Recent research demonstrated the promise of using resistive random access memory (ReRAM) as an emerging technology to perform inherently parallel analog domain in-situ matrix-vector multiplication\u2014the intensive and key computation in deep neural networks (DNNs). However, hardware failure, such as stuck-at-fault defects, is one of the main concerns that impedes the ReRAM devices to be a feasible solution for real implementations. The existing solutions to address this issue usually require an optimization to be conducted for each individual device, which is impractical for mass-produced products (e.g., IoT devices). In this paper, we rethink the value of weight pruning in ReRAM-based DNN design from the perspective of model fault tolerance. And a differential mapping scheme is proposed to improve the fault tolerance under a high stuck-on fault rate. Our method can tolerate almost an order of magnitude higher failure rate than the traditional two-column method in representative DNN tasks. More importantly, our method does not require extra hardware cost compared to the traditional two-column mapping scheme. The improvement is universal and does not require the optimization process for each individual device.", "venue": "2021 22nd International Symposium on Quality Electronic Design (ISQED)", "authors": ["Geng  Yuan", "Zhiheng  Liao", "Xiaolong  Ma", "Yuxuan  Cai", "Zhenglun  Kong", "Xuan  Shen", "Jingyan  Fu", "Zhengang  Li", "Chengming  Zhang", "Hongwu  Peng", "Ning  Liu", "Ao  Ren", "Jinhui  Wang", "Yanzhi  Wang"], "year": 2021, "n_citations": 1}
{"id": 5916916, "s2_id": "fc88d3abc739dba5bcb08dadc10905832ee01279", "title": "Opening the Black Box: Performance Estimation during Code Generation for GPUs", "abstract": "Automatic code generation is frequently used to create implementations of algorithms specifically tuned to particular hardware and application parameters. The code generation process involves the selection of adequate code transformations, tuning parameters, and parallelization strategies. To cover the huge search space, code generation frameworks may apply time-intensive autotuning, exploit scenario-specific performance models, or treat performance as an intangible black box that must be described via machine learning. This paper addresses the selection problem by identifying the relevant performance-defining mechanisms through a performance model coupled with an analytic hardware metric estimator. This enables a quick exploration of large configuration spaces to identify highly efficient candidates with high accuracy. Our current approach targets memory-intensive GPGPU applications and focuses on the correct modeling of data transfer volumes to all levels of the memory hierarchy. We show how our method can be coupled to the \u201cpystencils\u201d stencil code generator, which is used to generate kernels for a range four 3D25pt stencil and a complex two phase fluid solver based on the Lattice Boltzmann Method. For both, it delivers a ranking that can be used to select the best performing candidate. The method is not limited to stencil kernels, but can be integrated into any code generator that can generate the required address expressions.", "venue": "2021 IEEE 33rd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)", "authors": ["Dominik  Ernst", "Georg  Hager", "Markus  Holzer", "Matthias  Knorr", "Gerhard  Wellein"], "year": 2021, "n_citations": 0}
{"id": 5916950, "s2_id": "9bf12e243de3f63b93ea3664a44971072b5e46e4", "title": "Performance Assessment of OpenMP Compilers Targeting NVIDIA V100 GPUs", "abstract": "Heterogeneous systems are becoming increasingly prevalent. In order to exploit the rich compute resources of such systems, robust programming models are needed for application developers to seamlessly migrate legacy code from today's systems to tomorrow's. Over the past decade and more, directives have been established as one of the promising paths to tackle programmatic challenges on emerging systems. This work focuses on applying and demonstrating OpenMP offloading directives on five proxy applications. We observe that the performance varies widely from one compiler to the other; a crucial aspect of our work is reporting best practices to application developers who use OpenMP offloading compilers. While some issues can be worked around by the developer, there are other issues that must be reported to the compiler vendors. By restructuring OpenMP offloading directives, we gain an 18x speedup for the su3 proxy application on NERSC's Cori system when using the Clang compiler, and a 15.7x speedup by switching max reductions to add reductions in the laplace mini-app when using the Cray-llvm compiler on Cori.", "venue": "WACCPD@SC", "authors": ["Joshua Hoke Davis", "Christopher  Daley", "Swaroop  Pophale", "Thomas  Huber", "Sunita  Chandrasekaran", "Nicholas J. Wright"], "year": 2020, "n_citations": 5}
{"id": 5918244, "s2_id": "1332ad63ac8cf53189eadf94ffd0cc236707ccc9", "title": "Performance Analysis of CP2K Code for Ab Initio Molecular Dynamics", "abstract": "Using a realistic molecular catalyst system, we conduct scaling studies of ab initio molecular dynamics simulations using the CP2K code on both Intel Xeon CPU and NVIDIA V100 GPU architectures. We explore using process placement and affinity to gain additional performance improvements. We also use statistical methods to understand performance changes in spite of the variability in runtime for each molecular dynamics timestep. We found ideal conditions for CPU runs included at least four MPI ranks per node, bound evenly across each socket, and fully utilizing processing cores with one OpenMP thread per core, no benefit was shown from reserving cores for the system. The CPU-only simulations scaled at 70% or more of the ideal scaling up to 10 compute nodes, after which the returns began to diminish more quickly. Simulations on a single 40-core node with two NVIDIA V100 GPUs for acceleration achieved over 3.7x speedup compared to the fastest single 36-core node CPUonly version, and showed 13% speedup over the fastest time we achieved across five CPU-only nodes.", "venue": "ArXiv", "authors": ["Dewi  Yokelson", "Nikolay V. Tkachenko", "Robert  Robey", "Ying Wai Li", "Pavel A. Dub"], "year": 2021, "n_citations": 0}
{"id": 5919530, "s2_id": "a8c0df42313c6258fff910e0a00a8462d36b9562", "title": "The cost of address translation", "abstract": "Modern computers are not random access machines (RAMs). They have a memory hierarchy, multiple cores, and virtual memory. In this paper, we address the computational cost of address translation in virtual memory. Starting point for our work is the observation that the analysis of some simple algorithms (random scan of an array, binary search, heapsort) in either the RAM model or the EM model (external memory model) does not correctly predict growth rates of actual running times. We propose the VAT model (virtual address translation) to account for the cost of address translations and analyze the algorithms mentioned above and others in the model. The predictions agree with the measurements. We also analyze the VAT-cost of cache-oblivious algorithms.", "venue": "ALENEX", "authors": ["Tomasz  Jurkiewicz", "Kurt  Mehlhorn"], "year": 2013, "n_citations": 8}
{"id": 5921686, "s2_id": "9c4d937d600ca7080745e2cf4681b6acd3563080", "title": "Skip This Paper - RINASim: Your Recursive InterNetwork Architecture Simulator", "abstract": "Recursive InterNetwork Architecture is a clean-slate approach to how to deal with the current issues of the Internet based on the traditional TCP/IP networking stack. Instead of using a fixed number of layers with dedicated functionality, RINA proposes a single generic layer with programmable functionality that may be recursively stacked. We introduce a brand new framework for modeling and simulation of RINA that is intended for OMNeT++.", "venue": "ArXiv", "authors": ["Vladim\u00edr  Vesel\u00fd", "Marcel  Marek", "Tomas  Hykel", "Ondrej  Rysav\u00fd"], "year": 2015, "n_citations": 5}
{"id": 5921793, "s2_id": "c904d42a6aeb81c385dbc94d95380734dee43fbc", "title": "NVIDIA Tensor Core Programmability, Performance & Precision", "abstract": "The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.", "venue": "2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Stefano  Markidis", "Steven Wei Der Chien", "Erwin  Laure", "Ivy Bo Peng", "Jeffrey S. Vetter"], "year": 2018, "n_citations": 175}
{"id": 5922344, "s2_id": "b419d71d7f3b25785ea261496b9199b24e3f0b1f", "title": "Primary User Traffic Classification in Dynamic Spectrum Access Networks", "abstract": "This paper focuses on analytical studies of the primary user (PU) traffic classification problem. colorblack{Observing} that the gamma distribution can represent positively skewed data and exponential distribution (popular in communication networks performance analysis literature) it is considered here as the PU traffic descriptor. We investigate two PU traffic classifiers utilizing perfectly measured PU activity (busy) and inactivity (idle) periods: (i) maximum likelihood classifier (MLC) and (ii) multi-hypothesis sequential probability ratio test classifier (MSPRTC). Then, relaxing the assumption on perfect period measurement, we consider a PU traffic observation through channel sampling. For a special case of negligible probability of PU state change in between two samplings, we propose a minimum variance PU busy/idle period length estimator. Later, relaxing the assumption of the complete knowledge of the parameters of the PU period length distribution, we propose two PU traffic classification schemes: (i) estimate-then-classify (ETC), and (ii) average likelihood function (ALF) classifiers considering time domain fluctuation of the PU traffic parameters. Numerical results show that both MLC and MSPRTC are sensitive to the periods measurement errors when the distance among distribution hypotheses is small, and to the distribution parameter estimation errors when the distance among hypotheses is large. For PU traffic parameters with a partial prior knowledge of the distribution, the ETC outperforms ALF when the distance among hypotheses is small, while the opposite holds when the distance is large.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Chun-Hao  Liu", "Przemyslaw  Pawelczak", "Danijela  Cabric"], "year": 2013, "n_citations": 17}
{"id": 5922455, "s2_id": "f190eb528cfa222e012165dbb4c8d3be3f652be9", "title": "Anonymity Mixes as (Partial) Assembly Queues: Modeling and Analysis", "abstract": "Anonymity platforms route the traffic over a network of special routers that are known as mixes and implement various traffic disruption techniques to hide the communicating users\u2019 identities. Batch mixes in particular anonymize communicating peers by allowing message exchange to take place only after a sufficient number of messages (a batch) accumulate, thus introducing delay. We introduce a queueing model for batch mix and study its delay properties. Our analysis shows that delay of a batch mix grows quickly as the batch size gets close to the number of senders connected to the mix. We then propose a randomized batch mixing strategy and show that it achieves much better delay scaling in terms of the batch size. However, randomization is shown to reduce the anonymity preserving capabilities of the mix. We also observe that queueing models are particularly useful to study anonymity metrics that are more practically relevant such as the time-to-deanonymize metric.", "venue": "2019 IEEE Information Theory Workshop (ITW)", "authors": ["Mehmet Fatih Aktas", "Emina  Soljanin"], "year": 2019, "n_citations": 0}
{"id": 5925300, "s2_id": "a659ce65582d94776a64ed943d1891727f07ef23", "title": "Wireless Local Area Networks with Multiple-Packet Reception Capability", "abstract": "Thanks to its simplicity and cost efficiency, wireless local area network (WLAN) enjoys unique advantages in providing high-speed and low-cost wireless services in hot spots and indoor environments. Traditional WLAN medium-access-control (MAC) protocols assume that only one station can transmit at a time: simultaneous transmissions of more than one station causes the destruction of all packets involved. By exploiting recent advances in PHY-layer multiuser detection (MUD) techniques, it is possible for a receiver to receive multiple packets simultaneously. This paper argues that such multipacket reception (MPR) capability can greatly enhance the capacity of future WLANs. In addition, it provides the MAC-layer and PHY-layer designs needed to achieve the improved capacity. First, to demonstrate MUD/MPR as a powerful capacity-enhancement technique, we prove a \"super-linearity\" result, which states that the system throughput per unit cost increases as the MPR capability increases. Second, we show that the commonly deployed binary exponential backoff (BEB) algorithm in today's WLAN MAC may not be optimal in an MPR system, and that the optimal backoff factor increases with the MPR capability: the number of packets that can be received simultaneously. Third, based on the above insights, we design a joint MAC-PHY layer protocol for an IEEE 802.11-like WLAN that incorporates advanced PHY-layer blind detection and MUD techniques to implement MPR", "venue": "ArXiv", "authors": ["Ying Jun Zhang", "Peng Xuan Zheng", "Soung Chang Liew"], "year": 2007, "n_citations": 0}
{"id": 5926539, "s2_id": "eaf281dc1a52e0d4add2cd387a224ce613d7d1c5", "title": "Dynamic RF Combining for Multi-Antenna Ambient Energy Harvesting", "abstract": "Ambient radio frequency (RF) energy harvesting (EH) technology is key to realize self-sustainable, always-on, low-power, massive Internet of Things networks. Typically, rigid (non-adaptable to channel fluctuations) multi-antenna receive architectures are proposed to support reliable EH operation. Herein, we introduce a dynamic RF combining architecture for ambient RF EH use cases, and exemplify the attainable performance gains via three simple phase shifts\u2019 exploration mechanisms, namely, brute force (BF), sequential testing (ST) and codebook based (CB). Among the proposed mechanisms, BF demands the highest power consumption, while CB requires the highest-resolution phase shifters, thus tipping the scales in favor of ST. Finally, we show that the performance gains of ST over a rigid RF combining scheme increase with the number of receive antennas and energy transmitters\u2019 deployment density.", "venue": "IEEE Wireless Communications Letters", "authors": ["Onel Luis Alcaraz L'opez", "Bruno  Clerckx", "Matti  Latva-aho"], "year": 2021, "n_citations": 0}
{"id": 5930617, "s2_id": "352c2e6c625e637cff2572b258ffb360dd3dd32f", "title": "A Decentralized Parallelization-in-Time Approach with Parareal", "abstract": "With steadily increasing parallelism for high-performance architectures, simulations requiring a good strong scalability are prone to be limited in scalability with standard spatial-decomposition strategies at a certain amount of parallel processors. This can be a show-stopper if the simulation results have to be computed with wallclock time restrictions (e.g.\\,for weather forecasts) or as fast as possible (e.g. for urgent computing). Here, the time-dimension is the only one left for parallelization and we focus on Parareal as one particular parallelization-in-time method. \nWe discuss a software approach for making Parareal parallelization transparent for application developers, hence allowing fast prototyping for Parareal. Further, we introduce a decentralized Parareal which results in autonomous simulation instances which only require communicating with the previous and next simulation instances, hence with strong locality for communication. This concept is evaluated by a prototypical solver for the rotational shallow-water equations which we use as a representative black-box solver.", "venue": "ArXiv", "authors": ["Martin  Schreiber", "Adam  Peddle", "Terry  Haut", "Beth A. Wingate"], "year": 2015, "n_citations": 2}
{"id": 5940952, "s2_id": "4535427ad2b1bfb234a17145c3b8d3cb5b185d78", "title": "Distributing an Exact Algorithm for Maximum Clique: maximising the costup", "abstract": "We take an existing implementation of an algorithm for the maximum clique problem and modify it so that we can distribute it over an ad-hoc cluster of machines. Our goal was to achieve a signicant speedup in performance with minimal development eort, i.e. a maximum costup. We present a simple modication to a state-of-the-art exact algorithm for maximum clique that allows us to distribute it across many machines. An empirical study over large hard benchmarks shows that speedups of an order of magnitude are routine for 25 or more machines.", "venue": "ArXiv", "authors": ["Ciaran  McCreesh", "Patrick  Prosser"], "year": 2012, "n_citations": 16}
{"id": 5940966, "s2_id": "ed6d900734f8e709d8592ed441d2cc7f589f7af2", "title": "MDS coding is better than replication for job completion times", "abstract": "In a multi-server system, how can one get better performance than random assignment of jobs to servers if queue-states cannot be queried by the dispatcher? A replication strategy has recently been proposed where $d$ copies of each arriving job are sent to servers chosen at random. The job's completion time is the first time that the service of any of its copies is complete. On completion, redundant copies of the job are removed from other queues so as not to overburden the system. \nFor digital jobs, where the objects to be served can be algebraically manipulated, and for servers whose ouput is a linear function of their input, here we consider an alternate strategy: Maximum Distance Separable (MDS) codes. For every batch of $n$ digital jobs that arrive, $n+m$ linear combinations are created over the reals or a large finite field, and each coded job is sent to a random server. The batch completion time is the first time that any $n$ of the $n+m$ coded jobs are served, as the evaluation of $n$ original jobs can be recovered by Gaussian elimination. If redundant jobs can be removed from queues on batch completion, we establish that in order to get the improved response-time performance of sending $d$ copies of each of $n$ jobs via the replication strategy, with the MDS methodology it suffices to send $n+d$ jobs. That is, while replication is multiplicative, MDS is linear. If redunant jobs cannot be removed from queues on batch completion, the stability regions of the two strategies are distinct and the performance with MDS codes is better still.", "venue": "Oper. Res. Lett.", "authors": ["Ken  Duffy", "Seva  Shneer"], "year": 2021, "n_citations": 7}
{"id": 5941147, "s2_id": "7779985539bfedf0f4b531bcce7a805849c70864", "title": "Minimizing Cache Misses in Scientific Computing Using Isoperimetric Bodies", "abstract": "A number of known techniques for improving cache performance in scientific computations involve the reordering of the iteration space. Some of these reorderings can be considered coverings of the iteration space with sets having small surface-to-volume ratios. Use of such sets may reduce the number of cache misses in computations of local operators having the iteration space as their domain. First, we derive lower bounds on cache misses that any algorithm must suffer while computing a local operator on a grid. Then, we explore coverings of iteration spaces of structured and unstructured discretization grid operators which allow us to approach these lower bounds. For structured grids we introduce a covering by successive minima tiles based on the interference lattice of the grid. We show that the covering has a small surface-to-volume ratio and present a computer experiment showing actual reduction of the cache misses achieved by using these tiles. For planar unstructured grids we show existence of a covering which reduces the number of cache misses to the level of that of structured grids. Next, we introduce a class of multidimensional grids, called starry grids in this paper. These grids represent an abstraction of unstructured grids used in, for example, molecular simulations and the solution of partial differential equations. We show that starry grids can be covered by sets having a low surface-to-volume ratio and, hence have the same cache efficiency as structured grids. Finally, we present a triangulation of a three-dimensional cube that has the property that any local operator on the corresponding grid must incur a significantly larger number of cache misses than a similar operator on a structured grid of the same size.", "venue": "ArXiv", "authors": ["Michael A. Frumkin", "Rob F. Van der Wijngaart"], "year": 2002, "n_citations": 1}
{"id": 5942118, "s2_id": "8b59fac76ce8a3bcec1152e8bd2657b72df01c22", "title": "Performance Analysis of Zippers", "abstract": "A zipper is a powerful technique of representing a purely functional data structure in a way that allows fast access to a specific element. It is often used in cases where the imperative data structures would use a mutable pointer. However, the efficiency of zippers as a replacement for mutable pointers is not sufficiently explored. We attempt to address this issue by comparing the performance of zippers and mutable pointers in two common scenarios and three different languages: C++, C#, and Haskell.", "venue": "DECLARE", "authors": ["V\u00edt  Sefl"], "year": 2019, "n_citations": 0}
{"id": 5949972, "s2_id": "3ced5858f45512be94b7179bb88c637251eef923", "title": "Buffer occupancy asymptotics in rate proportional sharing networks with heterogeneous long-tailed inputs", "abstract": "In this paper, we consider a network of rate proportional processor sharing servers in which sessions with long-tailed duration arrive as Poisson processes. In particular, we assume that a session of type $n$ transmits at a rate $r_n$ bits per unit time and lasts for a random time $\\tau_n$ with a generalized Pareto distribution given by $P \\{\\tau_n > x\\} \\sim \\alpha_n x^{-(1+\\beta_n)}$ for large $x$, where $\\alpha_n, \\beta_n > 0$. The weights are taken to be the rates of the flows. The network is assumed to be loop-free with respect to source-destination routes. We characterize the order $O-$asymptotics of the complementary buffer occupancy distribution at each node in terms of the input characteristics of the sessions. In particular, we show that the distributions obey a power law whose exponent can be calculated via solving a fixed point and deterministic knapsack problem. The paper concludes with some canonical examples.", "venue": "ArXiv", "authors": ["Ozcan  Ozturk", "Ravi  Mazumdar", "Nikolay B. Likhanov"], "year": 2014, "n_citations": 0}
{"id": 5951028, "s2_id": "7767b0de634674d15732ce35127ad6174ebfc0b0", "title": "MoA Interpretation of the Iterative Conjugate Gradient Method with Psi Reduction - A Tutorial to teach the Mathematically literate in Linear and Tensor Algebra: Part I", "abstract": "It is often difficult to learn new mathematics semantically and syntactically, even when there are similarities in the words and meaning when discussed aloud. The goal of this document is to facilitate learning through explanations and definitions relating our common mathematical knowledge and highlighting what is new. It is meant to be a working document that will evolve based on feedback from target audiences, those mathematically literate in linear and tensor algebra, those that want to learn MoA, Psi Calculus, and its uses, those that want and need the ability to prove a design, either in hardware or software through the ONF, Operational Normal Form, and those wanting to exploit all resources optimally, especially when Tensor Algebra, i.e. algorithms foundational to their application,are needed: Knowledge Representation, Machine Learning, Signal Processing, AI, HPC, etc.", "venue": "ArXiv", "authors": ["Lenore  Mullin", "Paul  Sebexen"], "year": 2019, "n_citations": 0}
{"id": 5954286, "s2_id": "6462a7e3b3b8c432bbd31a4af83f79aa51da1f40", "title": "High-Throughput CNN Inference on Embedded ARM Big.LITTLE Multicore Processors", "abstract": "Internet of Things edge intelligence requires convolutional neural network (CNN) inference to take place in the edge devices itself. ARM big.LITTLE architecture is at the heart of prevalent commercial edge devices. It comprises of single-ISA heterogeneous cores grouped into multiple homogeneous clusters that enable power and performance tradeoffs. All cores are expected to be simultaneously employed in inference to attain maximal throughput. However, high communication overhead involved in parallelization of computations from convolution kernels across clusters is detrimental to throughput. We present an alternative framework called Pipe-it that employs pipelined design to split convolutional layers across clusters while limiting parallelization of their respective kernels to the assigned cluster. We develop a performance-prediction model that utilizes only the convolutional layer descriptors to predict the execution time of each layer individually on all permitted core configurations (type and count). Pipe-it then exploits the predictions to create a balanced pipeline using an efficient design space exploration algorithm. Pipe-it on average results in a 39% higher throughput than the highest antecedent throughput.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Siqi  Wang", "Gayathri  Ananthanarayanan", "Yifan  Zeng", "Neeraj  Goel", "Anuj  Pathania", "Tulika  Mitra"], "year": 2020, "n_citations": 30}
{"id": 5956235, "s2_id": "20591738d42c28abeeeacc7c7284a7098baa4239", "title": "Non-equilibrium information envelopes and the capacity-delay-error-tradeoff of source coding", "abstract": "This paper establishes a link between information and queueing theory using an envelope-based approach. Unlike classical, equilibrium information theory, information envelopes focus on the dynamics of sources and coders, using functions of time that bound the number of bits generated. In the limit the information envelopes converge to the expected value and recover the entropy of a source, respectively, the average codeword length of a coder. In contrast, on short time scales and for sources with memory it is shown that large deviations from known equilibrium results occur with non-negligible probability. These can cause significant network delays. Using results from the stochastic network calculus, the envelopes yield a characterization of the operating points of source coders by the triplet of capacity, delay, and error. In the limit, assuming an optimal coder the required capacity approaches the entropy with arbitrarily small probability of error if infinitely large delays are permitted. We derive a corresponding characterization of channels and prove that the model has the desirable property of additivity, that allows analyzing sources and channels as if in isolation.", "venue": "2012 IEEE International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM)", "authors": ["Ralf  L\u00fcbben", "Markus  Fidler"], "year": 2012, "n_citations": 10}
{"id": 5956945, "s2_id": "5c02b80ffcbf02ee98453677b60ca66558c10f49", "title": "High-Performance Routing With Multipathing and Path Diversity in Ethernet and HPC Networks", "abstract": "The recent line of research into topology design focuses on lowering network diameter. Many low-diameter topologies such as Slim Fly or Jellyfish that substantially reduce cost, power consumption, and latency have been proposed. A key challenge in realizing the benefits of these topologies is routing. On one hand, these networks provide shorter path lengths than established topologies such as Clos or torus, leading to performance improvements. On the other hand, the number of shortest paths between each pair of endpoints is much smaller than in Clos, but there is a large number of non-minimal paths between router pairs. This hampers or even makes it impossible to use established multipath routing schemes such as ECMP. In this article, to facilitate high-performance routing in modern networks, we analyze existing routing protocols and architectures, focusing on how well they exploit the diversity of minimal and non-minimal paths. We first develop a taxonomy of different forms of support for multipathing and overall path diversity. Then, we analyze how existing routing schemes support this diversity. Among others, we consider multipathing with both shortest and non-shortest paths, support for disjoint paths, or enabling adaptivity. To address the ongoing convergence of HPC and \u201cBig Data\u201d domains, we consider routing protocols developed for both HPC systems and for data centers as well as general clusters. Thus, we cover architectures and protocols based on Ethernet, InfiniBand, and other HPC networks such as Myrinet. Our review will foster developing future high-performance multipathing routing protocols in supercomputers and data centers.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Maciej  Besta", "Jens  Domke", "Marcel  Schneider", "Marek  Konieczny", "Salvatore Di Girolamo", "Timo  Schneider", "Ankit  Singla", "Torsten  Hoefler"], "year": 2021, "n_citations": 3}
{"id": 5962233, "s2_id": "0f40d0fbfc78fc46629cff19906c7396aa787179", "title": "Simulation-based Evaluation of a Synchronous Transaction Model for Time-Sensitive Software-Defined Networks", "abstract": "Real-time networks based on Ethernet require robust quality-of-service for time-critical traffic. The Time-Sensitive Networking (TSN) collection of standards enables this in real-time environments like vehicle on-board networks. Runtime reconfigurations in TSN must respect the deadlines of real-time traffic. Software-Defined Networking (SDN) moves the control plane of network devices to the SDN controller, making these networks programmable. This allows reconfigurations from a central point in the network. In this work, we present a transactional model for network reconfigurations that are synchronously executed in all network devices. We evaluate its performance in a case study against nontransactional reconfigurations and show that synchronous transactions enable consistency for reconfigurations in TSN without increased latencies for real-time frames.", "venue": "ArXiv", "authors": ["Tobias  Haugg", "Mohammad Fazel Soltani", "Timo  H\u00e4ckel", "Philipp  Meyer", "Franz  Korf", "Thomas C. Schmidt"], "year": 2021, "n_citations": 0}
{"id": 5966129, "s2_id": "ab1b1757da7708e518c0041a25822016bf9efa21", "title": "On Protocol and Physical Interference Models in Poisson Wireless Networks", "abstract": "This paper analyzes the connection between the protocol and physical interference models in the setting of Poisson wireless networks. A transmission is successful under the protocol model if there are no interferers within a parameterized guard zone around the receiver, while a transmission is successful under the physical model if the signal to interference plus noise ratio at the receiver is above a threshold. The parameterized protocol model forms a family of decision rules for predicting the success or failure of the same transmission attempt under the physical model. For Poisson wireless networks, we employ stochastic geometry to determine the prior, evidence, and posterior distributions associated with this estimation problem. With this in hand, we proceed to develop six sets of results: 1) the maximum correlation of protocol and physical model success indicators; 2) the minimum Bayes risk in estimating physical success from a protocol observation; 3) the receiver operating characteristic (ROC) of false rejection (Type I) and false acceptance (Type II) probabilities; 4) the impact of Rayleigh fading versus no fading on the correlation and ROC; 5) the impact of multiple prior protocol model observations in the setting of a wireless network with a fixed set of nodes in which the nodes employ the slotted Aloha protocol in each time slot; and 6) a numerical investigation of the effect of different pathloss models.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Jeffrey  Wildman", "Steven  Weber"], "year": 2018, "n_citations": 6}
{"id": 5970379, "s2_id": "a68110a8d84fcab7114bd649d6f3677aa54181d5", "title": "Accelerated Multiple Precision Direct Method and Mixed Precision Iterative Refinement on Python Programming Environment", "abstract": "Current Python programming environment does not have any reliable and efficient multiple precision floating-point (MPF) arithmetic except \u201cmpmath\u201d and \u201cgmpy2\u201d packages based on GNU MP(GMP) and MPFR libraries. Although it is well known that multi-componenttype MPF library can be utilized for middle length precision arithmetic under 200 bits, they are not widely used on Python environment. In this paper, we describe our accelerated MPF direct method with AVX2 techniques and its application to mixed precision iterative refinement combined with mpmath, and demonstrate their efficiency on x86 64 computational environments.", "venue": "ArXiv", "authors": ["Tomonori  Kouya"], "year": 2021, "n_citations": 0}
{"id": 5973517, "s2_id": "86216613dd11e1f6d0bf6ab7d0c056b1b2c4b9d6", "title": "Power Consumption Analysis of Parallel Algorithms on GPUs", "abstract": "Due to their highly parallel multi-cores architecture, GPUs are being increasingly used in a wide range of computationally intensive applications. Compared to CPUs, GPUs can achieve higher performances at accelerating the programs' execution in an energy-efficient way. Therefore GPGPU computing is useful for high performance computing applications and in many scientific research fields. In order to bring further performance improvements, GPU clusters are increasingly adopted. The energy consumed by GPUs cannot be neglected. Therefore, an energy-efficient time scheduling of the programs that are going to be executed by the parallel GPUs based on their deadline as well as the assigned priorities could be deployed to face their energetic avidity. For this reason, we present in this paper a model enabling the measure of the power consumption and the time execution of some elementary operations running on a single GPU using a new developed energy measurement protocol. Consequently, using our methodology, energy needs of a program could be predicted, allowing a better task scheduling.", "venue": "2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)", "authors": ["Fr\u00e9d\u00e9ric  Magoul\u00e8s", "Abal-Kassim Cheik Ahamed", "Alban  Desmaison", "Jean-Christophe  L\u00e9chenet", "Francois  Mayer", "Haifa Ben Salem", "Thomas  Zhu"], "year": 2014, "n_citations": 2}
{"id": 5979671, "s2_id": "05fb2dc509099b3c45c2df3d8b5d0f75a0c66976", "title": "A local parallel communication algorithm for polydisperse rigid body dynamics", "abstract": "The simulation of large ensembles of particles is usually parallelized by partitioning the domain spatially and using message passing to communicate between the processes handling neighboring subdomains. The particles are represented as individual geometric objects and are associated to the subdomains. Handling collisions and migrating particles between subdomains, as required for proper parallel execution, requires a complex communication protocol. Typically, the parallelization is restricted to handling only particles that are smaller than a subdomain. In many applications, however, particle sizes may vary drastically with some of them being larger than a subdomain. In this article we propose a new communication and synchronization algorithm that can handle the parallelization without size restrictions on the particles. Despite the additional complexity and extended functionality, the new algorithm introduces only minimal overhead. We demonstrate the scalability of the previous and the new communication algorithms up to almost two million parallel processes and for handling ten billion (1e10) geometrically resolved particles on a state-of-the-art petascale supercomputer. Different scenarios are presented to analyze the performance of the new algorithm and to demonstrate its capability to simulate polydisperse scenarios, where large individual particles can extend across several subdomains.", "venue": "Parallel Comput.", "authors": ["Sebastian  Eibl", "Ulrich  R\u00fcde"], "year": 2018, "n_citations": 9}
{"id": 5979966, "s2_id": "20c0c9ba73efb912dbe331786ba318a0f359a373", "title": "Gamers Private Network Performance Forecasting. From Raw Data to the Data Warehouse with Machine Learning and Neural Nets", "abstract": "Gamers Private Network (GPN\u00ae) is a client/server technology that guarantees a connection for online video games that is more reliable and lower latency than a standard internet connection. Users of the GPN\u00ae technology benefit from a stable and high-quality gaming experience for online games, which are hosted and played across the world. After transforming a massive volume of raw networking data collected by WTFast, we have structured the cleaned data into a special-purpose data warehouse and completed extensive analysis using machine learning and neural nets technologies, and business intelligence tools. These analyses demonstrate the ability to predict and quantify changes in the network, and demonstrate the benefits gained from the use of a GPN\u00ae for users when connected to an online game session.", "venue": "ArXiv", "authors": ["Albert  Wong", "Chun Yin Chiu", "Ga'etan  Hains", "Jack  Humphrey", "Hans  Fuhrmann", "Youry  Khmelevsky", "Chris  Mazur"], "year": 2021, "n_citations": 0}
{"id": 5980490, "s2_id": "f0435e28b3f8e6352fabbfe1aa0bca1ca324c0a5", "title": "Large-Scale Optimization-Based Non-negative Computational Framework for Diffusion Equations: Parallel Implementation and Performance Studies", "abstract": "It is well-known that the standard Galerkin formulation, which is often the formulation of choice under the finite element method for solving self-adjoint diffusion equations, does not meet maximum principles and the non-negative constraint for anisotropic diffusion equations. Recently, optimization-based methodologies that satisfy maximum principles and the non-negative constraint for steady-state and transient diffusion-type equations have been proposed. To date, these methodologies have been tested only on small-scale academic problems. The purpose of this paper is to systematically study the performance of the non-negative methodology in the context of high performance computing (HPC). PETSc and TAO libraries are, respectively, used for the parallel environment and optimization solvers. For large-scale problems, it is important for computational scientists to understand the computational performance of current algorithms available in these scientific libraries. The numerical experiments are conducted on the state-of-the-art HPC systems, and a single-core performance model is used to better characterize the efficiency of the solvers. Our studies indicate that the proposed non-negative computational framework for diffusion-type equations exhibits excellent strong scaling for real-world large-scale problems.Graphical AbstractThis figure shows the fate of chromium after 180 days using the single-field Galerkin formulation. The white regions indicate the violation of the non-negative constraint.", "venue": "J. Sci. Comput.", "authors": ["J.  Chang", "Satish  Karra", "K. B. Nakshatrala"], "year": 2017, "n_citations": 11}
{"id": 5983483, "s2_id": "9bb17d1468237f7e92d28897ef71473a3c7e0c89", "title": "On Big Data Benchmarking", "abstract": "Big data systems address the challenges of capturing, storing, managing, analyzing, and visualizing big data. Within this context, developing benchmarks to evaluate and compare big data systems has become an active topic for both research and industry communities. To date, most of the state-of-the-art big data benchmarks are designed for specific types of systems. Based on our experience, however, we argue that considering the complexity, diversity, and rapid evolution of big data systems, for the sake of fairness, big data benchmarks must include diversity of data and workloads. Given this motivation, in this paper, we first propose the key requirements and challenges in developing big data benchmarks from the perspectives of generating data with 4V properties (i.e. volume, velocity, variety and veracity) of big data, as well as generating tests with comprehensive workloads for big data systems. We then present the methodology on big data benchmarking designed to address these challenges. Next, the state-of-the-art are summarized and compared, following by our vision for future research directions.", "venue": "BPOE@ASPLOS/VLDB", "authors": ["Rui  Han", "Xiaoyi  Lu"], "year": 2014, "n_citations": 36}
{"id": 5986267, "s2_id": "f22fbe06d4a0de17150fcf5f020484c5b7eb5946", "title": "Optimistic Concurrency Control for Real-world Go Programs (Extended Version with Appendix)", "abstract": "We present a source-to-source transformation framework, GOCC, that consumes lock-based pessimistic concurrency programs in the Go language and transforms them into optimistic concurrency programs that use Hardware Transactional Memory (HTM). The choice of the Go language is motivated by the fact that concurrency is a first-class citizen in Go, and it is widely used in Go programs. GOCC performs rich interprocedural program analysis to detect and filter lock-protected regions and performs AST-level code transformation of the surrounding locks when profitable. Profitability is driven by both static analyses of critical sections and dynamic analysis via execution profiles. A custom HTM library, using perceptron, learns concurrency behavior and dynamically decides whether to use HTM in the rewritten lock/unlock points. Given the rich history of transactional memory research but its lack of adoption in any industrial setting, we believe this workflow, which ultimately produces source-code patches, is more apt for industry-scale adoption. Results on widely adopted Go libraries and applications demonstrate significant (up to 10\u00d7) and scalable performance gains resulting from our automated transformation while avoiding major performance regressions.", "venue": "USENIX Annual Technical Conference", "authors": ["Zhizhou  Zhang", "Milind  Chabbi", "Adam  Welc", "Timothy  Sherwood"], "year": 2021, "n_citations": 0}
{"id": 5999971, "s2_id": "a1533e8ae6ddcae3af10d4c78e52e0213ca505ae", "title": "A Loop-Based Methodology for Reducing Computational Redundancy in Workload Sets", "abstract": "The design of general purpose processors relies heavily on a workload gathering step in which representative programs are collected from various application domains. Processor performance, when running the workload set, is profiled using simulators that model the targeted processor architecture. However, simulating the entire workload set is prohibitively time-consuming, which precludes considering a large number of programs. To reduce simulation time, several techniques in the literature have exploited the internal program repetitiveness to extract and execute only representative code segments. Existing solutions are based on reducing cross-program computational redundancy or on eliminating internal-program redundancy to decrease execution time. In this paper, we propose an orthogonal and complementary loop-centric methodology that targets loop-dominant programs by exploiting internal-program characteristics to reduce cross-program computational redundancy. The approach employs a newly developed framework that extracts and analyzes core loops within workloads. The collected characteristics model memory behavior, computational complexity, and data structures of a program, and are used to construct a signature vector for each program. From these vectors, cross-workload similarity metrics are extracted, which are processed by a novel heuristic to exclude similar programs and reduce redundancy within the set. Finally, a reverse engineering approach that synthesizes executable micro-benchmarks having the same instruction mix as the loops in the original workload is introduced. A tool that automates the flow steps of the proposed methodology is developed. Simulation results demonstrate that applying the proposed methodology to a set of workloads reduces the set size by half, while preserving the main characterizations of the initial workloads.", "venue": "IEEE Access", "authors": ["Elie M. Shaccour", "Mohammad M. Mansour"], "year": 2018, "n_citations": 2}
{"id": 6001257, "s2_id": "8c187df4ba3b393f8ad369184a9b5abbf6ebff8d", "title": "Delayed Asynchronous Iterative Graph Algorithms", "abstract": "Iterative graph algorithms often compute intermediate values and update them as computation progresses. Updated output values are used as inputs for computations in current or subsequent iterations; hence the number of iterations required for values to converge can potentially reduce if the newest values are asynchronously made available to other updates computed in the same iteration. In a multi-threaded shared memory system, the immediate propagation of updated values can cause memory contention that may offset the benefit of propagating updates sooner. In some cases, the benefit of a smaller number of iterations may be diminished by each iteration taking longer. Our key idea is to combine the low memory contention that synchronous approaches have with the faster information sharing of asynchronous approaches. Our hybrid approach buffers updates from threads locally before committing them to the global store to control how often threads may cause conflicts for others while still sharing data within one iteration and hence speeding convergence. On a 112-thread CPU system, our hybrid approach attains up to 4.5% - 19.4% speedup over an asynchronous approach for Pagerank and up to 1.9% - 17% speedup over asynchronous Bellman Ford SSSP. Further, our hybrid approach attains 2.56x better performance than the synchronous approach. Finally, we provide insights as to why delaying updates is not helpful on certain graphs where connectivity is clustered on the main diagonal of the adjacency matrix.", "venue": "2021 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Mark P. Blanco", "Scott  McMillan", "Tze Meng Low"], "year": 2021, "n_citations": 0}
{"id": 6003510, "s2_id": "75e364df63217f908f89c571a4b3c2949cfd288a", "title": "ModiPick: SLA-aware Accuracy Optimization For Mobile Deep Inference", "abstract": "Mobile applications are increasingly leveraging complex deep learning models to deliver features, e.g., image recognition, that require high prediction accuracy. Such models can be both computation and memory-intensive, even for newer mobile devices, and are therefore commonly hosted in powerful remote servers. However, current cloud-based inference services employ static model selection approach that can be suboptimal for satisfying application SLAs (service level agreements), as they fail to account for inherent dynamic mobile environment. \nWe introduce a cloud-based technique called ModiPick that dynamically selects the most appropriate model for each inference request, and adapts its selection to match different SLAs and execution time budgets that are caused by variable mobile environments. The key idea of ModiPick is to make inference speed and accuracy trade-offs at runtime with a pool of managed deep learning models. As such, ModiPick masks unpredictable inference time budgets and therefore meets SLA targets, while improving accuracy within mobile network constraints. We evaluate ModiPick through experiments based on prototype systems and through simulations. We show that ModiPick achieves comparable inference accuracy to a greedy approach while improving SLA adherence by up to 88.5%.", "venue": "ArXiv", "authors": ["Samuel S. Ogden", "Tian  Guo"], "year": 2019, "n_citations": 2}
{"id": 6005373, "s2_id": "390e8c4327b7c16b83987b993c86d7c1acece253", "title": "A Parallel Task-Based Approach to Linear Algebra", "abstract": "Processors with large numbers of cores are becoming commonplace. In order to take advantage of the available resources in these systems, the programming paradigm has to move towards increased parallelism. However, increasing the level of concurrency in the program does not necessarily lead to better performance. Parallel programming models have to provide flexible ways of defining parallel tasks and at the same time, efficiently managing the created tasks. OpenMP is a widely accepted programming model for shared-memory architectures. In this paper we highlight some of the drawbacks in the OpenMP tasking approach, and propose an alternative model based on the Glasgow Parallel Reduction Machine (GPRM) programming framework. As the main focus of this study, we deploy our model to solve a fundamental linear algebra problem, LU factorisation of sparse matrices. We have used the SparseLU benchmark from the BOTS benchmark suite, and compared the results obtained from our model to those of the OpenMP tasking approach. The TILEPro64 system has been used to run the experiments. The results are very promising, not only because of the performance improvement for this particular problem, but also because they verify the task management efficiency, stability, and flexibility of our model, which can be applied to solve problems in future many-core systems.", "venue": "2014 IEEE 13th International Symposium on Parallel and Distributed Computing", "authors": ["Ashkan  Tousimojarad", "Wim  Vanderbauwhede"], "year": 2014, "n_citations": 6}
{"id": 6008449, "s2_id": "189e684d429ec3654adc4a4b8a65c3dba03e8e52", "title": "Low Overhead Instruction Latency Characterization for NVIDIA GPGPUs", "abstract": "The last decade has seen a shift in the computer systems industry where heterogeneous computing has become prevalent. Graphics Processing Units (GPUs) are now present in supercomputers to mobile phones and tablets. GPUs are used for graphics operations as well as general-purpose computing (GPGPUs) to boost the performance of compute-intensive applications. However, the percentage of undisclosed characteristics beyond what vendors provide is not small. In this paper, we introduce a very low overhead and portable analysis for exposing the latency of each instruction executing in the GPU pipeline(s) and the access overhead of the various memory hierarchies found in GPUs at the micro-architecture level. Furthermore, we show the impact of the various optimizations the CUDA compiler can perform over the various latencies. We perform our evaluation on seven different high-end NVIDIA GPUs from five different generations/architectures: Kepler, Maxwell, Pascal, Volta, and Turing. The results in this paper can help architects to have an accurate characterization of the latencies of these GPUs, which will help in modeling the hardware accurately. Also, software developers can perform informed optimizations to their applications.", "venue": "2019 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Yehia  Arafa", "Abdel-Hameed  Badawy", "Gopinath  Chennupati", "Nandakishore  Santhi", "Stephan  Eidenbenz"], "year": 2019, "n_citations": 11}
{"id": 6017610, "s2_id": "4d2089555232dde628c401361dea7cca3b9521e4", "title": "A Cross-Layer Design Based on Geographic Information for Cooperative Wireless Networks", "abstract": "Most of geographic routing approaches in wireless ad hoc and sensor networks do not take into consideration the medium access control (MAC) and physical layers when designing a routing protocol. In this paper, we focus on a cross-layer framework design that exploits the synergies between network, MAC, and physical layers. In the proposed CoopGeo, we use a beaconless forwarding scheme where the next hop is selected through a contention process based on the geographic position of nodes. We optimize this Network-MAC layer interaction using a cooperative relaying technique with a relay selection scheme also based on geographic information in order to improve the system performance in terms of reliability.", "venue": "2010 IEEE 71st Vehicular Technology Conference", "authors": ["Teck  Aguilar", "Mohamed Chedly Ghedira", "Syue-Ju  Syue", "Vincent  Gauthier", "Hossam  Afifi", "Chin-Liang  Wang"], "year": 2010, "n_citations": 14}
{"id": 6019449, "s2_id": "e0d9492f468e3da9faeff1fc769caadda4e56567", "title": "Joint Rate and Resource Allocation in Hybrid Digital\u2013Analog Transmission Over Fading Channels", "abstract": "In hybrid digital-analog (HDA) systems, resource allocation has been utilized to achieve the desired distortion performance. However, existing studies on this issue assume error-free digital transmission, which is not valid for fading channels. With time-varying channel fading, the exact channel state information is not available at the transmitter. Thus, random outage and resulting digital distortion cannot be ignored. Moreover, rate allocation should be considered in resource allocation, since it not only determines the amount of information for digital transmission and that for analog transmission but also affects the outage probability. Based on above-mentioned observations, in this paper, we attempt to perform joint rate and resource allocation strategies to optimize system distortion in HDA systems over fading channels. Consider a bandwidth expansion scenario where a memoryless Gaussian source is transmitted in an HDA system with the entropy-constrained scalar quantizer. First, we formulate the joint allocation problem as an expected system distortion minimization problem where both analog and digital distortions are considered. Then, in the limit of low outage probability, we decompose the problem into two coupled subproblems based on the block coordinate descent method and propose an iterative gradient algorithm to approach the optimal solution. Furthermore, we extend our work to the multivariate Gaussian source scenario where a two-stage fast algorithm integrating rounding and greedy strategies is proposed to optimize the joint rate and resource allocation problem. Finally, simulation results demonstrate that the proposed algorithms can achieve up to 2.3\u00a0dB gains in terms of the signal-to-distortion ratio over existing schemes under the single Gaussian source scenario, and up to 3.5\u00a0dB gains under the multivariate Gaussian source scenario.", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Xiaoda  Jiang", "Hancheng  Lu"], "year": 2018, "n_citations": 3}
{"id": 6020282, "s2_id": "e5adc0b6245c412ab5d221782e7d42d4e0c16eb7", "title": "Throughput Prediction of Asynchronous SGD in TensorFlow", "abstract": "Modern machine learning frameworks can train neural networks using multiple nodes in parallel, each computing parameter updates with stochastic gradient descent (SGD) and sharing them asynchronously through a central parameter server. Due to communication overhead and bottlenecks, the total throughput of SGD updates in a cluster scales sublinearly, saturating as the number of nodes increases. In this paper, we present a solution to predicting training throughput from profiling traces collected from a single-node configuration. Our approach is able to model the interaction of multiple nodes and the scheduling of concurrent transmissions between the parameter server and each node. By accounting for the dependencies between received parts and pending computations, we predict overlaps between computation and communication and generate synthetic execution traces for configurations with multiple nodes. We validate our approach on TensorFlow training jobs for popular image classification neural networks, on AWS and on our in-house cluster, using nodes equipped with GPUs or only with CPUs. We also investigate the effects of data transmission policies used in TensorFlow and the accuracy of our approach when combined with optimizations of the transmission schedule.", "venue": "ICPE", "authors": ["Zhuojin  Li", "Wumo  Yan", "Marco  Paolieri", "Leana  Golubchik"], "year": 2020, "n_citations": 2}
{"id": 6024971, "s2_id": "87c2d7505cb23f57a771ea589b05b349992df896", "title": "New Thread Migration Strategies for NUMA Systems", "abstract": "Multicore systems present on-board memory hierarchies and communication networks that influence performance when executing shared memory parallel codes. Characterising this influence is complex, and understanding the effect of particular hardware configurations on different codes is of paramount importance. In previous works, monitoring information extracted from hardware counters at runtime has been used to characterise the behaviour of each thread in the parallel code in terms of the number of floating point operations per second, operational intensity, and latency of memory access. We propose to use this information to guide thread migration strategies that improve execution efficiency by increasing locality and affinity. Different configurations of NAS Parallel OpenMP benchmarks on multicores were used to validate the benefits of the proposed thread migration strategies. Our proposed strategies produce up to 70% improvement in scenarios where locality and affinity are low, there being a small degradation in performance for codes with high locality and affinity.", "venue": "ArXiv", "authors": ["Oscar G. Lorenzo", "M. L. Beco\u00f1a", "Tom\u00e1s F. Pena", "Jos\u00e9 Carlos Cabaleiro", "J. A. Lorenzo", "Francisco F. Rivera"], "year": 2018, "n_citations": 0}
{"id": 6030648, "s2_id": "51aadc87e4a5ea0ba023880641973efd6148e542", "title": "Developing and experimenting with LEO satellite constellations in OMNeT++", "abstract": "In this paper, we present our work in designing and implementing a LEO satellite constellation simulation model within OMNeT++ and INET, which is validated by comparing the results with existing work. Our model builds upon the fundamentals of the Open Source Satellite Simulator (OS ), which was ported to INET 4.3. We describe how the model was integrated on top of the INET and ported OS Framework. We then experiment with the simulation model to demonstrate its viability in simulating LEO satellite constellations. This involved simulating both outdated and more recent satellite constellations, using FCC filing information, to validate latency results.", "venue": "ArXiv", "authors": ["Aiden  Valentine", "George  Parisis"], "year": 2021, "n_citations": 0}
{"id": 6042652, "s2_id": "94a59ac05c824bfe205834276fb8935feafc0b13", "title": "An Enhanced Mathematical Model for Performance Evaluation of Optical Burst Switched Networks", "abstract": "An enhanced mathematical model is introduced to study and evaluate the performance of a core node in an optical burst switched network. In the proposed model, the exact Poisson traffic arrivals to the OBS node is approximated by assuming that the maximum allowed number of arrivals to the OBS node, in a given time slot, is two (instead of infinity). A detailed state diagram is outlined to illustrate the problem, and then a mathematical model based on the equilibrium point analysis (EPA) technique is presented. The steady-state system throughput is derived from the model which is built in the absence of wavelength conversion capability. Our proposed model is aided by a simulation work which studies the performance of an OBS core node under the assumption of Poisson traffic arrivals (the exact case) and calculates the steady- state system throughput. The results obtained from the proposed mathematical model are consistent with that of simulation when assuming Poisson traffic arrivals and this consistency holds for a wide range of traffic load.", "venue": "IEEE GLOBECOM 2008 - 2008 IEEE Global Telecommunications Conference", "authors": ["Mohamed H. S. Morsy", "Mohamad Y. S. Sowailem", "Hossam M. H. Shalaby"], "year": 2008, "n_citations": 1}
{"id": 6045290, "s2_id": "e5e109688b1f70be7605482b493cfbc60717d35b", "title": "Task Scheduling for Heterogeneous Multicore Systems", "abstract": "In recent years, as the demand for low energy and high performance computing has steadily increased, heterogeneous computing has emerged as an important and promising solution. Because most workloads can typically run most efficiently on certain types of cores, mapping tasks on the best available resources can not only save energy but also deliver high performance. However, optimal task scheduling for performance and/or energy is yet to be solved for heterogeneous platforms. The work presented herein mathematically formulates the optimal heterogeneous system task scheduling as an optimization problem using queueing theory. We analytically solve for the common case of two processor types, e.g., CPU+GPU, and give an optimal policy (CAB). We design the GrIn heuristic to efficiently solve for near-optimal policy for any number of processor types (within 1.6% of the optimal). Both policies work for any task size distribution and processing order, and are therefore, general and practical. We extensively simulate and validate the theory, and implement the proposed policy in a CPU-GPU real platform to show the optimal throughput and energy improvement. Comparing to classic policies like load-balancing, our results range from 1.08x~2.24x better performance or 1.08x~2.26x better energy efficiency in simulations, and 2.37x~9.07x better performance in experiments.", "venue": "ArXiv", "authors": ["Zhuo  Chen", "Diana  Marculescu"], "year": 2017, "n_citations": 1}
{"id": 6047005, "s2_id": "4a2b0cb0f5c89595692b1dfa1690f61747deab86", "title": "Nudge: Stochastically Improving upon FCFS", "abstract": "The First-Come First-Served (FCFS) scheduling policy is the most popular scheduling algorithm used in practice. Furthermore, its usage is theoretically validated: for light-tailed job size distributions, FCFS has weakly optimal asymptotic tail of response time. But what if we don't just care about the asymptotic tail? What if we also care about the 99th percentile of response time, or the fraction of jobs that complete in under one second? Is FCFS still best? Outside of the asymptotic regime, only loose bounds on the tail of FCFS are known, and optimality is completely open. In this paper, we introduce a new policy, Nudge, which is the first policy to provably stochastically improve upon FCFS. We prove that Nudge simultaneously improves upon FCFS at every point along the tail, for light-tailed job size distributions. As a result, Nudge outperforms FCFS for every moment and every percentile of response time. Moreover, Nudge provides a multiplicative improvement over FCFS in the asymptotic tail. This resolves a long-standing open problem by showing that, counter to previous conjecture, FCFS is not strongly asymptotically optimal.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Isaac  Grosof", "Kunhe  Yang", "Ziv  Scully", "Mor  Harchol-Balter"], "year": 2021, "n_citations": 0}
{"id": 6047333, "s2_id": "e19499198b3cc12f55166798adc34afae9854a0b", "title": "Dynamic Bandwidth Management in Distributed VoD based on the User Class Using Agents", "abstract": "This paper proposes a dynamic bandwidth management algorithm in which more bandwidth is allocated for higher class users and also higher priority is given to the videos with higher popularity within a class using agent technology. The popularity and weight profile of the videos which is used for efficiently allocating bandwidth is periodically updated by a mobile agent. The proposed approach allocates more bandwidth for higher class users and gives higher priority for higher weight videos [popular videos] so that they can be served with high QoS, reduces the load on the central multimedia server and maximizes the channel utilization between the neighboring proxy servers and the central multimedia server and lower video rejection ratio. The simulation results prove the reduction of load on central multimedia server by load sharing among the neighboring proxy servers, maximum bandwidth utilization, and more bandwidth allocation for higher class users.", "venue": "ArXiv", "authors": ["H. S. Guruprasad", "H. D. Maheshappa"], "year": 2009, "n_citations": 0}
{"id": 6049662, "s2_id": "cc632d9fe9ad692ca576107fa32112b6ff33747e", "title": "Adaptive Mesh Approach for Predicting Algorithm Behavior with Application to Visibility Culling in Computer Graphics", "abstract": "We propose a concise approximate description, and a method for efficiently obtaining this description, via adaptive random sampling of the performance (running time, memory consumption, or any other profileable numerical quantity) of a given algorithm on some low-dimensional rectangular grid of inputs. The formal correctness is proven under reasonable assumptions on the algorithm under consideration; and the approach's practical benefit is demonstrated by predicting for which observer positions and viewing directions an occlusion culling algorithm yields a net performance benefit or loss compared to a simple brute force renderer.", "venue": "ArXiv", "authors": ["Matthias  Fischer", "Claudius  J\u00e4hn", "Martin  Ziegler"], "year": 2009, "n_citations": 1}
{"id": 6059018, "s2_id": "467615591c9df14fd009103a52d318a3394665e2", "title": "Neuro-Symbolic AI: An Emerging Class of AI Workloads and their Characterization", "abstract": "Neuro-symbolic artificial intelligence is a novel area of AI research which seeks to combine traditional rules-based AI approaches with modern deep learning techniques. Neurosymbolic models have already demonstrated the capability to outperform state-of-the-art deep learning models in domains such as image and video reasoning. They have also been shown to obtain high accuracy with significantly less training data than traditional models. Due to the recency of the field\u2019s emergence and relative sparsity of published results, the performance characteristics of these models are not well understood. In this paper, we describe and analyze the performance characteristics of three recent neuro-symbolic models. We find that symbolic models have less potential parallelism than traditional neural models due to complex control flow and low-operational-intensity operations, such as scalar multiplication and tensor addition. However, the neural aspect of computation dominates the symbolic part in cases where they are clearly separable. We also find that data movement poses a potential bottleneck, as it does in many ML workloads.", "venue": "ArXiv", "authors": ["Zachary  Susskind", "Bryce  Arden", "Lizy K. John", "Patrick  Stockton", "Eugene B. John"], "year": 2021, "n_citations": 0}
{"id": 6059572, "s2_id": "5d2affdb7d2369062aeb54d4387c44d789e10ee2", "title": "Collaborative Uploading in Heterogeneous Networks: Optimal and Adaptive Strategies", "abstract": "Collaborative uploading describes a type of crowd-sourcing scenario in networked environments where a device utilizes multiple paths over neighboring devices to upload content to a centralized processing entity such as a cloud service. Intermediate devices may aggregate and preprocess this data stream. Such scenarios arise in the composition and aggregation of information, e.g., from smart phones or sensors. We use a queuing theoretic description of the collaborative uploading scenario, capturing the ability to split data into chunks that are then transmitted over multiple paths, and finally merged at the destination. We analyze replication and allocation strategies that control the mapping of data to paths and provide closed-form expressions that pinpoint the optimal strategy given a description of the paths' service distributions. Finally, we provide an online path-aware adaptation of the allocation strategy that uses statistical inference to sequentially minimize the expected waiting time for the uploaded data. Numerical results show the effectiveness of the adaptive approach compared to the proportional allocation and a variant of the join-the-shortest-queue allocation, especially for bursty path conditions.", "venue": "IEEE INFOCOM 2018 - IEEE Conference on Computer Communications", "authors": ["Wasiur R. KhudaBukhsh", "Bastian  Alt", "Sounak  Kar", "Amr  Rizk", "Heinz  Koeppl"], "year": 2018, "n_citations": 4}
{"id": 6060942, "s2_id": "8459d429e0b0de3c1e9e59215a2ec641b0220133", "title": "Window Flow Control Systems with Random Service", "abstract": "We present an extension of the window flow control analysis by R. Agrawal et.al. (Reference [1]), C.-S. Chang (Reference [6]), and C.-S. Chang et. al. (Reference [8]) to a system with random service time and fixed feedback delay. We consider two network service models. In the first model, the network service process itself has no time correlations. The second model addresses a two-state Markov-modulated service.", "venue": "ArXiv", "authors": ["Alireza  Shekaramiz", "J\u00f6rg  Liebeherr", "Almut  Burchard"], "year": 2015, "n_citations": 7}
{"id": 6068220, "s2_id": "8eef02c8b78995f7b38e54d30770e62c2ebdc7aa", "title": "Graph Computing based Distributed State Estimation with PMUs", "abstract": "Power system state estimation plays a fundamental and critical role in the energy management system (EMS). To achieve a high performance and accurate system states estimation, a graph computing based distributed state estimation approach is proposed in this paper. Firstly, a power system network is divided into multiple areas. Reference buses are selected with PMUs being installed at these buses for each area. Then, the system network is converted into multiple independent areas. In this way, the power system state estimation could be conducted in parallel for each area and the estimated system states are obtained without compromise of accuracy. IEEE 118-bus system and MP 10790-bus system are employed to verify the results accuracy and present the promising computation performance.", "venue": "2020 IEEE Power & Energy Society General Meeting (PESGM)", "authors": ["Yi  Lu", "Chen  Yuan", "Xiang  Zhang", "Hua  Huang", "Guangyi  Liu", "Renchang  Dai", "Zhiwei  Wang"], "year": 2020, "n_citations": 1}
{"id": 6070290, "s2_id": "6017e3a5bbc56932d0a007cb8318e34625398fb6", "title": "In Situ Network and Application Performance Measurement on Android Devices and the Imperfections", "abstract": "Understanding network and application performance are essential for debugging, improving user experience, and performance comparison. Meanwhile, modern mobile systems are optimized for energy-efficient computation and communications that may limit the performance of network and applications. In recent years, several tools have emerged that analyze network performance of mobile applications in~situ with the help of the VPN service. There is a limited understanding of how these measurement tools and system optimizations affect the network and application performance. In this study, we first demonstrate that mobile systems employ energy-aware system hardware tuning, which affects application performance and network throughput. We next show that the VPN-based application performance measurement tools, such as Lumen, PrivacyGuard, and Video Optimizer, aid in ambiguous network performance measurements and degrade the application performance. Our findings suggest that sound application and network performance measurement on Android devices requires a good understanding of the device, networks, measurement tools, and applications.", "venue": "ArXiv", "authors": ["Mohammad A. Hoque", "Ashwin  Rao", "Sasu  Tarkoma"], "year": 2020, "n_citations": 2}
{"id": 6070924, "s2_id": "1dc8459b1e9f98a09f2c07edb6f358b17ac1f89f", "title": "Power Consumption Analysis of a Modern Smartphone", "abstract": "This paper presents observations about power consumption of a latest smartphone. Modern smartphones are powerful devices with different choices of data connections and other functional modes. This paper provides analysis of power utilization for these different operation modes. Also, we present power consumption by vital operating system (OS) components.", "venue": "ArXiv", "authors": ["Muhammad Yasir Malik"], "year": 2012, "n_citations": 13}
{"id": 6071042, "s2_id": "49fd973cca9a0fffe69f75924e15f9f8bc11ae0a", "title": "Stochastic service curve and delay bound analysis: A single node case", "abstract": "A packet-switched network node with constant capacity (in bps) is considered, where packets within each flow are served in the first in first out (FIFO) manner. While this single node system is perhaps the simplest computer communication system, its stochastic service curve characterization and independent case analysis in the context of stochastic network calculus (snetcal) are still basic and many crucial questions surprisingly remain open. Specifically, when the input is a single flow, what stochastic service curve and delay bound does the node provide? When the considered flow shares the node with another flow, what stochastic service curve and delay bound does the node provide to the considered flow, and if the two flows are independent, can this independence be made use of and how? The aim of this paper is to provide answers to these fundamental questions.", "venue": "Proceedings of the 2013 25th International Teletraffic Congress (ITC)", "authors": ["Yuming  Jiang"], "year": 2013, "n_citations": 11}
{"id": 6076280, "s2_id": "3a37dab6c5287844669fa1061d69757d95e3a0b7", "title": "Computing the sparse matrix vector product using block-based kernels without zero padding on processors with AVX-512 instructions", "abstract": "The sparse matrix-vector product (SpMV) is a fundamental operation in many scientific applications from various fields. The High Performance Computing (HPC) community has therefore continuously invested a lot of effort to provide an efficient SpMV kernel on modern CPU architectures. Although it has been shown that block-based kernels help to achieve high performance, they are difficult to use in practice because of the zero padding they require. In the current paper, we propose new kernels using the AVX-512 instruction set, which makes it possible to use a blocking scheme without any zero padding in the matrix memory storage. We describe mask-based sparse matrix formats and their corresponding SpMV kernels highly optimized in assembly language. Considering that the optimal blocking size depends on the matrix, we also provide a method to predict the best kernel to be used utilizing a simple interpolation of results from previous executions. We compare the performance of our approach to that of the Intel MKL CSR kernel and the CSR5 open-source package on a set of standard benchmark matrices. We show that we can achieve significant improvements in many cases, both for sequential and for parallel executions. Finally, we provide the corresponding code in an open source library, called SPC5.", "venue": "PeerJ Comput. Sci.", "authors": ["B\u00e9renger  Bramas", "Pavel  Kus"], "year": 2018, "n_citations": 5}
{"id": 6077418, "s2_id": "047a313fc617169a683242d3427d7cf47c9de159", "title": "Towards Parallel Computing on the Internet: Applications, Architectures, Models and Programming Tools", "abstract": "The development of Internet wide resources for general purpose parallel computing poses the challenging task of matching computation and communication complexity. A number of parallel computing models exist that address this for traditional parallel architectures, and there are a number of emerging models that attempt to do this for large scale Internet-based systems like computational grids. In this survey we cover the three fundamental aspects -- application, architecture and model, and we show how they have been developed over the last decade. We also cover programming tools that are currently being used for parallel programming in computational grids. The trend in conventional computational models are to put emphasis on efficient communication between participating nodes by adapting different types of communication to network conditions. Effects of dynamism and uncertainties that arise in large scale systems are evidently important to understand and yet there is currently little work that addresses this from a parallel computing perspective.", "venue": "ArXiv", "authors": ["Elankovan  Sundararajan", "Aaron  Harwood"], "year": 2006, "n_citations": 1}
{"id": 6078594, "s2_id": "3a0d1781768a86690f2ffe49d3c29c9b1d457a94", "title": "Optimizing Xeon Phi for Interactive Data Analysis", "abstract": "The Intel Xeon Phi manycore processor is designed to provide high performance matrix computations of the type often performed in data analysis. Common data analysis environments include Matlab, GNU Octave, Julia, Python, and R. Achieving optimal performance of matrix operations within data analysis environments requires tuning the Xeon Phi OpenMP settings, process pinning, and memory modes. This paper describes matrix multiplication performance results for Matlab and GNU Octave over a variety of combinations of process counts and OpenMP threads and Xeon Phi memory modes. These results indicate that using KMP_AFFINITY=granlarity=fine, taskset pinning, and all2all cache memory mode allows both Matlab and GNU Octave to achieve 66% of the practical peak performance for process counts ranging from 1 to 64 and OpenMP threads ranging from 1 to 64. These settings have resulted in generally improved performance across a range of applications and has enabled our Xeon Phi system to deliver significant results in a number of real-world applications.", "venue": "2019 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Chansup  Byun", "Jeremy  Kepner", "William  Arcand", "David  Bestor", "William  Bergeron", "Matthew  Hubbell", "Vijay  Gadepally", "Michael  Houle", "Michael  Jones", "Anne  Klein", "Lauren  Milechin", "Peter  Michaleas", "Julie  Mullen", "Andrew  Prout", "Antonio  Rosa", "Siddharth  Samsi", "Charles  Yee", "Albert  Reuther"], "year": 2019, "n_citations": 5}
{"id": 6078654, "s2_id": "da717e4edb663926c995dab84012bce985e2f904", "title": "Performance-oriented DevOps: A Research Agenda", "abstract": "DevOps is a trend towards a tighter integration between development (Dev) and operations (Ops) teams. The need for such an integration is driven by the requirement to continuously adapt enterprise applications (EAs) to changes in the business environment. As of today, DevOps concepts have been primarily introduced to ensure a constant flow of features and bug fixes into new releases from a functional perspective. In order to integrate a non-functional perspective into these DevOps concepts this report focuses on tools, activities, and processes to ensure one of the most important quality attributes of a software system, namely performance. \nPerformance describes system properties concerning its timeliness and use of resources. Common metrics are response time, throughput, and resource utilization. Performance goals for EAs are typically defined by setting upper and/or lower bounds for these metrics and specific business transactions. In order to ensure that such performance goals can be met, several activities are required during development and operation of these systems as well as during the transition from Dev to Ops. Activities during development are typically summarized by the term Software Performance Engineering (SPE), whereas activities during operations are called Application Performance Management (APM). SPE and APM were historically tackled independently from each other, but the newly emerging DevOps concepts require and enable a tighter integration between both activity streams. This report presents existing solutions to support this integration as well as open research challenges in this area.", "venue": "ArXiv", "authors": ["Andreas  Brunnert", "Andr\u00e9 van Hoorn", "Felix  Willnecker", "Alexandru  Danciu", "Wilhelm  Hasselbring", "Christoph  Heger", "Nikolas Roman Herbst", "Pooyan  Jamshidi", "Reiner  Jung", "J\u00f3akim von Kistowski", "Anne  Koziolek", "Johannes  Kro\u00df", "Simon  Spinner", "Christian  V\u00f6gele", "J\u00fcrgen  Walter", "Alexander  Wert"], "year": 2015, "n_citations": 84}
{"id": 6084870, "s2_id": "c0a0997c57e48db1247daba7d96ebbd6158264f9", "title": "A High-Throughput Solver for Marginalized Graph Kernels on GPU", "abstract": "We present the design and optimization of a linear solver on General Purpose GPUs for the efficient and high-throughput evaluation of the marginalized graph kernel between pairs of labeled graphs. The solver implements a preconditioned conjugate gradient (PCG) method to compute the solution to a generalized Laplacian equation associated with the tensor product of two graphs. To cope with the gap between the instruction throughput and the memory bandwidth of current generation GPUs, our solver forms the tensor product linear system on-the-fly without storing it in memory when performing matrix-vector dot product operations in PCG. Such on-the-fly computation is accomplished by using threads in a warp to cooperatively stream the adjacency and edge label matrices of individual graphs by small square matrix blocks called tiles, which are then staged in registers and the shared memory for later reuse. Warps across a thread block can further share tiles via the shared memory to increase data reuse. We exploit the sparsity of the graphs hierarchically by storing only non-empty tiles using a coordinate format and nonzero elements within each tile using bitmaps. Besides, we propose a new partition-based reordering algorithm for aggregating nonzero elements of the graphs into fewer but denser tiles to improve the efficiency of the sparse format.We carry out extensive theoretical analyses on the graph tensor product primitives for tiles of various density and evaluate their performance on synthetic and real-world datasets. Our solver delivers three to four orders of magnitude speedup over existing CPU-based solvers such as GraKeL and GraphKernels. The capability of the solver enables kernel-based learning tasks at unprecedented scales.", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Yu-Hang  Tang", "Oguz  Selvitopi", "Doru  Popovici", "Aydin  Bulu\u00e7"], "year": 2020, "n_citations": 6}
{"id": 6086681, "s2_id": "9398745c73fc9824abe5a3f70ef7b9d5d5617ccc", "title": "A flexible Patch-based lattice Boltzmann parallelization approach for heterogeneous GPU-CPU clusters", "abstract": "Sustaining a large fraction of single GPU performance in parallel computations is considered to be the major problem of GPU-based clusters. We address this issue in the context of a lattice Boltzmann flow solver that is integrated in the WaLBerla software framework. Our multi-GPU implementation uses a block-structured MPI parallelization and is suitable for load balancing and heterogeneous computations on CPUs and GPUs. The overhead required for multi-GPU simulations is discussed in detail. It is demonstrated that a large fraction of the kernel performance can be sustained for weak scaling on InfiniBand clusters, leading to excellent parallel efficiency. However, in strong scaling scenarios using multiple GPUs is much less efficient than running CPU-only simulations on IBM BG/P and x86-based clusters. Hence, a cost analysis must determine the best course of action for a particular simulation task and hardware configuration. Finally we present weak scaling results of heterogeneous simulations conducted on CPUs and GPUs simultaneously, using clusters equipped with varying node configurations.", "venue": "Parallel Comput.", "authors": ["Christian  Feichtinger", "Johannes  Habich", "Harald  K\u00f6stler", "Georg  Hager", "Ulrich  R\u00fcde", "Gerhard  Wellein"], "year": 2011, "n_citations": 53}
{"id": 6088248, "s2_id": "4c06738539c33588785aec277e121e6dc1e393de", "title": "On the Secrecy Performance of Random VLC Networks With Imperfect CSI and Protected Zone", "abstract": "This article investigates the physical-layer security for a random indoor visible light communication (VLC) network with imperfect channel state information (CSI) and a protected zone. The VLC network consists of three nodes, i.e., a transmitter (Alice), a legitimate receiver (Bob), and an eavesdropper (Eve). Alice is fixed in the center of the ceiling, and the emitted signal at Alice satisfies the nonnegativity and the dimmable average optical intensity constraint. Bob and Eve are randomly deployed on the receiver plane. By employing the protected zone and considering the imperfect CSI, the stochastic characteristics of the channel gains for both the main and the eavesdropping channels is first analyzed. After that, the closed-form expressions of the average secrecy capacity and the lower bound of secrecy outage probability are derived, respectively. Finally, Monte Carlo simulations are provided to verify the accuracy of the derived theoretical expressions. Moreover, the impacts of the nominal optical intensity, the dimming target, the protected zone, and the imperfect CSI on secrecy performance are discussed, respectively.", "venue": "IEEE Systems Journal", "authors": ["Jin-Yuan  Wang", "Yu  Qiu", "Sheng-Hong  Lin", "Jun-Bo  Wang", "Min  Lin", "Cheng  Liu"], "year": 2020, "n_citations": 10}
{"id": 6090206, "s2_id": "90a700cc9a0dded9138b38539e774e28d799dbd1", "title": "Performance Analysis of Connection Admission Control Scheme in IEEE 802.16 OFDMA Networks", "abstract": "IEEE 802.16 OFDMA (Orthogonal Frequency Division Multiple Access) technology has emerged as a promising technology for broadband access in a Wireless Metropolitan Area Network (WMAN) environment. In this paper, we address the problem of queueing theoretic performance modeling and analysis of OFDMA under broad-band wireless networks. We consider a single-cell IEEE 802.16 environment in which the base station allocates subchannels to the subscriber stations in its coverage area. The subchannels allocated to a subscriber station are shared by multiple connections at that subscriber station. To ensure the Quality of Service (QoS) performances, a Connection Admission Control (CAC) scheme is considered at a subscriber station. A queueing analytical framework for these admission control schemes is presented considering OFDMA-based transmission at the physical layer. Then, based on the queueing model, both the connection-level and the packet-level performances are studied and compared with their analogues in the case without CAC. The connection arrival is modeled by a Poisson process and the packet arrival for a connection by a two-state Markov Modulated Poisson Process (MMPP). We determine analytically and numerically different performance parameters, such as connection blocking probability, average number of ongoing connections, average queue length, packet dropping probability, queue throughput and average packet delay.", "venue": "ArXiv", "authors": ["Abdelali El Bouchti", "Said El Kafhali", "Abdelkrim  Haqiq"], "year": 2013, "n_citations": 11}
{"id": 6092034, "s2_id": "0152fe36691c013f85d0267e118c2d494d66b7a7", "title": "Effective RAT Selection Approach for 5G Dense Wireless Networks", "abstract": "Dense Networks (DenseNet) and Multi-Radio Access Technologies (Multi-RATs) are considered as key features of the emerging fifth generation (5G) wireless systems. A Multi- RAT DenseNet is characterized by a very dense deployment of low- power base stations (BSs) and by a multi-tier architecture consisting of heterogeneous radio access technologies. In this work, we propose an effective RAT selection algorithm that efficiently manages the RAT handover procedure by (i) choosing the most suitable RAT that guarantees high system and user performance, and (ii) reducing unnecessary handover events. In particular, the decision to trigger a handover is based on a new system parameter named Reference Base Station Efficiency (RBSE). This parameter takes into account metrics related to both the system and the user: the BS transmitted power, the BS traffic load and the users' spectral efficiency. We compare, by simulation, the proposed scheme with the standardized 3GPP policies. Results show that the proposed RAT selection scheme significantly reduces the number of handovers and the end-to- end delay while maintaining high system throughput and user spectral efficiency.", "venue": "2015 IEEE 81st Vehicular Technology Conference (VTC Spring)", "authors": ["Antonino  Orsino", "Giuseppe  Araniti", "Antonella  Molinaro", "Antonio  Iera"], "year": 2015, "n_citations": 27}
{"id": 6092657, "s2_id": "a4219ee332dca98c8c9c061222fbee9e627fd2fc", "title": "Robust benchmarking in noisy environments", "abstract": "We propose a benchmarking strategy that is robust in the presence of timer error, OS jitter and other environmental fluctuations, and is insensitive to the highly nonideal statistics produced by timing measurements. We construct a model that explains how these strongly nonideal statistics can arise from environmental fluctuations, and also justifies our proposed strategy. We implement this strategy in the BenchmarkTools Julia package, where it is used in production continuous integration (CI) pipelines for developing the Julia language and its ecosystem.", "venue": "ArXiv", "authors": ["Jiahao  Chen", "Jarrett  Revels"], "year": 2016, "n_citations": 35}
{"id": 6095928, "s2_id": "c8ebaa931779e31655d47ff18360f5714ef098f1", "title": "LCD: Low Latency Command Dissemination for a Platoon of Vehicles", "abstract": "In a vehicular platoon, a lead vehicle that is responsible for managing the platoon's moving directions and velocity periodically disseminates control commands to following vehicles based on vehicle-to-vehicle communications. However, reducing command dissemination latency with multiple vehicles while ensuring successful message delivery to the tail vehicle is challenging. We propose a new linear dynamic programming algorithm using backward induction and interchange arguments to minimize the dissemination latency of the vehicles. Furthermore, a closed form of dissemination latency in vehicular platoon is obtained by utilizing Markov chain with M/M/1 queuing model. Simulation results confirm that the proposed dynamic programming algorithm improves the dissemination rate by at least 50.9%, compared to similar algorithms in the literature. Moreover, it also approximates the best performance with the maximum gap of up to 0.2 second in terms of latency.", "venue": "2018 IEEE International Conference on Communications (ICC)", "authors": ["Kai  Li", "Wei  Ni", "Eduardo  Tovar", "Mohsen  Guizani"], "year": 2018, "n_citations": 11}
{"id": 6096233, "s2_id": "a8c49bdb481923759895357468e080aacd6227c8", "title": "On the Throughput Optimization in Large-Scale Batch-Processing Systems", "abstract": "We analyze a data-processing system with $n$ clients producing jobs which are processed in \\textit{batches} by $m$ parallel servers; the system throughput critically depends on the batch size and a corresponding sub-additive speedup function. In practice, throughput optimization relies on numerical searches for the optimal batch size, a process that can take up to multiple days in existing commercial systems. In this paper, we model the system in terms of a closed queueing network; a standard Markovian analysis yields the optimal throughput in $\\omega\\left(n^4\\right)$ time. Our main contribution is a mean-field model of the system for the regime where the system size is large. We show that the mean-field model has a unique, globally attractive stationary point which can be found in closed form and which characterizes the asymptotic throughput of the system as a function of the batch size. Using this expression we find the \\textit{asymptotically} optimal throughput in $O(1)$ time. Numerical settings from a large commercial system reveal that this asymptotic optimum is accurate in practical finite regimes.", "venue": "Perform. Evaluation", "authors": ["Sounak  Kar", "Robin  Rehrmann", "Arpan  Mukhopadhyay", "Bastian  Alt", "Florin  Ciucu", "Heinz  Koeppl", "Carsten  Binnig", "Amr  Rizk"], "year": 2020, "n_citations": 2}
{"id": 6097151, "s2_id": "fdb25b9d81ed6f624a7e32da75d1b4ee3edab453", "title": "Covert Cycle Stealing in a Single FIFO Server", "abstract": "Consider a setting where Willie generates a Poisson stream of jobs and routes them to a single server that follows the first-in first-out discipline. Suppose there is an adversary Alice, who desires to receive service without being detected. We ask the question: What is the number of jobs that she can receive covertly, i.e., without being detected by Willie? In the case where both Willie and Alice jobs have exponential service times with respective rates \u03bc1 and \u03bc2, we demonstrate a phase-transition when Alice adopts the strategy of inserting a single job probabilistically when the server idles: over n busy periods, she can achieve a covert throughput, measured by the expected number of jobs covertly inserted, of O(\u221a n) when \u03bc1 < 2 \u03bc2, O(\u221a n log n) when \u03bc1 = 2\u03bc2, and O(n\u03bc2/\u03bc1) when \u03bc1 > 2\u03bc2. When both Willie and Alice jobs have general service times, we establish an upper bound for the number of jobs Alice can execute covertly. This bound is related to the Fisher information. More general insertion policies are also discussed.", "venue": "ACM Trans. Model. Perform. Evaluation Comput. Syst.", "authors": ["Bo  Jiang", "Philippe  Nain", "Don  Towsley"], "year": 2021, "n_citations": 4}
{"id": 6100654, "s2_id": "17d3ab99d03039ff1d8b13e35ac1926e1bf41c6c", "title": "A figure of merit for describing the performance of scaling of parallelization", "abstract": "With the spread of multi- and many-core processors more and more typical task is to re-implement some source code written originally for a single processor to run on more than one cores. Since it is a serious investment, it is important to decide how much efforts pays off, and whether the resulting implementation has as good performability as it could be. The Amdahl's law provides some theoretical upper limits for the performance gain reachable through parallelizing the code, but it needs the detailed architectural knowledge of the program code, does not consider the housekeeping activity needed for parallelization and cannot tell how the actual stage of parallelization implementation performs. The present paper suggests a quantitative measure for that goal. This figure of merit is derived experimentally, from measured running time, and number of threads/cores. It can be used to quantify the used parallelization technology, the connection between the computing units, the acceleration technology under the given conditions, communication method within SoC, or the performance of the software team/compiler.", "venue": "ArXiv", "authors": ["J\u00e1nos  V\u00e9gh", "P\u00e9ter  Moln\u00e1r", "J\u00f3zsef  V\u00e1s\u00e1rhelyi"], "year": 2016, "n_citations": 7}
{"id": 6103758, "s2_id": "1fbd0155366c495563f44d1546270dc78d21bc97", "title": "A Comparative Study of Full-Duplex Relaying Schemes for Low Latency Applications", "abstract": "Various sectors are likely to carry a set of emerging applications while targeting a reliable communication with low latency transmission. To address this issue, upon a spectrally-efficient transmission, this paper investigates the performance of a one full-dulpex (FD) relay system, and considers for that purpose, two basic relaying schemes, namely the symbol-by-symbol transmission, i.e., amplify-and-forward (AF) and the block-by-block transmission, i.e., selective decode-and-forward (SDF). The conducted analysis presents an exhaustive comparison, covering both schemes, over two different transmission modes, i.e., the non combining mode where the best link, direct or relay link is decoded and the signals combining mode, where direct and relay links are combined at the receiver side. While targeting latency purpose as a necessity, simulations show a refined results of performed comparisons, and reveal that AF relaying scheme is more adapted to combining mode, whereas the SDF relaying scheme is more suitable for non combining mode.", "venue": "Int. J. Commun. Syst.", "authors": ["Fatima Ezzahra Airod", "Houda  Chafnaji", "Ahmed  Tamtaoui"], "year": 2018, "n_citations": 2}
{"id": 6107167, "s2_id": "1dbcd4e34b1887e944da71cdecbf5a170e56b097", "title": "A Visual Web Tool to Perform What-If Analysis of Optimization Approaches", "abstract": "In Operation Research, practical evaluation is essential to validate the efficacy of optimization approaches. This paper promotes the usage of performance profiles as a standard practice to visualize and analyze experimental results. It introduces a Web tool to construct and export performance profiles as SVG or HTML files. In addition, the application relies on a methodology to estimate the benefit of hypothetical solver improvements. Therefore, the tool allows one to employ what-if analysis to screen possible research directions, and identify those having the best potential. The approach is showcased on two Operation Research technologies: Constraint Programming and Mixed Integer Linear Programming.", "venue": "ArXiv", "authors": ["Sascha Van Cauwelaert", "Michele  Lombardi", "Pierre  Schaus"], "year": 2017, "n_citations": 6}
{"id": 6108299, "s2_id": "78481dd4b46acfe8fd14751aa7e3b079f0232682", "title": "Download time analysis for distributed storage systems with node failures", "abstract": "We consider a distributed storage system which stores several hot (popular) and cold (less popular) data files across multiple nodes or servers. Hot files are stored using repetition codes while cold files are stored using erasure codes. The nodes are prone to failure and hence at any given time, we assume that only a fraction of the nodes are available. Using a cavity process based mean field framework, we analyze the download time for users accessing hot or cold data in the presence of failed nodes. Our work also illustrates the impact of the choice of the storage code on the download time performance of users in the system.", "venue": "2021 IEEE International Symposium on Information Theory (ISIT)", "authors": ["Tim  Hellemans", "Arti  Yardi", "Tejas  Bodas"], "year": 2021, "n_citations": 0}
{"id": 6108953, "s2_id": "01267115b883c0bb49341c6cdde970efe33bdb28", "title": "Deriving Pareto-optimal performance bounds for 1 and 2-relay wireless networks", "abstract": "This work addresses the problem of deriving fundamental trade-off bounds for a 1-relay and a 2-relay wireless network when multiple performance criteria are of interest. It proposes a simple MultiObjective (MO) performance evaluation framework composed of a broadcast and interference-limited network model; capacity, delay and energy performance metrics and an associated MO optimization problem. Pareto optimal performance bounds between end-to-end delay and energy for a capacity-achieving network are given for 1-relay and 2-relay topologies and assessed through simulations. Moreover, we also show in this paper that these bounds are tight since they can be reached by simple practical coding strategies performed by the source and the relays. Two different types of network coding strategies are investigated. Practical performance bounds for both strategies are compared to the theoretical upper bound. Results confirm that the proposed upper bound on delay and energy performance is tight and can be reached with the proposed combined source and network coding strategies.", "venue": "ICCCN 2013", "authors": ["Qi  Wang", "Katia  Jaffr\u00e8s-Runser", "Claire  Goursaud", "Jean-Marie  Gorce"], "year": 2012, "n_citations": 5}
{"id": 6109685, "s2_id": "a95275a8e639b924364f9c63d56a5cae882fde6f", "title": "When Backpressure Meets Predictive Scheduling", "abstract": "Motivated by the increasing popularity of learning and predicting human user behavior in communication and computing systems, in this paper, we investigate the fundamental benefit of predictive scheduling, i.e., predicting and pre-serving arrivals, in controlled queueing systems. Based on a lookahead-window prediction model, we first establish a novel queue-equivalence between the predictive queueing system with a fully efficient scheduling scheme and an equivalent queueing system without prediction. This result allows us to analytically demonstrate that predictive scheduling necessarily improves system delay performance and drives it to zero with increasing prediction power. It also enables us to exactly determine the required prediction power for different systems and study its impact on tail delay. We then propose the Predictive Backpressure (PBP) algorithm for achieving optimal utility performance in such predictive systems. PBP efficiently incorporates prediction into stochastic system control and avoids the great complication due to the exponential state space growth in the prediction window size. We show that PBP achieves a utility performance that is within O(\u03b5) of the optimal, for any \u03b5 > 0, while guaranteeing that the system delay distribution is a shifted-to-the-left version of that under the original Backpressure algorithm. Hence, the average delay under PBP is strictly better than that under Backpressure, and vanishes with increasing prediction window size. This implies that the resulting utility-delay tradeoff with predictive scheduling can beat the known optimal [O(\u03b5),O(log(1/\u03b5))] tradeoff for systems without prediction. We also develop the Predictable-Only PBP (POPBP) algorithm and show that it effectively reduces packet delay in systems where traffic can only be predicted but not pre-served.", "venue": "IEEE/ACM Transactions on Networking", "authors": ["Longbo  Huang", "Shaoquan  Zhang", "Minghua  Chen", "Xin  Liu"], "year": 2016, "n_citations": 50}
{"id": 6111220, "s2_id": "9300f65af853bda31fdba7d43b0b3af482d950b3", "title": "75,000,000,000 Streaming Inserts/Second Using Hierarchical Hypersparse GraphBLAS Matrices", "abstract": "The SuiteSparse GraphBLAS C-library implements high performance hypersparse matrices with bindings to a variety of languages (Python, Julia, and Matlab/Octave). GraphBLAS provides a lightweight in-memory database implementation of hypersparse matrices that are ideal for analyzing many types of network data, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of hypersparse matrices put enormous pressure on the memory hierarchy. This work benchmarks an implementation of hierarchical hypersparse matrices that reduces memory pressure and dramatically increases the update rate into a hypersparse matrices. The parameters of hierarchical hypersparse matrices rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve optimal performance for a variety of applications. Hierarchical hypersparse matrices achieve over 1,000,000 updates per second in a single instance. Scaling to 31,000 instances of hierarchical hypersparse matrices arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 75,000,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Jeremy  Kepner", "Tim  Davis", "Chansup  Byun", "William  Arcand", "David  Bestor", "William  Bergeron", "Vijay  Gadepally", "Matthew  Hubbell", "Michael  Houle", "Michael  Jones", "Anna  Klein", "Peter  Michaleas", "Lauren  Milechin", "Julie  Mullen", "Andrew  Prout", "Antonio  Rosa", "Siddharth  Samsi", "Charles  Yee", "Albert  Reuther"], "year": 2020, "n_citations": 9}
{"id": 6124323, "s2_id": "c1774dd8f8d5ecb20aff14f8d6162888b8e32256", "title": "Algorithm-Based Fault Tolerance for Parallel Stencil Computations", "abstract": "The increase in HPC systems size and complexity, together with increasing on-chip transistor density, power limitations, and number of components, render modern HPC systems subject to soft errors. Silent data corruptions (SDCs) are typically caused by such soft errors in the form of bit-flips in the memory subsystem and hinder the correctness of scientific applications. This work addresses the problem of protecting a class of iterative computational kernels, called stencils, against SDCs when executing on parallel HPC systems. Existing SDC detection and correction methods are in general either inaccurate, inefficient, or targeting specific application classes that do not include stencils. This work proposes a novel algorithm-based fault tolerance (ABFT) method to protect scientific applications that contain arbitrary stencil computations against SDCs. The ABFT method can be applied both online and offline to accurately detect and correct SDCs in 2D and 3D parallel stencil computations. We present a formal model for the proposed method including theorems and proofs for the computation of the associated check-sums as well as error detection and correction. We experimentally evaluate the use of the proposed ABFT method on a real 3D stencil-based application (HotSpot3D) via a fault-injection, detection, and correction campaign. Results show that the proposed ABFT method achieves less than 8% overhead compared to the performance of the unprotected stencil application. Moreover, it accurately detects and corrects SDCs. While the offline ABFT version corrects errors more accurately, it may incur a small additional overhead than its online counterpart.", "venue": "2019 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Aur'elien  Cavelan", "Florina M. Ciorba"], "year": 2019, "n_citations": 2}
{"id": 6124785, "s2_id": "e4cd5ddfa0ce555bdcf040e6884db19ca958d7ed", "title": "Etalumis: bringing probabilistic programming to scientific simulators at scale", "abstract": "Probabilistic programming languages (PPLs) are receiving widespread attention for performing Bayesian inference in complex generative models. However, applications to science remain limited because of the impracticability of rewriting complex scientific simulators in a PPL, the computational cost of inference, and the lack of scalable implementations. To address these, we present a novel PPL framework that couples directly to existing scientific simulators through a cross-platform probabilistic execution protocol and provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference compilation (IC) engines for tractable inference. To guide IC inference, we perform distributed training of a dynamic 3DCNN-LSTM architecture with a PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori supercomputer with a global mini-batch size of 128k: achieving a performance of 450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron Collider (LHC) use-case with the C++ Sherpa simulator and achieve the largest-scale posterior inference in a Turing-complete PPL.", "venue": "SC", "authors": ["Atilim G\u00fcnes Baydin", "Lei  Shao", "Wahid  Bhimji", "Lukas  Heinrich", "Lawrence  Meadows", "Jialin  Liu", "Andreas  Munk", "Saeid  Naderiparizi", "Bradley  Gram-Hansen", "Gilles  Louppe", "Mingfei  Ma", "Xiaohui  Zhao", "Philip H. S. Torr", "Victor W. Lee", "Kyle  Cranmer", "Prabhat", "Frank  Wood"], "year": 2019, "n_citations": 31}
{"id": 6127031, "s2_id": "8e5cc516f0e6b62af0ef0fdb1069a93113539beb", "title": "ThrottleBot - Performance without Insight", "abstract": "Large scale applications are increasingly built by composing sets of microservices. In this model the functionality for a single application might be split across 100s or 1000s of microservices. Resource provisioning for these applications is complex, requiring administrators to understand both the functioning of each microservice, and dependencies between microservices in an application. In this paper we present ThrottleBot, a system that automates the process of determining what resource when allocated to which microservice is likely to have the greatest impact on application performance. We demonstrate the efficacy of our approach by applying ThrottleBot to both synthetic and real world applications. We believe that ThrottleBot when combined with existing microservice orchestrators, e.g., Kubernetes, enables push-button deployment of web scale applications.", "venue": "ArXiv", "authors": ["Michael Alan Chang", "Aurojit  Panda", "Yuan-Cheng  Tsai", "Hantao  Wang", "Scott  Shenker"], "year": 2017, "n_citations": 0}
{"id": 6129998, "s2_id": "88cc17a72958b578de8bee5c2588b74964f21907", "title": "On the Performance Prediction of BLAS-based Tensor Contractions", "abstract": "Tensor operations are surging as the computational building blocks for a variety of scientific simulations and the development of high-performance kernels for such operations is known to be a challenging task. While for operations on one- and two-dimensional tensors there exist standardized interfaces and highly-optimized libraries (BLAS), for higher dimensional tensors neither standards nor highly-tuned implementations exist yet. In this paper, we consider contractions between two tensors of arbitrary dimensionality and take on the challenge of generating high-performance implementations by resorting to sequences of BLAS kernels. The approach consists in breaking the contraction down into operations that only involve matrices or vectors. Since in general there are many alternative ways of decomposing a contraction, we are able to methodically derive a large family of algorithms. The main contribution of this paper is a systematic methodology to accurately identify the fastest algorithms in the bunch, without executing them. The goal is instead accomplished with the help of a set of cache-aware micro-benchmarks for the underlying BLAS kernels. The predictions we construct from such benchmarks allow us to reliably single out the best-performing algorithms in a tiny fraction of the time taken by the direct execution of the algorithms.", "venue": "PMBS@SC", "authors": ["Elmar  Peise", "Diego  Fabregat-Traver", "Paolo  Bientinesi"], "year": 2014, "n_citations": 15}
{"id": 6132842, "s2_id": "c94d454214690d772fca1d53767b6f10c7a4e8db", "title": "High Performance GPU Code Generation for Matrix-Matrix Multiplication using MLIR: Some Early Results", "abstract": "This report presents some early results on code generation targeting tensor cores on NVIDIA GPUs using the MLIR compiler infrastructure. The state-of-the-art in high-performance deep learning today is primarily driven by highly tuned libraries. These libraries are often hand-optimized and tuned by expert programmers using low-level abstractions with significant effort. A lot of this effort may have to be repeated for similar hardware and future ones. This process is thus not modular or reusable to the same extent that compiler infrastructure like LLVM are. Manual optimization typically does not use a standard intermediate representation (IR), although the optimizations performed can be encoded as a sequence of transformation steps and customized passes on an IR. Hand tuning may also miss exploration of design points only reachable easily by automatic code generation. We believe that until the recent introduction of MLIR [14, 16] (Multi-level intermediate representation), IR infrastructure was not geared to tackle the problem of automatic generation of domain-specific libraries in an effective manner. In particular, it was hard to represent and transform compute abstractions at high, middle, and low levels using a single IR. With suitable abstractions in MLIR, we build an experimental lowering pipeline that is able to automatically generate code for matrix-matrix multiplication on NVIDIA GPUs targeting its tensor cores. On a set of problem sizes we evaluated, initial performance results show that we are able to attain performance that is 95-119% and 80-160% of CuBLAS [19] for F32 and F16 accumulate respectively on NVIDIA\u2019s Ampere [18] microarchitecture-based Geforce 3090 RTX. We believe that these results could be used as motivation for further research and development on automatic code and library generation using IR infrastructure for similar specialized accelerators.", "venue": "ArXiv", "authors": ["Navdeep  Katel", "Vivek  Khandelwal", "Uday  Bondhugula"], "year": 2021, "n_citations": 0}
{"id": 6133258, "s2_id": "ac740e9043a0573ba5011ca1bb93b6f66dde688f", "title": "Are Distributed Ledger Technologies Ready for Smart Transportation Systems?", "abstract": "The aim of this paper is to understand whether Distributed Ledger Technologies (DLTs) are ready to support complex services, such as those related to Intelligent Transportation Systems (ITS). In smart transportation services, a huge amount of sensed data is generated by a multitude of vehicles. While DLTs provide very interesting features, such as immutability, traceability and verifiability of data, some doubts on the scalability and responsiveness of these technologies appear to be well-founded. We propose an architecture for ITS that resorts to DLT features. Moreover, we provide experimental results of a real test-bed over IOTA, a promising DLT for IoT. Results clearly show that, while the viability of the proposal cannot be rejected, further work is needed on the responsiveness of DLT infrastructures.", "venue": "ArXiv", "authors": ["Mirko  Zichichi", "Stefano  Ferretti", "Gabriele  D'Angelo"], "year": 2020, "n_citations": 1}
{"id": 6139083, "s2_id": "7efb99e4fa4ceb7c09d1c5a1a55fe028257667de", "title": "NekRS, a GPU-Accelerated Spectral Element Navier-Stokes Solver", "abstract": "The development of NekRS, a GPU-oriented thermal-fluids simulation code based on the spectral element method (SEM) is described. For performance portability, the code is based on the open concurrent compute abstraction and leverages scalable developments in the SEM code Nek5000 and in libParanumal, which is a library of high-performance kernels for high-order discretizations and PDE-based miniapps. Critical performance sections of the Navier\u2013Stokes time advancement are addressed. Performance results on several platforms are presented, including scaling to 27,648 V100s on OLCF Summit, for calculations of up to 60B gridpoints.", "venue": "ArXiv", "authors": ["Paul  Fischer", "Stefan  Kerkemeier", "Misun  Min", "Yu-Hsiang  Lan", "Malachi  Phillips", "Thilina  Rathnayake", "Elia  Merzari", "Ananias  Tomboulides", "Ali  Karakus", "Noel  Chalmers", "Tim  Warburton"], "year": 2021, "n_citations": 5}
{"id": 6139996, "s2_id": "8dc8ce191a565f665f2319b024baac9a7cbe521f", "title": "An Efficient Hybrid I/O Caching Architecture Using Heterogeneous SSDs", "abstract": "Storage subsystem is considered as the performance bottleneck of computer systems in data-intensive applications. Solid-State Drives (SSDs) are emerging storage devices which unlike Hard Disk Drives (HDDs), do not have mechanical parts and therefore, have superior performance compared to HDDs. Due to the high cost of SSDs, entirely replacing HDDs with SSDs is not economically justified. Additionally, SSDs can endure a limited number of writes before failing. To mitigate the shortcomings of SSDs while taking advantage of their high performance, SSD caching is practiced in both academia and industry. Previously proposed caching architectures have only focused on either performance or endurance and neglected to address both parameters in suggested architectures. Moreover, the cost, reliability, and power consumption of such architectures is not evaluated. This paper proposes a hybrid I/O caching architecture that while offers higher performance than previous studies, it also improves power consumption with a similar budget. The proposed architecture uses DRAM, Read-Optimized SSD (RO-SSD), and Write-Optimized SSD (WO-SSD) in a three-level cache hierarchy and tries to efficiently redirect read requests to either DRAM or RO-SSD while sending writes to WO-SSD. To provide high reliability, dirty pages are written to at least two devices which removes any single point of failure. The power consumption is also managed by reducing the number of accesses issued to SSDs. The proposed architecture reconfigures itself between performance- and endurance-optimized policies based on the workload characteristics to maintain an effective tradeoff between performance and endurance. We have implemented the proposed architecture on a server equipped with industrial SSDs and HDDs. The experimental results show that as compared to state-of-the-art studies, the proposed architecture improves performance and power consumption by an average of 8 and 28 percent, respectively, and reduces the cost by 5 percent while increasing the endurance cost by 4.7 percent and negligible reliability penalty.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Reza  Salkhordeh", "Mostafa  Hadizadeh", "Hossein  Asadi"], "year": 2019, "n_citations": 9}
{"id": 6144089, "s2_id": "563165a399644749005899e304acc7649625ed9d", "title": "XPipe: Efficient Pipeline Model Parallelism for Multi-GPU DNN Training", "abstract": "We propose XPipe, an efficient asynchronous pipeline model parallelism approach for multi-GPU DNN training. XPipe is designed to make use of multiple GPUs to concurrently and continuously train different parts of a DNN model. To improve GPU utilization and achieve high throughput, it splits a mini-batch into a set of micro-batches and allows the overlapping of the pipelines of multiple micro-batches, including those belonging to different mini-batches. Most importantly, the novel weight prediction strategy adopted by XPipe enables it to effectively address the weight inconsistency and staleness issues incurred by the asynchronous pipeline parallelism. As a result, XPipe incorporates the advantages of both synchronous and asynchronous pipeline model parallelism approaches. Concretely, it can achieve very comparable (even slightly better) model accuracy as its synchronous counterpart, while obtaining higher throughput than it. Experimental results show that XPipe outperforms other state-of-the-art synchronous and asynchronous model parallelism approaches.", "venue": "ArXiv", "authors": ["Lei  Guan", "Wotao  Yin", "Dongsheng  Li", "Xicheng  Lu"], "year": 2019, "n_citations": 12}
{"id": 6146661, "s2_id": "ab3b08871d36b7f7afd8b13749bd899ce5f1d66d", "title": "Mean-payoff Optimization in Continuous-time Markov Chains with Parametric Alarms", "abstract": "Continuous-time Markov chains with alarms (ACTMCs) allow for alarm events that can be non-exponentially distributed. Within parametric ACTMCs, the parameters of alarm-event distributions are not given explicitly and can be the subject of parameter synthesis. In this line, an algorithm is presented that solves the \u03b5-optimal parameter synthesis problem for parametric ACTMCs with long-run average optimization objectives. The approach provided in this article is based on a reduction of the problem to finding long-run average optimal policies in semi-Markov decision processes (semi-MDPs) and sufficient discretization of the parameter (i.e., action) space. Since the set of actions in the discretized semi-MDP can be very large, a straightforward approach based on an explicit action-space construction fails to solve even simple instances of the problem. The presented algorithm uses an enhanced policy iteration on symbolic representations of the action space. Soundness of the algorithm is established for parametric ACTMCs with alarm-event distributions that satisfy four mild assumptions, fulfilled by many kinds of distributions. Exemplifying proofs for the satisfaction of these requirements are provided for Dirac, uniform, exponential, Erlang, and Weibull distributions in particular. An experimental implementation shows that the symbolic technique substantially improves the efficiency of the synthesis algorithm and allows us to solve instances of realistic size.", "venue": "ACM Trans. Model. Comput. Simul.", "authors": ["Christel  Baier", "Clemens  Dubslaff", "L'ubo\u0161  Koren\u010diak", "Anton\u00edn  ku\u010dera", "Vojt\u011bch  \u0158eh\u00e1k"], "year": 2019, "n_citations": 2}
{"id": 6151109, "s2_id": "f911453163da8a3e1c44582fa447c9c3d330fd11", "title": "Cache Analysis of Non-uniform Distribution Sorting Algorithms", "abstract": "We analyse the average-case cache performance of distribution sorting algorithms in the case when keys are independently but not necessarily uniformly distributed. The analysis is for both `in-place' and `out-of-place' distribution sorting algorithms and is more accurate than the analysis presented in \\cite{RRESA00}. In particular, this new analysis yields tighter upper and lower bounds when the keys are drawn from a uniform distribution. \nWe use this analysis to tune the performance of the integer sorting algorithm MSB radix sort when it is used to sort independent uniform floating-point numbers (floats). Our tuned MSB radix sort algorithm comfortably outperforms a cache-tuned implementations of bucketsort \\cite{RR99} and Quicksort when sorting uniform floats from $[0, 1)$.", "venue": "ArXiv", "authors": ["Naila  Rahman", "Rajeev  Raman"], "year": 2007, "n_citations": 1}
{"id": 6151620, "s2_id": "4b9f9879e1100a4daff2c9e6d77ebe040d084ca5", "title": "Uniform Bounds for Scheduling with Job Size Estimates", "abstract": "We consider the problem of scheduling to minimize mean response time in M/G/1 queues where only estimated job sizes (processing times) are known to the scheduler, where a job of true size s has estimated size in the interval [\u03b2s, \u03b1s] for some \u03b1 \u2265 \u03b2 > 0. We evaluate each scheduling policy by its approximation ratio, which we define to be the ratio between its mean response time and that of Shortest Remaining Processing Time (SRPT), the optimal policy when true sizes are known. Our question: is there a scheduling policy that (a) has approximation ratio near 1 when \u03b1 and \u03b2 are near 1, (b) has approximation ratio bounded by some function of \u03b1 and \u03b2 even when they are far from 1, and (c) can be implemented without knowledge of \u03b1 and \u03b2? We first show that naively running SRPT using estimated sizes in place of true sizes is not such a policy: its approximation ratio can be arbitrarily large for any fixed \u03b2 < 1. We then provide a simple variant of SRPT for estimated sizes that satisfies criteria (a), (b), and (c). In particular, we prove its approximation ratio approaches 1 uniformly as \u03b1 and \u03b2 approach 1. This is the first result showing this type of convergence for M/G/1 scheduling. We also study the Preemptive Shortest Job First (PSJF) policy, a cousin of SRPT. We show that, unlike SRPT, naively running PSJF using estimated sizes in place of true sizes satisfies criteria (b) and (c), as well as a weaker version of (a). 2012 ACM Subject Classification Theory of computation \u2192 Scheduling algorithms; Mathematics of computing \u2192 Queueing theory; Theory of computation \u2192 Online algorithms", "venue": "ArXiv", "authors": ["Ziv  Scully", "Isaac  Grosof", "Michael  Mitzenmacher"], "year": 2021, "n_citations": 0}
{"id": 6151817, "s2_id": "bab7ba6e4c50125b9356461cbe12ede37d417b26", "title": "Dynamic Index NAT as a Mobility Solution in OMNeT++", "abstract": "Abstract \u0097Mobility in wireless networks causes a major issuefrom the IP-addressing perspective. When a Mobile Node (MN)moves to another subnet, it will probably get assigned a newIP address. This causes a routing problem since the MN willnot be reachable with its previous IP address known to theother communication party. Real time applications might sufferfrom connection drops, which is recognized as inconvenience inthe currently used service, unless some solution is provided.An approach to maintain session continuity while traversingheterogeneous networks of different subnet addresses is proposed.Here, a cross-layer module is implemented in OMNeT++ withNAT functionality to provide a seamless handover. A proof ofconcept is also shown with analogy to the Mobile IPv6 protocolprovided in INET.Index Terms \u0097Handover; Network Address Translation; Cross-layer. I. I NTRODUCTION Wireless communication networks can be categorized asheterogeneous based on different aspects, like having aninfrastructure, type of radio access technology, and subnetaddress. While nowadays communication devices are equippedwith multiple interfaces and due to the evolving applicationsand usage scenarios that require IP connectivity anywhereanytime, switching the connection to another access point(handover) and getting assigned a new IP address is a commoncommunication scenario.In this paper, we try to handle this issue from the networklayer point of view and observe its impact on running applica-tions and the provided Quality of Service (QoS). Nevertheless,we also present our design for vertical handover (VHO)management module, which is however not yet provided withperformance analysis. The module manages VHO from abovethe link layer and between independent wireless interfaces.For a running session, changing the IP address introduces arouting problem where the MN is not available any more forits communication party, unless a robust mobility solution isdeployed. Our proposed approach makes use of the fact thatdue to the limited address space of IPv4 and the increasednumber of IP-connected users, Network Address Translation(NAT) became a de facto standard in almost all communica-tion networks. This work implements our concept previouslypresented in a use case in [1]. Also it contributes to the INETframework of OMNeT++ [2] by implementing NAT operationin network layer with an update mechanism achieved througha cross layer module as will be described in more details later.Performance results are provided in comparison to the knownmobility solution (MIPv6) provided in INET. The programcode introduced here is a part of a PhD research work andcan be available for sharing after the defense of the PhD,to be presented as a VHO protocol contributing to the INETframework.II. R", "venue": "ArXiv", "authors": ["Atheer Al Rubaye", "Jochen  Seitz"], "year": 2015, "n_citations": 2}
{"id": 6152171, "s2_id": "9d7703721c827ff37076dcf8d6b8e3d55af5484b", "title": "Robust methods for LTE and WiMAX dimensioning", "abstract": "This paper proposes an analytic model for dimensioning OFDMA based networks like WiMAX and LTE systems. In such a system, users require a number of subchannels which depends on their SNR, hence of their position and the shadowing they experience. The system is overloaded when the number of required subchannels is greater than the number of available subchannels. We give an exact though not closed expression of the loss probability and then give an algorithmic method to derive the number of subchannels which guarantees a loss probability less than a given threshold. We show that Gaussian approximation lead to optimistic values and are thus unusable. We then introduce Edgeworth expansions with error bounds and show that by choosing the right order of the expansion, one can have an approximate dimensioning value easy to compute but with guaranteed performance. As the values obtained are highly dependent from the parameters of the system, which turned to be rather undetermined, we provide a procedure based on concentration inequality for Poisson functionals, which yields to conservative dimensioning. This paper relies on recent results on concentration inequalities and establish new results on Edgeworth expansions.", "venue": "6th International ICST Conference on Performance Evaluation Methodologies and Tools", "authors": ["Laurent  Decreusefond", "Eduardo  Ferraz", "Philippe  Martins", "Than-Tung  Vu"], "year": 2012, "n_citations": 15}
{"id": 6154697, "s2_id": "06a6cc18c28a64819f240921cf274192a2a27755", "title": "Thousands of DebitCredit Transactions-Per-Second: Easy and Inexpensive", "abstract": "A $2k computer can execute about 8k transactions per second. This is 80x more than one of the largest US bank's 1970's traffic - it approximates the total US 1970's financial transaction volume. Very modest modern computers can easily solve yesterday's problems.", "venue": "ArXiv", "authors": ["Jim  Gray", "Charles  Levine"], "year": 2007, "n_citations": 10}
{"id": 6158962, "s2_id": "5ac8e6517ae9ee50b2126852b61a2f629a731b26", "title": "Parallel Simulations for Analysing Portfolios of Catastrophic Event Risk", "abstract": "At the heart of the analytical pipeline of a modern quantitative insurance/reinsurance company is a stochastic simulation technique for portfolio risk analysis and pricing process referred to as Aggregate Analysis. Support for the computation of risk measures including Probable Maximum Loss (PML) and the Tail Value at Risk (TVAR) for a variety of types of complex property catastrophe insurance contracts including Cat eXcess of Loss (XL), or Per-Occurrence XL, and Aggregate XL, and contracts that combine these measures is obtained in Aggregate Analysis. In this paper, we explore parallel methods for aggregate risk analysis. A parallel aggregate risk analysis algorithm and an engine based on the algorithm is proposed. This engine is implemented in C and OpenMP for multi-core CPUs and in C and CUDA for many-core GPUs. Performance analysis of the algorithm indicates that GPUs offer an alternative HPC solution for aggregate risk analysis that is cost effective. The optimised algorithm on the GPU performs a 1 million trial aggregate simulation with 1000 catastrophic events per trial on a typical exposure set and contract structure in just over 20 seconds which is approximately 15x times faster than the sequential counterpart. This can sufficiently support the real-time pricing scenario in which an underwriter analyses different contractual terms and pricing while discussing a deal with a client over the phone.", "venue": "2012 SC Companion: High Performance Computing, Networking Storage and Analysis", "authors": ["Aman Kumar Bahl", "Oliver  Baltzer", "Andrew  Rau-Chaplin", "Blesson  Varghese"], "year": 2012, "n_citations": 20}
{"id": 6161273, "s2_id": "88736e76d0380f14082db5037ba48f85fb4a1d35", "title": "An Approximation of the Outage Probability for Multi-hop AF Fixed Gain Relay", "abstract": "In this letter, we present a closed-form approximation of the outage probability for the multi-hop amplify-and-forward (AF) relaying systems with fixed gain in Rayleigh fading channel. The approximation is derived from the outage event for each hop. The simulation results show the tightness of the proposed approximation in low and high signal-to-noise ratio (SNR) region.", "venue": "ArXiv", "authors": ["Jun Kyoung Lee", "Janghoon  Yang", "Dong Ku Kim"], "year": 2008, "n_citations": 0}
{"id": 6167150, "s2_id": "2cb48593731fff11e5f6b382380536d69f4fd5b0", "title": "An EM algorithm for continuous-time bivariate Markov chains", "abstract": "We study properties and parameter estimation of a finite-state, homogeneous, continuous-time, bivariate Markov chain. Only one of the two processes of the bivariate Markov chain is assumed observable. The general form of the bivariate Markov chain studied here makes no assumptions on the structure of the generator of the chain. Consequently, simultaneous jumps of the observable and underlying processes are possible, neither process is necessarily Markov, and the time between jumps of each of the two processes has a phase-type distribution. Examples of bivariate Markov chains include the Markov modulated Poisson process and the batch Markovian arrival process when appropriate modulo counts are used in each case. We develop an expectation-maximization (EM) procedure for estimating the generator of a bivariate Markov chain, and we demonstrate its performance. The procedure does not rely on any numerical integration or sampling scheme of the continuous-time bivariate Markov chain. The proposed EM algorithm is equally applicable to multivariate Markov chains.", "venue": "Comput. Stat. Data Anal.", "authors": ["Brian L. Mark", "Yariv  Ephraim"], "year": 2013, "n_citations": 21}
{"id": 6168779, "s2_id": "b815f9652717ad55625b9e9ded486962aff654dc", "title": "Towards Green Computing: A Survey of Performance and Energy Efficiency of Different Platforms using OpenCL", "abstract": "When considering hardware platforms, not just time to solution can be of importance but also the energy necessary to reach it. This is not only the case with battery powered mobile devices but also with HPC cluster systems due to financial and practical limits on power consumption a nd cooling. With a variety of hardware options available, the question arises which combination of devices is best suited for a given problem. The answer depends not only on the runtime but also on the energy to solution and the price of the hardware. The energy required to reach a solution becomes increasingly important as battery powered systems have to handle computationally intensive tasks e.g. image processing or machine learning. Even for data centers or HPC facilities the energy cost over the lifetime of the systems can be higher than the acquisition cost. To showcase the differences and give a basic outlook on the applicability of different architectures, devices ranging from ARM systems to server CPUs and GPUs have been used with diverse benchmarking test cases taken from applied research applications.", "venue": "ArXiv", "authors": ["Philip  Heinisch", "Katharina  Ostaszewski", "Hendrik  Ranocha"], "year": 2020, "n_citations": 1}
{"id": 6175625, "s2_id": "b413e73ea9d158f5ec1f147997fdb1fd7df000aa", "title": "Towards a Statistical Methodology to Evaluate Program Speedups and their Optimisation Techniques", "abstract": "The community of program optimisation and analysis, code performance evaluation, parallelisation and optimising compilation has published since many decades hundreds of research and engineering articles in major conferences and journals. These articles study efficient algorithms, strategies and techniques to accelerate programs execution times, or optimise other performance metrics (MIPS, code size, energy/power, MFLOPS, etc.). Many speedups are published, but nobody is able to reproduce them exactly. The non-reproducibility of our research results is a dark point of the art, and we cannot be qualified as {\\it computer scientists} if we do not provide rigorous experimental methodology. This article provides a first effort towards a correct statistical protocol for analysing and measuring speedups. As we will see, some common mistakes are done by the community inside published articles, explaining part of the non-reproducibility of the results. Our current article is not sufficient by its own to deliver a complete experimental methodology, further efforts must be done by the community to decide about a common protocol for our future experiences. Anyway, our community should take care about the aspect of reproducibility of the results in the future.", "venue": "ArXiv", "authors": ["Sid  Touati"], "year": 2009, "n_citations": 4}
{"id": 6177950, "s2_id": "402b6eed11b6c8718b4ae319703caf93d62b68bf", "title": "uops.info: Characterizing Latency, Throughput, and Port Usage of Instructions on Intel Microarchitectures", "abstract": "Modern microarchitectures are some of the world's most complex man-made systems. As a consequence, it is increasingly difficult to predict, explain, let alone optimize the performance of software running on such microarchitectures. As a basis for performance predictions and optimizations, we would need faithful models of their behavior, which are, unfortunately, seldom available. In this paper, we present the design and implementation of a tool to construct faithful models of the latency, throughput, and port usage of x86 instructions. To this end, we first discuss common notions of instruction throughput and port usage, and introduce a more precise definition of latency that, in contrast to previous definitions, considers dependencies between different pairs of input and output operands. We then develop novel algorithms to infer the latency, throughput, and port usage based on automatically-generated microbenchmarks that are more accurate and precise than existing work. To facilitate the rapid construction of optimizing compilers and tools for performance prediction, the output of our tool is provided in a machine-readable format. We provide experimental results for processors of all generations of Intel's Core architecture, i.e., from Nehalem to Coffee Lake, and discuss various cases where the output of our tool differs considerably from prior work.", "venue": "ASPLOS", "authors": ["Andreas  Abel", "Jan  Reineke"], "year": 2019, "n_citations": 29}
{"id": 6179285, "s2_id": "6c0388f903e8ad03eaf0492290d8e832ecd208ce", "title": "Parameter Sensitivity Analysis of the Energy/Frequency Convexity Rule for Nanometer-scale Application Processors", "abstract": "Both theoretical and experimental evidence are presented in this work in order to validate the existence of an Energy/Frequency Convexity Rule, which relates energy consumption and microprocessor frequency for nanometer-scale microprocessors. Data gathered during several month-long experimental acquisition campaigns, supported by several independent publications, suggest that energy consumed is indeed depending on the microprocessor's clock frequency, and, more interestingly, the curve exhibits a clear minimum over the processor's frequency range. An analytical model for this behavior is presented and motivated, which fits well with the experimental data. A parameter sensitivity analysis shows how parameters affect the energy minimum in the clock frequency space. The conditions are discussed under which this convexity rule can be exploited, and when other methods are more effective, with the aim of improving the computer system's energy management efficiency. We show that the power requirements of the computer system, besides the microprocessor, and the overhead affect the location of the energy minimum the most. The sensitivity analysis of the Energy/Frequency Convexity Rule puts forward a number of simple guidelines especially for by low-power systems, such as battery-powered and embedded systems, and less likely by high-performance computer systems.", "venue": "Sustain. Comput. Informatics Syst.", "authors": ["Karel De Vogeleer", "G\u00e9rard  Memmi", "Pierre  Jouvelot"], "year": 2017, "n_citations": 4}
{"id": 6180267, "s2_id": "3d9b137cce1292d16a1c2e72e3db50e13c1113a9", "title": "Simulation methodology for analysis of substrate noise impact on analog/RF circuits including interconnect resistance", "abstract": "The paper reports a novel simulation methodology for the analysis and prediction of substrate noise impact on analog/RF circuits taking into account the role of the parasitic resistance of the on-chip interconnect in the impact mechanism. This methodology allows investigation of the role of the separate devices (also parasitic devices) in the analog/RF circuit in the overall impact. In this way, it is revealed which devices have to be taken care of (shielding, topology change) to protect the circuit against substrate noise. The developed methodology is used to analyze the impact of substrate noise on a 3 GHz LC-tank voltage controlled oscillator (VCO) designed in a high-ohmic 0.18 /spl mu/m 1 PM6 CMOS technology. For this VCO (in the investigated frequency range from DC to 15 MHz) impact is mainly caused by resistive coupling of noise from the substrate to the non-ideal on-chip ground interconnect, resulting in analog ground bounce and frequency modulation. Hence, the presented test-case reveals the important role of the on-chip interconnect in the phenomenon of substrate noise impact.", "venue": "Design, Automation and Test in Europe", "authors": ["Charlotte  Soens", "Geert Van der Plas", "Piet  Wambacq", "St\u00e9phane  Donnay"], "year": 2005, "n_citations": 13}
{"id": 6180309, "s2_id": "9d939383dcf303a67568efc552f89b6362eb8484", "title": "A Study on the Influence of Caching: Sequences of Dense Linear Algebra Kernels", "abstract": "It is universally known that caching is critical to attain high-performance implementations: In many situations, data locality (in space and time) plays a bigger role than optimizing the (number of) arithmetic floating point operations. In this paper, we show evidence that at least for linear algebra algorithms, caching is also a crucial factor for accurate performance modeling and performance prediction.", "venue": "VECPAR", "authors": ["Elmar  Peise", "Paolo  Bientinesi"], "year": 2014, "n_citations": 9}
{"id": 6183211, "s2_id": "d29d342952a4d14334d83b260c886dec97a5b1be", "title": "Efficient Modular Arithmetic for SIMD Devices", "abstract": "This paper describes several new improvements of modular arithmetic and how to exploit them in order to gain more efficient implementations of commonly used algorithms, especially in cryptographic applications. We further present a new record for modular multiplications per second on a single desktop computer as well as a new record for the ECM factoring algorithm. This new results allow building personal computers which can handle more than 3 billion modular multiplications per second for a 192 bit module at moderate costs using modern graphic cards.", "venue": "IACR Cryptol. ePrint Arch.", "authors": ["Wilke  Trei"], "year": 2013, "n_citations": 3}
{"id": 6188028, "s2_id": "5c9b04e675b8a76dea18a4a045161635cabc535e", "title": "Anonymity and Confidentiality in Secure Distributed Simulation", "abstract": "Research on data confidentiality, integrity and availability is gaining momentum in the ICT community, due to the intrinsically insecure nature of the Internet. While many distributed systems and services are now based on secure communication protocols to avoid eavesdropping and protect confidentiality\u2019 the techniques usually employed in distributed simulations do not consider these issues at all. This is probably due to the fact that many real-world simulators rely on monolithic, offline approaches and therefore the issues above do not apply. However, the complexity of the systems to be simulated, and the rise of distributed and cloud based simulation, now impose the adoption of secure simulation architectures. This paper presents a solution to ensure both anonymity and confidentiality in distributed simulations. A performance evaluation based on an anonymized distributed simulator is used for quantifying the performance penalty for being anonymous. The obtained results show that this is a viable solution.", "venue": "2018 IEEE/ACM 22nd International Symposium on Distributed Simulation and Real Time Applications (DS-RT)", "authors": ["Antonio  Magnani", "Gabriele  D'Angelo", "Stefano  Ferretti", "Moreno  Marzolla"], "year": 2018, "n_citations": 2}
{"id": 6188711, "s2_id": "c8042cb13bfed61dc5a52d6b1824fbe696d2db3b", "title": "An efficient, secure and trusted channel protocol for avionics wireless networks", "abstract": "Avionics networks rely on a set of stringent reliability and safety requirements. In existing deployments, most of these networks are based on a wired technology, which supports these requirements. Furthermore, this technology simplifies the security management of the network since certain assumptions can be safely made, including the inability of an attacker to access the network, and the fact that it is almost impossible for an attacker to introduce a node into the network. The proposal for Avionics Wireless Networks (AWNs, currently under consideration by multiple aerospace working groups, promises a reduction in the complexity of electrical wiring harness design and fabrication, a reduction in the total weight of wires, increased customization possibilities, and the capacity to monitor otherwise inaccessible moving or rotating aircraft parts such as landing gear and some sections of the aircraft engines. While providing these benefits, the AWN must ensure that it provides levels of safety that are at minimum equivalent to those offered by the wired equivalent. In this paper, we propose a secure and trusted channel protocol that satisfies the stated security and operational requirements for an AWN protocol. There are three main objectives for this protocol. First, the protocol has to provide the assurance that all communicating entities can trust each other, and can trust their internal (secure) software and hardware states. Second, the protocol has to establish a fair key exchange between all communicating entities so as to provide a secure channel. Finally, the third objective is to be efficient for both the initial start-up of the network and when resuming a session after a cold and/or warm restart of a node. The proposed protocol is implemented within a demo AWN, and performance measurements are presented based on this implementation. In addition, we formally verify our proposed protocol using CasperFDR.", "venue": "2016 IEEE/AIAA 35th Digital Avionics Systems Conference (DASC)", "authors": ["Raja Naeem Akram", "Konstantinos  Markantonakis", "Keith  Mayes", "Pierre-Fran\u00e7ois  Bonnefoi", "Damien  Sauveron", "Serge  Chaumette"], "year": 2016, "n_citations": 13}
{"id": 6193159, "s2_id": "13d10c39cb1bbef9ab654a14b9d24517ac6a8c3e", "title": "Open-MPI over MOSIX: paralleled computing in a clustered world", "abstract": "Recent increased interest in Cloud computing emphasizes the need to find an adequate solution to the load-balancing problem in parallel computing -- efficiently running several jobs concurrently on a cluster of shared computers (nodes). One approach to solve this problem is by preemptive process migration -- the transfer of running processes between nodes. A possible drawback of this approach is the increased overhead between heavily communicating processes. This project presents a solution to this last problem by incorporating the process migration capability of MOSIX into Open-MPI and by reducing the resulting communication overhead. Specifically, we developed a module for direct communication (DiCOM) between migrated Open-MPI processes, to overcome the increased communication latency of TCP/IP between such processes. The outcome is reduced run-time by improved resource allocation.", "venue": "ArXiv", "authors": ["Adam  Lev-Libfeld", "Alexander  Margolin", "Amnon  Barak"], "year": 2019, "n_citations": 0}
{"id": 6193783, "s2_id": "4cd454d5c68652e9cd9a8ee072534331348ea2f0", "title": "Memory Slices: A Modular Building Block for Scalable, Intelligent Memory Systems", "abstract": "While reduction in feature size makes computation cheaper in terms of latency, area, and power consumption, performance of emerging data-intensive applications is determined by data movement. These trends have introduced the concept of scalability as reaching a desirable performance per unit cost by using as few number of units as possible. Many proposals have moved compute closer to the memory. However, these efforts ignored maintaining a balance between bandwidth and compute rate of an architecture, with those of applications, which is a key principle in designing scalable large systems. This paper proposes the use of memory slices, a modular building block for scalable memory systems integrated with compute, in which performance scales with memory size (and volume of data). The slice architecture utilizes a programmable memory interface feeding a systolic compute engine with high reuse rate. The modularity feature of slice-based systems is exploited with a partitioning and data mapping strategy across allocated memory slices where training performance scales with the data size. These features enable shifting the most pressure to cheap compute units rather than expensive memory accesses or transfers via interconnection network. An application of the memory slices to a scale-out memory system is accelerating the training of recurrent, convolutional, and hybrid neural networks (RNNs and RNNs+CNN) that are forming cloud workloads. The results of our cycle-level simulations show that memory slices exhibits a superlinear speedup when the number of slices increases. Furthermore, memory slices improve power efficiency to 747 GFLOPs/J for training LSTMs. While our current evaluation uses memory slices with 3D packaging, a major value is that slices can also be constructed with a variety of packaging options, for example with DDR-based memory units.", "venue": "ArXiv", "authors": ["Bahar  Asgari", "Saibal  Mukhopadhyay", "Sudhakar  Yalamanchili"], "year": 2018, "n_citations": 1}
{"id": 6198953, "s2_id": "e94b6d7fbc75acff68f7493d31916a4dc1ba56ff", "title": "Mitigating network noise on Dragonfly networks through application-aware routing", "abstract": "System noise can negatively impact the performance of HPC systems, and the interconnection network is one of the main factors contributing to this problem. To mitigate this effect, adaptive routing sends packets on non-minimal paths if they are less congested. However, while this may mitigate interference caused by congestion, it also generates more traffic since packets traverse additional hops, causing in turn congestion on other applications and on the application itself. In this paper, we first describe how to estimate network noise. By following these guidelines, we show how noise can be reduced by using routing algorithms which select minimal paths with a higher probability. We exploit this knowledge to design an algorithm which changes the probability of selecting minimal paths according to the application characteristics. We validate our solution on microbenchmarks and real-world applications on two systems relying on a Dragonfly interconnection network, showing noise reduction and performance improvement.", "venue": "SC", "authors": ["Daniele De Sensi", "Salvatore Di Girolamo", "Torsten  Hoefler"], "year": 2019, "n_citations": 13}
{"id": 6202504, "s2_id": "6b791658b90614d4f9b28bdd8d13ebc9302ee967", "title": "Towards Cost-Optimal Policies for DAGs to Utilize IaaS Clouds with Online Learning", "abstract": "Premier cloud service providers (CSPs) offer two types of purchase options, namely on-demand and spot instances, with time-varying features in availability and price. Users like startups have to operate on a limited budget and similarly others hope to reduce their costs. While interacting with a CSP, central to their concerns is the process of cost-effectively utilizing different purchase options possibly in addition to self-owned instances. A job in data intensive applications is typically represented by a directed acyclic graph which can further be transformed into a chain of tasks. The key to achieving cost efficiency is determining the allocation of a specific deadline to each task, as well as the allocation of different types of instances to the task. In this paper, we propose a framework that determines the optimal allocation of deadlines to tasks. The framework also features an optimal policy to determine the allocation of spot and on-demand instances in a predefined time window, and a near-optimal policy for allocating self-owned instances. The policies are designed to be parametric to support the usage of online learning to infer the optimal values against the dynamics of cloud markets. Finally, several intuitive heuristics are used as baselines to validate the cost improvement brought by the proposed solutions. We show that the cost improvement over the state-of-the-art is up to 24.87% when spot and on-demand instances are considered and up to 59.05% when self-owned instances are considered.", "venue": "ArXiv", "authors": ["Xiaohu  Wu", "Han  Yu", "Giuliano  Casale", "Guanyu  Gao"], "year": 2021, "n_citations": 0}
{"id": 6202650, "s2_id": "16b5384f4697f6d9c64c18b988b0b7ca2908bd8b", "title": "LIRS: Enabling efficient machine learning on NVM-based storage via a lightweight implementation of random shuffling", "abstract": "Machine learning algorithms, such as Support Vector Machine (SVM) and Deep Neural Network (DNN), have gained a lot of interests recently. When training a machine learning algorithm, randomly shuffle all the training data can improve the testing accuracy and boost the convergence rate. Nevertheless, realizing training data random shuffling in a real system is not a straightforward process due to the slow random accesses in hard disk drive (HDD). To avoid frequent random disk access, the effect of random shuffling is often limited in existing approaches. With the emerging non-volatile memory-based storage device, such as Intel Optane SSD, which provides fast random accesses, we propose a lightweight implementation of random shuffling (LIRS) to randomly shuffle the indexes of the entire training dataset, and the selected training instances are directly accessed from the storage and packed into batches. Experimental results show that LIRS can reduce the total training time of SVM and DNN by 49.9% and 43.5% on average, and improve the final testing accuracy on DNN by 1.01%.", "venue": "ArXiv", "authors": ["Zhi-Lin  Ke", "Hsiang-Yun  Cheng", "Chia-Lin  Yang"], "year": 2018, "n_citations": 4}
{"id": 6204585, "s2_id": "da36227a6b35e474bcd7a09fa4e17862fa1e743a", "title": "On open problems in polling systems", "abstract": "In the present paper we address two open problems concerning polling systems, viz., queueing systems consisting of multiple queues attended by a single server that visits the queues one at a time. The first open problem deals with a system consisting of two queues, one of which has gated service, while the other receives 1-limited service. The second open problem concerns polling systems with general (renewal) arrivals and deterministic switch-over times that become infinitely large. We discuss related, known results for both problems, and the difficulties encountered when trying to solve them.", "venue": "Queueing Syst. Theory Appl.", "authors": ["Marko A. A. Boon", "Onno J. Boxma", "Erik M. M. Winands"], "year": 2011, "n_citations": 8}
{"id": 6205119, "s2_id": "d7df1afd6d15e0e4021dde020f08ea33c4c32815", "title": "Assessing the Feasibility of Web-Request Prediction Models on Mobile Platforms", "abstract": "Prefetching web pages is a well-studied solution to reduce network latency by predicting users\u2019 future actions based on their past behaviors. However, such techniques are largely unexplored on mobile platforms. Today\u2019s privacy regulations make it infeasible to explore prefetching with the usual strategy of amassing large amounts of data over long periods and constructing conventional, \"large\" prediction models. Our work is based on the observation that this may not be necessary: Given previously reported mobile-device usage trends (e.g., repetitive behaviors in brief bursts), we hypothesized that prefetching should work effectively with \"small\" models trained on mobile-user requests collected during much shorter time periods. To test this hypothesis, we constructed a framework for automatically assessing prediction models, and used it to conduct an extensive empirical study based on over 15 million HTTP requests collected from nearly 11,500 mobile users during a 24-hour period, resulting in over 7 million models. Our results demonstrate the feasibility of prefetching with small models on mobile platforms, directly motivating future work in this area. We further introduce several strategies for improving prediction models while reducing the model size. Finally, our framework provides the foundation for future explorations of effective prediction models across a range of usage scenarios.", "venue": "2021 IEEE/ACM 8th International Conference on Mobile Software Engineering and Systems (MobileSoft)", "authors": ["Yixue  Zhao", "Siwei  Yin", "Adriana  Sejfia", "Marcelo Schmitt Laser", "Haoyu  Wang", "Nenad  Medvidovic"], "year": 2021, "n_citations": 0}
{"id": 6212243, "s2_id": "0438b701de7f8f0cdec80bfc90ca421e186c5e8b", "title": "A High-Performance Persistent Memory Key-Value Store with Near-Memory Compute", "abstract": "MCAS (Memory Centric Active Storage) is a persistent memory tier for high-performance durable data storage. It is designed from the ground-up to provide a key-value capability with low-latency guarantees and data durability through memory persistence and replication. To reduce data movement and make further gains in performance, we provide support for user-defined \u201cpush-down\u201d operations (known as Active Data Objects) that can execute directly and safely on the value-memory associated with one or more keys. The ADO mechanism allows complex pointer-based dynamic data structures (e.g., trees) to be stored and operated on in persistent memory. To this end, we examine a real-world use case for MCAS-ADO in the handling of enterprise storage system metadata for Continuous Data Protection (CDP). This requires continuously updated complex metadata that must be kept consistent and durable. In this paper, we i.) present the MCAS-ADO system architecture, ii.) show how the CDP use case is implemented, and finally iii.) give an evaluation of system performance in the context of this use case.", "venue": "ArXiv", "authors": ["Daniel  Waddington", "Clem  Dickey", "Luna  Xu", "Moshik  Hershcovitch", "Sangeetha  Seshadri"], "year": 2021, "n_citations": 0}
{"id": 6212586, "s2_id": "9c25be282f1e8e3d92eef2c8a388d7c7bbbdeacf", "title": "Scalable Load Balancing in the Presence of Heterogeneous Servers", "abstract": "Abstract Heterogeneity is becoming increasingly ubiquitous in modern large-scale computer systems. Developing good load balancing policies for systems whose resources have varying speeds is crucial in achieving low response times. Indeed, how best to dispatch jobs to servers is a classical and well-studied problem in the queueing literature. Yet the bulk of existing work on large-scale systems assumes homogeneous servers; unfortunately, policies that perform well in the homogeneous setting can cause unacceptably poor performance in heterogeneous systems. We adapt the \u201cpower-of- d \u201d versions of both the Join-the-Idle-Queue and Join-the-Shortest-Queue policies to design two corresponding families of heterogeneity-aware dispatching policies, each of which is parameterized by a pair of routing probabilities. Unlike their heterogeneity-unaware counterparts, our policies use server speed information both when choosing which servers to query and when probabilistically deciding where (among the queried servers) to dispatch jobs. Both of our policy families are analytically tractable: our mean response time and queue length distribution analyses are exact as the number of servers approaches infinity, under standard assumptions. Furthermore, our policy families achieve maximal stability and outperform well-known dispatching rules\u2014including heterogeneity-aware policies such as Shortest-Expected-Delay\u2014with respect to mean response time.", "venue": "Perform. Evaluation", "authors": ["Kristen  Gardner", "Jazeem Abdul Jaleel", "Alexander  Wickeham", "Sherwin  Doroudi"], "year": 2021, "n_citations": 10}
{"id": 6212762, "s2_id": "3416758841aefb9b7e9b9d1857c893b512c5897a", "title": "Application-aware Retiming of Accelerators: A High-level Data-driven Approach", "abstract": "Flexibility at hardware level is the main driving force behind adaptive systems whose aim is to realise microarhitecture deconfiguration 'online'. This feature allows the software/hardware stack to tolerate drastic changes of the workload in data centres. With emerge of FPGA reconfigurablity this technology is becoming a mainstream computing paradigm. Adaptivity is usually accompanied by the high-level tools to facilitate multi-dimensional space exploration. An essential aspect in this space is memory orchestration where on-chip and off-chip memory distribution significantly influences the architecture in coping with the critical spatial and timing constraints, e.g. Place and Route. This paper proposes a memory smart technique for a particular class of adaptive systems: Elastic Circuits which enjoy slack elasticity at fine level of granularity. We explore retiming of a set of popular benchmarks via investigating the memory distribution within and among accelerators. The area, performance and power patterns are adopted by our high-level synthesis framework, with respect to the behaviour of the input descriptions, to improve the quality of the synthesised elastic circuits.", "venue": "ArXiv", "authors": ["Ana  Lava", "Mahdi Jelodari Mamaghani", "Siamak  Mohammadi", "Steve B. Furber"], "year": 2016, "n_citations": 0}
{"id": 6217654, "s2_id": "492562ea9565bb9638032bf7efa0f6895065b56b", "title": "A Simple Model for Portable and Fast Prediction of Execution Time and Power Consumption of GPU Kernels", "abstract": "Characterizing compute kernel execution behavior on GPUs for efficient task scheduling is a non-trivial task. We address this with a simple model enabling portable and fast predictions among different GPUs using only hardware-independent features. This model is built based on random forests using 189 individual compute kernels from benchmarks such as Parboil, Rodinia, Polybench-GPU, and SHOC. Evaluation of the model performance using cross-validation yields a median Mean Average Percentage Error (MAPE) of 8.86\u201352.0% for time and 1.84\u20132.94% for power prediction across five different GPUs, while latency for a single prediction varies between 15 and 108 ms.", "venue": "ACM Trans. Archit. Code Optim.", "authors": ["Lorenz  Braun", "Sotirios  Nikas", "Chen  Song", "Vincent  Heuveline", "Holger  Fr\u00f6ning"], "year": 2021, "n_citations": 4}
{"id": 6220641, "s2_id": "958449f3506cc733d735bc15c3d3c1a4d2e1113c", "title": "Limits of responsiveness concerning human-readable knowledge bases: an operational analysis", "abstract": "Introduction. The purpose of this work is the evaluation of responsiveness when remote users communicate with a human-readable knowledge base (KB). Responsiveness [R(s)] is considered here as a measure of service quality. Method. The preferred method is operational analysis, a variation of classical stochastic theory, which allows for the study of user-system interaction with minimal computational effort. Analysis. The analysis is based on well-known performance metrics, such as service ability, elapsed time, and throughput: from these metrics estimates of R(s) are derived analytically. Results. Critical points indicating congestion are obtained: these are limits on the number of admissible requests and the number of connected users. Also obtained is a sufficient condition for achieving flow balance between the KB host and the request-relaying servers. Conclusions. When R(s) is within normal limits, users should appreciate the benefits from using the services offered by their KB host. When bottlenecks are formed, R(s) declines, and the whole communication system heads for saturation. Flow balancing procedures are necessary for the elimination of bottlenecks, which leads to a better resource management.", "venue": "ArXiv", "authors": ["G. C. Pentzaropoulos"], "year": 2010, "n_citations": 0}
{"id": 6223041, "s2_id": "64679b0a8547bb9430e4d43bdce3571e1229a2af", "title": "Ball-Larus path profiling across multiple loop iterations", "abstract": "Identifying the hottest paths in the control flow graph of a routine can direct optimizations to portions of the code where most resources are consumed. This powerful methodology, called path profiling, was introduced by Ball and Larus in the mid 90's [4] and has received considerable attention in the last 15 years for its practical relevance. A shortcoming of the Ball-Larus technique was the inability to profile cyclic paths, making it difficult to mine execution patterns that span multiple loop iterations. Previous results, based on rather complex algorithms, have attempted to circumvent this limitation at the price of significant performance losses even for a small number of iterations. In this paper, we present a new approach to multi-iteration path profiling, based on data structures built on top of the original Ball-Larus numbering technique. Our approach allows the profiling of all executed paths obtained as a concatenation of up to k Ball-Larus acyclic paths, where k is a user-defined parameter. We provide examples showing that this method can reveal optimization opportunities that acyclic-path profiling would miss. An extensive experimental investigation on a large variety of Java benchmarks on the Jikes RVM shows that our approach can be even faster than Ball-Larus due to fewer operations on smaller hash tables, producing compact representations of cyclic paths even for large values of k.", "venue": "OOPSLA 2013", "authors": ["Daniele Cono D'Elia", "Camil  Demetrescu", "Irene  Finocchi"], "year": 2013, "n_citations": 12}
{"id": 6223775, "s2_id": "e1497dbe84c989c2eab3423e789b08f6475e027f", "title": "Load Balancing Guardrails: Keeping Your Heavy Traffic on the Road to Low Response Times", "abstract": "Load balancing systems, comprising a central dispatcher and a scheduling policy at each server, are widely used in practice, and their response time has been extensively studied in the theoretical literature. While much is known about the scenario where the scheduling at the servers is First-Come-First-Served (FCFS), to minimize mean response time we must use Shortest-Remaining-Processing-Time (SRPT) scheduling at the servers. Much less is known about dispatching polices when SRPT scheduling is used. Unfortunately, traditional dispatching policies that are used in practice in systems with FCFS servers often have poor performance in systems with SRPT servers. In this paper, we devise a simple fix that can be applied to any dispatching policy. This fix, called guardrails, ensures that the dispatching policy yields optimal mean response time under heavy traffic when used in a system with SRPT servers. Any dispatching policy, when augmented with guardrails, becomes heavy-traffic optimal. Our results yield the first analytical bounds on mean response time for load balancing systems with SRPT scheduling at the servers.", "venue": "Abstracts of the 2019 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems", "authors": ["Isaac  Grosof", "Ziv  Scully", "Mor  Harchol-Balter"], "year": 2019, "n_citations": 11}
{"id": 6227812, "s2_id": "4cca49e45f2b815c6da309f7d4deda03af30993b", "title": "Decay of Tails at Equilibrium for FIFO Join the Shortest Queue Networks", "abstract": "In join the shortest queue networks, incoming jobs are assigned to the shortest queue from among a randomly chosen subset of $D$ queues, in a system of $N$ queues; after completion of service at its queue, a job leaves the network. We also assume that jobs arrive into the system according to a rate-$\\alpha N$ Poisson process, $\\alpha 1$. We show under the above ansatz that, as $N\\rightarrow\\infty$, the tail of the equilibrium queue size exhibits a wide range of behavior depending on the relationship between $\\beta$ and $D$. In particular, if $\\beta>D/(D-1)$, the tail is doubly exponential and, if $\\beta<D/(D-1)$, the tail has a power law. When $\\beta=D/(D-1)$, the tail is exponentially distributed.", "venue": "ArXiv", "authors": ["Maury  Bramson", "Yi  Lu", "Balaji  Prabhakar"], "year": 2011, "n_citations": 43}
{"id": 6228015, "s2_id": "1cbb1741f65ad33dfee492be40aa0dbac772a468", "title": "Congestion control of TCP flows in Internet routers by means of index policy", "abstract": "In this paper we address the problem of fast and fair transmission of flows in a router, which is a fundamental issue in networks like the Internet. We model the interaction between a source using the Transmission Control Protocol (TCP) and a bottleneck router with the objective of designing optimal packet admission controls in the router queue. We focus on the relaxed version of the problem obtained by relaxing the fixed buffer capacity constraint that must be satisfied at all time epoch. The relaxation allows us to reduce the multi-flow problem into a family of single-flow problems, for which we can analyze both theoretically and numerically the existence of optimal control policies of special structure. In particular, we show that for a variety of parameters, TCP flows can be optimally controlled in routers by so-called index policies, but not always by threshold policies. We have also implemented the index policy in Network Simulator-3 and tested in a simple topology their applicability in real networks. The simulation results show that the index policy achieves a wide range of desirable properties with respect to fairness between different TCP versions, across users with different round-trip-time and minimum buffer required to achieve full utility of the queue.", "venue": "Comput. Networks", "authors": ["K.  Avrachenkov", "U.  Ayesta", "J.  Doncel", "P.  Jacko"], "year": 2013, "n_citations": 30}
{"id": 6230091, "s2_id": "e77060f985ba9516fc3b43d63e50570515170c92", "title": "Asymptotic Optimality of the Static Frequency Caching in the Presence of Correlated Requests", "abstract": "Renewed interest in caching algorithms stems from their application to content distribution on the Web. When documents are of equal size and their requests are independent and equally distributed, it is well known that static algorithm that keeps the most frequently requested documents in the cache is optimal. However, there are no explicit caching algorithms that are provably optimal when the requests are statistically correlated. In this paper, we show, maybe somewhat surprisingly, that keeping the most frequently requested documents in the cache is still optimal for large cache sizes even if the requests are strongly correlated. We model the statistical dependency of requests using semi-Markov modulated processes that can capture strong statistical correlation, including the empirically observed long-range dependence in the Web access sequences. \n \nAlthough frequency algorithm and its practical version least-frequently-used policy is not commonly used in practice due to their complexity and static nature, our result provides a benchmark for evaluating the popular heuristic schemes. In particular, an important corollary of our main theorem and recent result from [9] is that the widely used least-recently-used heuristic is asymptotically near-optimal under the semi-Markov modulated requests and generalized Zipf's law document frequencies.", "venue": "ANALCO", "authors": ["Predrag R. Jelenkovic", "Ana  Radovanovic"], "year": 2006, "n_citations": 5}
{"id": 6231064, "s2_id": "62d92aac93c3dfb7c377a44c1ef532a165f6661e", "title": "ACCBench: A Framework for Comparing Causality Algorithms", "abstract": "Modern socio-technical systems are increasingly complex. A fundamental problem is that \nthe borders of such systems are often not well-defined a-priori, which among other problems \ncan lead to unwanted behavior during runtime. Ideally, unwanted behavior should \nbe prevented. If this is not possible the system shall at least be able to help determine \npotential cause(s) a-posterori, identify responsible parties and make them accountable for \ntheir behavior. Recently, several algorithms addressing these concepts have been proposed. \nHowever, the applicability of the corresponding approaches, specifically their effectiveness \nand performance, is mostly unknown. Therefore, in this paper, we propose ACCBench, a \nbenchmark tool that allows to compare and evaluate causality algorithms under a consistent \nsetting. Furthermore, we contribute an implementation of the two causality algorithms \nby [7] and [6] as well as of a policy compliance approach based on some concepts of [16]. \nLastly, we conduct a case study of an Intelligent Door Control System, which exposes concrete \nstrengths and weaknesses of all algorithms under different aspects. In the course of \nthis, we show that the effectiveness of the algorithms in terms of cause detection as well as \ntheir performance differ to some extent. In addition, our analysis reports on some qualitative \naspects that should be considered when evaluating each algorithm. For example, the \nhuman effort needed to configure the algorithm and model the use case is analyzed.", "venue": "CREST@ETAPS", "authors": ["Simon  Rehwald", "Amjad  Ibrahim", "Kristian  Beckers", "Alexander  Pretschner"], "year": 2017, "n_citations": 3}
{"id": 6232127, "s2_id": "2b4f932d418aacdf8dec3db8335c1e3d6ecee49a", "title": "Accelerating Empowerment Computation with UCT Tree Search", "abstract": "Models of intrinsic motivation present an important means to produce sensible behaviour in the absence of extrinsic rewards. Applications in video games are varied, and range from intrinsically motivated general game-playing agents to non-player characters such as companions and enemies. The information-theoretic quantity of Empowerment is a particularly promising candidate motivation to produce believable, generic and robust behaviour. However, while it can be used in the absence of external reward functions that would need to be crafted and learned, empowerment is computationally expensive. In this paper, we propose a modified UCT tree search method to mitigate empowerment\u2019s computational complexity in discrete and deterministic scenarios. We demonstrate how to modify a Monte-Carlo Search Tree with UCT to realise empowerment maximisation, and discuss three additional modifications that facilitate better sampling. We evaluate the approach both quantitatively, by analysing how close our approach gets to the baseline of exhaustive empowerment computation with varying amounts of computational resources, and qualitatively, by analysing the resulting behaviour in a Minecraft-like scenario.", "venue": "2018 IEEE Conference on Computational Intelligence and Games (CIG)", "authors": ["Christoph  Salge", "Christian  Guckelsberger", "Rodrigo  Canaan", "Tobias  Mahlmann"], "year": 2018, "n_citations": 2}
{"id": 6232145, "s2_id": "b75b21f440083603a1c6c6d14770a04e74483ffe", "title": "HPS: A C++11 High Performance Serialization Library", "abstract": "Data serialization is a common and crucial component in high performance computing. In this paper, I present a C++11 based serialization library for performance critical systems. It provides an interface similar to Boost but up to 150% faster and beats several popular serialization libraries.", "venue": "ArXiv", "authors": ["Junhao  Li"], "year": 2018, "n_citations": 0}
{"id": 6240255, "s2_id": "58ffd627fd8d382d6deaff71fb1af905464dabc6", "title": "Bridging the Architecture Gap: Abstracting Performance-Relevant Properties of Modern Server Processors", "abstract": "We describe a universal modeling approach for predicting single- and multicore runtime of steady-state loops on server processors. To this end we strictly differentiate between application and machine models: An application model comprises the loop code, problem sizes, and other runtime parameters, while a machine model is an abstraction of all performance-relevant properties of a CPU. We introduce a generic method for determining machine models and present results for relevant server-processor architectures by Intel, AMD, IBM, and Marvell/Cavium. Considering this wide range of architectures, the set of features required for adequate performance modeling is surprisingly small. To validate our approach, we compare performance predictions to empirical data for an OpenMP-parallel preconditioned CG algorithm, which includes compute- and memory-bound kernels. Both single- and multicore analysis shows that the model exhibits average and maximum relative errors of 5% and 10%. Deviations from the model and insights gained are discussed in detail.", "venue": "Supercomput. Front. Innov.", "authors": ["Johannes  Hofmann", "Christie L. Alappat", "Georg  Hager", "Dietmar  Fey", "Gerhard  Wellein"], "year": 2020, "n_citations": 14}
{"id": 6242113, "s2_id": "675b706ef2cdb96d6ea179a3ab0cfc8e09d4bcf2", "title": "Architecture, implementation and parallelization of the software to search for periodic gravitational wave signals", "abstract": "Abstract The parallelization, design and scalability of the PolGrawAllSky code to search for periodic gravitational waves from rotating neutron stars is discussed. The code is based on an efficient implementation of the F -statistic using the Fast Fourier Transform algorithm. To perform an analysis of data from the advanced LIGO and Virgo gravitational wave detectors\u2019 network, which will start operating in 2015, hundreds of millions of CPU hours will be required\u2014the code utilizing the potential of massively parallel supercomputers is therefore mandatory. We have parallelized the code using the Message Passing Interface standard, implemented a mechanism for combining the searches at different sky-positions and frequency bands into one extremely scalable program. The parallel I/O interface is used to escape bottlenecks, when writing the generated data into file system. This allowed to develop a highly scalable computation code, which would enable the data analysis at large scales on acceptable time scales. Benchmarking of the code on a Cray XE6 system was performed to show efficiency of our parallelization concept and to demonstrate scaling up to 50 thousand cores in parallel. Program summary Program title: parallel PolGrawAllSky Catalogue identifier: AEUX_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEUX_v1_0.html Program obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 163747 No. of bytes in distributed program, including test data, etc.: 28989030 Distribution format: tar.gz Programming language: C. Computer: Any parallel computing platform supporting MPI standard. Operating system: Linux as well any other supporting MPI standard. Has the code been vectorized or parallelized?: Yes, using MPI. Tested with up to 50208 processors RAM: 1 Gigabyte per parallel task Classification: 1.5. External routines: MPI v.2 or newer, FFTW v.3 or newer Nature of problem: Search for periodic gravitational waves from rotating neutron stars. Solution method: The F -statistic method using the Fast Fourier Transform algorithm. Running time: The example provided takes approximately 30 mins with 256 processors.", "venue": "Comput. Phys. Commun.", "authors": ["Gevorg  Poghosyan", "Sanchit  Matta", "Achim  Streit", "Michal  Bejger", "Andrzej  Kr\u00f3lak"], "year": 2015, "n_citations": 10}
{"id": 6245005, "s2_id": "497dc0fe5d55ff16bc6cdcbf0410ea42376c9f6b", "title": "Queueing Analysis of a Large-Scale Bike Sharing System through Mean-Field Theory", "abstract": "The bike sharing systems are fast increasing as a public transport mode in urban short trips, and have been developed in many major cities around the world. A major challenge in the study of bike sharing systems is that large-scale and complex queueing networks have to be applied through multi-dimensional Markov processes, while their discussion always suffers a common difficulty: State space explosion. For this reason, this paper provides a mean-field computational method to study such a large-scale bike sharing steps: Firstly, a multi-dimensional Markov process is set up for expressing the states of the bike sharing system, and the empirical process of the multi-dimensional Markov process is given to partly overcome the difficulty of state space explosion. Based on this, the mean-field equations are derived by means of a virtual time-inhomogeneous M(t)/M(t)/1/K queue whose arrival and service rates are determined by the mean-field computation. Secondly, the martingale limit is employed to investigate the limiting behavior of the empirical process, the fixed point is proved to be unique so that it can be computed by means of a nonlinear birth-death process, the asymptotic independence of this system is discussed simply, and specifically, these lead to numerical computation of the steady-state probability of the problematic (empty or full) stations. Finally, some numerical examples are given for valuable observation on how the steady-state probability of the problematic stations depends on some crucial parameters of the bike sharing system.", "venue": "ArXiv", "authors": ["Quan-Lin  Li", "Chang  Chen", "Rui-Na  Fan", "Liang  Xu", "Jing-Yu  Ma"], "year": 2016, "n_citations": 10}
{"id": 6247460, "s2_id": "562ed6c9e6cf4875e61f9eed701fa8a0e3cff204", "title": "On the Delay-Storage Trade-Off in Content Download from Coded Distributed Storage Systems", "abstract": "We study how coding in distributed storage reduces expected download time, in addition to providing reliability against disk failures. The expected download time is reduced because when a content file is encoded with redundancy and distributed across multiple disks, reading only a subset of the disks is sufficient for content reconstruction. For the same total storage used, coding exploits the diversity in storage better than simple replication, and hence gives faster download. We use a novel fork-join queueing framework to model multiple users requesting the content simultaneously, and derive bounds on the expected download time. Our system model and results are a novel generalization of the fork-join system that is studied in queueing theory literature. Our results demonstrate the fundamental trade-off between the expected download time and the amount of storage space. This trade-off can be used for design of the amount of redundancy required to meet the delay constraints on content delivery.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Gauri  Joshi", "Yanpei  Liu", "Emina  Soljanin"], "year": 2014, "n_citations": 169}
{"id": 6252637, "s2_id": "0f9080d297fc22dcf24dfd8ffcd3de5cea04c689", "title": "Quantifying Performance Bottlenecks of Stencil Computations Using the Execution-Cache-Memory Model", "abstract": "Stencil algorithms on regular lattices appear in many fields of computational science, and much effort has been put into optimized implementations. Such activities are usually not guided by performance models that provide estimates of expected speedup. Understanding the performance properties and bottlenecks by performance modeling enables a clear view on promising optimization opportunities. In this work we refine the recently developed Execution-Cache-Memory (ECM) model and use it to quantify the performance bottlenecks of stencil algorithms on a contemporary Intel processor. This includes applying the model to arrive at single-core performance and scalability predictions for typical \"corner case\" stencil loop kernels. Guided by the ECM model we accurately quantify the significance of \"layer conditions,\" which are required to estimate the data traffic through the memory hierarchy, and study the impact of typical optimization approaches such as spatial blocking, strength reduction, and temporal blocking for their expected benefits. We also compare the ECM model to the widely known Roofline model.", "venue": "ICS", "authors": ["Holger  Stengel", "Jan  Treibig", "Georg  Hager", "Gerhard  Wellein"], "year": 2015, "n_citations": 94}
{"id": 6254535, "s2_id": "17cf330bb7ab1163c664155623a13028aac29c23", "title": "A partitioned shift-without-invert algorithm to improve parallel eigensolution efficiency in real-space electronic transport", "abstract": "Abstract We present an eigenspectrum partitioning scheme without inversion for the recently described real-space electronic transport code, TRANSEC. The primary advantage of TRANSEC is its highly parallel algorithm, which enables studying conductance in large systems. The present scheme adds a new source of parallelization, significantly enhancing TRANSEC\u2019s parallel scalability, especially for systems with many electrons. In principle, partitioning could enable super-linear parallel speedup, as we demonstrate in calculations within TRANSEC. In practical cases, we report better than five-fold improvement in CPU time and similar improvements in wall time, compared to previously-published large calculations. Importantly, the suggested scheme is relatively simple to implement. It can be useful for general large Hermitian or weakly non-Hermitian eigenvalue problems, whenever relatively accurate inversion via direct or iterative linear solvers is impractical.", "venue": "Comput. Phys. Commun.", "authors": ["Baruch  Feldman", "Yunkai  Zhou"], "year": 2016, "n_citations": 2}
{"id": 6255417, "s2_id": "9829e82d7defa82ed229e32d32e8cd036a05ce74", "title": "On Determinism of Game Engines used for Simulation-based Autonomous Vehicle Verification", "abstract": "Game engines are increasingly used as simulation platforms by the autonomous vehicle (AV) community to develop vehicle control systems and test environments. A key requirement for simulation-based development and verification is determinism, since a deterministic process will always produce the same output given the same initial conditions and event history. Thus, in a deterministic simulation environment, tests are rendered repeatable and yield simulation results that are trustworthy and straightforward to debug. However, game engines are seldom deterministic. This paper reviews and identifies the potential causes and effects of non-deterministic behaviours in game engines. A case study using CARLA, an open-source autonomous driving simulation environment powered by Unreal Engine, is presented to highlight its inherent shortcomings in providing sufficient precision in experimental results. Different configurations and utilisations of the software and hardware are explored to determine an operational domain where the simulation precision is sufficiently high i.e. variance between repeated executions becomes negligible for development and testing work. Finally, a method of a general nature is proposed, that can be used to find the domains of permissible variance in game engine simulations for any given system configuration.", "venue": "ArXiv", "authors": ["Greg  Chance", "Abanoub  Ghobrial", "Kevin  McAreavey", "Severin  Lemaignan", "Tony  Pipe", "Kerstin  Eder"], "year": 2021, "n_citations": 1}
{"id": 6256652, "s2_id": "3e0495fa8a67b38ab77af7479ae200878f60b5dd", "title": "Minimizing network bandwidth under latency constraints: The single node case", "abstract": "Much of today\u2019s traffic flows between datacenters over private networks. The operators of those networks have access to detailed traffic profiles with performance goals that need to be met as efficiently as possible, e.g., realizing latency guarantees with minimal network bandwidth. Of particular interest is the extent to which traffic (re)shaping can be of benefit. The paper focuses on the most basic network configuration, namely, a single link network, with extensions to more general, multi-node networks discussed in a companion paper. The main results are in the form of optimal solutions for different types of schedulers of varying complexity. They demonstrate how judicious traffic shaping can help lower complexity schedulers perform nearly as well as more complex ones.", "venue": "2021 33th International Teletraffic Congress (ITC-33)", "authors": ["Jiayi  Song", "Roch  Gu'erin", "Henry  Sariowan"], "year": 2021, "n_citations": 0}
{"id": 6269443, "s2_id": "164f127fe77e6ee566dff67bf93d252e620cb618", "title": "Scaling Hyperledger Fabric Using Pipelined Execution and Sparse Peers", "abstract": "Many proofs of concept blockchain applications built using Hyperledger Fabric, a permissioned blockchain platform, have recently been transformed into production. However, the performance provided by Hyperledger Fabric is of significant concern for enterprises due to steady growth in network usage. Hence, in this paper, we study the performance achieved in a Fabric network using vertical scaling (i.e., by adding more vCPUs) and horizontal scaling (i.e., by adding more nodes) techniques. We observe that network scales very poorly with both of these techniques. With vertical scaling, due to serial execution of validation & commit phases of transactions, the allocated vCPUs are underutilized. With horizontal scaling, due to redundant work between nodes, allocated resources are wasted though it is utilized. Further, we identify these techniques to be unsuited for dynamically scaling a network quickly to mitigate an overload situation, and hence, it results in a 30% drop in the performance. To increase the CPU utilization and hence the performance, we re-architect Fabric to enable pipelined execution of validation & commit phases by introducing dirty state management using a trie data structure. Additionally, we facilitated the validation phase to validate transactions in parallel by introducing a waiting-transactions dependency graph. To avoid redundant work performed between nodes and to quickly scale up a network, we propose a new type of peer node called sparse peer, which selective commits transactions. Overall, we improved the throughput by 3x and reduced the time taken to scale up a network by 96%.", "venue": "ArXiv", "authors": ["Parth  Thakkar", "Senthil  Nathan"], "year": 2020, "n_citations": 11}
{"id": 6269648, "s2_id": "a35fe84c36cdf692153988cab460f7c5665928ad", "title": "Type-Directed Program Synthesis and Constraint Generation for Library Portability", "abstract": "Fast numerical libraries have been a cornerstone of scientific computing for decades, but this comes at a price. Programs may be tied to vendor specific software ecosystems resulting in polluted, non-portable code. As we enter an era of heterogeneous computing, there is an explosion in the number of accelerator libraries required to harness specialized hardware. We need a system that allows developers to exploit ever-changing accelerator libraries, without over-specializing their code. As we cannot know the behavior of future libraries ahead of time, this paper develops a scheme that assists developers in matching their code to new libraries, without requiring the source code for these libraries. Furthermore, it can recover equivalent code from programs that use existing libraries and automatically port them to new interfaces. It first uses program synthesis to determine the meaning of a library, then maps the synthesized description into generalized constraints which are used to search the program for replacement opportunities to present to the developer. We applied this approach to existing large applications from the scientific computing and deep learning domains. Using our approach, we show speedups ranging from 1.1x to over 10x on end to end performance when using accelerator libraries.", "venue": "2019 28th International Conference on Parallel Architectures and Compilation Techniques (PACT)", "authors": ["Bruce  Collie", "Philip  Ginsbach", "Michael F. P. O'Boyle"], "year": 2019, "n_citations": 5}
{"id": 6269937, "s2_id": "eab376ed9f075f06d13608aaf151c30f8aa66744", "title": "Efficient Adaptive Implementation of the Serial Schedule Generation Scheme Using Preprocessing and Bloom Filters", "abstract": "The majority of scheduling metaheuristics use indirect representation of solutions as a way to efficiently explore the search space. Thus, a crucial part of such metaheuristics is a \u201cschedule generation scheme\u201d \u2013 procedure translating the indirect solution representation into a schedule. Schedule generation scheme is used every time a new candidate solution needs to be evaluated. Being relatively slow, it eats up most of the running time of the metaheuristic and, thus, its speed plays significant role in performance of the metaheuristic. Despite its importance, little attention has been paid in the literature to efficient implementation of schedule generation schemes. We give detailed description of serial schedule generation scheme, including new improvements, and propose a new approach for speeding it up, by using Bloom filters. The results are further strengthened by automated control of parameters. Finally, we employ online algorithm selection to dynamically choose which of the two implementations to use. This hybrid approach significantly outperforms conventional implementation on a wide range of instances.", "venue": "LION", "authors": ["Daniel  Karapetyan", "Alexei  Vernitski"], "year": 2017, "n_citations": 0}
{"id": 6270294, "s2_id": "3a9899ac1f40ecce6189f2849fab11a76db38064", "title": "Parallelism-Aware Memory Interference Delay Analysis for COTS Multicore Systems", "abstract": "In modern Commercial Off-The-Shelf (COTS) multicore systems, each core can generate many parallel memory requests at a time. The processing of these parallel requests in the DRAM controller greatly affects the memory interference delay experienced by running tasks on the platform. In this paper, we present a new parallelism-aware worst-case memory interference delay analysis for COTS multicore systems. The analysis considers a COTS processor that can generate multiple outstanding requests and a COTS DRAM controller that has a separate read and write request buffer, prioritizes reads over writes, and supports out-of-order request processing. Focusing on LLC and DRAM bank partitioned systems, our analysis computes worst-case upper bounds on memory-interference delays, caused by competing memory requests. We validate our analysis on a Gem5 full-system simulator modeling a realistic COTS multicore platform, with a set of carefully designed synthetic benchmarks as well as SPEC2006benchmarks. The evaluation results show that our analysis produces safe upper bounds in all tested benchmarks, while the current state-of-the-art analysis significantly under-estimates the delays.", "venue": "2015 27th Euromicro Conference on Real-Time Systems", "authors": ["Heechul  Yun", "Rodolfo  Pellizzoni", "Prathap Kumar Valsan"], "year": 2015, "n_citations": 62}
{"id": 6270859, "s2_id": "13e6f5c2090d5ef519e52273b07a02d90dd78f20", "title": "On the Cluster Admission Problem for Cloud Computing", "abstract": "Cloud computing providers face the problem of matching heterogeneous customer workloads to resources that will serve them. This is particularly challenging if customers, who are already running a job on a cluster, scale their resource usage up and down over time. The provider therefore has to continuously decide whether she can add additional workloads to a given cluster or if doing so would impact existing workloads\u2019 ability to scale. Currently, this is often done using simple threshold policies to reserve large parts of each cluster, which leads to low efficiency (i.e., low average utilization of the cluster). We propose more sophisticated policies for controlling admission to a cluster and demonstrate that they significantly increase cluster utilization. We first introduce the cluster admission problem and formalize it as a constrained Partially Observable Markov Decision Process (POMDP). As it is infeasible to solve the POMDP optimally, we then systematically design admission policies that estimate moments of each workload\u2019s distribution of future resource usage. Via extensive simulations grounded in a trace from Microsoft Azure, we show that our admission policies lead to a substantial improvement over the simple threshold policy. We then show that substantial further gains are possible if high-quality information is available about arriving workloads. Based on this, we propose an information elicitation approach to incentivize users to provide this information and simulate its effects.", "venue": "J. Artif. Intell. Res.", "authors": ["Ludwig  Dierks", "Ian A. Kash", "Sven  Seuken"], "year": 2021, "n_citations": 1}
{"id": 6272474, "s2_id": "dded3517bfbf7240ecec1b50d876d0cf4f7f8e98", "title": "An Analytical Model of Information Dissemination for a Gossip-Based Protocol", "abstract": "We develop an analytical model of information dissemination for a gossip protocol. With this model we analyse how fast an item is replicated through a network. We also determine the optimal size of the exchange buffer, to obtain fast replication. Our results are confirmed by large-scale simulation experiments.", "venue": "ICDCN", "authors": ["Rena  Bakhshi", "Daniela  Gavidia", "Wan  Fokkink", "Maarten van Steen"], "year": 2009, "n_citations": 17}
{"id": 6274555, "s2_id": "6dbbdd72d0b5cb5e369943ca9cd36a86b2937418", "title": "Dynamic Channel Allocation for Class-Based QoS Provisioning and Call Admission in Visible Light Communication", "abstract": "Provisioning of quality of service (QoS) is a key issue in wireless communication systems. Owing to the fact that QoS requirements are not very strict for all traffic types, more calls of higher priority traffic classes can be accommodated by blocking a slightly greater number of calls of lower priority traffic classes. Diverse types of high data rate traffic are supported by existing wireless communication systems, although resources are limited. Hence, priority-based resource allocation can ensure sufficient service quality for calls of important traffic classes. However, the use of fixed guard channels to prioritize any class of calls always reduces channel utilization. Hence, we propose a priority-based dynamic channel reservation scheme for higher priority calls that does not reduce channel utilization significantly. The number of reserved channels for each individual traffic class is calculated using real-time observation of call arrival rates for all traffic. The scheme reduces the call blocking probability for higher priority calls while simultaneously increasing channel utilization. The proposed channel reservation scheme can be efficiently applied for visible light communication (VLC) systems as well as for other wireless communication systems. The proposed Markov Chain model is expected to be very effective for queuing analysis and particularly for implementing a priority-based scheme for any number of traffic classes. We consider VLC as the system model for performance analysis. The numerical results show that the proposed scheme is able to attain a reasonable call blocking probability for higher priority calls, without sacrificing channel utilization.", "venue": "ArXiv", "authors": ["Mostafa Zaman Chowdhury", "Muhammad Shahin Uddin", "Yeong Min Jang"], "year": 2014, "n_citations": 24}
{"id": 6278387, "s2_id": "f8be13c0a971db4a73b1aa525d96dfc8e4569c95", "title": "Simulation of the Internet of Things", "abstract": "This paper presents main concepts and issues concerned with the simulation of Internet of Things (IoT). The heterogeneity of possible scenarios, arising from the massive deployment of an enormous amount of sensors and devices, imposes the use of sophisticated modeling and simulation techniques. In fact, the simulation of IoT introduces several issues from both quantitative and qualitative aspects. We discuss novel simulation techniques to enhance scalability and to permit the real-time execution of massively populated IoT environments (e.g., large-scale smart cities). In particular, we claim that agent-based, adaptive Parallel and Distributed Simulation (PADS) approaches are needed, together with multi-level simulation, which provide means to perform highly detailed simulations, on demand. We present a use case concerned with the simulation of smart territories.", "venue": "2016 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Gabriele  D'Angelo", "Stefano  Ferretti", "Vittorio  Ghini"], "year": 2016, "n_citations": 57}
{"id": 6279262, "s2_id": "d27322e17f69b2602a34b5abfa63e377cf96511d", "title": "Mean Waiting Time in Large-Scale and Critically Loaded Power of d Load Balancing Systems", "abstract": "Mean field models are a popular tool used to analyse load balancing policies. In some exceptional cases the waiting time distribution of the mean field limit has an explicit form. In other cases it can be computed as the solution of a set of differential equations. In this paper we study the limit of the mean waiting time E[W\u03bb] as the arrival rate \u03bb approaches 1 for a number of load balancing policies in a large-scale system of homogeneous servers which finish work at a constant rate equal to one and exponential job sizes with mean 1 (i.e. when the system gets close to instability). As E[W\u03bb] diverges to infinity, we scale with -log(1-\u03bb) and present a method to compute the limit lim\u03bb-> 1- -E[W\u03bb]/l(1-\u03bb). We show that this limit has a surprisingly simple form for the load balancing algorithms considered. More specifically, we present a general result that holds for any policy for which the associated differential equation satisfies a list of assumptions. For the well-known LL(d) policy which assigns an incoming job to a server with the least work left among d randomly selected servers these assumptions are trivially verified. For this policy we prove the limit is given by 1/d-1. We further show that the LL(d,K) policy, which assigns batches of K jobs to the K least loaded servers among d randomly selected servers, satisfies the assumptions and the limit is equal to K/d-K. For a policy which applies LL(di) with probability pi, we show that the limit is given by 1/ \u2211i pi di - 1. We further indicate that our main result can also be used for load balancers with redundancy or memory. In addition, we propose an alternate scaling -l(p\u03bb) instead of -l(1-\u03bb), where p\u03bb is adapted to the policy at hand, such that lim\u03bb-> 1- -E[W\u03bb]/l(1-\u03bb)=lim\u03bb-> 1- -E[W\u03bb]/l(p\u03bb), where the limit lim\u03bb-> 0+ -E[W\u03bb]/l(p\u03bb) is well defined and non-zero (contrary to lim\u03bb-> 0+ -E[W\u03bb]/l(1-\u03bb)). This allows to obtain relatively flat curves for -E[W\u03bb]/l(p\u03bb) for \u03bb \u2208 [0,1] which indicates that the low and high load limits can be used as an approximation when \u03bb is close to one or zero. Our results rely on the earlier proven ansatz which asserts that for certain load balancing policies the workload distribution of any finite set of queues becomes independent of one another as the number of servers tends to infinity.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Tim  Hellemans", "Benny Van Houdt"], "year": 2021, "n_citations": 2}
{"id": 6283012, "s2_id": "28883f889481cf14f088f8c68f1b66e41503efc4", "title": "A Hybrid Neuro-Wavelet Predictor for QoS Control and Stability", "abstract": "For distributed systems to properly react to peaks of requests, their adaptation activities would benefit from the estimation of the amount of requests. This paper proposes a solution to produce a short-term forecast based on data characterising user behaviour of online services. We use wavelet analysis, providing compression and denoising on the observed time series of the amount of past user requests; and a recurrent neural network trained with observed data and designed so as to provide well-timed estimations of future requests. The said ensemble has the ability to predict the amount of future user requests with a root mean squared error below 0.06%. Thanks to prediction, advance resource provision can be performed for the duration of a request peak and for just the right amount of resources, hence avoiding over-provisioning and associated costs. Moreover, reliable provision lets users enjoy a level of availability of services unaffected by load variations.", "venue": "AI*IA", "authors": ["Christian  Napoli", "Giuseppe  Pappalardo", "Emiliano  Tramontana"], "year": 2013, "n_citations": 30}
{"id": 6287960, "s2_id": "b951550f8f9830048214a3ee4057f9892cd36947", "title": "Efficient FFT mapping on GPU for radar processing application: modeling and implementation", "abstract": "General-purpose multiprocessors (as, in our case, Intel IvyBridge and Intel Haswell) increasingly add GPU computing power to the former multicore architectures. When used for embedded applications (for us, Synthetic aperture radar) with intensive signal processing requirements, they must constantly compute convolution algorithms, such as the famous Fast Fourier Transform. Due to its \"fractal\" nature (the typical buttery shape, with larger FFTs dened as combination of smaller ones with auxiliary data array transpose functions), one can hope to compute analytically the size of the largest FFT that can be performed locally on an elementary GPU compute block. Then, the full application must be organized around this given building block size. Now, due to phenomena involved in the data transfers between various memory levels across CPUs and GPUs, the optimality of such a scheme is only loosely predictable (as communications tend to overcome in time the complexity of computations). Therefore a mix of (theoretical) analytic approach and (practical) runtime validation is here needed. As we shall illustrate, this occurs at both stage,", "venue": "ArXiv", "authors": ["Mohamed Amine Bergach", "Emilien  Kofman", "Robert de Simone", "Serge  Tissot", "Michel  Syska"], "year": 2015, "n_citations": 1}
{"id": 6288520, "s2_id": "ae1408f3b9d6b5c913f1dca6647cf65ad3a3bcb4", "title": "Optimal Synthesis of Multiple Algorithms", "abstract": "In this paper we give a definition of \"algorithm,\" \"finite algorithm,\" \"equivalent algorithms,\" and what it means for a single algorithm to dominate a set of algorithms. We define a derived algorithm which may have a smaller mean execution time than any of its component algorithms. We give an explicit expression for the mean execution time (when it exists) of the derived algorithm. We give several illustrative examples of derived algorithms with two component algorithms. We include mean execution time solutions for two-algorithm processors whose joint density of execution times are of several general forms. For the case in which the joint density for a two-algorithm processor is a step function, we give a maximum-likelihood estimation scheme with which to analyze empirical processing time data.", "venue": "ArXiv", "authors": ["Kerry Michael Soileau"], "year": 2007, "n_citations": 0}
{"id": 6289964, "s2_id": "99ad042e52b52252dc201dd0f1172ccfda0b5a50", "title": "MedPerf: Open Benchmarking Platform for Medical Artificial Intelligence using Federated Evaluation", "abstract": "Medical AI has tremendous potential to advance healthcare by supporting the evidence-based practice of medicine, personalizing patient treatment, reducing costs, and improving provider and patient experience. We argue that unlocking this potential requires a systematic way to measure the performance of medical AI models on large-scale heterogeneous data. To meet this need, we are building MedPerf, an open framework for benchmarking machine learning in the medical domain. MedPerf will enable federated evaluation in which models are securely distributed to different facilities for evaluation, thereby empowering healthcare organizations to assess and verify the performance of AI models in an efficient and human-supervised process, while prioritizing privacy. We describe the current challenges healthcare and AI communities face, the need for an open platform, the design philosophy of MedPerf, its current implementation status, and our roadmap. We call for researchers and organizations to join us in creating the MedPerf open benchmarking platform. Code availability: we made all code available under an Apache license at https://github.com/mlcommons", "venue": "ArXiv", "authors": ["Alexandros  Karargyris", "Renato  Umeton", "Micah J. Sheller", "Alejandro  Aristizabal", "Johnu  George", "Srini  Bala", "Daniel J. Beutel", "Victor  Bittorf", "Akshay  Chaudhari", "Alexander  Chowdhury", "Cody  Coleman", "Bala  Desinghu", "Gregory  Diamos", "Debo  Dutta", "Diane  Feddema", "Grigori  Fursin", "Junyi  Guo", "Xinyuan  Huang", "David  Kanter", "Satyananda  Kashyap", "Nicholas  Lane", "Indranil  Mallick", "Pietro  Mascagni", "Virendra  Mehta", "Vivek  Natarajan", "Nikola  Nikolov", "Nicolas  Padoy", "Gennady  Pekhimenko", "Vijay Janapa Reddi", "G Anthony Reina", "Pablo  Ribalta", "Jacob  Rosenthal", "Abhishek  Singh", "Jayaraman J. Thiagarajan", "Anna  Wuest", "Maria  Xenochristou", "Daguang  Xu", "Poonam  Yadav", "Michael  Rosenthal", "Massimo  Loda", "Jason M. Johnson", "Peter  Mattson"], "year": 2021, "n_citations": 0}
{"id": 6290442, "s2_id": "6313f104df803b1cb6667dbb6e0b2f52fa000dfa", "title": "First experiences with the Intel MIC architecture at LRZ", "abstract": "With the rapidly growing demand for computing power new accelerator based architectures have entered the world of high performance computing since around 5 years. In particular GPGPUs have recently become very popular, however programming GPGPUs using programming languages like CUDA or OpenCL is cumbersome and error-prone. Trying to overcome these difficulties, Intel developed their own Many Integrated Core (MIC) architecture which can be programmed using standard parallel programming techniques like OpenMP and MPI. In the beginning of 2013, the first production-level cards named Intel Xeon Phi came on the market. LRZ has been considered by Intel as a leading research centre for evaluating coprocessors based on the MIC architecture since 2010 under strict NDA. Since the Intel Xeon Phi is now generally available, we can share our experience on programming Intel's new MIC architecture.", "venue": "ArXiv", "authors": ["Volker  Weinberg", "Momme  Allalen"], "year": 2013, "n_citations": 0}
{"id": 6291345, "s2_id": "67a458329b7d64e10e1d0f1eb18252f1ecf9ebba", "title": "Hot-Rodding the Browser Engine: Automatic Configuration of JavaScript Compilers", "abstract": "Modern software systems in many application areas offer to the user a multitude of parameters, switches and other customisation hooks. Humans tend to have difficulties determining the best configurations for particular applications. Modern optimising compilers are an example of such software systems; their many parameters need to be tuned for optimal performance, but are often left at the default values for convenience. In this work, we automatically determine compiler parameter settings that result in optimised performance for particular applications. Specifically, we apply a state-of-the-art automated parameter configuration procedure based on cutting-edge machine learning and optimisation techniques to two prominent JavaScript compilers and demonstrate that significant performance improvements, more than 35% in some cases, can be achieved over the default parameter settings on a diverse set of benchmarks.", "venue": "ArXiv", "authors": ["Chris  Fawcett", "Lars  Kotthoff", "Holger H. Hoos"], "year": 2017, "n_citations": 0}
{"id": 6296498, "s2_id": "0c736b6f2be7ba4986bd68d2479ebb43856271c8", "title": "Performance Analysis and Optimization of Sparse Matrix-Vector Multiplication on Modern Multi- and Many-Core Processors", "abstract": "This paper presents a low-overhead optimizer for the ubiquitous sparse matrix-vector multiplication (SpMV) kernel. Architectural diversity among different processors together with structural diversity among different sparse matrices lead to bottleneck diversity. This justifies an SpMV optimizer that is both matrix- and architecture-adaptive through runtime specialization. To this direction, we present an approach that first identifies the performance bottlenecks of SpMV for a given sparse matrix on the target platform either through profiling or by matrix property inspection, and then selects suitable optimizations to tackle those bottlenecks. Our optimization pool is based on the widely used Compressed Sparse Row (CSR) sparse matrix storage format and has low preprocessing overheads, making our overall approach practical even in cases where fast decision making and optimization setup is required. We evaluate our optimizer on three x86-based computing platforms and demonstrate that it is able to distinguish and appropriately optimize SpMV for the majority of matrices in a representative test suite, leading to significant speedups over the CSR and Inspector-Executor CSR SpMV kernels available in the latest release of the Intel MKL library.", "venue": "2017 46th International Conference on Parallel Processing (ICPP)", "authors": ["Athena  Elafrou", "Georgios I. Goumas", "Nectarios  Koziris"], "year": 2017, "n_citations": 14}
{"id": 6297147, "s2_id": "5c15c2864bb995bff0453bd10e6727790a2c6490", "title": "The ELAPS framework: Experimental Linear Algebra Performance Studies", "abstract": "In scientific computing, optimal use of computing resources comes at the cost of extensive coding, tuning, and benchmarking. While the classic approach of \u201cfeatures first, performance later\u201d is supported by a variety of tools such as Tau, Vampir, and Scalasca, the emerging performance-centric approach, in which both features and performance are primary objectives, is still lacking suitable development tools. For dense linear algebra applications, we fill this gap with the Experimental Linear Algebra Performance Studies (ELAPS) framework, a multi-platform open-source environment for easy, fast, and yet powerful performance experimentation and prototyping. In contrast to many existing tools, ELAPS targets the beginning of the development process, assisting application developers in both algorithmic and optimization decisions. With ELAPS, users construct experiments to investigate how performance and efficiency depend on factors such as caching, algorithmic parameters, problem size, and parallelism. Experiments are designed either through Python scripts or a specialized Graphical User Interface (GUI), and run on a spectrum of architectures, ranging from laptops to accelerators and clusters. The resulting reports provide various metrics and statistics that can be analyzed both numerically and visually. In this article, we introduce ELAPS and illustrate its practical value in guiding critical performance decisions already in early development stages.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Elmar  Peise", "Paolo  Bientinesi"], "year": 2019, "n_citations": 7}
{"id": 6297596, "s2_id": "5de6c427378f2c1d7686062f2d86c3040773ce8c", "title": "Benchmarking OpenCL, OpenACC, OpenMP, and CUDA: programming productivity, performance, and energy consumption", "abstract": "Many modern parallel computing systems are heterogeneous at their node level. Such nodes may comprise general purpose CPUs and accelerators (such as, GPU, or Intel Xeon Phi) that provide high performance with suitable energy-consumption characteristics. However, exploiting the available performance of heterogeneous architectures may be challenging. There are various parallel programming frameworks (such as, OpenMP, OpenCL, OpenACC, CUDA) and selecting the one that is suitable for a target context is not straightforward. In this paper, we study empirically the characteristics of OpenMP, OpenACC, OpenCL, and CUDA with respect to programming productivity, performance, and energy. To evaluate the programming productivity we use our homegrown tool CodeStat, which enables us to determine the percentage of code lines required to parallelize the code using a specific framework. We use our tools MeterPU and x-MeterPU to evaluate the energy consumption and the performance. Experiments are conducted using the industry-standard SPEC benchmark suite and the Rodinia benchmark suite for accelerated computing on heterogeneous systems that combine Intel Xeon E5 Processors with a GPU accelerator or an Intel Xeon Phi co-processor.", "venue": "ARMS-CC@PODC", "authors": ["Suejb  Memeti", "Lu  Li", "Sabri  Pllana", "Joanna  Kolodziej", "Christoph W. Kessler"], "year": 2017, "n_citations": 63}
{"id": 6299274, "s2_id": "308388616c12158423fbf8bd8c441d11d1f432a2", "title": "Stochastic superoptimization", "abstract": "We formulate the loop-free binary superoptimization task as a stochastic search problem. The competing constraints of transformation correctness and performance improvement are encoded as terms in a cost function, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program. Although our method sacrifices completeness, the scope of programs we are able to consider, and the resulting quality of the programs that we produce, far exceed those of existing superoptimizers. Beginning from binaries compiled by llvm -O0 for 64-bit x86, our prototype implementation, STOKE, is able to produce programs which either match or outperform the code produced by gcc -O3, icc -O3, and in some cases, expert handwritten assembly.", "venue": "ASPLOS '13", "authors": ["Eric  Schkufza", "Rahul  Sharma", "Alexander  Aiken"], "year": 2013, "n_citations": 239}
{"id": 6299606, "s2_id": "db4a175cac37041f9c927ea6e208b5331a5db46d", "title": "A Language for Describing Optimization Strategies", "abstract": "Optimizing programs to run efficiently on modern parallel hardware is hard but crucial for many applications. The predominantly used imperative languages - like C or OpenCL - force the programmer to intertwine the code describing functionality and optimizations. This results in a nightmare for portability which is particularly problematic given the accelerating trend towards specialized hardware devices to further increase efficiency. \nMany emerging DSLs used in performance demanding domains such as deep learning, automatic differentiation, or image processing attempt to simplify or even fully automate the optimization process. Using a high-level - often functional - language, programmers focus on describing functionality in a declarative way. In some systems such as Halide or TVM, a separate schedule specifies how the program should be optimized. Unfortunately, these schedules are not written in well-defined programming languages. Instead, they are implemented as a set of ad-hoc predefined APIs that the compiler writers have exposed. \nIn this paper, we present Elevate: a functional language for describing optimization strategies. Elevate follows a tradition of prior systems used in different contexts that express optimization strategies as composition of rewrites. In contrast to systems with scheduling APIs, in Elevate programmers are not restricted to a set of built-in optimizations but define their own optimization strategies freely in a composable way. We show how user-defined optimization strategies in Elevate enable the effective optimization of programs expressed in a functional data-parallel language demonstrating competitive performance with Halide and TVM.", "venue": "ArXiv", "authors": ["Bastian  Hagedorn", "Johannes  Lenfers", "Thomas  Koehler", "Sergei  Gorlatch", "Michel  Steuwer"], "year": 2020, "n_citations": 4}
{"id": 6306022, "s2_id": "1601727e9d919af14b4319b156657b08e73fab0b", "title": "CLTune: A Generic Auto-Tuner for OpenCL Kernels", "abstract": "This work presents CLTune, an auto-tuner for OpenCL kernels. It evaluates and tunes kernel performance of a generic, user-defined search space of possible parameter-value combinations. Example parameters include the OpenCL workgroup size, vector data-types, tile sizes, and loop unrolling factors. CLTune can be used in the following scenarios: 1) when there are too many tunable parameters to explore manually, 2) when performance portability across OpenCL devices is desired, or 3) when the optimal parameters change based on input argument values (e.g. matrix dimensions). The auto-tuner is generic, easy to use, open-source, and supports multiple search strategies including simulated annealing and particle swarm optimisation. CLTune is evaluated on two GPU case-studies inspired by the recent successes in deep learning: 2D convolution and matrix-multiplication (GEMM). For 2D convolution, we demonstrate the need for auto-tuning by optimizing for different filter sizes, achieving performance on-par or better than the state-of-the-art. For matrix-multiplication, we use CLTune to explore a parameter space of more than two-hundred thousand configurations, we show the need for device-specific tuning, and outperform the clBLAS library on NVIDIA, AMD and Intel GPUs.", "venue": "2015 IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip", "authors": ["Cedric  Nugteren", "Valeriu  Codreanu"], "year": 2015, "n_citations": 72}
{"id": 6311455, "s2_id": "a21f35a8f06ef661bd24efc11e667907c2e4fa25", "title": "Brewing Analytics Quality for Cloud Performance", "abstract": "Cloud computing has become increasingly popular. Many options of cloud deployments are available. Testing cloud performance would enable us to choose a cloud deployment based on the requirements. In this paper, we present an innovative process, implemented in software, to allow us to assess the quality of the cloud performance data. The process combines performance data from multiple machines, spanning across user experience data, workload performance metrics, and readily available system performance data. Furthermore, we discuss the major challenges of bringing raw data into tidy data formats in order to enable subsequent analysis, and describe how our process has several layers of assessment to validate the quality of the data processing procedure. We present a case study to demonstrate the effectiveness of our proposed process, and conclude our paper with several future research directions worth investigating.", "venue": "ArXiv", "authors": ["Li  Chen", "Pooja  Jain", "Kingsum  Chow", "Emad  Guirguis", "Tony  Wu"], "year": 2015, "n_citations": 3}
{"id": 6312296, "s2_id": "9701794e0dc3dec94f4a4a71e0562cb5ad9c5b9d", "title": "Dissecting demand response mechanisms: The role of consumption forecasts and personalized offers", "abstract": "Abstract Demand-Response (DR) programs, whereby users of an electricity network are encouraged by economic incentives to re-arrange their consumption in order to reduce production costs, are envisioned to be a key feature of the smart grid paradigm. Several recent works proposed DR mechanisms and used analytical models to derive optimal incentives. Most of these works, however, rely on a macroscopic description of the population that does not model individual choices of users. In this paper, we conduct a detailed analysis of those models and we argue that the macroscopic descriptions hide important assumptions that can jeopardize the mechanisms\u2019 implementation (such as the ability to make personalized offers and to perfectly estimate the demand that is moved from a timeslot to another). Then, we start from a microscopic description that explicitly models each user\u2019s decision. We introduce four DR mechanisms with various assumptions on the provider\u2019s capabilities. Contrarily to previous studies, we find that the optimization problems that result from our mechanisms are complex and can be solved numerically only through a heuristic. We present numerical simulations that compare the different mechanisms and their sensitivity to forecast errors. At a high level, our results show that the performance of DR mechanisms under reasonable assumptions on the provider\u2019s capabilities are significantly lower than those suggested by previous studies, but that the gap reduces when the population\u2019s flexibility increases.", "venue": "Sustainable Energy, Grids and Networks", "authors": ["Alberto  Benegiamo", "Patrick  Loiseau", "Giovanni  Neglia"], "year": 2018, "n_citations": 4}
{"id": 6312636, "s2_id": "0fa18769946b4704d0de20060577d6b40ea0d719", "title": "A Metric for DISH Networks: Analysis, Implications, and Applications", "abstract": "In wireless networks, node cooperation has been exploited as a data relaying mechanism for decades. However, the wireless channel allows for much richer interaction among nodes. In particular, Distributed Information SHaring (DISH) represents a new improvement to multichannel MAC protocol design by using a cooperative element at the control plane. In this approach, nodes exchange control information to make up for other nodes' insufficient knowledge about the environment, and thereby aid in their decision making. To date, what is lacking is a theoretical understanding of DISH. In this paper, we view cooperation as a network resource and evaluate the availability of cooperation, p_{co}. We first analyze p_{co} in the context of a multichannel multihop wireless network, and then perform simulations which show that the analysis accurately characterizes p_{co} as a function of underlying network parameters. Next, we investigate the correlation between p_{co} and network metrics such as collision rate, packet delay, and throughput. We find a near-linear relationship between p_{co} and the metrics, which suggests that p_{co} can be used as an appropriate performance indicator itself. Finally, we apply our analysis to solving a channel bandwidth allocation problem, where we derive optimal schemes and provide general guidelines on bandwidth allocation for DISH networks.", "venue": "IEEE Transactions on Mobile Computing", "authors": ["Tie  Luo", "Vikram  Srinivasan", "Mehul  Motani"], "year": 2010, "n_citations": 8}
{"id": 6314912, "s2_id": "f33c28be74f5ce18579d8fcbbe4dff38d36831c8", "title": "Detection and Performance Analysis for Non-Coherent DF Relay Networks with Optimized Generalized Differential Modulation", "abstract": "This paper studies the detection and performance analysis problems for a relay network with $N$ parallel decode-and-forward (DF) relays. Due to the distributed nature of this network, it is practically very challenging to fulfill the requirement of instantaneous channel state information for coherent detection. To bypass this requirement, we consider the use of non-coherent DF relaying based on a generalized differential modulation (GDM) scheme, in which transmission power allocation over the $M$-ary phase shift keying symbols is exploited when performing differential encoding. In this paper, a novel detector at the destination of such a non-coherent DF relay network is proposed. It is an accurate approximation of the state-of-the-art detector, called the almost maximum likelihood detector (AMLD), but the detection complexity is considerably reduced from $\\mathcal{O}(M^2N)$ to $\\mathcal{O}(MN)$. By characterizing the dominant error terms, we derive an accurate approximate symbol error rate (SER) expression. An optimized power allocation scheme for GDM is further designed based on this SER expression. Our simulation demonstrates that the proposed non-coherent scheme can perform close to the coherent counterpart as the block length increases. Additionally, we prove that the diversity order of both the proposed detector and the AMLD is exactly $\\lceil N/2 \\rceil + 1$. Extensive simulation results further verify the accuracy of our results in various scenarios.", "venue": "ArXiv", "authors": ["Yuxin  Lu", "Wai Ho Mow"], "year": 2020, "n_citations": 1}
{"id": 6318466, "s2_id": "a240db97cf8cc46b65bc17b6e6d95d6345e46089", "title": "A Foreground-Background queueing model with speed or capacity modulation", "abstract": "The models studied in the steady state involve two queues which are served either by a single server whose speed depends on the number of jobs present, or by several parallel servers whose number may be controlled dynamically. Job service times have a two-phase Coxian distribution and the second phase is given lower priority than the first. The trade-offs between holding costs and energy consumption costs are examined by means of a suitable cost functions. Two different two-dimensional Markov process are solved exactly. The solutions are used in several numerical experiments. Some counter-intuitive results are observed.", "venue": "ArXiv", "authors": ["Andrea  Marin", "Isi  Mitrani"], "year": 2021, "n_citations": 0}
{"id": 6320681, "s2_id": "289d0558ec6f21ba6cba5e33afa0194aecd3b112", "title": "Deep Learning Training with Simulated Approximate Multipliers", "abstract": "This paper presents by simulation how approximate multipliers can be utilized to enhance the training performance of convolutional neural networks (CNNs). Approximate multipliers have significantly better performance in terms of speed, power, and area compared to exact multipliers. However, approximate multipliers have an inaccuracy which is defined in terms of the Mean Relative Error (MRE). To assess the applicability of approximate multipliers in enhancing CNN training performance, a simulation for the impact of approximate multipliers error on CNN training is presented. The paper demonstrates that using approximate multipliers for CNN training can significantly enhance the performance in terms of speed, power, and area at the cost of a small negative impact on the achieved accuracy. Additionally, the paper proposes a hybrid training method which mitigates this negative impact on the accuracy. Using the proposed hybrid method, the training can start using approximate multipliers then switches to exact multipliers for the last few epochs. Using this method, the performance benefits of approximate multipliers in terms of speed, power, and area can be attained for a large portion of the training stage. On the other hand, the negative impact on the accuracy is diminished by using the exact multipliers for the last epochs of training.", "venue": "2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)", "authors": ["Issam  Hammad", "Kamal  El-Sankary", "Jason  Gu"], "year": 2019, "n_citations": 3}
{"id": 6328457, "s2_id": "b187856c91d97a76c1be78057bcc4c7c4f4fe014", "title": "Implementation of a Wake-up Radio Cross-Layer Protocol in OMNeT++ / MiXiM", "abstract": "This paper presents the DoRa protocol, which is a new cross-layer protocol for handling the double radio of nodes in wake-up radio scenario. The implementation details in OMNET++/MiXiM are also given, with a focus on the implemented MAC layers. The main goal of the DoRa protocol is to reduce energy consumption in wireless sensor network, by taking full advantage of the passive wake-up scheme. The performance of the DoRa protocol is then evaluated and results are compared with B-MAC and IEEE 802.15.4 protocols.", "venue": "ArXiv", "authors": ["Jean  Lebreton", "Nour  Murad"], "year": 2015, "n_citations": 12}
{"id": 6330412, "s2_id": "39f91813f8e434824422c70d7029b0f6b8e03d6e", "title": "Predicting intermediate storage performance for workflow applications", "abstract": "System configuration decisions for I/O-intensive workflow applications can be complex even for expert users. Users face decisions to configure several parameters optimally (e.g., replication level, chunk size, number of storage node) - each having an impact on overall application performance. This paper presents our progress on addressing the problem of supporting storage system configuration decisions for workflow applications. Our approach accelerates the exploration of the configuration space based on a low-cost performance predictor that estimates turn-around time of a workflow application in a given setup. Our evaluation shows that the predictor is effective in identifying the desired system configuration, and it is lightweight using 2000-5000\u00d7 less resources (machines \u00d7 time) than running the actual benchmarks.", "venue": "PDSW@SC", "authors": ["Lauro Beltr\u00e3o Costa", "Samer  Al-Kiswany", "Abmar  Barros", "Hao  Yang", "Matei  Ripeanu"], "year": 2013, "n_citations": 5}
{"id": 6332912, "s2_id": "a08e731085e0a3717d5b16dbd6e4d85c86d75ac9", "title": "Performance Evaluation of Deep Learning Tools in Docker Containers", "abstract": "With the success of deep learning techniques in a broad range of application domains, many deep learning software frameworks have been developed and are being updated frequently to adapt to new hardware features and software libraries, which bring a big challenge for end users and system administrators. To address this problem, container techniques are widely used to simplify the deployment and management of deep learning software. However, it remains unknown whether container techniques bring any performance penalty to deep learning applications. The purpose of this work is to systematically evaluate the impact of docker container on the performance of deep learning applications. We first benchmark the performance of system components (IO, CPU and GPU) in a docker container and the host system and compare the results to see if there's any difference. According to our results, we find that computational intensive jobs, either running on CPU or GPU, have small overhead indicating docker containers can be applied to deep learning programs. Then we evaluate the performance of some popular deep learning tools deployed in a docker container and the host system. It turns out that the docker container will not cause noticeable drawbacks while running those deep learning tools. So encapsulating deep learning tool in a container is a feasible solution.", "venue": "2017 3rd International Conference on Big Data Computing and Communications (BIGCOM)", "authors": ["Pengfei  Xu", "Shaohuai  Shi", "Xiaowen  Chu"], "year": 2017, "n_citations": 20}
{"id": 6333865, "s2_id": "d2d61cc9a175f247a6d1633bb99f5a8495d805a7", "title": "A sequential approximation framework for coded distributed optimization", "abstract": "Building on the previous work of Lee et al. [2] and Ferdinand et al. [3] on coded computation, we propose a sequential approximation framework for solving optimization problems in a distributed manner. In a distributed computation system, latency caused by individual processors (\u201cstragglers\u201d) usually causes a significant delay in the overall process. The proposed method is powered by a sequential computation scheme, which is designed specifically for systems with stragglers. This scheme has the desirable property that the user is guaranteed to receive useful (approximate) computation results whenever a processor finishes its subtask, even in the presence of uncertain latency. In this paper, we give a coding theorem for sequentially computing matrix-vector multiplications, and the optimality of this coding scheme is also established. As an application of the results, we demonstrate solving optimization problems using a sequential approximation approach, which accelerates the algorithm in a distributed system with stragglers.", "venue": "2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "authors": ["Jingge  Zhu", "Ye  Pu", "Vipul  Gupta", "Claire J. Tomlin", "Kannan  Ramchandran"], "year": 2017, "n_citations": 18}
{"id": 6335225, "s2_id": "ea5d2c502338102b5d968cbe1cb1f0dcdbcb9eb2", "title": "Soft cache hits and the impact of alternative content recommendations on mobile edge caching", "abstract": "Caching popular content at the edge of future mobile networks has been widely considered in order to alleviate the impact of the data tsunami on both the access and backhaul networks. A number of interesting techniques have been proposed, including femto-caching and \"delayed\" or opportunistic cache access. Nevertheless, the majority of these approaches suffer from the rather limited storage capacity of the edge caches, compared to the tremendous and rapidly increasing size of the Internet content catalog. We propose to depart from the assumption of hard cache misses, common in most existing works, and consider \"soft\" cache misses, where if the original content is not available, an alternative content that is locally cached can be recommended. Given that Internet content consumption is increasingly entertainment-oriented, we believe that a related content could often lead to complete or at least partial user satisfaction, without the need to retrieve the original content over expensive links. In this paper, we formulate the problem of optimal edge caching with soft cache hits, in the context of delayed access, and analyze the expected gains. We then show using synthetic and real datasets of related video contents that promising caching gains could be achieved in practice.", "venue": "CHANTS@MOBICOM", "authors": ["Thrasyvoulos  Spyropoulos", "Pavlos  Sermpezis"], "year": 2016, "n_citations": 16}
{"id": 6338907, "s2_id": "e50274674346b08b6eef0b377b9619f9835463fe", "title": "The L-CSC cluster: greenest supercomputer in the world in Green500 list of November 2014", "abstract": "The L-CSC (Lattice Computer for Scientific Computing) is a general purpose compute cluster built of commodity hardware installed at GSI. Its main operational purpose is Lattice QCD (LQCD) calculations for physics simulations. Quantum Chromo Dynamics (QCD) is the physical theory describing the strong force, one of the four known fundamental interactions in the universe. L-CSC leverages a multi-GPU design accommodating the huge demand of LQCD for memory bandwidth. In recent years, heterogeneous clusters with accelerators such as GPUs have become more and more powerful while supercomputers in general have shown enormous increases in power consumption making electricity costs and cooling a significant factor in the total cost of ownership. Using mainly GPUs for processing, L-CSC is very power efficient, and its architecture was optimized to provide the greatest possible power efficiency. This paper presents the cluster design as well as optimizations to improve the power efficiency. It examines the power measurements performed for the Green500 list of the most power efficient supercomputers in the world which led to the number 1 position as the greenest supercomputer in November 2014.", "venue": "ArXiv", "authors": ["David  Rohr", "Gvozden  Neskovic", "M.  Radtke", "Volker  Lindenstruth"], "year": 2017, "n_citations": 0}
{"id": 6340063, "s2_id": "d973ae50f88405ebfe6487b48d31c52b27cbb2ab", "title": "Speeding simulation analysis up with yt and Intel Distribution for Python", "abstract": "As modern scientific simulations grow ever more in size and complexity, even their analysis and post-processing becomes increasingly demanding, calling for the use of HPC resources and methods. yt is a parallel, open source post-processing python package for numerical simulations in astrophysics, made popular by its cross-format compatibility, its active community of developers and its integration with several other professional Python instruments. The Intel Distribution for Python enhances yt's performance and parallel scalability, through the optimization of lower-level libraries Numpy and Scipy, which make use of the optimized Intel Math Kernel Library (Intel-MKL) and the Intel MPI library for distributed computing. The library package yt is used for several analysis tasks, including integration of derived quantities, volumetric rendering, 2D phase plots, cosmological halo analysis and production of synthetic X-ray observation. In this paper, we provide a brief tutorial for the installation of yt and the Intel Distribution for Python, and the execution of each analysis task. Compared to the Anaconda python distribution, using the provided solution one can achieve net speedups up to 4.6x on Intel Xeon Scalable processors (codename Skylake).", "venue": "ArXiv", "authors": ["Salvatore  Cielo", "Luigi  Iapichino", "Fabio  Baruffa"], "year": 2019, "n_citations": 2}
{"id": 6340965, "s2_id": "9a5dd73fab178dfa6e9da2227af1862d3b0615b0", "title": "On the Power of Centralization in Distributed Processing", "abstract": "In this thesis, we propose and analyze a multi-server model that captures a performance trade-off between centralized and distributed processing. In our model, a fraction $p$ of an available resource is deployed in a centralized manner (e.g., to serve a most-loaded station) while the remaining fraction $1-p$ is allocated to local servers that can only serve requests addressed specifically to their respective stations. \nUsing a fluid model approach, we demonstrate a surprising phase transition in the steady-state delay, as $p$ changes: in the limit of a large number of stations, and when any amount of centralization is available ($p>0$), the average queue length in steady state scales as $\\log_{1/(1-p)} 1/(1-\\lambda)$ when the traffic intensity $\\lambda$ goes to 1. This is exponentially smaller than the usual M/M/1-queue delay scaling of $1/(1-\\lambda)$, obtained when all resources are fully allocated to local stations ($p=0$). This indicates a strong qualitative impact of even a small degree of centralization. \nWe prove convergence to a fluid limit, and characterize both the transient and steady-state behavior of the finite system, in the limit as the number of stations $N$ goes to infinity. We show that the sequence of queue-length processes converges to a unique fluid trajectory (over any finite time interval, as $N$ approaches infinity, and that this fluid trajectory converges to a unique invariant state $v^I$, for which a simple closed-form expression is obtained. We also show that the steady-state distribution of the $N$-server system concentrates on $v^I$ as $N$ goes to infinity.", "venue": "ArXiv", "authors": ["Kuang  Xu"], "year": 2012, "n_citations": 3}
{"id": 6341003, "s2_id": "8488765cfa484a6da5eda369a24be7f51d4e31af", "title": "LDPC Code Design for Distributed Storage: Balancing Repair Bandwidth, Reliability, and Storage Overhead", "abstract": "Distributed storage systems suffer from significant repair traffic generated due to the frequent storage node failures. This paper shows that properly designed low-density parity-check (LDPC) codes can substantially reduce the amount of required block downloads for repair thanks to the sparse nature of their factor graph representation. In particular, with a careful construction of the factor graph, both low repair-bandwidth and high reliability can be achieved for a given code rate. First, a formula for the average repair bandwidth of LDPC codes is developed. This formula is then used to establish that the minimum repair bandwidth can be achieved by forcing a regular check node degree in the factor graph. Moreover, it is shown that given a fixed code rate, the variable node degree should also be regular to yield minimum repair bandwidth, under some reasonable minimum variable node degree constraint. It is also shown that for a given repair-bandwidth requirement, LDPC codes can yield substantially higher reliability than the currently utilized Reed\u2013Solomon codes. Our reliability analysis is based on a formulation of the general equation for the mean-time-to-data-loss (MTTDL) associated with LDPC codes. The formulation reveals that the stopping number is closely related to the MTTDL. It is further shown that LDPC codes can be designed such that a small loss of repair-bandwidth optimality may be traded for a large improvement in erasure-correction capability and thus the MTTDL.", "venue": "IEEE Transactions on Communications", "authors": ["Hyegyeong  Park", "Dongwon  Lee", "Jaekyun  Moon"], "year": 2018, "n_citations": 13}
{"id": 6343156, "s2_id": "4ea289d3ea0a910c7bb7857d352cafe05caf22d7", "title": "Power grid vulnerability to geographically correlated failures \u2014 Analysis and control implications", "abstract": "We consider line outages in the transmission network of the power grid, and specifically those caused by natural disasters or large-scale physical attacks. In such networks, an outage of a line may lead to overload on other lines, thereby leading to their outage. Such a cascade may have devastating effects not only on the power grid but also on the interconnected communication networks. We study a model of such failures and show that it differs from other models used to analyze cascades (e.g., epidemic/percolation-based models). Inspired by methods developed for network-survivability analysis, we show how to identify the most vulnerable locations in the network. We also perform extensive numerical experiments with real grid data to estimate the effects of geographically correlated outages and briefly discuss mitigation methods. The developed techniques can indicate potential locations for grid monitoring, and hence, will have impact on the deployment of the smart-grid networking infrastructure.", "venue": "IEEE INFOCOM 2014 - IEEE Conference on Computer Communications", "authors": ["Andrey  Bernstein", "Daniel  Bienstock", "David  Hay", "Meric  Uzunoglu", "Gil  Zussman"], "year": 2014, "n_citations": 171}
{"id": 6343622, "s2_id": "8f2c6456dc96947345ae99aaf7ccc5c2ae06ffbf", "title": "A Lightweight Distributed Solution to Content Replication in Mobile Networks", "abstract": "Performance and reliability of content access in mobile networks is conditioned jointly by the number and location of content replicas deployed at the network nodes. The endeavour of this work is to address such an optimization problem with a distributed, lightweight solution that handles network dynamics. We devise a mechanism that lets nodes share the burden of storing and providing content, so as to achieve load balancing, and decide whether to replicate or drop the information so as to adapt to a dynamic content demand and time-varying topology. Simulation results show that our mechanism, which uses local measurements only, is: (i) extremely precise in approximating an optimal solution to content placement and replication; (ii) robust against network mobility; (iii) flexible in accommodating variation in time and space of the content demand.", "venue": "2010 IEEE Wireless Communication and Networking Conference", "authors": ["Chi-Anh  La", "Pietro  Michiardi", "Claudio  Casetti", "Carla-Fabiana  Chiasserini", "Marco  Fiore"], "year": 2010, "n_citations": 10}
{"id": 6345416, "s2_id": "82c1bc0c546f68ba573b3b85e232de633a7a5bf7", "title": "Exploring System Performance of Continual Learning for Mobile and Embedded Sensing Applications", "abstract": "Continual learning approaches help deep neural network models adapt and learn incrementally by trying to solve catastrophic forgetting. However, whether these existing approaches, applied traditionally to image-based tasks, work with the same efficacy to the sequential time series data generated by mobile or embedded sensing systems remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the performance of three predominant continual learning schemes (i.e., regularization, replay, and replay with examples) on six datasets from three mobile and embedded sensing applications in a range of scenarios having different learning complexities. More specifically, we implement an end-to-end continual learning framework on edge devices. Then we investigate the generalizability, trade-offs between performance, storage, computational costs, and memory footprint of different continual learning methods. Our findings suggest that replay with exemplars-based schemes such as iCaRL has the best performance trade-offs, even in complex scenarios, at the expense of some storage space (few MBs) for training examples (1% to 5%). We also demonstrate for the first time that it is feasible and practical to run continual learning ondevice with a limited memory budget. In particular, the latency on two types of mobile and embedded devices suggests that both incremental learning time (few seconds 4 minutes) and training time (1 75 minutes) across datasets are acceptable, as training could happen on the device when the embedded device is charging thereby ensuring complete data privacy. Finally, we present some guidelines for practitioners who want to apply a continual learning paradigm for mobile sensing tasks.", "venue": "ArXiv", "authors": ["Young D. Kwon", "Jagmohan  Chauhan", "Abhishek  Kumar", "Pan  Hui", "Cecilia  Mascolo"], "year": 2021, "n_citations": 1}
{"id": 6346647, "s2_id": "99dd5a07bfb03c469463b99f7a3f654640dc90c7", "title": "Stochastic service guarantee analysis based on time-domain models", "abstract": "Stochastic network calculus is a theory for stochastic service guarantee analysis of computer communication networks. In the current stochastic network calculus literature, its traffic and server models are typically defined based on the cumulative amount of traffic and cumulative amount of service respectively. However, there are network scenarios where the applicability of such models is limited, and hence new ways of modeling traffic and service are needed to address this limitation. This paper presents time-domain models and results for stochastic network calculus. Particularly, we define traffic models, which are defined based on probabilistic lower-bounds on cumulative packet inter-arrival time, and server models, which are defined based on probabilistic upper-bounds on cumulative packet service time. In addition, examples demonstrating the use of the proposed time-domain models are provided. On the basis of the proposed models, the five basic properties of stochastic network calculus are also proved, which implies broad applicability of the proposed time-domain approach.", "venue": "2009 IEEE International Symposium on Modeling, Analysis & Simulation of Computer and Telecommunication Systems", "authors": ["Jing  Xie", "Yuming  Jiang"], "year": 2009, "n_citations": 20}
{"id": 6352601, "s2_id": "e01fc328621257a9ffc7e091bbeba59d0fbfccc7", "title": "Leveraging Shared Caches for Parallel Temporal Blocking of Stencil Codes on Multicore Processors and Clusters", "abstract": "Bandwidth-starved multicore chips have become ubiquitous. It is well known that the performance of stencil codes can be improved by temporal blocking, lessening the pressure on the memory interface. We introduce a new pipelined approach that makes explicit use of shared caches in multicore environments and minimizes synchronization and boundary overhead. Benchmark results are presented for three current x86-based microprocessors, showing clearly that our optimization works best on designs with high-speed shared caches and low memory bandwidth per core. We furthermore demonstrate that simple bandwidth-based performance models are inaccurate for this kind of algorithm and employ a more elaborate, synthetic modeling procedure. Finally we show that temporal blocking can be employed successfully in a hybrid shared/distributed-memory environment, albeit with limited benefit at strong scaling.", "venue": "Parallel Process. Lett.", "authors": ["Markus  Wittmann", "Georg  Hager", "Jan  Treibig", "Gerhard  Wellein"], "year": 2010, "n_citations": 26}
{"id": 6354578, "s2_id": "ea2d6f5776f470ef9edb7b3bd2eda409c5e6157c", "title": "Communication Patterns in Mean Field Models for Wireless Sensor Networks", "abstract": "Wireless sensor networks are usually composed of a large number of nodes, and with the increasing processing power and power consumption effi- ciency they are expected to run more complex protocols in the future. These pose problems in the field of verification and performance evaluation of wireless net- works. In this paper, we tailor the mean-field theory as a modeling technique to analyze their behavior. We apply this method to the slotted ALOHA protocol, and establish results on the long term trends of the protocol within a very large network, specially regarding the stability of ALOHA-type protocols.", "venue": "ArXiv", "authors": ["Mahmoud  Talebi", "Jan Friso Groote", "Jean-Paul M. G. Linnartz"], "year": 2015, "n_citations": 0}
{"id": 6355459, "s2_id": "aafc5a0bad847eef4b053ba932b38002edc5f4ed", "title": "A Droplet Approach Based on Raptor Codes for Distributed Computing With Straggling Servers", "abstract": "We propose a coded distributed computing scheme based on Raptor codes to address the straggler problem. In particular, we consider a scheme where each server computes intermediate values, referred to as droplets, that are either stored locally or sent over the network. Once enough droplets are collected, the computation can be completed. Compared to previous schemes in the literature, our proposed scheme achieves lower computational delay when the decoding time is taken into account.", "venue": "2018 IEEE 10th International Symposium on Turbo Codes & Iterative Information Processing (ISTC)", "authors": ["Albin  Severinson", "Alexandre Graell i Amat", "Eirik  Rosnes", "Francisco  L\u00e1zaro", "Gianluigi  Liva"], "year": 2018, "n_citations": 8}
{"id": 6357784, "s2_id": "4f5f8b0f798d895a3f0470f91ec8cc48a67b907f", "title": "Design and Implementation of Secret Key Agreement for Platoon-based Vehicular Cyber-physical Systems", "abstract": "In a platoon-based vehicular cyber-physical system (PVCPS), a lead vehicle that is responsible for managing the platoon\u2019s moving directions and velocity periodically disseminates control messages to the vehicles that follow. Securing wireless transmissions of the messages between the vehicles is critical for privacy and confidentiality of the platoon\u2019s driving pattern. However, due to the broadcast nature of radio channels, the transmissions are vulnerable to eavesdropping. In this article, we propose a cooperative secret key agreement (CoopKey) scheme for encrypting/decrypting the control messages, where the vehicles in PVCPS generate a unified secret key based on the quantized fading channel randomness. Channel quantization intervals are optimized by dynamic programming to minimize the mismatch of keys. A platooning testbed is built with autonomous robotic vehicles, where a TelosB wireless node is used for onboard data processing and multi-hop dissemination. Extensive real-world experiments demonstrate that CoopKey achieves significantly low secret bit mismatch rate in a variety of settings. Moreover, the standard NIST test suite is employed to verify randomness of the generated keys, where the p-values of our CoopKey pass all the randomness tests. We also evaluate CoopKey with an extended platoon size via simulations to investigate the effect of system scalability on performance.", "venue": "ACM Trans. Cyber Phys. Syst.", "authors": ["Kai  Li", "Wei  Ni", "Yousef  Emami", "Yiran  Shen", "Ricardo  Severino", "David  Pereira", "Eduardo  Tovar"], "year": 2020, "n_citations": 9}
{"id": 6358386, "s2_id": "ea730c19efbd5a50aaec9ef1b105a61f761bc1d8", "title": "GPGPU Performance Estimation With Core and Memory Frequency Scaling", "abstract": "Contemporary graphics processing units (GPUs) support dynamic voltage and frequency scaling to balance computational performance and energy consumption. However, accurate and straightforward performance estimation for a given GPU kernel under different frequency settings is still lacking for real hardware, which is essential to determine the best frequency configuration for energy saving. In this article, we reveal a fine-grained analytical model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Compared to the cycle-level simulators, which are too slow to apply on real hardware, our model only needs simple and one-off micro-benchmarks to extract a set of hardware parameters and kernel performance counters without any source code analysis. Our experimental results show that the proposed performance model can capture the kernel performance scaling behaviors under different frequency settings and achieve decent accuracy (average errors of 3.85, 8.6, 8.82, and 8.83 percent on a set of 20 GPU kernels with four modern Nvidia GPUs).", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Qiang  Wang", "Xiaowen  Chu"], "year": 2020, "n_citations": 3}
{"id": 6359446, "s2_id": "170eeef7c9b4be938cb084aea9944adba56c52e1", "title": "Leveraging Transprecision Computing for Machine Vision Applications at the Edge", "abstract": "Machine vision tasks present challenges for resource constrained edge devices, particularly as they execute multiple tasks with variable workloads. A robust approach that can dynamically adapt in runtime while maintaining the maximum quality of service (QoS) within resource constraints, is needed. The paper presents a lightweight approach that monitors the runtime workload constraint and leverages accuracy-throughput trade-off. Optimisation techniques are included which find the configurations for each task for optimal accuracy, energy and memory and manages transparent switching between configurations. For an accuracy drop of 1%, we show a 1.6\u00d7 higher achieved frame processing rate with further improvements possible at lower accuracy.", "venue": "2021 IEEE Workshop on Signal Processing Systems (SiPS)", "authors": ["Umar Ibrahim Minhas", "Lev  Mukhanov", "Georgios  Karakonstantis", "Hans  Vandierendonck", "Roger  Woods"], "year": 2021, "n_citations": 0}
{"id": 6362129, "s2_id": "ade8f6bc1edab082a16b62a09d6b3342f0954cb9", "title": "High Level Programming for Heterogeneous Architectures", "abstract": "This work presents an effort to bridge the gap between abstract high level programming and OpenCL by extending an existing high level Java programming framework (APARAPI), based on OpenCL, so that it can be used to program FPGAs at a high level of abstraction and increased ease of programmability. We run several real world algorithms to assess the performance of the framework on both a low end and a high end system. On the low end and high end systems respectively we observed up to 78-80 percent power reduction and 4.8X-5.3X speed increase running NBody simulation, as well as up to 65-80 percent power reduction and 6.2X-7X speed increase for a KMeans, MapReduce algorithm running on top of the Hadoop framework and APARAPI.", "venue": "ArXiv", "authors": ["Oren  Segal", "Martin  Margala", "Sai Rahul Chalamalasetti", "Mitch  Wright"], "year": 2014, "n_citations": 12}
{"id": 6366817, "s2_id": "8df5bb5704c4458dca0a51bba49ec9a04d4e3611", "title": "A New Approach to Capacity Scaling Augmented With Unreliable Machine Learning Predictions", "abstract": "Modern data centers suffer from immense power consumption. As a result, data center operators have heavily invested in capacity scaling solutions, which dynamically deactivate servers if the demand is low and activate them again when the workload increases. We analyze a continuoustime model for capacity scaling, where the goal is to minimize the weighted sum of flow-time, switching cost, and power consumption in an online fashion. We propose a novel algorithm, called the Adaptive Balanced Capacity Scaling (ABCS) algorithm, that has access to black-box machine learning predictions. ABCS aims to adapt to the predictions and is also robust against unpredictable surges in the workload. In particular, we prove that the ABCS algorithm is (1 + \u03b5)-competitive if the predictions are accurate, and yet, it has a uniformly bounded competitive ratio even if the predictions are completely inaccurate.", "venue": "ArXiv", "authors": ["Daan  Rutten", "Debankur  Mukherjee"], "year": 2021, "n_citations": 0}
{"id": 6367511, "s2_id": "36952d722b42b34940ee0fdfb0d3bce9d8d878f9", "title": "Generating Hard Instances for Robust Combinatorial Optimization", "abstract": "While research in robust optimization has attracted considerable interest over the last decades, its algorithmic development has been hindered by several factors. One of them is a missing set of benchmark instances that make algorithm performance better comparable, and makes reproducing instances unnecessary. Such a benchmark set should contain hard instances in particular, but so far, the standard approach to produce instances has been to sample values randomly from a uniform distribution. \nIn this paper we introduce a new method to produce hard instances for min-max combinatorial optimization problems, which is based on an optimization model itself. Our approach does not make any assumptions on the problem structure and can thus be applied to any combinatorial problem. Using the Selection and Traveling Salesman problems as examples, we show that it is possible to produce instances which are up to 500 times harder to solve for a mixed-integer programming solver than the current state-of-the-art instances.", "venue": "Eur. J. Oper. Res.", "authors": ["Marc  Goerigk", "Stephen J. Maher"], "year": 2020, "n_citations": 2}
{"id": 6369345, "s2_id": "ae569a55d3de2681407b1fab127e23d00b769ef9", "title": "Dynamic Scheduling of Virtual Machines Running HPC Workloads in Scientific Grids", "abstract": "The primary motivation for uptake of virtualization has been resource isolation, capacity management and resource customization allowing resource providers to consolidate their resources in virtual machines. Various approaches have been taken to integrate virtualization in to scientific Grids especially in the arena of High Performance Computing (HPC) to run grid jobs in virtual machines, thus enabling better provisioning of the underlying resources and customization of the execution environment on runtime. Despite the gains, virtualization layer also incur a performance penalty and its not very well understood that how such an overhead will impact the performance of systems where jobs are scheduled with tight deadlines. Since this overhead varies the types of workload whether they are memory intensive, CPU intensive or network I/O bound, and could lead to unpredictable deadline estimation for the running jobs in the system. In our study, we have attempted to tackle this problem by developing an intelligent scheduling technique for virtual machines which monitors the workload types and deadlines, and calculate the system over head in real time to maximize number of jobs finishing within their agreed deadlines.", "venue": "2009 3rd International Conference on New Technologies, Mobility and Security", "authors": ["Omer  Khalid", "Ivo  Maljevic", "Richard J. Anthony", "Miltos  Petridis", "Kevin  Parrott", "Markus  Schulz"], "year": 2009, "n_citations": 93}
{"id": 6371477, "s2_id": "6822c6294847673dac513032f667b9630b10e799", "title": "An Extensible Timing Infrastructure for Adaptive Large-Scale Applications", "abstract": "Real-time access to accurate and reliable timing information is necessary to profile scientific applications, and crucial as simulations become increasingly complex, adaptive, and large-scale. The Cactus Framework provides flexible and extensible capabilities for timing information through a well designed infrastructure and timing API. Applications built with Cactus automatically gain access to built-in timers, such as gettimeofday and getrusage, system-specific hardware clocks, and high-level interfaces such as PAPI. We describe the Cactus timer interface, its motivation, and its implementation. We then demonstrate how this timing information can be used by an example scientific application to profile itself, and to dynamically adapt itself to a changing environment at run time.", "venue": "PPAM", "authors": ["Dylan  Stark", "Gabrielle  Allen", "Tom  Goodale", "Thomas  Radke", "Erik  Schnetter"], "year": 2007, "n_citations": 3}
{"id": 6378741, "s2_id": "783b2ef5d5b902b32c238ac66c63a91665871deb", "title": "Modeling the Invariance of Virtual Pointers in LLVM", "abstract": "Devirtualization is a compiler optimization that replaces indirect (virtual) function calls with direct calls. It is particularly effective in object-oriented languages, such as Java or C++, in which virtual methods are typically abundant. \nWe present a novel abstract model to express the lifetimes of C++ dynamic objects and invariance of virtual table pointers in the LLVM intermediate representation. The model and the corresponding implementation in Clang and LLVM enable full devirtualization of virtual calls whenever the dynamic type is statically known and elimination of redundant virtual table loads in other cases. \nDue to the complexity of C++, this has not been achieved by any other C++ compiler so far. Although our model was designed for C++, it is also applicable to other languages that use virtual dispatch. Our benchmarks show an average of 0.8% performance improvement on real-world C++ programs, with more than 30% speedup in some cases. The implementation is already a part of the upstream LLVM/Clang and can be enabled with the -fstrict-vtable-pointers flag.", "venue": "ArXiv", "authors": ["Piotr  Padlewski", "Krzysztof  Pszeniczny", "Richard  Smith"], "year": 2020, "n_citations": 1}
{"id": 6379610, "s2_id": "3656774a740ca946ad68ca8e2107172993127026", "title": "Efficient Timeout Synthesis in Fixed-Delay CTMC Using Policy Iteration", "abstract": "We consider the fixed-delay synthesis problem for continuous-time Markov chains extended with fixed-delay transitions (fdCTMC). The goal is to synthesize concrete values of the fixed-delays (timeouts) that minimize the expected total cost incurred before reaching a given set of target states. The same problem has been considered and solved in previous works by computing an optimal policy in a certain discrete-time Markov decision process (MDP) with a huge number of actions that correspond to suitably discretized values of the timeouts. In this paper, we design a symbolic fixed-delay synthesis algorithm which avoids the explicit construction of large action spaces. Instead, the algorithm computes a small sets of \"promising\" candidate actions on demand. The candidate actions are selected by minimizing a certain objective function by computing its symbolic derivative and extracting a univariate polynomial whose roots are precisely the points where the derivative takes zero value. Since roots of high degree univariate polynomials can be isolated very efficiently using modern mathematical software, we achieve not only drastic memory savings but also speedup by three orders of magnitude compared to the previous methods.", "venue": "2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)", "authors": ["Lubos  Korenciak", "Anton\u00edn  Kucera", "Vojtech  Reh\u00e1k"], "year": 2016, "n_citations": 2}
{"id": 6382977, "s2_id": "25578957034044c6a977ec69edbe6bfcbef91371", "title": "Highly Efficient Memory Failure Prediction using Mcelog-based Data Mining and Machine Learning", "abstract": "In the data center, unexpected downtime caused by memory failures can lead to a decline in the stability of the server and even the entire information technology infrastructure, which harms the business. Therefore, whether the memory failure can be accurately predicted in advance has become one of the most important issues to be studied in the data center. However, for the memory failure prediction in the production system, it is necessary to solve technical problems such as huge data noise and extreme imbalance between positive and negative samples, and at the same time ensure the long-term stability of the algorithm. This paper compares and summarizes some commonly used skills and the improvement they can bring. The single model we proposed won the top 14th in the 2nd Alibaba Cloud AIOps Competition belonging to the 25th PAKDD conference. It takes only 30 minutes to pass the online test, while most of the other contestants\u2019 solution need more than 3 hours. Codes has been open source to https://www.github.com/ycd2016/acaioc2.", "venue": "ArXiv", "authors": ["Chengdong  Yao"], "year": 2021, "n_citations": 0}
{"id": 6387414, "s2_id": "c9d1641119da1617eab51488b70194a40f5dee07", "title": "SOAP: One Clean Analysis of All Age-Based Scheduling Policies", "abstract": "We consider an extremely broad class of M/G/1 scheduling policies called SOAP: Schedule Ordered by Age-based Priority. The SOAP policies include almost all scheduling policies in the literature as well as an infinite number of variants which have never been analyzed, or maybe not even conceived. SOAP policies range from classic policies, like first-come, first-serve (FCFS), foreground-background (FB), class-based priority, and shortest remaining processing time (SRPT); to much more complicated scheduling rules, such as the famously complex Gittins index policy and other policies in which a job's priority changes arbitrarily with its age. While the response time of policies in the former category is well understood, policies in the latter category have resisted response time analysis. We present a universal analysis of all SOAP policies, deriving the mean and Laplace-Stieltjes transform of response time. The full version of this work appears in POMACS [Scully et al., 2018].", "venue": "SIGMETRICS 2018", "authors": ["Ziv  Scully", "Mor  Harchol-Balter", "Alan  Scheller-Wolf"], "year": 2018, "n_citations": 11}
{"id": 6392100, "s2_id": "91cf4823081f594d53d1e92345f919b58893f513", "title": "TCD-NPE: A Re-configurable and Efficient Neural Processing Engine, Powered by Novel Temporal-Carry-deferring MACs", "abstract": "In this paper, we first propose the design of Temporal-Carry-deferring MAC (TCD-MAC) and illustrate how our proposed solution can gain significant energy and performance benefit when utilized to process a stream of input data. We then propose using the TCD-MAC to build a reconfigurable, high speed, and low power Neural Processing Engine (TCD-NPE). We, further, propose a novel scheduler that lists the sequence of needed processing events to process an MLP model in the least number of computational rounds in our proposed TCD-NPE. We illustrate that our proposed TCD-NPE significantly outperform similar neural processing solutions that use conventional MACs in terms of both energy consumption and execution time.", "venue": "2019 International Conference on ReConFigurable Computing and FPGAs (ReConFig)", "authors": ["Ali  Mirzaeian", "Houman  Homayoun", "Avesta  Sasan"], "year": 2019, "n_citations": 10}
{"id": 6392931, "s2_id": "82f76d55baf50ac584e86ebc0b6352f63dec1b00", "title": "Machine Learning-Based Link Fault Identification and Localization in Complex Networks", "abstract": "With the proliferation of network devices and rapid development in information technology, networks such as Internet of Things are increasing in size and becoming more complex with heterogeneous wired and wireless links. In such networks, link faults may result in a link disconnection without immediate replacement or a link reconnection, e.g., a wireless node changes its access point. Identifying whether a link disconnection or a link reconnection has occurred and localizing the failed link become a challenging problem. An active probing approach requires a long time to probe the network by sending signaling messages on different paths, thus incurring significant communication delay and overhead. In this paper, we adopt a passive approach and develop a three-stage machine learning-based technique for link fault identification and localization (ML-LFIL) by analyzing the measurements captured from the normal traffic flows, including aggregate flow rate, end-to-end delay, and packet loss. ML-LFIL learns the traffic behavior in normal working conditions and different link fault scenarios. We train the learning model using support vector machine, multilayer perceptron, and random forest. We implement ML-LFIL and carry out extensive experiments using Mininet platform. Performance studies show that ML-LFIL achieves high accuracy while requiring much lower fault localization time compared to the active probing approach.", "venue": "IEEE Internet of Things Journal", "authors": ["Srinikethan Madapuzi Srinivasan", "Tram  Truong-Huu", "Mohan  Gurusamy"], "year": 2019, "n_citations": 24}
{"id": 6395860, "s2_id": "dbf5a0a907e59018e2067a533ea36db6622999a7", "title": "Substitute valuations: Generation and structure", "abstract": "Substitute valuations (in some contexts called gross substitute valuations) are prominent in combinatorial auction theory. An algorithm is given in this paper for generating a substitute valuation using a random number generator. In addition, the geometry of the set of all substitute valuations for a fixed number of goods K is investigated. The set consists of a union of polyhedrons, and the maximal polyhedrons are identified for K=4. It is shown that the maximum dimension of the polyhedrons increases with K nearly as fast as two to the power K. Consequently, under broad conditions, if a combinatorial algorithm can present an arbitrary substitute valuation given a list of input numbers, the list must grow nearly as fast as two to the power K.", "venue": "Perform. Evaluation", "authors": ["Bruce E. Hajek"], "year": 2008, "n_citations": 2}
{"id": 6396303, "s2_id": "9f6c41ca797e45803001bea932cb490c0d179077", "title": "Learning Aided Optimization for Energy Harvesting Devices with Outdated State Information", "abstract": "This paper considers utility optimal power control for energy harvesting wireless devices with a finite capacity battery. The distribution information of the underlying wireless environment and harvestable energy is unknown and only outdated system state information is known at the device controller. This scenario shares similarity with Lyapunov opportunistic optimization and online learning but is different from both. By a novel combination of Zinkevich's online gradient learning technique and the drift-plus-penalty technique from Lyapunov opportunistic optimization, this paper proposes a learning-aided algorithm that achieves utility within O (\u220a) of the optimal, for any desired \u220a > 0, by using a battery with an 0 (1/\u220a) capacity. The proposed algorithm has low complexity and makes power investment decisions based on system history, without requiring knowledge of the system state or its probability distribution.", "venue": "IEEE INFOCOM 2018 - IEEE Conference on Computer Communications", "authors": ["Hao  Yu", "Michael J. Neely"], "year": 2018, "n_citations": 17}
{"id": 6403033, "s2_id": "a0de85249efdb1153fdb0de54f12d717d253eef7", "title": "Boosting Metrics for Cloud Services Evaluation -- The Last Mile of Using Benchmark Suites", "abstract": "Benchmark suites are significant for evaluating various aspects of Cloud services from a holistic view. However, there is still a gap between using benchmark suites and achieving holistic impression of the evaluated Cloud services. Most Cloud service evaluation work intended to report individual benchmarking results without delivering summary measures. As a result, it could be still hard for customers with such evaluation reports to understand an evaluated Cloud service from a global perspective. Inspired by the boosting approaches to machine learning, we proposed the concept Boosting Metrics to represent all the potential approaches that are able to integrate a suite of benchmarking results. This paper introduces two types of preliminary boosting metrics, and demonstrates how the boosting metrics can be used to supplement primary measures of individual Cloud service features. In particular, boosting metrics can play a summary Response role in applying experimental design to Cloud services evaluation. Although the concept Boosting Metrics was refined based on our work in the Cloud Computing domain, we believe it can be easily adapted to the evaluation work of other computing paradigms.", "venue": "2013 IEEE 27th International Conference on Advanced Information Networking and Applications (AINA)", "authors": ["Zheng  Li", "Liam  O'Brien", "Rainbow  Cai", "He  Zhang"], "year": 2013, "n_citations": 13}
{"id": 6403604, "s2_id": "3577c56bc2949e2aa318b74e12543c384ddc7bd8", "title": "State Dependent Attempt Rate Modeling of Single Cell IEEE~802.11 WLANs with Homogeneous Nodes and Poisson Packet Arrivals", "abstract": "Analytical models of IEEE 802.11-based WLANs are invariably based on approximations, such as the well-known mean-field approximations proposed by Bianchi for saturated nodes. In this paper, we provide a new approach for modeling the situation when the nodes are not saturated. We study a State Dependent Attempt Rate (SDAR) approximation to model M queues (one queue per node) served by the CSMA/CA protocol as standardized in the IEEE 802.11 DCF. The approximation is that, when n of the M queues are non-empty, the attempt probability of the n non-empty nodes is given by the long-term attempt probability of n saturated nodes as provided by Bianchi's model. This yields a coupled queue system. When packets arrive to the M queues according to independent Poisson processes, we provide an exact model for the coupled queue system with SDAR service. The main contribution of this paper is to provide an analysis of the coupled queue process by studying a lower dimensional process and by introducing a certain conditional independence approximation. We show that the numerical results obtained from our finite buffer analysis are in excellent agreement with the corresponding results obtained from ns-2 simulations. We replace the CSMA/CA protocol as implemented in the ns-2 simulator with the SDAR service model to show that the SDAR approximation provides an accurate model for the CSMA/CA protocol. We also report the simulation speed-ups thus obtained by our model-based simulation.", "venue": "COMSNETS 2009", "authors": ["Manoj  Panda", "Anurag  Kumar"], "year": 2009, "n_citations": 9}
{"id": 6405027, "s2_id": "9f2a6fe6a60fb4929efed0ad3c9d0476498baf7e", "title": "Wireless node cooperation with resource availability constraints", "abstract": "Base station cooperation is a promising scheme to improve network performance for next generation cellular networks. Up to this point research has focused on station grouping criteria based solely on geographic proximity. However, for the cooperation to be meaningful, each station participating in a group should have sufficient available resources to share with others. In this work we consider an alternative grouping criterion based on a distance that considers both geographic proximity and available resources of the stations. When the network is modelled by a Poisson Point Process, we derive analytical formulas on the proportion of cooperative pairs or single stations, and the expected sum interference from each of the groups. The results illustrate that cooperation gains strongly depend on the distribution of available resources over the network.", "venue": "2017 15th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)", "authors": ["Luis David Alvarez Corrales", "Anastasios  Giovanidis", "Philippe  Martins", "Laurent  Decreusefond"], "year": 2017, "n_citations": 1}
{"id": 6406314, "s2_id": "7dc517886684032cf133a8854b894091f56243d4", "title": "Less is More: Exploiting the Standard Compiler Optimization Levels for Better Performance and Energy Consumption", "abstract": "This paper presents the interesting observation that by performing fewer of the optimizations available in a standard compiler optimization level such as -02, while preserving their original ordering, significant savings can be achieved in both execution time and energy consumption. This observation has been validated on two embedded processors, namely the ARM Cortex-M0 and the ARM Cortex-M3, using two different versions of the LLVM compilation framework; v3.8 and v5.0. Experimental evaluation with 71 embedded benchmarks demonstrated performance gains for at least half of the benchmarks for both processors. An average execution time reduction of 2.4% and 5.3% was achieved across all the benchmarks for the Cortex-M0 and Cortex-M3 processors, respectively, with execution time improvements ranging from 1% up to 90% over the -02. The savings that can be achieved are in the same range as what can be achieved by the state-of-the-art compilation approaches that use iterative compilation or machine learning to select flags or to determine phase orderings that result in more efficient code. In contrast to these time consuming and expensive to apply techniques, our approach only needs to test a limited number of optimization configurations, less than 64, to obtain similar or even better savings. Furthermore, our approach can support multi-criteria optimization as it targets execution time, energy consumption and code size at the same time.", "venue": "SCOPES", "authors": ["Kyriakos  Georgiou", "Craig  Blackmore", "Samuel Xavier de Souza", "Kerstin  Eder"], "year": 2018, "n_citations": 11}
{"id": 6407695, "s2_id": "9242b0998a3788d2a33325900caf0e4cd940c4f6", "title": "Early Performance Prediction of Web Services", "abstract": "Web Service is an interface which implements business logic. Performance is an important quality aspect of Web services because of their distributed nature. Predicting the performance of web services during early stages of software development is significant. In this paper we model web service using Unified Modeling Language, Use Case Diagram, Sequence Diagram, Deployment Diagram. We obtain the Performance metrics by simulating the web services model using a simulation tool Simulation of Multi-Tier Queuing Architecture. We have identified the bottle neck resources.", "venue": "ArXiv", "authors": ["Ch. Ram Mohan Reddy", "D. Evangelin Geetha", "K. G. Srinivasa", "T. V. Suresh Kumar", "K. Rajani Kanth"], "year": 2012, "n_citations": 11}
{"id": 6411914, "s2_id": "686bb1a44b8b9400a4bbcb86f07f02ed100d92d9", "title": "Critically loaded k-limited polling systems", "abstract": "We consider a two-queue polling model with switch-over times and $k$-limited service (serve at most $k_i$ customers during one visit period to queue $i$) in each queue. The major benefit of the $k$-limited service discipline is that it - besides bounding the cycle time - effectuates prioritization by assigning different service limits to different queues. System performance is studied in the heavy-traffic regime, in which one of the queues becomes critically loaded with the other queue remaining stable. By using a singular-perturbation technique, we rigorously prove heavy-traffic limits for the joint queue-length distribution. Moreover, it is observed that an interchange exists among the first two moments in service and switch-over times such that the HT limits remain unchanged. Not only do the rigorously proven results readily carry over to $N$($\\geq2$) queue polling systems, but one can also easily relax the distributional assumptions. The results and insights of this note prove their worth in the performance analysis of Wireless Personal Area Networks (WPAN) and mobile networks, where different users compete for access to the shared scarce resources.", "venue": "EAI Endorsed Trans. Collab. Comput.", "authors": ["Marko A. A. Boon", "Erik M. M. Winands"], "year": 2016, "n_citations": 2}
{"id": 6416214, "s2_id": "f491c4d10f9d656f64a2ad1269588ec2a5a54025", "title": "Daydream: Accurately Estimating the Efficacy of Optimizations for DNN Training", "abstract": "Modern deep neural network (DNN) training jobs use complex and heterogeneous software/hardware stacks. The efficacy of software-level optimizations can vary significantly when used in different deployment configurations. It is onerous and error-prone for ML practitioners and system developers to implement each optimization separately, and determine which ones will improve performance in their own configurations. Unfortunately, existing profiling tools do not aim to answer predictive questions such as \"How will optimization X affect the performance of my model?\". We address this critical limitation, and proposes a new profiling tool, Daydream, to help programmers efficiently explore the efficacy of DNN optimizations. Daydream models DNN execution with a fine-grained dependency graph based on low-level traces collected by CUPTI, and predicts runtime by simulating execution based on the dependency graph. Daydream maps the low-level traces using DNN domain-specific knowledge, and introduces a set of graph-transformation primitives that can easily model a wide variety of optimizations. We show that Daydream is able to model most mainstream DNN optimization techniques, and accurately predict the efficacy of optimizations that will result in significant performance improvements.", "venue": "USENIX Annual Technical Conference", "authors": ["Hongyu  Zhu", "Amar  Phanishayee", "Gennady  Pekhimenko"], "year": 2020, "n_citations": 10}
{"id": 6417247, "s2_id": "c6c4c972f71d2a7ad9c2289c65f1d030b91f3ecc", "title": "New Results on Parameter Estimation via Dynamic Regressor Extension and Mixing: Continuous and Discrete-Time Cases", "abstract": "We present some new results on the dynamic regressor extension and mixing parameter estimators for linear regression models recently proposed in the literature. This technique has proven instrumental in the solution of several open problems in system identification and adaptive control. The new results include the following, first, a unified treatment of the continuous and the discrete-time cases; second, the proposal of two new extended regressor matrices, one which guarantees a quantifiable transient performance improvement, and the other exponential convergence under conditions that are strictly weaker than regressor persistence of excitation; and, third, an alternative estimator ensuring convergence in finite-time whose adaptation gain, in contrast with the existing one, does not converge to zero. Simulations that illustrate our results are also presented.", "venue": "IEEE Transactions on Automatic Control", "authors": ["Romeo  Ortega", "Stanislav  Aranovskiy", "Anton A. Pyrkin", "Alessandro  Astolfi", "Alexey A. Bobtsov"], "year": 2021, "n_citations": 34}
{"id": 6418807, "s2_id": "95c84c346c316afd54787b50a2c5831b1c5c28a3", "title": "Toward a New Protocol to Evaluate Recommender Systems", "abstract": "In this paper, we propose an approach to analyze the performance and the added value of automatic recommender systems in an industrial context. We show that recommender systems are multifaceted and can be organized around 4 structuring functions: help users to decide, help users to compare, help users to discover, help users to explore. A global off line protocol is then proposed to evaluate recommender systems. This protocol is based on the definition of appropriate evaluation measures for each aforementioned function. The evaluation protocol is discussed from the perspective of the usefulness and trust of the recommendation. A new measure called Average Measure of Impact is introduced. This measure evaluates the impact of the personalized recommendation. We experiment with two classical methods, K-Nearest Neighbors (KNN) and Matrix Factorization (MF), using the well known dataset: Netflix. A segmentation of both users and items is proposed to finely analyze where the algorithms perform well or badly. We show that the performance is strongly dependent on the segments and that there is no clear correlation between the RMSE and the quality of the recommendation.", "venue": "RUE@RecSys", "authors": ["Frank  Meyer", "Fran\u00e7oise  Fessant", "Fabrice  Cl\u00e9rot", "\u00c9ric  Gaussier"], "year": 2012, "n_citations": 15}
{"id": 6427226, "s2_id": "2673df16bd509ab12113dfef3cd6d16c088a6f48", "title": "Unstable Throughput: When the Difficulty Algorithm Breaks", "abstract": "In Proof-of-Work blockchains, difficulty algorithms serve the crucial purpose of maintaining a stable transaction throughput by dynamically adjusting the block difficulty in response to the miners\u2019 constantly changing computational power. Blockchains that may experience severe hash rate fluctuations need difficulty algorithms that quickly adapt the mining difficulty. However, without careful design, the system could be gamed by miners using coin-hopping strategies to manipulate the block difficulty for profit. Such miner behavior results in an unreliable system due to the unstable processing of transactions.We provide an empirical analysis of how Bitcoin Cash\u2019s difficulty algorithm design leads to cyclicality in block solve times as a consequence of a positive feedback loop. In response, we mathematically derive a difficulty algorithm using a negative exponential filter which prohibits the formation of positive feedback and exhibits additional desirable properties, such as history agnosticism. We compare the described algorithm to that of Bitcoin Cash in a simulated mining environment and verify that the former would eliminate the severe oscillations in transaction throughput.", "venue": "2021 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)", "authors": ["Sam M. Werner", "Dragos I. Ilie", "Iain  Stewart", "William J. Knottenbelt"], "year": 2021, "n_citations": 4}
{"id": 6430505, "s2_id": "84427128b0d61d453d0815798240e9506b64a97f", "title": "Convex Optimization of Real Time SoC", "abstract": "Convex optimization methods are employed to optimize a real-time (RT) system-on-chip (SoC) under a variety of physical resource-driven constraints, demonstrated on an industry MPEG2 encoder SoC. The power optimization is compared to conventional performance-optimization framework, showing a factor of two and a half saving in power. Convex optimization is shown to be very efficient in a high-level early stage design exploration, guiding computer architects as to the choice of area, voltage, and frequency of the individual components of the Chip Multiprocessor (CMP).", "venue": "ArXiv", "authors": ["Leonid  Yavits", "Amir  Morad", "Ran  Ginosar", "Uri C. Weiser"], "year": 2016, "n_citations": 2}
{"id": 6440154, "s2_id": "d99a9d78d0e9cec14b5e50ad7b1ffddeb7cc2184", "title": "Ciw: An open-source discrete event simulation library", "abstract": "This paper introduces Ciw, an open-source library for conducting discrete event simulations that has been developed in Python. The strengths of the library are illustrated in terms of best practice and reproducibility for computational research. An analysis of Ciw\u2019s performance and comparison to several alternative discrete event simulation frameworks is presented.", "venue": "J. Simulation", "authors": ["Geraint I. Palmer", "Vincent A. Knight", "Paul R. Harper", "Asyl L. Hawa"], "year": 2019, "n_citations": 8}
{"id": 6443109, "s2_id": "ced94e5b71f70eff747e9944549c93f124de4ddd", "title": "Performance analysis of a 240 thread tournament level MCTS Go program on the Intel Xeon Phi", "abstract": "In 2013 Intel introduced the Xeon Phi, a new parallel co-processor board. The Xeon Phi is a cache-coherent many-core shared memory architecture claiming CPU-like versatility, programmability, high performance, and power efficiency. The first published micro-benchmark studies indicate that many of Intel's claims appear to be true. The current paper is the first study on the Phi of a complex artificial intelligence application. It contains an open source MCTS application for playing tournament quality Go (an oriental board game). We report the first speedup figures for up to 240 parallel threads on a real machine, allowing a direct comparison to previous simulation studies. After a substantial amount of work, we observed that performance scales well up to 32 threads, largely confirming previous simulation results of this Go program, although the performance surprisingly deteriorates between 32 and 240 threads. Furthermore, we report (1) unexpected performance anomalies between the Xeon Phi and Xeon CPU for small problem sizes and small numbers of threads, and (2) that performance is sensitive to scheduling choices. Achieving good performance on the Xeon Phi for complex programs is not straightforward; it requires a deep understanding of (1) search patterns, (2) of scheduling, and (3) of the architecture and its many cores and caches. In practice, the Xeon Phi is less straightforward to program for than originally envisioned by Intel.", "venue": "ArXiv", "authors": ["Sayyed Ali Mirsoleimani", "Aske  Plaat", "J. A. M. Vermaseren", "H. Jaap van den Herik"], "year": 2014, "n_citations": 6}
{"id": 6443411, "s2_id": "7b68f2ae6d00fc3e07ba12428a42c029eb195366", "title": "Performance Evaluation of Treecode Algorithm for N-Body Simulation Using GridRPC System", "abstract": "This paper is aimed at improving the performance of the treecode algorithm for N-Body simulation by employing the NetSolve GridRPC programming model to exploit the use of multiple clusters. N-Body is a classical problem, and appears in many areas of science and engineering, including astrophysics, molecular dynamics, and graphics. In the simulation of N-Body, the specific routine for calculating the forces on the bodies which accounts for upwards of 90% of the cycles in typical computations is eminently suitable for obtaining parallelism with GridRPC calls. It is divided among the compute nodes by simultaneously calling multiple GridRPC requests to them. The performance of the GridRPC implementation is then compared to that of the MPI version and hybrid MPI-OpenMP version for the treecode algorithm on individual clusters.", "venue": "ArXiv", "authors": ["Truong Vinh Truong Duy", "Katsuhiro  Yamazaki", "Shigeru  Oyanagi"], "year": 2012, "n_citations": 0}
{"id": 6447637, "s2_id": "5512a7c735c0328d23762a8b2dbe346aa2341906", "title": "Mean Field and Refined Mean Field Approximations for Heterogeneous Systems: It Works!", "abstract": "Mean field approximation is a powerful technique to study the performance of large stochastic systems represented as n interacting objects. Applications include load balancing models, epidemic spreading, cache replacement policies, or large-scale data centers. Mean field approximation is asymptotically exact for systems composed of n homogeneous objects under mild conditions. In this paper, we study what happens when objects are heterogeneous. This can represent servers with different speeds or contents with different popularities. We define an interaction model that allows obtaining asymptotic convergence results for stochastic systems with heterogeneous object behavior, and show that the error of the mean field approximation is of order O (1/n). More importantly, we show how to adapt the refined mean field approximation, developed by the authors of [14], and show that the error of this approximation is reduced to O (1/n2). To illustrate the applicability of our result, we present two examples. The first addresses a list-based cache replacement model RANDOM(m), which is an extension of the RANDOM policy. The second is a heterogeneous supermarket model. These examples show that the proposed approximations are computationally tractable and very accurate. They also show that for moderate system sizes (n \u2248 30) the refined mean field approximation tends to be more accurate than simulations for any reasonable simulation time.", "venue": "ArXiv", "authors": ["Sebastian  Allmeier", "Nicolas  Gast"], "year": 2021, "n_citations": 0}
{"id": 6447957, "s2_id": "10889b5c5fdffdd545277cb30ce8073d34f9fa79", "title": "Teaching an Old Elephant New Tricks", "abstract": "In recent years, column stores (or C-stores for short) have emerged as a novel approach to deal with read-mostly data warehousing applications. Experimental evidence suggests that, for certain types of queries, the new features of C-stores result in orders of magnitude improvement over traditional relational engines. At the same time, some C-store proponents argue that C-stores are fundamentally different from traditional engines, and therefore their benefits cannot be incorporated into a relational engine short of a complete rewrite. In this paper we challenge this claim and show that many of the benefits of C-stores can indeed be simulated in traditional engines with no changes whatsoever. We then identify some limitations of our ?pure-simulation? approach for the case of more complex queries. Finally, we predict that traditional relational engines will eventually leverage most of the benefits of C-stores natively, as is currently happening in other domains such as XML data.", "venue": "CIDR", "authors": ["Nicolas  Bruno"], "year": 2009, "n_citations": 28}
{"id": 6448988, "s2_id": "10700f1c54ecb535627ddf787475ae8e71fd08a4", "title": "Metamorphic IOTA", "abstract": "IOTA opened recently a new line of research in distributed ledgers area by targeting algorithms that ensure a high throughput for the transactions generated in IoT systems. Transactions are continuously appended to an acyclic structure called tangle and each new transaction selects as parents two existing transactions (called tips) that it approves. G-IOTA, a very recent improvement of IOTA, targets to protect tips left behind offering hence a good confidence level. However, this improvement had a cost: the use of an additional tip selection mechanism which may be critical in IoT systems since it needs additional energy consumption. In this paper we propose a new metamorphic algorithm for tip selection that offers the best guaranties of both IOTA and G-IOTA. Our contribution is two fold. First, we propose a parameterized algorithm, E-IOTA, for tip selection which targets to reduce the number of random walks executed in previous versions (IOTA and G-IOTA) while maintaining the same security guaranties as IOTA and the same confidence level and fairness with respect to tips selection as G-IOTA. Then we propose a formal analysis of the security guaranties offered by E-IOTA against various attacks mentioned in the original IOTA proposal (e.g. large weight attack, parasite chain attack and splitting attack). Interestingly, to the best of our knowledge this is the first formal analysis of the security guaranties of IOTA and its derivatives.", "venue": "ArXiv", "authors": ["Gewu  Bu", "Wassim  Hana", "Maria Gradinariu Potop-Butucaru"], "year": 2019, "n_citations": 5}
{"id": 6450867, "s2_id": "3823c361fdd6683ee3a6d670516236784cbebd63", "title": "Optimal Discriminant Functions Based on Sampled Distribution Distance for Modulation Classification", "abstract": "In this letter, we derive the optimal discriminant functions for modulation classification based on the sampled distribution distance. The proposed method classifies various candidate constellations using a low complexity approach based on the distribution distance at specific testpoints along the cumulative distribution function. This method, based on the Bayesian decision criteria, asymptotically provides the minimum classification error possible given a set of testpoints. Testpoint locations are also optimized to improve classification performance. The method provides significant gains over existing approaches that also use the distribution distance of the signal features.", "venue": "IEEE Communications Letters", "authors": ["Paulo  Urriza", "Eric  Rebeiz", "Danijela  Cabric"], "year": 2013, "n_citations": 10}
{"id": 6456814, "s2_id": "4dbf4359da59e7091983fc1be7dae091a502c59e", "title": "Performance Analysis of  $l_0$ Norm Constraint Least Mean Square Algorithm", "abstract": "As one of the recently proposed algorithms for sparse system identification, I0 norm constraint Least Mean Square (io-LMS) algorithm modifies the cost function of the traditional method with a penalty of tap-weight sparsity. The performance of I0-LMS is quite attractive compared with its various precursors. However, there has been no detailed study of its performance. This paper presents comprehensive theoretical performance analysis of I0-LMS for white Gaussian input data based on some reasonable assumptions, which are reasonable in a large range of parameter setting. Expressions for steady-state mean square deviation (MSD) are derived and discussed with respect to algorithm parameters and system sparsity. The parameter selection rule is established for achieving the best performance. Approximated with Taylor series, the instantaneous behavior is also derived. In addition, the relationship between I0-LMS and some previous arts and the sufficient conditions for I0-LMS to accelerate convergence are set up. Finally, all of the theoretical results are compared with simulations and are shown to agree well in a wide range of parameters.", "venue": "IEEE Transactions on Signal Processing", "authors": ["Guolong  Su", "Jian  Jin", "Yuantao  Gu", "Jian  Wang"], "year": 2012, "n_citations": 129}
{"id": 6460419, "s2_id": "a1b46ca204eca6a208990c3d7b87331179b735d0", "title": "DW&C: Dollops Wise Curtail IPv4/IPv6 Transition Mechanism using NS2", "abstract": "BD-SIIT and DSTM are widely deployed IPv4/IPv6 Transition mechanism to improve the performance of the computer network in terms of Throughput,End to End Delay(EED) and Packet Drop Rate(PDR).In this journal paper we have Implemented and Compared the Performance Issues of our newly proposed Dollops Wise Curtail(DW&C)IPv4/IPv6 Migration Mechanism with BD-SIIT and DSTM in NS2. Implementation and Comparison Performance Analysis between Dollops Wise Curtail,BD-SIIT and DSTM shows that Dollops Wise Curtail IPv4/IPv6 migration algorithm performance outperforms than BD-SIIT and DSTM.Based on extensive simulations,we show that DW&C algorithm reduces the Packet Drop Rate(PDR),End to End Delay(EED) and achieves better Throughput than BD-SIIT and DSTM.In our research work observation,the performance metrics such as Throughput,EED and PLR for DW&C,BD-SIIT and DSTM are measured using TCP,UDP,FTP and CBR Traffics", "venue": "ArXiv", "authors": ["J.  Hanumanthappa", "H.  Annaiah"], "year": 2014, "n_citations": 3}
{"id": 6461547, "s2_id": "ef14fa73dc4981e231e94bab04a357f1b4d97c8d", "title": "Beyond Scaling: Calculable Error Bounds of the Power-of-Two-Choices Mean-Field Model in Heavy-Traffic", "abstract": "This paper provides a recipe for deriving calculable approximation errors of mean-field models in heavy-traffic with the focus on the well-known load balancing algorithm --- power-of-two-choices (Po2). The recipe combines Stein's method for linearized mean-field models and State Space Concentration (SSC) based on geometric tail bounds. In particular, our approach divides the state space into two regions, a neighborhood near the mean-field equilibrium and the complement of that. We first use a tail bound to show that the steady-state probability being outside the neighborhood is small. Then, we use a linearized mean-field model and Stein's method to characterize the generator difference, which provides the dominant term of the approximation error. From the dominant term, we are able to obtain an asymptotically-tight bound and a nonasymptotic upper bound, both are calculable bounds, not order-wise scaling results like most results in the literature. Finally, we compare the theoretical bounds with numerical evaluations to show the effectiveness of our results. We note that the simulation results show that both bounds are valid even for small size systems such as a system with only ten servers.", "venue": "MobiHoc", "authors": ["Hairi", "Xin  Liu", "Lei  Ying"], "year": 2021, "n_citations": 2}
{"id": 6462268, "s2_id": "f687d64bc0289fbf8c6ba7106b86904c7af9b521", "title": "With Great Freedom Comes Great Opportunity: Rethinking Resource Allocation for Serverless Functions", "abstract": "Current serverless offerings give users a limited degree of flexibility for configuring the resources allocated to their function invocations by either coupling memory and CPU resources together or providing no knobs at all. These configuration choices simplify resource allocation decisions on behalf of users, but at the same time, create deployments that are resource inefficient. In this paper, we take a principled approach to the problem of resource allocation for serverless functions, allowing this choice to be made in an automatic way that leads to the best combination of performance and cost. In particular, we systematically explore the opportunities that come with decoupling memory and CPU resource allocations and also enabling the use of different VM types. We find a rich trade-off space between performance and cost. The provider can use this in a number of ways: from exposing all these parameters to the user, to eliciting preferences for performance and cost from users, or by simply offering the same performance with lower cost. This flexibility can also enable the provider to optimize its resource utilization and enable a cost-effective service with predictable performance. Our results show that, by decoupling memory and CPU allocation, there is potential to have up to 40% lower execution cost than the preset coupled configurations that are the norm in current serverless offerings. Similarly, making the correct choice of VM instance type can provide up to 50% better execution time. Furthermore, we demonstrate that providers can utilize different instance types for the same functions to maximize resource utilization while providing performance within 10-20% of the best resource configuration for each respective function.", "venue": "ArXiv", "authors": ["Muhammad  Bilal", "Marco  Canini", "Rodrigo  Fonseca", "Rodrigo  Rodrigues"], "year": 2021, "n_citations": 0}
{"id": 6464935, "s2_id": "3f20dc7e841571d1160a821150e2ce448274aa1f", "title": "Delay-based Back-Pressure scheduling in multi-hop wireless networks", "abstract": "Scheduling is a critical and challenging resource allocation mechanism for multi-hop wireless networks. It is well known that scheduling schemes that give a higher priority to the link with larger queue length can achieve high throughput performance. However, this queue-length-based approach could potentially suffer from large (even infinite) packet delays due to the well-known last packet problem, whereby packets may get excessively delayed due to lack of subsequent packet arrivals. Delay-based schemes have the potential to resolve this last packet problem by scheduling the link based on the delay for the packet has encountered. However, the throughput performance of delay-based schemes has largely been an open problem except in limited cases of single-hop networks. In this paper, we investigate delaybased scheduling schemes for multi-hop traffic scenarios. We view packet delays from a different perspective, and develop a scheduling scheme based on a new delay metric. Through rigorous analysis, we show that the proposed scheme achieves the optimal throughput performance. Finally, we conduct extensive simulations to support our analytical results, and show that the delay-based scheduler successfully removes excessive packet delays, while it achieves the same throughput region as the queue-length-based scheme.", "venue": "INFOCOM", "authors": ["Bo  Ji", "Changhee  Joo", "Ness B. Shroff"], "year": 2011, "n_citations": 45}
{"id": 6465547, "s2_id": "16ebbd3af9919a0caa4ada829e1452e64a978a86", "title": "A Lower Bound on the stability region of Redundancy-d with FIFO service discipline", "abstract": "Redundancy-d (R(d)) is a load balancing method used to route incoming jobs to K servers, each with its own queue. Every arriving job is replicated into 2<=d<=K tasks, which are then routed to d servers chosen uniformly at random. When the first task finishes service, the remaining d-1 tasks are cancelled and the job departs the system. \nDespite the fact that R(d) is known, under certain conditions, to substantially improve job completion times compared to not using redundancy at all, little is known on a more fundamental performance criterion: what is the set of arrival rates under which the R(d) queueing system with FIFO service discipline is stable? In this context, due to the complex dynamics of systems with redundancy and cancellations, existing results are scarce and are limited to very special cases with respect to the joint service time distribution of tasks. \nIn this paper we provide a non-trivial, closed form lower bound on the stability region of R(d) for a general joint service time distribution of tasks with finite first and second moments. We consider a discrete time system with Bernoulli arrivals and assume that jobs are processed by their order of arrival. We use the workload processes and a quadratic Lyapunov function to characterize the set of arrival rates for which the system is stable. While simulation results indicate our bound is not tight, it provides an easy-to-check performance guarantee.", "venue": "Oper. Res. Lett.", "authors": ["Gal  Mendelson"], "year": 2021, "n_citations": 5}
{"id": 6468813, "s2_id": "5de350863cb9b85c4d99199cbda8d94723690c2f", "title": "Orpheus: A New Deep Learning Framework for Easy Deployment and Evaluation of Edge Inference", "abstract": "Optimising deep learning inference across edge devices and optimisation targets such as inference time, memory footprint and power consumption is a key challenge due to the ubiquity of neural networks. Today, production deep learning frameworks provide useful abstractions to aid machine learning engineers and systems researchers. However, in exchange they can suffer from compatibility challenges (especially on constrained platforms), inaccessible code complexity, or design choices that otherwise limit research from a systems perspective. This paper presents Orpheus, a new deep learning framework for easy prototyping, deployment and evaluation of inference optimisations. Orpheus features a small codebase, minimal dependencies, and a simple process for integrating other third party systems. We present some preliminary evaluation results.", "venue": "2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Perry  Gibson", "Jos'e  Cano"], "year": 2020, "n_citations": 0}
{"id": 6469868, "s2_id": "69ee893fbac5b45972374063dc1cfd83f8c2c0c4", "title": "Space-Efficient Scheduling of Stochastically Generated Tasks", "abstract": "We study the problem of scheduling tasks for execution by a processor when the tasks can stochastically generate new tasks. Tasks can be of different types, and each type has a fixed, known probability of generating other tasks. We present results on the random variable S\u03c3 modeling the maximal space needed by the processor to store the currently active tasks when acting under the scheduler \u03c3. We obtain tail bounds for the distribution of S\u03c3 for both offline and online schedulers, and investigate the expected value E[S\u03c3].", "venue": "ICALP", "authors": ["Tom\u00e1s  Br\u00e1zdil", "Javier  Esparza", "Stefan  Kiefer", "Michael  Luttenberger"], "year": 2010, "n_citations": 5}
{"id": 6470225, "s2_id": "c8889cc13667160dbd16285ef2b267560ff7edfa", "title": "The role of commutativity in constraint propagation algorithms", "abstract": "Constraing propagation algorithms form an important part of most of the constraint programming systems. We provide here a simple, yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way. In this framework we proceed in two steps. First, we introduce a generic iteration algorithm on partial orderings and prove its correctness in an abstract setting. Then we instantiate this algorithm with specific partial orderings and functions to obtain specific constraint propagation algorithms. In particular, using the notions commutativity and semi-commutativity, we show that the AC-3, PC-2, DAC, and DPC algorithms for achieving (directional) arc consistency and (directional) path consistency are instances of a single generic algorithm. The work reported here extends and simplifies that of Apt [1999a].", "venue": "TOPL", "authors": ["Krzysztof R. Apt"], "year": 2000, "n_citations": 29}
{"id": 6474897, "s2_id": "8ed7d91281d82447d19d79e996ac15c8294b77e1", "title": "Outage Probability and Capacity for Two-Tier Femtocell Networks by Approximating Ratio of Rayleigh and Log Normal Random Variables", "abstract": "This paper presents the derivation for per-tier outage probability of a randomly deployed femtocell network over an existing macrocell network. The channel characteristics of macro user and femto user are addressed by considering different propagation modeling for outdoor and indoor links. Location based outage probability analysis and capacity of the system with outage constraints are used to analyze the system performance. To obtain the simplified expressions, approximations of ratios of Rayleigh random variables (RVs), Rayleigh to log normal RVs and their weighted summations, are derived with the verifications using simulations.", "venue": "2013 IEEE 77th Vehicular Technology Conference (VTC Spring)", "authors": ["Sumudu  Samarakoon", "R. M. A. P. Rajatheva", "Mehdi  Bennis", "Matti  Latva-aho"], "year": 2013, "n_citations": 3}
{"id": 6475689, "s2_id": "ee037d56a574cc655d16ca3f36a9ab0e6bb09d29", "title": "On Coding for Reliable VNF Chaining in DCNs", "abstract": "We study how erasure coding can improve service reliability in Data Center Networks (DCN). To this end, we find that coding can be best deployed in systems, where i) traffic is split into multiple parallel sub-flows, ii) each sub-flow is encoded; iii) Virtual Network Functions (VNF) concatenated into Service Function Chain (SFC) are replicated into at least as many VNF instances as there are sub-flows, resulting in parallel sub-SFCs; and iv) all coded sub-flows are distributed over parallel paths and processed in parallel. We study service reliability as function of the level of parallelization within DCN and the resulting amount of redundancy. Based on the probability theory and by considering failures of path segments, VNF and server failures, we analytically derive the probability that parallel sub-flows are successfully processed by the parallelized SFC and that the original serial traffic can be successfully recovered without service interruptions. We compare the proposed failure protection with coding and the standard backup protection and evaluate the related overhead of both methods, including decoding, traffic redirection and VNF migration. The results not only show the benefit of our scheme for reliability, but also a reduced overhead required in comparison to backup protection.", "venue": "2019 15th International Conference on the Design of Reliable Communication Networks (DRCN)", "authors": ["Anna  Engelmann", "Admela  Jukan", "Rastin  Pries"], "year": 2019, "n_citations": 3}
{"id": 6477174, "s2_id": "be75cbccabb18b81c3ac6ca1c36a55e5d13819b0", "title": "ContainerStress: Autonomous Cloud-Node Scoping Framework for Big-Data ML Use Cases", "abstract": "Deploying big-data Machine Learning (ML) services in a cloud environment presents a challenge to the cloud vendor with respect to the cloud container configuration sizing for any given customer use case. OracleLabs has developed an automated framework that uses nested-loop Monte Carlo simulation to autonomously scale any size customer ML use cases across the range of cloud CPU-GPU \"Shapes\" (configurations of CPUs and/or GPUs in Cloud containers available to end customers). Moreover, the OracleLabs and NVIDIA authors have collaborated on a ML benchmark study which analyzes the compute cost and GPU acceleration of any ML prognostic algorithm and assesses the reduction of compute cost in a cloud container comprising conventional CPUs and NVIDIA GPUs.", "venue": "2019 International Conference on Computational Science and Computational Intelligence (CSCI)", "authors": ["Guang Chao Wang", "Kenny  Gross", "Akshay  Subramaniam"], "year": 2019, "n_citations": 1}
{"id": 6480293, "s2_id": "e9d13b70f983f64f602cb647814ca3908668a2ea", "title": "Mathematical Model for the Optimal Utilization Percentile in M/M/1 Systems: A Contribution about Knees in Performance Curves", "abstract": "Performance curves of queuing systems can be analyzed by separating them into three regions: the flat region, the knee region, and the exponential region. Practical considerations, usually locate the knee region between 70-90% of the theoretical maximum utilization. However, there is not a clear agreement about where the boundaries between regions are, and where exactly the utilization knee is located. An open debate about knees in performance curves was undertaken at least 20 years ago. This historical debate is mainly divided between those who claim that a knee in the curve is not a well-defined term in mathematics, or it is a subjective and not really meaningful concept, and those who define knees mathematically and consider their relevance and application. In this paper, we present a mathematical model and analysis for identifying the three mentioned regions on performance curves for M/M/1 systems; specifically, we found the knees, or optimal utilization percentiles, at the vertices of the hyperbolas that relate response time as a function of utilization. Using these results, we argue that an adaptive and optimal queuing system could be deployed by keeping load and throughput within the knee region.", "venue": "ArXiv", "authors": ["Francisco A. Gonzalez Horta", "Rogerio A. Enr\u00edquez-Caldera", "Juan Manuel Ram\u00edrez-Cort\u00e9s", "Jorge  Mart\u00ednez-Carballido", "Eldamira  Buenfil-Alpuche"], "year": 2011, "n_citations": 5}
{"id": 6481277, "s2_id": "6a338327653959f9186cd88cc1e12e122a82b3a2", "title": "Parallelism Via Concurrency at Multiple Levels", "abstract": "In this paper we examine the key elements determining the best performance of computing by increasing the frequency of a single chip and to get the minimum latency during execution of the programs to achieve best possible output. It is not enough to provide concurrent improvements in the hardware as Software also have to introduce concurrency in order to exploit the parallelism. The software parallelism is defined by the control and data dependency of programs whereas Hardware refers to the type of parallelism defined by the machine architecture and hardware multiplicity.", "venue": "ArXiv", "authors": ["Kamran  Latif"], "year": 2014, "n_citations": 1}
{"id": 6484925, "s2_id": "64f8f8508ceefc2c29b1c0b77b699dbc904eb24e", "title": "Quicker ADC : Unlocking the Hidden Potential of Product Quantization With SIMD", "abstract": "Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a foundation of many multimedia retrieval systems. A common approach is to rely on Product Quantization, which allows the storage of large vector databases in memory and efficient distance computations. Yet, implementations of nearest neighbor search with Product Quantization have their performance limited by the many memory accesses they perform. Following this observation, Andr\u00e9 et al. proposed Quick ADC with up to <inline-formula><tex-math notation=\"LaTeX\">$6{\\times }$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>6</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"lescouarnec-ieq1-2952606.gif\"/></alternatives></inline-formula> faster implementations of PQ <inline-formula><tex-math notation=\"LaTeX\">${m}{\\times }{4}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>m</mml:mi><mml:mo>\u00d7</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"lescouarnec-ieq2-2952606.gif\"/></alternatives></inline-formula> product quantizers (PQ) leveraging specific SIMD instructions. Quicker ADC is a generalization of Quick ADC not limited to PQ <inline-formula><tex-math notation=\"LaTeX\">${m}{\\times }{4}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>m</mml:mi><mml:mo>\u00d7</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"lescouarnec-ieq3-2952606.gif\"/></alternatives></inline-formula> codes and supporting AVX-512, the latest revision of SIMD instruction set. In doing so, Quicker ADC faces the challenge of using efficiently 5,6 and 7-bit shuffles that do not align to computer bytes or words. To this end, we introduce (i) <italic>irregular product quantizers</italic> combining sub-quantizers of different granularity and (ii) <italic>split tables</italic> allowing lookup tables larger than registers. We evaluate Quicker ADC with multiple indexes including Inverted Multi-Indexes and IVF HNSW and show that it outperforms the reference optimized implementations (i.e., FAISS and polysemous codes) for numerous configurations. Finally, we release an open-source fork of FAISS enhanced with Quicker ADC.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": ["Fabien  Andr\u00e9", "Anne-Marie  Kermarrec", "Nicolas  Le Scouarnec"], "year": 2021, "n_citations": 5}
{"id": 6486848, "s2_id": "65c8e1e68c88b935b1c084267b2f3c9313afb5ed", "title": "A Minmax Utilization Algorithm for Network Traffic Scheduling of Industrial Robots", "abstract": "Emerging 5G and beyond wireless industrial virtualized networks are expected to support a significant number of robotic manipulators. Depending on the processes involved, these industrial robots might result in significant volume of multi-modal traffic that will need to traverse the network all the way to the (public/private) edge cloud, where advanced processing, control and service orchestration will be taking place. In this paper, we perform the traffic engineering by capitalizing on the underlying pseudo-deterministic nature of the repetitive processes of robotic manipulators in an industrial environment and propose an integer linear programming (ILP) model to minimize the maximum aggregate traffic in the network. The task sequence and time gap requirements are also considered in the proposed model. To tackle the curse of dimensionality in ILP, we provide a random search algorithm with quadratic time complexity. Numerical investigations reveal that the proposed scheme can reduce the peak data rate up to 53.4% compared with the nominal case where robotic manipulators operate in an uncoordinated fashion, resulting in significant improvement in the utilization of the underlying network resources.", "venue": "ArXiv", "authors": ["Yantong  Wang", "Vasilis  Friderikos", "Sebastian  Andraos"], "year": 2021, "n_citations": 0}
{"id": 6487658, "s2_id": "90db0a582ac3e41251bdd8c30628e9fc5638b270", "title": "Dynamic Load Balancing with Tokens", "abstract": "Efficiently exploiting the resources of data centers is a complex task that requires efficient and reliable load balancing and resource allocation algorithms. The former are in charge of assigning jobs to servers upon their arrival in the system, while the latter are responsible for sharing server resources between their assigned jobs. These algorithms should take account of various constraints, such as data locality, that restrict the feasible job assignments. In this paper, we propose a token-based mechanism that efficiently balances load between servers without requiring any knowledge on job arrival rates and server capacities. Assuming a balanced fair sharing of the server resources, we show that the resulting dynamic load balancing is insensitive to the job size distribution. Its performance is compared to that obtained under the best static load balancing and in an ideal system that would constantly optimize the resource utilization.", "venue": "2018 IFIP Networking Conference (IFIP Networking) and Workshops", "authors": ["C\u00e9line  Comte"], "year": 2018, "n_citations": 3}
{"id": 6496812, "s2_id": "b4455948e9786853204c39fdd64b34434b5ef9a5", "title": "Software model refactoring based on performance analysis: better working on software or performance side?", "abstract": "Several approaches have been introduced in the last few years to tackle the problem of interpreting model-based performance analysis results and translating them into architectural feedback. Typically the interpretation can take place by browsing either the software model or the performance model. In this paper, we compare two approaches that we have recently introduced for this goal: one based on the detection and solution of performance antipatterns, and another one based on bidirectional model transformations between software and performance models. We apply both approaches to the same example in order to illustrate the differences in the obtained performance results. Thereafter, we raise the level of abstraction and we discuss the pros and cons of working on the software side and on the performance side.", "venue": "FESCA", "authors": ["Davide  Arcelli", "Vittorio  Cortellessa"], "year": 2013, "n_citations": 23}
{"id": 6497013, "s2_id": "43e24651b7e0af8233f0cfb721cfb8baeedb74a3", "title": "Tracking Performance Limitations of MIMO Networked Control Systems With Multiple Communication Constraints", "abstract": "In this paper, the tracking performance limitation of networked control systems (NCSs) is studied. The NCSs are considered as continuous-time linear multi-input multioutput (MIMO) systems with random reference noises. The controlled plants include unstable poles and nonminimum phase (NMP) zeros. The output feedback path is affected by multiple communication constraints. We focus on some basic communication constraints, including additive white noise (AWN), quantization noise, bandwidth, as well as encoder-decoder. The system performance is evaluated with the tracking error energy, and used a two-degree-of-freedom (2DOF) controller. The explicit representation of the tracking performance is given in this paper. The results indicate the tracking performance limitations rely to internal characteristics of the plant (unstable poles and NMP zeros), reference noises [the reference noise power distribution (RNPD) and its directions], and the characteristics of communication constraints. The characteristics of communication constraints include communication noise power distribution (CNPD); quantization noise power distribution (QNPD), and their distribution directions; transform bandwidth allocation (TBA); transform encoder-decoder allocation (TEA), and their allocation directions; and NMP zeros and MP part of bandwidth. Moreover, the tracking performance limitations are also affected by the angles between the each transform NMP zero direction and RNPD direction, and these angles between each transform unstable poles direction and the direction of communication constraint distribution/allocation. In addition, for MIMO NCSs, bandwidth (there are not identical two channels) can always affect the direction of unstable poles, and the channel allocation of bandwidth and encode-decode may be used for a feasible method for the performance allocation of each channel. Finally, an instance is given for verifying the effectiveness of the theoretical outcomes.", "venue": "IEEE Transactions on Cybernetics", "authors": ["Chao-Yang  Chen", "Weihua  Gui", "Lianghong  Wu", "Zhaohua  Liu", "Huaicheng  Yan"], "year": 2020, "n_citations": 19}
{"id": 6498475, "s2_id": "a87f425a339b25c8b55ddca60bf878ed7001904f", "title": "JArena: Partitioned Shared Memory for NUMA-awareness in Multi-threaded Scientific Applications", "abstract": "The distributed shared memory (DSM) architecture is widely used in today's computer design to mitigate the ever-widening processing-memory gap, and inevitably exhibits non-uniform memory access (NUMA) to shared-memory parallel applications. Failure to achieve full NUMA-awareness can significantly downgrade application performance, especially on today's manycore platforms with tens to hundreds of cores. Yet traditional approaches such as first-touch and memory policy fail short in either false page-sharing, fragmentation, or ease-of-use. In this paper, we propose a partitioned shared memory approach which allows multi-threaded applications to achieve full NUMA-awareness with only minor code changes and develop a companying NUMA-aware heap manager which eliminates false page-sharing and minimizes fragmentation. Experiments on a 256-core cc-NUMA computing node show that the proposed approach achieves true NUMA-awareness and improves the performance of typical multi-threaded scientific applications up to 4.3 folds with the increased use of cores.", "venue": "ArXiv", "authors": ["Zhang  Yang", "Aiqing  Zhang", "Zeyao  Mo"], "year": 2019, "n_citations": 1}
{"id": 6498703, "s2_id": "cd1128c9bad8586c715212a65c05d74cc8d0b264", "title": "Optimization strategies for parallel CPU and GPU implementations of a meshfree particle method", "abstract": "Much of the current focus in high performance computing (HPC) for computational fluid dynamics (CFD) deals with grid based methods. However, parallel implementations for new meshfree particle methods such as Smoothed Particle Hydrodynamics (SPH) are less studied. In this work, we present optimizations for both central processing unit (CPU) and graphics processing unit (GPU) of a SPH method. These optimization strategies can be further applied to many other meshfree methods. The obtained performance for each architecture and a comparison between the most efficient implementations for CPU and GPU are shown.", "venue": "ArXiv", "authors": ["Jos\u00e9 M. Dom\u00ednguez", "Alejandro J. C. Crespo", "Moncho  G\u00f3mez-Gesteira"], "year": 2011, "n_citations": 7}
{"id": 6502073, "s2_id": "842d7d377dd51e1b949b139c928cba5e2841fb2d", "title": "In Datacenter Performance, The Only Constant Is Change", "abstract": "All computing infrastructure suffers from performance variability, be it bare-metal or virtualized. This phenomenon originates from many sources: some transient, such as noisy neighbors, and others more permanent but sudden, such as changes or wear in hardware, changes in the underlying hypervisor stack, or even undocumented interactions between the policies of the computing resource provider and the active workloads. Thus, performance measurements obtained on clouds, HPC facilities, and, more generally, datacenter environments are almost guaranteed to exhibit performance regimes that evolve over time, which leads to undesirable nonstationarities in application performance. In this paper, we present our analysis of performance of the bare-metal hardware available on the CloudLab testbed where we focus on quantifying the evolving performance regimes using changepoint detection. We describe our findings, backed by a dataset with nearly 6.9M benchmark results collected from over 1600 machines over a period of 2 years and 9 months. These findings yield a comprehensive characterization of real-world performance variability patterns in one computing facility, a methodology for studying such patterns on other infrastructures, and contribute to a better understanding of performance variability in general.", "venue": "2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID)", "authors": ["Dmitry  Duplyakin", "Alexandru  Uta", "Aleksander  Maricq", "Robert  Ricci"], "year": 2020, "n_citations": 3}
{"id": 6502427, "s2_id": "8e6a86163eb9c0248473817f5e0bed2f00be467b", "title": "Towards Autonomic Service Provisioning Systems", "abstract": "This paper discusses our experience in building SPIRE, an autonomic system for service provision. The architecture consists of a set of hosted Web Services subject to QoS constraints, and a certain number of servers used to run session-based traffic. Customers pay for having their jobs run, but require in turn certain quality guarantees: there are different SLAs specifying charges for running jobs and penalties for failing to meet promised performance metrics. The system is driven by an utility function, aiming at optimizing the average earned revenue per unit time. Demand and performance statistics are collected, while traffic parameters are estimated in order to make dynamic decisions concerning server allocation and admission control. Different utility functions are introduced and a number of experiments aiming at testing their performance are discussed. Results show that revenues can be dramatically improved by imposing suitable conditions for accepting incoming traffic, the proposed system performs well under different traffic settings, and it successfully adapts to changes in the operating environment.", "venue": "2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing", "authors": ["Michele  Mazzucco"], "year": 2010, "n_citations": 15}
{"id": 6508772, "s2_id": "3ad099af772920b68aaff715ff59793d5def48de", "title": "Differential Diversity Reception of MDPSK over Independent Rayleigh Channels with Nonidentical Branch Statistics and Asymmetric Fading Spectrum", "abstract": "This paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (DPSK) with differential detection over nonselective, independent, nonidentically distributed, Rayleigh fading channels. The fading process in each branch is assumed to have an arbitrary Doppler spectrum with arbitrary Doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. Using 8-DPSK as an example, the average bit error probability (BEP) of the optimum diversity receiver is obtained by calculating the BEP for each of the three individual bits. The BEP results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "venue": "2007 IEEE International Symposium on Information Theory", "authors": ["Hua  Fu", "Pooi Yuen Kam"], "year": 2007, "n_citations": 0}
{"id": 6510784, "s2_id": "a72a6bbef914c7177d8eb5ef168527c6d0ad1b67", "title": "Branch-Avoiding Graph Algorithms", "abstract": "This paper quantifies the impact of branches and branch mispredictions on the single-core performance of certain graph problems, specifically for computing connected components. We show that branch mispredictions are costly and can reduce performance by as much as 30%-50%. This insight suggests that one should seek graph algorithms and implementations that avoid branches. As a proof-of-concept, we devise such branch-avoiding implementations of the Shiloach-Vishkin algorithm for computing connected components. We evaluate these implementations on current x86 and ARM-based processors to show the efficacy of the approach. Our results suggest how both compiler writers and architects might exploit this insight to improve graph processing systems more broadly and create better systems for such problems.", "venue": "SPAA", "authors": ["Oded  Green", "Marat  Dukhan", "Richard W. Vuduc"], "year": 2015, "n_citations": 23}
{"id": 6512209, "s2_id": "01cc1bc4d5c2135fc5aa5d7049b3fbbebfd0a3fe", "title": "Applicability of a Novel Integer Programming Model for Wireless Sensor Networks", "abstract": "This paper presents an applicability analysis over a novel integer programming model devoted to optimize power consumption efficiency in heterogeneous wireless sensor networks. This model is based upon a schedule of sensor allocation plans in multiple time intervals subject to coverage and connectivity constraints. By turning off a specific set of redundant sensors in each time interval, it is possible to reduce the total energy consumption in the network and, at the same time, avoid partitioning the whole network by losing some strategic sensors too prematurely. Since the network is heterogeneous, sensors can sense different phenomena from different demand points, with different sample rates. As the problem instances grows the time spent to the execution turns impracticable.", "venue": "ArXiv", "authors": ["Alexei Barbosa de Aguiar", "Alvaro de Menezes S. Neto", "Pl\u00e1cido Rog\u00e9rio Pinheiro", "Andr\u00e9 L. V. Coelho"], "year": 2009, "n_citations": 9}
{"id": 6520525, "s2_id": "f7e634a2824c53e9ad22ffd9578b9887965e28f0", "title": "Scalar Arithmetic Multiple Data: Customizable Precision for Deep Neural Networks", "abstract": "Quantization of weights and activations in Deep Neural Networks (DNNs) is a powerful technique for network compression, and has enjoyed significant attention and success. However, much of the inference-time benefit of quantization is accessible only through customized hardware accelerators or with an FPGA implementation of quantized arithmetic. Building on prior work, we show how to construct very fast implementations of arbitrary bit-precise signed and unsigned integer operations using a software technique which logically embeds a vector architecture with custom bit-width lanes in fixed-width scalar arithmetic. At the strongest level of quantization, our approach yields a maximum speedup of ~ 6\u00d7 on an x86 platform, and ~ 10\u00d7 on an ARM platform versus quantization to native 8-bit integers.", "venue": "2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)", "authors": ["Andrew  Anderson", "David  Gregg"], "year": 2019, "n_citations": 1}
{"id": 6520621, "s2_id": "24b12c02cf6016274f82aabfe540e7e24c2e9a07", "title": "Performance Evaluation of Snapshot Methods to Warm the Serverless Cold Start", "abstract": "The serverless computing model strengthens the cloud computing tendency to abstract resource management. Serverless platforms are responsible for deploying and scaling the developer's applications. Serverless also incorporated the pay-as-you-go billing model, which only considers the time spent processing client requests. Such a decision created a natural incentive for improving the platform's efficient resource usage. This search for efficiency can lead to the cold start problem, which represents a delay to execute serverless applications. Among the solutions proposed to deal with the cold start, those based on the snapshot method stand out. Despite the rich exploration of the technique, there is a lack of research that evaluates the solution's trade-offs. In this direction, this work compares two solutions to mitigate the cold start: Prebaking and SEUSS. We analyzed the solution's performance with functions of different levels of complexity: NoOp, a function that renders Markdown to HTML, and a function that loads 41 MB of dependencies. Preliminary results indicated that Prebaking showed a 33% and 25% superior performance to startup the NoOp and Markdown functions, respectively. Further analysis also revealed that Prebaking's warmup mechanism reduced the Markdown first request processing time by 69%.", "venue": "ArXiv", "authors": ["Paulo  Silva", "Thiago Emmanuel Pereira"], "year": 2021, "n_citations": 0}
{"id": 6523610, "s2_id": "6fdfc9d97b48674d11ee7f424158ae3561c10fdf", "title": "On the Security of the Yi-Tan-Siew Chaos-Based Cipher", "abstract": "This paper analyzes the security of the Yi-Tan-Siew chaos-based cipher proposed in [1]. It is found that the clai med key (\u03b1, \u03b2, \u03b3, K) collapses to be(\u03b1, \u03b3) under a differential chosen-plaintext attack, and that an intermediate variabl e (called noise vector) used in the encryption function does not have a uniform distribution, which will downgrade the security of the cipher. Also, analysis shows that the security of this ciphe r is independent of the use of the chaotic tent map, so that the proposed cipher literally does not provide useful experien ce on the design of the chaos-based ciphers.", "venue": "ArXiv", "authors": ["Shujun  Li", "Guanrong  Chen", "Xuanqin  Mou"], "year": 2004, "n_citations": 7}
{"id": 6523676, "s2_id": "013b12cdf801517230b8770c282c5f54a9b6c6f4", "title": "A TTL-based Approach for Content Placement in Edge Networks", "abstract": "Edge networks are promising to provide better services to users by provisioning computing and storage resources at the edge of networks. However, due to the uncertainty and diversity of user interests, content popularity, distributed network structure, cache sizes, it is challenging to decide where to place the content, and how long it should be cached. In this paper, we study the utility optimization of content placement at edge networks through timer-based (TTL) policies. We propose provably optimal distributed algorithms that operate at each network cache to maximize the overall network utility. Our TTL-based optimization model provides theoretical answers to how long each content must be cached, and where it should be placed in the edge network. Extensive evaluations show that our algorithm significantly outperforms path replication with conventional caching algorithms over some network topologies.", "venue": "VALUETOOLS", "authors": ["Nitish K. Panigrahy", "Jian  Li", "Faheem  Zafari", "Don  Towsley", "Paul  Yu"], "year": 2021, "n_citations": 2}
{"id": 6528613, "s2_id": "3e1b395a0a0bd5b163ecd9591a97dc4995e9dea4", "title": "On the impact of Performance Antipatterns in multi-objective software model refactoring optimization", "abstract": "Software quality estimation is a challenging and time-consuming activity, and models are crucial to face the complexity of such activity on modern software applications. One main challenge is that the improvement of distinctive quality attributes may require contrasting refactoring actions on an application, as for trade-off between performance and reliability. In such cases, multi-objective optimization can provide the designer with a wider view on these trade-offs and, consequently, can lead to identify suitable actions that take into account independent or even competing objectives. In this paper, we present an approach that exploits the NSGA - II multi-objective evolutionary algorithm to search optimal Pareto solution frontiers for software refactoring while considering as objectives: i) performance variation, ii) reliability, iii) amount of performance antipatterns, and iv) architectural distance. The algorithm combines randomly generated refactoring actions into solutions (i.e., sequences of actions) and compares them according to the objectives. We have applied our approach on a train ticket booking service case study, and we have focused the analysis on the impact of performance antipatterns on the quality of solutions. Indeed, we observe that the approach finds better solutions when antipatterns enter the multi-objective optimization. In particular, performance antipatterns objective leads to solutions improving the performance by up to 15% with respect to the case where antipatterns are not considered, without affecting the solution quality on other objectives.", "venue": "2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)", "authors": ["Vittorio  Cortellessa", "Daniele Di Pompeo", "Vincenzo  Stoico", "Michele  Tucci"], "year": 2021, "n_citations": 0}
{"id": 6530299, "s2_id": "a3cf224f7ac2d78df299adf6a9b774fa3a02de44", "title": "Fault Tolerant Real Time Systems", "abstract": "Real time systems are systems in which there is a commitment for timely response by the computer to external stimuli. Real time applications have to function correctly even in presence of faults. Fault tolerance can be achieved by either hardware or software or time redundancy. Safety-critical applications have strict time and cost constraints, which means that not only faults have to be tolerated but also the constraints should be satisfied. Deadline scheduling means that the taskwith the earliest required response time is processed. The most common scheduling algorithms are :Rate Monotonic(RM) and Earliest deadline first(EDF).This paper deals with the interaction between the fault tolerant strategy and the EDF real time scheduling strategy.", "venue": "ArXiv", "authors": ["A. Christy Persya", "T. R. Gopalakrishnan Nair"], "year": 2010, "n_citations": 7}
{"id": 6533449, "s2_id": "7d5df4ef96c098139880752f67253bb506ad46ed", "title": "A Note on Uniform Power Connectivity in the SINR Model", "abstract": "In this paper we study the connectivity problem for wireless networks under the Signal to Interference plus Noise Ratio (SINR) model. Given a set of radio transmitters distributed in some area, we seek to build a directed strongly connected communication graph, and compute an edge coloring of this graph such that the transmitter-receiver pairs in each color class can communicate simultaneously. Depending on the interference model, more or less colors, corresponding to the number of frequencies or time slots, are necessary. We consider the SINR model that compares the received power of a signal at a receiver to the sum of the strength of other signals plus ambient noise . The strength of a signal is assumed to fade polynomially with the distance from the sender, depending on the so-called path-loss exponent ?. \n \nWe show that, when all transmitters use the same power, the number of colors needed is constant in one-dimensional grids if ?> 1 as well as in two-dimensional grids if ?> 2. For smaller path-loss exponents and two-dimensional grids we prove upper and lower bounds in the order of $\\mathcal{O}(\\log n)$ and ?(logn/loglogn) for ?= 2 and ?(n 2/?? 1) for ?< 2 respectively. If nodes are distributed uniformly at random on the interval [0,1], a regular coloring of $\\mathcal{O}(\\log n)$ colors guarantees connectivity, while ?(loglogn) colors are required for any coloring.", "venue": "ALGOSENSORS", "authors": ["Chen  Avin", "Zvi  Lotker", "Francesco  Pasquale", "Yvonne Anne Pignolet"], "year": 2009, "n_citations": 46}
{"id": 6537418, "s2_id": "ef66fa322ebc2a02a33cef05c1d40c51dcf013d1", "title": "Learnability and Robustness of Shallow Neural Networks Learned With a Performance-Driven BP and a Variant PSO For Edge Decision-Making", "abstract": "In many cases, the computing resources are limited without the benefit from GPU, especially in the edge devices of IoT enabled systems. It may not be easy to implement complex AI models in edge devices. The Universal Approximation Theorem states that a shallow neural network (SNN) can represent any nonlinear function. However, how fat is an SNN enough to solve a nonlinear decision-making problem in edge devices? In this paper, we focus on the learnability and robustness of SNNs, obtained by a greedy tight force heuristic algorithm (performance driven BP) and a loose force meta-heuristic algorithm (a variant of PSO). Two groups of experiments are conducted to examine the learnability and the robustness of SNNs with Sigmoid activation, learned/optimised by KPI-PDBPs and KPI-VPSOs, where, KPIs (key performance indicators: error (ERR), accuracy (ACC) and $F_1$ score) are the objectives, driving the searching process. An incremental approach is applied to examine the impact of hidden neuron numbers on the performance of SNNs, learned/optimised by KPI-PDBPs and KPI-VPSOs. From the engineering prospective, all sensors are well justified for a specific task. Hence, all sensor readings should be strongly correlated to the target. Therefore, the structure of an SNN should depend on the dimensions of a problem space. The experimental results show that the number of hidden neurons up to the dimension number of a problem space is enough; the learnability of SNNs, produced by KPI-PDBP, is better than that of SNNs, optimized by KPI-VPSO, regarding the performance and learning time on the training data sets; the robustness of SNNs learned by KPI-PDBPs and KPI-VPSOs depends on the data sets; and comparing with other classic machine learning models, ACC-PDBPs win for almost all tested data sets.", "venue": "Neural Comput. Appl.", "authors": ["Hongmei  He", "Mengyuan  Chen", "Gang  Xu", "Zhilong  Zhu", "Zhenhuan  Zhu"], "year": 2021, "n_citations": 1}
{"id": 6542789, "s2_id": "e49a88ee303b60d50d1e5949ae48e4d3e99da841", "title": "Efficiency Metrics for Data-Driven Models: A Text Summarization Case Study", "abstract": "Using data-driven models for solving text summarization or similar tasks has become very common in the last years. Yet most of the studies report basic accuracy scores only, and nothing is known about the ability of the proposed models to improve when trained on more data. In this paper, we define and propose three data efficiency metrics: data score efficiency, data time deficiency and overall data efficiency. We also propose a simple scheme that uses those metrics and apply it for a more comprehensive evaluation of popular methods on text summarization and title generation tasks. For the latter task, we process and release a huge collection of 35 million abstract-title pairs from scientific articles. Our results reveal that among the tested models, the Transformer is the most efficient on both tasks.", "venue": "INLG", "authors": ["Erion  \u00c7ano", "Ondrej  Bojar"], "year": 2019, "n_citations": 7}
{"id": 6548280, "s2_id": "591811ba3afe6250454a73b9bd4e76f7cbd2089a", "title": "On the model transform in stochastic network calculus", "abstract": "Stochastic network calculus requires special care in the search of proper stochastic traffic arrival models and stochastic service models. Tradeoff must be considered between the feasibility for the analysis of performance bounds, the usefulness of performance bounds, and the ease of their numerical calculation. In theory, transform between different traffic arrival models and transform between different service models are possible. Nevertheless, the impact of the model transform on performance bounds has not been thoroughly investigated. This paper is to investigate the effect of the model transform and to provide practical guidance in the model selection in stochastic network calculus.", "venue": "2010 IEEE 18th International Workshop on Quality of Service (IWQoS)", "authors": ["Kui  Wu", "Yuming  Jiang", "Jie  Li"], "year": 2010, "n_citations": 15}
{"id": 6548575, "s2_id": "35b4bc8fd8e729bd4504fe4b73e9a6170414bdce", "title": "Exploiting the past to reduce delay in CSMA scheduling: a high-order markov chain approach", "abstract": "Recently several CSMA algorithms based on the Glauber dynamics model have been proposed for multihop wireless scheduling, as viable solutions to achieve the throughput optimality, yet are simple to implement. However, their delay performances still remain unsatisfactory, mainly due to the nature of the underlying Markov chains that imposes a fundamental constraint on how the link state can evolve over time. In this paper, we propose a new approach toward better queueing and delay performance, based on our observation that the algorithm needs not be Markovian, as long as it can be implemented in a distributed manner, achieving the same throughput optimality and better delay performance. Our approach hinges upon utilizing past state information observed by local link and then constructing a high-order Markov chain for the evolution of the feasible link schedules. Our proposed algorithm, named delayed CSMA, adds virtually no additional overhead onto the existing CSMA-based algorithms, achieves the throughput optimality under the usual choice of link weight as a function of queue length, and also provides much better delay performance by effectively resolving temporal link starvation problem. From our extensive simulations we observe that the delay under our algorithm can be often reduced by a factor of 20 over a wide range of scenarios, compared to the standard Glauber-dynamics-based CSMA algorithm.", "venue": "SIGMETRICS '13", "authors": ["Jaewook  Kwak", "Chul-Ho  Lee", "Do Young Eun"], "year": 2013, "n_citations": 9}
{"id": 6558319, "s2_id": "13d693f8d512a4b7fa9e31bff4e6ea078aaced96", "title": "SPH-EXA: Enhancing the Scalability of SPH codes Via an Exascale-Ready SPH Mini-App", "abstract": "Numerical simulations of fluids in astrophysics and computational fluid dynamics (CFD) are among the most computationally-demanding calculations, in terms of sustained floating-point operations per second, or FLOP/s. It is expected that these numerical simulations will significantly benefit from the future Exascale computing infrastructures, that will perform 10^18 FLOP/s. The performance of the SPH codes is, in general, adversely impacted by several factors, such as multiple time-stepping, long-range interactions, and/or boundary conditions. In this work an extensive study of three SPH implementations SPHYNX, ChaNGa, and XXX is performed, to gain insights and to expose any limitations and characteristics of the codes. These codes are the starting point of an interdisciplinary co-design project, SPH-EXA, for the development of an Exascale-ready SPH mini-app. We implemented a rotating square patch as a joint test simulation for the three SPH codes and analyzed their performance on a modern HPC system, Piz Daint. The performance profiling and scalability analysis conducted on the three parent codes allowed to expose their performance issues, such as load imbalance, both in MPI and OpenMP. Two-level load balancing has been successfully applied to SPHYNX to overcome its load imbalance. The performance analysis shapes and drives the design of the SPH-EXA mini-app towards the use of efficient parallelization methods, fault-tolerance mechanisms, and load balancing approaches.", "venue": "ArXiv", "authors": ["Danilo  Guerrera", "Aur\u00e9lien  Cavelan", "Rub\u00e9n M. Cabez\u00f3n", "David  Imbert", "Jean-Guillaume  Piccinali", "Ali  Mohammed", "Lucio  Mayer", "Darren S. Reed", "Florina M. Ciorba"], "year": 2019, "n_citations": 0}
{"id": 6558807, "s2_id": "266895a21090d4754fa04e6d89e91a07421ab53f", "title": "On Polynomial Multiplication in Chebyshev Basis", "abstract": "In a recent paper, Lima, Panario, and Wang have provided a new method to multiply polynomials expressed in Chebyshev basis which reduces the total number of multiplication for small degree polynomials. Although their method uses Karatsuba's multiplication, a quadratic number of operations are still needed. In this paper, we extend their result by providing a complete reduction to polynomial multiplication in monomial basis, which therefore offers many subquadratic methods. Our reduction scheme does not rely on basis conversions and we demonstrate that it is efficient in practice. Finally, we show a linear time equivalence between the polynomial multiplication problem under monomial basis and under Chebyshev basis.", "venue": "IEEE Transactions on Computers", "authors": ["Pascal  Giorgi"], "year": 2012, "n_citations": 17}
{"id": 6561467, "s2_id": "91046e4d5e06d9b1184fd02f6010f6223dd15964", "title": "High availability using virtualization", "abstract": "............................................................................................. IV INDEX ........................................................................................................ V", "venue": "ArXiv", "authors": ["Federico  Calzolari"], "year": 2009, "n_citations": 14}
{"id": 6564875, "s2_id": "e6678ffbcbd186bc338a008259b9232c12b48b74", "title": "Online Evaluation for Effective Web Service Development", "abstract": "Development of the majority of the leading web services and software products today is generally guided by data-driven decisions based on evaluation that ensures a steady stream of updates, both in terms of quality and quantity. Large internet companies use online evaluation on a day-to-day basis and at a large scale. The number of smaller companies using A/B testing in their development cycle is also growing. Web development across the board strongly depends on quality of experimentation platforms. In this tutorial, we overview state-of-the-art methods underlying everyday evaluation pipelines at some of the leading Internet companies. Software engineers, designers, analysts, service or product managers --- beginners, advanced specialists, and researchers --- can learn how to make web service development data-driven and do it effectively.", "venue": "ArXiv", "authors": ["Roman  Budylin", "Alexey  Drutsa", "Gleb  Gusev", "Pavel  Serdyukov", "Igor  Yashkov"], "year": 2018, "n_citations": 5}
{"id": 6565324, "s2_id": "88be581edfe6d1f62f0bb71a425405adff448efb", "title": "Disks, Partitions, Volumes and RAID Performance with the Linux Operating System", "abstract": "Block devices in computer operating systems typically correspond to disks or disk partitions, and are used to store files in a filesystem. Disks are not the only real or virtual device which adhere to the block accessible stream of bytes block device model. Files, remote devices, or even RAM may be used as a virtual disks. This article examines several common combinations of block device layers used as virtual disks in the Linux operating system: disk partitions, loopback files, software RAID, Logical Volume Manager, and Network Block Devices. It measures their relative performance using different filesystems: Ext2, Ext3, ReiserFS, JFS, XFS,NFS.", "venue": "ArXiv", "authors": ["Michel  Dagenais"], "year": 2005, "n_citations": 3}
{"id": 6566079, "s2_id": "9e91821a82f863afddda6467b1bb0b1b8b4fb312", "title": "Stabilizing Maximal Independent Set in Unidirectional Networks is Hard", "abstract": "A distributed algorithm is self-stabilizing if after faults and attacks hit the system and place it in some arbitrary global state, the system recovers from this catastrophic situation without external intervention in finite time. In this paper, we consider the problem of constructing self-stabilizingly a \\emph{maximal independent set} in uniform unidirectional networks of arbitrary shape. On the negative side, we present evidence that in uniform networks, \\emph{deterministic} self-stabilization of this problem is \\emph{impossible}. Also, the \\emph{silence} property (\\emph{i.e.} having communication fixed from some point in every execution) is impossible to guarantee, either for deterministic or for probabilistic variants of protocols. On the positive side, we present a deterministic protocol for networks with arbitrary unidirectional networks with unique identifiers that exhibits polynomial space and time complexity in asynchronous scheduling. We complement the study with probabilistic protocols for the uniform case: the first probabilistic protocol requires infinite memory but copes with asynchronous scheduling, while the second probabilistic protocol has polynomial space complexity but can only handle synchronous scheduling. Both probabilistic solutions have expected polynomial time complexity.", "venue": "ArXiv", "authors": ["Toshimitsu  Masuzawa", "S\u00e9bastien  Tixeuil"], "year": 2009, "n_citations": 2}
{"id": 6567470, "s2_id": "33166a55e2a0784f1fda00a050fec4f7253ba851", "title": "A Novel Unified Expression for the Capacity and Bit Error Probability of Wireless Communication Systems over Generalized Fading Channels", "abstract": "Analysis of the average binary error probabilities (ABEP) and average capacity (AC) of wireless communications systems over generalized fading channels have been considered separately in past years. This paper introduces a novel moment generating function (MGF)-based unified expression for the ABEP and AC of single and multiple link communications with maximal ratio combining. In addition, this paper proposes the hyper-Fox's H fading model as a unified fading distribution of a majority of the well-known generalized fading environments. As such, the authors offer a generic unified performance expression that can be easily calculated, and that is applicable to a wide variety of fading scenarios. The mathematical formulism is illustrated with some selected numerical examples that validate the correctness of the authors' newly derived results.", "venue": "IEEE Transactions on Communications", "authors": ["Ferkan  Yilmaz", "Mohamed-Slim  Alouini"], "year": 2012, "n_citations": 64}
{"id": 6568581, "s2_id": "adc61079cce18b538572b5e43fe0235d146e5438", "title": "A Precise Program Phase Identification Method Based on Frequency Domain Analysis", "abstract": "In this paper, we present a systematic approach that transforms the program execution trace into frequency domain and precisely identifies program phases. The analyzed results can be embedded into program code to mark the starting point and execution characteristics, such as CPI (Cycles per Instruction), of each phase. The so generated information can be applied to runtime program phase prediction. With the precise program phase information, more intelligent software and system optimization techniques can be further explored and developed. Keywords\u2014program phase; frequency domain analysis; basic block;", "venue": "ArXiv", "authors": ["Hsuan-Yi  Lin", "Ren-Song  Tsay"], "year": 2021, "n_citations": 0}
{"id": 6573489, "s2_id": "2378d0042f2902623de9876bd46c68ae2dcb65a7", "title": "Stress-Testing Memcomputing on Hard Combinatorial Optimization Problems", "abstract": "Memcomputing is a novel computing paradigm that employs time non-local dynamical systems to compute with and in memory. The digital version of these machines [digital memcomputing machines or (DMMs)] is scalable, and is particularly suited to solve combinatorial optimization problems. One of its possible realizations is by means of standard electronic circuits, with and without memory. Since these elements are non-quantum, they can be described by ordinary differential equations. Therefore, the circuit representation of DMMs can also be simulated efficiently on our traditional computers. We have indeed previously shown that these simulations only require time and memory resources that scale linearly with the problem size when applied to finding a good approximation to the optimum of hard instances of the maximum-satisfiability problem. The state-of-the-art algorithms, instead, require exponential resources for the same instances. However, in that work, we did not push the simulations to the limit of the processor used. Since linear scalability at smaller problem sizes cannot guarantee linear scalability at much larger sizes, we have extended these results in a stress-test up to $64\\times 10^{6}$ variables (corresponding to about 1 billion literals), namely the largest case that we could fit on a single core of an Intel Xeon E5-2860 with 128 GB of dynamic random-access memory (DRAM). For this test, we have employed a commercial simulator, Falcon of MemComputing, Inc. We find that the simulations of DMMs still scale linearly in both time and memory up to these very large problem sizes versus the exponential requirements of the state-of-the-art solvers. These results further reinforce the advantages of the physics-based memcomputing approach compared with traditional ones.", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "authors": ["Forrest  Sheldon", "Pietro  Cicotti", "Fabio L. Traversa", "Massimiliano  Di Ventra"], "year": 2020, "n_citations": 8}
{"id": 6573660, "s2_id": "bf6a0c6756a6c22265a7ee489f3f1d379f374a20", "title": "Distributed Resource Allocation Optimization for User-Centric Cell-Free MIMO Networks", "abstract": "We develop two distributed downlink resource allocation algorithms for user-centric, cell-free, spatially-distributed, multiple-input multiple-output (MIMO) networks. In such networks, each user is served by a subset of nearby transmitters that we call distributed units or DUs. The operation of the DUs in a region is controlled by a central unit (CU). Our first scheme is implemented at the DUs, while the second is implemented at the CUs controlling these DUs. We define a hybrid quality of service metric that enables distributed optimization of system resources in a proportional fair manner. Specifically, each of our algorithms performs user scheduling, beamforming, and power control while accounting for channel estimation errors. Importantly, our algorithm does not require information exchange amongst DUs (CUs) for the DU-distributed (CU-distributed) system, while also smoothly converging. Our results show that our CU-distributed system provides 1.3to 1.8-fold network throughput compared to the DU-distributed system, with minor increases in complexity and front-haul load and substantial gains over benchmark schemes like local zero-forcing. We also analyze the trade-offs provided by the CU-distributed system, hence highlighting the significance of deploying multiple CUs in user-centric cell-free networks.", "venue": "IEEE Transactions on Wireless Communications", "authors": ["Hussein A. Ammar", "Raviraj  Adve", "Shahram  Shahbazpanahi", "Gary  Boudreau", "Kothapalli Venkata Srinivas"], "year": 2021, "n_citations": 2}
{"id": 6574199, "s2_id": "270fb6b316a4d0ea2a6fd7060483a52ba497c9a5", "title": "Exploiting Parallelism on Shared Memory in the QED Particle-in-Cell Code PICADOR with Greedy Load Balancing", "abstract": "State-of-the-art numerical simulations of laser plasma by means of the Particle-in-Cell method are often extremely computationally intensive. Therefore there is a growing need for development of approaches for efficient utilization of resources of modern supercomputers. In this paper, we address the problem of a substantially non-uniform and dynamically varying distribution of macroparticles in a computational area in simulating quantum electrodynamic (QED) cascades. We propose and evaluate a load balancing scheme for shared memory systems, which allows subdividing individual cells of the computational domain into work portions with subsequent dynamic distribution of these portions between OpenMP threads. Computational experiments on 1D, 2D, and 3D QED simulations show that the proposed scheme outperforms the previously developed standard and custom schemes in the PICADOR code by 2.1 to 10 times when employing several Intel Cascade Lake CPUs.", "venue": "PPAM", "authors": ["Iosif  Meyerov", "Sergei  Bastrakov", "Aleksei  Bashinov", "Evgeny  Efimenko", "Alexander  Panov", "Elena  Panova", "Igor  Surmin", "Valentin  Volokitin", "Arkady  Gonoskov"], "year": 2019, "n_citations": 1}
{"id": 6575482, "s2_id": "6e16cda2096dcfb3d191a1ab48f80d7825298e9a", "title": "CBR: Controlled Burst Recording", "abstract": "Collecting traces from software running in the field is both useful and challenging. Traces may indeed help revealing unexpected usage scenarios, detecting and reproducing failures, and building behavioral models that reflect how the software is actually used. On the other hand, recording traces is an intrusive activity that may annoy users, negatively affecting the usability of the applications, if not properly designed.In this paper we address field monitoring by introducing Controlled Burst Recording, a monitoring solution that can collect comprehensive runtime data without compromising the quality of the user experience. The technique encodes the knowledge extracted from the monitored application as a finite state model that both represents the sequences of operations that can be executed by the users and the corresponding internal computations that might be activated by each operation.Our initial assessment with information extracted from ArgoUML shows that Controlled Burst Recording can reconstruct behavioral information more effectively than competing sampling techniques, with a low impact on the system response time.", "venue": "2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)", "authors": ["Oscar  Cornejo", "Daniela  Briola", "Daniela  Micucci", "Leonardo  Mariani"], "year": 2020, "n_citations": 0}
{"id": 6579467, "s2_id": "41ab23ecdaadcecc0112b35b788e5d480e9f5c35", "title": "Approximate analysis of biological systems by hybrid switching jump diffusion", "abstract": "In this paper we consider large state space continuous time Markov chains (MCs) arising in the field of systems biology. For density dependent families of MCs that represent the interaction of large groups of identical objects, Kurtz has proposed two kinds of approximations. One is based on ordinary differential equations, while the other uses a diffusion process. The computational cost of the deterministic approximation is significantly lower, but the diffusion approximation retains stochasticity and is able to reproduce relevant random features like variance, bimodality, and tail behavior. In a recent paper, for particular stochastic Petri net models, we proposed a jump diffusion approximation that aims at being applicable beyond the limits of Kurtz's diffusion approximation, namely when the process reaches the boundary with non-negligible probability. Other limitations of the diffusion approximation in its original form are that it can provide inaccurate results when the number of objects in some groups is often or constantly low and that it can be applied only to pure density dependent Markov chains. In order to overcome these drawbacks, in this paper we propose to apply the jump-diffusion approximation only to those components of the model that are in density dependent form and are associated with high population levels. The remaining components are treated as discrete quantities. The resulting process is a hybrid switching jump diffusion. We show that the stochastic differential equations that characterize this process can be derived automatically both from the description of the original Markov chains or starting from a higher level description language, like stochastic Petri nets. The proposed approach is illustrated on three models: one modeling the so called crazy clock reaction, one describing viral infection kinetics and the last considering transcription regulation.", "venue": "Theor. Comput. Sci.", "authors": ["Alessio  Angius", "Gianfranco  Balbo", "Marco  Beccuti", "Enrico  Bibbona", "Andr\u00e1s  Horv\u00e1th", "Roberta  Sirovich"], "year": 2015, "n_citations": 19}
{"id": 6579831, "s2_id": "8149306af1d327db727c38a217e7089c0b276d9f", "title": "Towards Informative Statistical Flow Inversion", "abstract": "This is the accepted version of 'Towards Informative Statistical Flow Inversion', archived originally at arXiv:0705.1939v1 [cs.NI] 14 May 2007.", "venue": "ArXiv", "authors": ["Richard G. Clegg", "Hamed  Haddadi", "Raul  Landa", "Miguel  Rio"], "year": 2007, "n_citations": 1}
{"id": 6580864, "s2_id": "f1043b902460fb09a3cd303303e33aefecd35503", "title": "Groups of Repairmen and Repair-based Load Balancing in Supermarket Models with Repairable Servers", "abstract": "Supermarket models are a class of interesting parallel queueing networks with dynamic randomized load balancing and real-time resource management. When the parallel servers are subject to breakdowns and repairs, analysis of such a supermarket model becomes more difficult and challenging. In this paper, we apply the mean-field theory to studying four interrelated supermarket models with repairable servers, and numerically indicate impact of the different repairman groups on performance of the systems. First, we set up the systems of mean-field equations for the supermarket models with repairable servers. Then we prove the asymptotic independence of the supermarket models through the operator semi-group and the mean-field limit. Furthermore, we show that the fixed points of the supermarket models satisfy the systems of nonlinear equations. Finally, we use the fixed points to give numerical computation for performer analysis, and provide valuable observations on model improvement. Therefore, this paper provides a new and effective method in the study of complex supermarket models.", "venue": "ArXiv", "authors": ["Na  Li", "Quan-Lin  Li", "Zhe George Zhang"], "year": 2017, "n_citations": 0}
{"id": 6585935, "s2_id": "e25c0140824641bcb62acb783d1d14e1be060277", "title": "MKPipe: a compiler framework for optimizing multi-kernel workloads in OpenCL for FPGA", "abstract": "OpenCL for FPGA enables developers to design FPGAs using a programming model similar for processors. Recent works have shown that code optimization at the OpenCL level is important to achieve high computational efficiency. However, existing works either focus primarily on optimizing single kernels or solely depend on channels to design multi-kernel pipelines. In this paper, we propose a source-to-source compiler framework, MKPipe, for optimizing multi-kernel workloads in OpenCL for FPGA. Besides channels, we propose new schemes to enable multi-kernel pipelines. Our optimizing compiler employs a systematic approach to explore the tradeoffs of these optimizations methods. To enable more efficient overlapping between kernel execution, we also propose a novel workitem/workgroup-id remapping technique. Furthermore, we propose new algorithms for throughput balancing and resource balancing to tune the optimizations upon individual kernels in the multi-kernel workloads. Our results show that our compiler-optimized multi-kernels achieve up to 3.6x (1.4x on average) speedup over the baseline, in which the kernels have already been optimized individually.", "venue": "ICS", "authors": ["Ji  Liu", "Abdullah-Al  Kafi", "Xipeng  Shen", "Huiyang  Zhou"], "year": 2020, "n_citations": 1}
{"id": 6586151, "s2_id": "c93f4cda0730935d6c74a83b964d5bda823ac8b4", "title": "Simple and effective dynamic provisioning for power-proportional data centers", "abstract": "Energy consumption represents a significant cost in data center operation. A large fraction of the energy, however, is used to power idle servers when the workload is low. Dynamic provisioning techniques aim at saving this portion of the energy, by turning off unnecessary servers. In this paper, we explore how much gain knowing future workload information can bring to dynamic provisioning. In particular, we develop online dynamic provisioning solutions with and without future workload information available. We first reveal an elegant structure of the off-line dynamic provisioning problem, which allows us to characterize the optimal solution in a \u201cdivide-and-conquer\u201d manner. We then exploit this insight to design two online algorithms with competitive ratios 2 \u2212 \u03b1 and e/ (e \u2212 1 + \u03b1), respectively, where 0 \u2264 \u03b1 \u2264 1 is the normalized size of a look-ahead window in which future workload information is available. A fundamental observation is that future workload information beyond the full-size look-ahead window (corresponding to \u03b1= 1) will not improve dynamic provisioning performance. Our algorithms are decentralized and easy to implement. We demonstrate their effectiveness in simulations using real-world traces.", "venue": "CISS", "authors": ["Tan  Lu", "Minghua  Chen"], "year": 2012, "n_citations": 13}
{"id": 6587535, "s2_id": "d802409f75f1d3a9c98b9fcfa788a96e3b3a9357", "title": "When queueing meets coding: Optimal-latency data retrieving scheme in storage clouds", "abstract": "Storage clouds, such as Amazon S3, are being widely used for web services and Internet applications. It has been observed that the delay for retrieving data from and placing data into the clouds is quite random, and exhibits weak correlations between different read/write requests. This inspires us to investigate a key problem: can we reduce the delay by transmitting data replications in parallel or using powerful erasure codes? In this paper, we study the problem of reducing the delay of downloading data from cloud storage systems by leveraging multiple parallel threads, assuming that the data has been encoded and stored in the clouds using fixed rate forward error correction (FEC) codes with parameters (n, k). That is., each file is divided into k equal-sized chunks, which are then expanded into n chunks such that any k chunks out of the n are sufficient to successfully restore the original file. The model can be depicted as a multiple-server queue with arrivals of data retrieving requests and a server corresponding to a thread. However, this is not a typical queueing model because a server can terminate its operation, depending on when other servers complete their service (due to the redundancy that is spread across the threads). Hence, to the best of our knowledge, the analysis of this queueing model remains quite uncharted. Real traces from Amazon S3 show that the time to retrieve a fixed size chunk is random and can be accurately approximated as an i.i.d. exponentially distributed random variable. We show that any work-conserving scheme is delay-optimal when k = 1. When k > 1, we find that a simple greedy scheme, which allocates all available threads to the head of line request, is delay optimal, which appears surprising.", "venue": "IEEE INFOCOM 2014 - IEEE Conference on Computer Communications", "authors": ["Shengbo  Chen", "Yin  Sun", "Ulas C. Kozat", "Longbo  Huang", "Prasun  Sinha", "Guanfeng  Liang", "Xin  Liu", "Ness B. Shroff"], "year": 2014, "n_citations": 82}
{"id": 6590065, "s2_id": "8b52a1fff81cd20ea4801779fd029cf8ef7946f0", "title": "Scalable Load Balancing in Networked Systems: Universality Properties and Stochastic Coupling Methods", "abstract": "We present an overview of scalable load balancing algorithms which provide favorable delay performance in large-scale systems, and yet only require minimal implementation overhead. Aimed at a broad audience, the paper starts with an introduction to the basic load balancing scenario, consisting of a single dispatcher where tasks arrive that must immediately be forwarded to one of $N$ single-server queues. \nA popular class of load balancing algorithms are so-called power-of-$d$ or JSQ($d$) policies, where an incoming task is assigned to a server with the shortest queue among $d$ servers selected uniformly at random. This class includes the Join-the-Shortest-Queue (JSQ) policy as a special case ($d = N$), which has strong stochastic optimality properties and yields a mean waiting time that vanishes as $N$ grows large for any fixed subcritical load. However, a nominal implementation of the JSQ policy involves a prohibitive communication burden in large-scale deployments. In contrast, a random assignment policy ($d = 1$) does not entail any communication overhead, but the mean waiting time remains constant as $N$ grows large for any fixed positive load. \nIn order to examine the fundamental trade-off between performance and implementation overhead, we consider an asymptotic regime where $d(N)$ depends on $N$. We investigate what growth rate of $d(N)$ is required to match the performance of the JSQ policy on fluid and diffusion scale. The results demonstrate that the asymptotics for the JSQ($d(N)$) policy are insensitive to the exact growth rate of $d(N)$, as long as the latter is sufficiently fast, implying that the optimality of the JSQ policy can asymptotically be preserved while dramatically reducing the communication overhead. We additionally show how the communication overhead can be reduced yet further by the so-called Join-the-Idle-Queue scheme, leveraging memory at the dispatcher.", "venue": "Proceedings of the International Congress of Mathematicians (ICM 2018)", "authors": ["Mark van der Boor", "Sem C. Borst", "Johan van Leeuwaarden", "Debankur  Mukherjee"], "year": 2019, "n_citations": 25}
{"id": 6590785, "s2_id": "205a2ba8f6c835a7628ae8847ad48a5ea2c0a0a4", "title": "Performance analysis for energy harvesting communication protocols with fixed rate transmission", "abstract": "Energy Harvesting (EH) has emerged as a promising technique for Green Communications and it is a novel technique to prolong the lifetime of the wireless networks with replenishable nodes. In this paper, we consider the energy shortage analysis of fixed rate transmission in communication systems with energy harvesting nodes. First, we study the finite-horizon transmission and provide the general formula for the energy shortage probability. We also give some examples as benchmarks. Then, we continue to derive a closed-form expression for infinite-horizon transmission, which is a lower bound for the energy shortage probability of any finite-horizon transmission. These results are proposed for both Additive White Gaussian Noise (AWGN) and fading channels. Moreover, we show that even under \\emph{random energy arrival}, one can transmit at a fixed rate equal to capacity in the AWGN channels with negligible aggregate shortage time. We achieve this result using our practical transmission schemes, proposed for finite-horizon. Also, comprehensive numerical simulations are performed in AWGN and fading channels with no Channel State Information (CSI) available at the transmitter, which corroborate our theoretical findings. Furthermore, we improve the performance of our transmission schemes in the fading channel with no CSI at the transmitter by optimizing the transmission initiation threshold.", "venue": "IET Commun.", "authors": ["Mahmood Mohassel Feghhi", "Aliazam  Abbasfar", "Mahtab  Mirmohseni"], "year": 2014, "n_citations": 4}
{"id": 6591103, "s2_id": "25293c65df40327e8e3f3bdc9b8f0c50ab87f732", "title": "Hoard: A Distributed Data Caching System to Accelerate Deep Learning Training on the Cloud", "abstract": "Deep Learning system architects strive to design a balanced system where the computational accelerator -- FPGA, GPU, etc, is not starved for data. Feeding training data fast enough to effectively keep the accelerator utilization high is difficult when utilizing dedicated hardware like GPUs. As accelerators are getting faster, the storage media \\& data buses feeding the data have not kept pace and the ever increasing size of training data further compounds the problem. We describe the design and implementation of a distributed caching system called Hoard that stripes the data across fast local disks of multiple GPU nodes using a distributed file system that efficiently feeds the data to ensure minimal degradation in GPU utilization due to I/O starvation. Hoard can cache the data from a central storage system before the start of the job or during the initial execution of the job and feeds the cached data for subsequent epochs of the same job and for different invocations of the jobs that share the same data requirements, e.g. hyper-parameter tuning. Hoard exposes a POSIX file system interface so the existing deep learning frameworks can take advantage of the cache without any modifications. We show that Hoard, using two NVMe disks per node and a distributed file system for caching, achieves a 2.1x speed-up over a 10Gb/s NFS central storage system on a 16 GPU (4 nodes, 4 GPUs per node) cluster for a challenging AlexNet ImageNet image classification benchmark with 150GB of input dataset. As a result of the caching, Hoard eliminates the I/O bottlenecks introduced by the shared storage and increases the utilization of the system by 2x compared to using the shared storage without the cache.", "venue": "ArXiv", "authors": ["Christian  Pinto", "Yiannis  Gkoufas", "Andrea  Reale", "Seetharami  Seelam", "Steven  Eliuk"], "year": 2018, "n_citations": 7}
{"id": 6593124, "s2_id": "c4e17e676f705b29ee343c9851f8033c9220de0c", "title": "Resource Allocation in One-Dimensional Distributed Service Networks", "abstract": "We consider assignment policies that allocate resources to users, where both resources and users are located on a one-dimensional line (0, \u221e). First, we consider unidirectional assignment policies that allocate resources only to users located to their left. We propose the Move to Right (MTR) policy, which scans from left to right assigning the nearest available resource located to the right of a user, and contrast it to the Unidirectional Gale-Shapley (UGS) matching policy. While both policies among all unidirectional policies, minimize the expected distance traveled by a request, MTR is fairer. Moreover, we show that when user and resource locations are modeled by statistical point processes, and resources are allowed to satisfy more than one user, the spatial system under unidirectional policies can be mapped into bulk service queueing systems, thus allowing the application of many queueing theory results that yield closed form expressions. As we consider a case where different resources can satisfy different numbers of users, we also generate new results for bulk service queues. We also consider bidirectional policies where there are no directional restrictions on resource allocation and develop an algorithm for computing the optimal assignment which is more efficient than known algorithms in the literature when there are more resources than users. Finally, numerical evaluation of performance of unidirectional and bidirectional allocation schemes yields design guidelines beneficial for resource placement.", "venue": "2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)", "authors": ["Nitish K. Panigrahy", "Prithwish  Basu", "Philippe  Nain", "Don  Towsley", "Ananthram  Swami", "Kevin S. Chan", "Kin K. Leung"], "year": 2019, "n_citations": 0}
{"id": 6594865, "s2_id": "8476baa01290d2b76cee4d2b325739218a2fcced", "title": "A Gain Function for Architectural Decision-Making in Scientific Computing", "abstract": "Scientific Computing typically requires large computational needs which have been addressed with High Performance Distributed Computing. It is essential to efficiently deploy a number of complex scientific applications, which have different characteristics, and so require distinct computational resources too. However, in many research laboratories, this high performance architecture is not dedicated. So, the architecture must be shared to execute a set of scientific applications, with so many different execution times and relative importance to research. Also, the high performance architectures have different characteristics and costs. When a new infrastructure has to be acquired to meet the needs of this scenario, the decision-making is hard and complex. In this work, we present a Gain Function as a model of an utility function, with which it is possible a decision-making with confidence. With the function is possible to evaluate the best architectural option taking into account aspects of applications and architectures, including the executions time, cost of architecture, the relative importance of each application and also the relative importance of performance and cost on the final evaluation. This paper presents the Gain Function, examples, and a real case showing their applicabilities.", "venue": "ArXiv", "authors": ["Mariza  Ferro", "Antonio Roberto Mury", "Bruno  Schulze"], "year": 2016, "n_citations": 0}
{"id": 6600161, "s2_id": "30c53af217278515129531941a769c6251557739", "title": "Address Translation Design Tradeoffs for Heterogeneous Systems", "abstract": "This paper presents a broad, pathfinding design space exploration of memory management units (MMUs) for heterogeneous systems. We consider a variety of designs, ranging from accelerators tightly coupled with CPUs (and using their MMUs) to fully independent accelerators that have their own MMUs. We find that regardless of the CPU-accelerator communication, accelerators should not rely on the CPU MMU for any aspect of address translation, and instead must have its own, local, fully-fledged MMU. That MMU, however, can and should be as application-specific as the accelerator itself, as our data indicates that even a 100% hit rate in a small, standard L1 Translation Lookaside Buffer (TLB) presents a substantial accelerator performance overhead. Furthermore, we isolate the benefits of individual MMU components (e.g., TLBs versus page table walkers) and discover that their relative performance, area, and energy are workload dependent, with their interplay resulting in different area-optimal and energy-optimal configurations.", "venue": "ArXiv", "authors": ["Yunsung  Kim", "Guilherme  Cox", "Martha A. Kim", "Abhishek  Bhattacharjee"], "year": 2017, "n_citations": 0}
{"id": 6601141, "s2_id": "d18ab489ed5d3296c31b6c1bbbd70a8861c0b175", "title": "Fast and Simple Relational Processing of Uncertain Data", "abstract": "This paper introduces U-relations, a succinct and purely relational representation system for uncertain databases. U-relations support attribute-level uncertainty using vertical partitioning. If we consider positive relational algebra extended by an operation for computing possible answers, a query on the logical level can be translated into, and evaluated as, a single relational algebra query on the U-relational representation. The translation scheme essentially preserves the size of the query in terms of number of operations and, in particular, number of joins. Standard techniques employed in off-the-shelf relational database management systems are effective for optimizing and processing queries on U-relations. In our experiments we show that query evaluation on U-relations scales to large amounts of data with high degrees of uncertainty.", "venue": "2008 IEEE 24th International Conference on Data Engineering", "authors": ["Lyublena  Antova", "Thomas  Jansen", "Christoph  Koch", "Dan  Olteanu"], "year": 2008, "n_citations": 290}
{"id": 6602488, "s2_id": "148c9a2ac4102d39fc68d593f231ef337e281ffe", "title": "Age of information distribution under dynamic service preemption", "abstract": "Age of Information (AoI) has emerged as an important quality-of-service measure for applications that prioritize delivery of the freshest information, e.g., virtual or augmented reality over mobile devices and wireless sensor networks used in the control of cyber-physical systems. We derive the Laplace transform of the stationary AoI for the M/GI/1/2 system with a \u201cdynamic\u201d service preemption and pushout policy depending on the existing service time of the in-service message. Thus, our system generalizes both the static M/GI/1/2 queue-pushout system without service preemption and the M/GI/1/1 bufferless system with service preemption two systems considered to provide very good AoI performance. Based on our analysis, for a service-time distribution that is a mixture of deterministic and exponential, we numerically show that the dynamic policy has lower mean AoI than that of these two static policies and also that of the well studied M/GI/1/1 blocking system.", "venue": "ArXiv", "authors": ["George  Kesidis", "Takis  Konstantopoulos", "Michael A. Zazanis"], "year": 2021, "n_citations": 3}
{"id": 6604040, "s2_id": "bae5d46966b63e3b3c9591f1cf5dab6a2ec3f2bd", "title": "Radio Irregularity Model in OMNeT++", "abstract": "Radio irregularity is a non-negligible phenomenon that has an impact on protocol performances. For instance, irregularity in radio range leads to asymmetric links that cause the loss of packets in different directions. In order to investigate its effect, the Radio Irregularity Model (RIM) is proposed that takes into account the irregularity of a radio range and estimates path losses in an anisotropic environment. The purpose of this paper is to provide details of the RIM model developed in the INET Framework of the OMNeT++ simulator that can be used to investigate the impact of radio irregularity on protocol performance.", "venue": "ArXiv", "authors": ["Behruz  Khalilov", "Anna  F\u00f6rster", "Asanga  Udugama"], "year": 2017, "n_citations": 3}
{"id": 6609734, "s2_id": "c6a3b51f45e7da08a51570b4880607cef142e35f", "title": "GSGP-CUDA - a CUDA framework for Geometric Semantic Genetic Programming", "abstract": "Geometric Semantic Genetic Programming (GSGP) is a state-of-the-art machine learning method based on evolutionary computation. GSGP performs search operations directly at the level of program semantics, which can be done more efficiently then operating at the syntax level like most GP systems. Efficient implementations of GSGP in C++ exploit this fact, but not to its full potential. This paper presents GSGP-CUDA, the first CUDA implementation of GSGP and the most efficient, which exploits the intrinsic parallelism of GSGP using GPUs. Results show speedups greater than 1,000\u00d7 relative to the state-of-the-art sequential implementation.", "venue": "ArXiv", "authors": ["Leonardo  Trujillo", "Jose Manuel Munoz Contreras", "Daniel E Hernandez", "Mauro  Castelli", "Juan J Tapia"], "year": 2021, "n_citations": 0}
{"id": 6618825, "s2_id": "64ecf37a4fb1161f38fd54734a83e5926589fbbc", "title": "Characterizing Workload of Web Applications on Virtualized Servers", "abstract": "With the ever increasing demands of cloud computing services, planning and management of cloud resources has become a more and more important issue which directed affects the resource utilization and SLA and customer satisfaction. But before any management strategy is made, a good understanding of applications\u2019 workload in virtualized environment is the basic fact and principle to the resource management methods. Unfortunately, little work has been focused on this area. Lack of raw data could be one reason; another reason is that people still use the traditional models or methods shared under non-virtualized environment. The study of applications\u2019 workload in virtualized environment should take on some of its peculiar features comparing to the non-virtualized environment. In this paper, we are open to analyze the workload demands that reflect applications\u2019 behavior and the impact of virtualization. The results are obtained from an experimental cloud testbed running web applications, specifically the RUBiS benchmark application. We profile the workload dynamics on both virtualized and non-virtualized environments and compare the findings. The experimental results are valuable for us to estimate the performance of applications on computer architectures, to predict SLA compliance or violation based on the projected application workload and to guide the decision making to support applications with the right hardware.", "venue": "BPOE@ASPLOS/VLDB", "authors": ["Xiajun  Wang", "Song  Huang", "Song  Fu", "Krishna  Kavi"], "year": 2014, "n_citations": 2}
{"id": 6624676, "s2_id": "9e8880a4d35ae84c7a35ef5714f44f92631ceab1", "title": "Parallel 3DPIFCM Algorithm for Noisy Brain MRI Images", "abstract": "In this paper we implemented the algorithm we developed in [1] called 3DPIFCM in a parallel environment by using CUDA on a GPU. In our previous work we introduced 3DPIFCM which performs segmentation of images in noisy conditions and uses particle swarm optimization for finding the optimal algorithm parameters to account for noise. This algorithm achieved state of the art segmentation accuracy when compared to FCM (Fuzzy C-Means), IFCMPSO (Improved Fuzzy C-Means with Particle Swarm Optimization), GAIFCM (Genetic Algorithm Improved Fuzzy C-Means) on noisy MRI images of an adult Brain. \nWhen using a genetic algorithm or PSO (Particle Swarm Optimization) on a single machine for optimization we witnessed long execution times for practical clinical usage. Therefore, in the current paper our goal was to speed up the execution of 3DPIFCM by taking out parts of the algorithm and executing them as kernels on a GPU. The algorithm was implemented using the CUDA [13] framework from NVIDIA and experiments where performed on a server containing 64GB RAM , 8 cores and a TITAN X GPU with 3072 SP cores and 12GB of GPU memory. \nOur results show that the parallel version of the algorithm performs up to 27x faster than the original sequential version and 68x faster than GAIFCM algorithm. We show that the speedup of the parallel version increases as we increase the size of the image due to better utilization of cores in the GPU. Also, we show a speedup of up to 5x in our Brainweb experiment compared to other generic variants such as IFCMPSO and GAIFCM.", "venue": "ArXiv", "authors": ["Arie  Agranonik", "Maya  Herman", "Mark  Last"], "year": 2020, "n_citations": 0}
{"id": 6624906, "s2_id": "0b258247f3e85a176f1ed521a4adcbc555c2d380", "title": "On Conditional Branches in Optimal Decision Trees", "abstract": "The decision tree is one of the most fundamental programming abstractions. A commonly used type of decision tree is the alphabetic binary tree, which uses (without loss of generality) \"less than\" versus \"greater than or equal to\" tests in order to determine one of n outcome events. The process of finding an optimal alphabetic binary tree for a known probability distribution on outcome events usually has the underlying assumption that the cost (time) per decision is uniform and thus independent of the outcome of the decision. This assumption, however, is incorrect in the case of software to be optimized for a given microprocessor, e.g., in compiling switch statements or in fine-tuning program bottlenecks. The operation of the microprocessor generally means that the cost for the more likely decision outcome can or will be less - often far less -than the less likely decision outcome. Here we formulate a variety of O(n3)-time O(n2)-space dynamic programming algorithms to solve such optimal binary decision tree problems, optimizing for the behavior of processors with predictive branch capabilities, both static and dynamic. In the static case, we use existing results to arrive at entropy-based performance bounds. Solutions to this formulation are often faster in practice than \";optimal\"; decision trees as formulated in the literature, and, for small problems, are easily worth the extra complexity in finding the better solution. This can be applied in fast implementation of decoding Huffman codes.", "venue": "2007 IEEE International Symposium on Information Theory", "authors": ["Michael B. Baer"], "year": 2007, "n_citations": 2}
{"id": 6625239, "s2_id": "3e456f06dc8f0eb1aa8a2ece23ee8becaa3986ba", "title": "SCALABLE INTERNETWORKING: Final Technical Report", "abstract": "This document describes the work completed at the University of California, Santa Cruz under the project Scalable Internetworking sponsored by ARPA under Contract No. F19628-93-C-0175. This report covers work performed from 1 April 1993 to 31 December 1995. Results on routing and multicasting for large-scale internets are summarized. The technical material discussed assumes familiarity with the content of our proposal and previous quarterly reports submitted in this project.", "venue": "ArXiv", "authors": ["J. J. Garcia-Luna-Aceves", "A.  Varma"], "year": 2019, "n_citations": 0}
{"id": 6625822, "s2_id": "46e7dbcc5db8f46db4c670c342b6dd8d91ee0bee", "title": "HMTT: A Hybrid Hardware/Software Tracing System for Bridging Memory Trace's Semantic Gap", "abstract": "Memory trace analysis is an important technology for architecture research, system software (i.e., OS, compiler) optimization, and application performance improvements. Hardware-snooping is an effective and efficient approach to monitor and collect memory traces. Compared with software-based approaches, memory traces collected by hardware-based approaches are usually lack of semantic information, such as process/function/loop identifiers, virtual address and I/O access. In this paper we propose a hybrid hardware/software mechanism which is able to collect memory reference trace as well as semantic information. Based on this mechanism, we designed and implemented a prototype system called HMTT (Hybrid Memory Trace Tool) which adopts a DIMMsnooping mechanism to snoop on memory bus and a software-controlled tracing mechanism to inject semantic information into normal memory trace. To the best of our knowledge, the HMTT system is the first hardware tracing system capable of correlating memory trace with high-level events. Comprehensive validations and evaluations show that the HMTT system has both hardware's (e.g., no distortion or pollution) and software's advantages (e.g., flexibility and more information).", "venue": "ArXiv", "authors": ["Yungang  Bao", "Jinyong  Zhang", "Yan  Zhu", "Dan  Tang", "Yuan  Ruan", "Mingyu  Chen", "Jianping  Fan"], "year": 2011, "n_citations": 3}
{"id": 6626478, "s2_id": "aa46950f59ac101cb955dec1afbe473c0b24f250", "title": "Right buffer sizing matters: Some dynamical and statistical studies on Compound TCP", "abstract": "Motivated by recent concerns that queuing delays in the Internet are on the rise, we conduct a performance evaluation of Compound TCP (C-TCP) in two topologies: a single bottleneck and a multi-bottleneck topology, under different traffic scenarios. The first topology consists of a single bottleneck router, and the second consists of two distinct sets of TCP flows, regulated by two edge routers, feeding into a common core router. We focus on some dynamical and statistical properties of the underlying system. From a dynamical perspective, we develop fluid models in a regime wherein the number of flows is large, bandwidth-delay product is high, buffers are dimensioned small (independent of the bandwidth-delay product) and routers deploy a Drop-Tail queue policy. A detailed local stability analysis for these models yields the following key insight: smaller buffers favour stability. Additionally, we highlight that larger buffers, in addition to increasing latency, are prone to inducing limit cycles in the system dynamics, via a Hopf bifurcation. These limit cycles in turn cause synchronisation among the TCP flows, and also result in a loss of link utilisation. For the topologies considered, we also empirically analyse some statistical properties of the bottleneck queues. These statistical analyses serve to validate an important modelling assumption: that in the regime considered, each bottleneck queue may be approximated as either an $M/M/1/B$ or an $M/D/1/B$ queue. This immediately makes the modelling perspective attractive and the analysis tractable. Finally, we show that smaller buffers, in addition to ensuring stability and low latency, would also yield fairly good system performance, in terms of throughput and flow completion times.", "venue": "Perform. Evaluation", "authors": ["Debayani  Ghosh", "Krishna  Jagannathan", "Gaurav  Raina"], "year": 2020, "n_citations": 2}
{"id": 6627127, "s2_id": "ec637df46dfe787f32b4d2fa89969bcf6c7c59fb", "title": "Fault Tolerant Adaptive Parallel and Distributed Simulation through Functional Replication", "abstract": "Abstract This paper presents FT-GAIA, a software-based fault-tolerant parallel and distributed simulation middleware. FT-GAIA has being designed to reliably handle Parallel And Distributed Simulation (PADS) models, which are needed to properly simulate and analyze complex systems arising in any kind of scientific or engineering field. PADS takes advantage of multiple execution units run in multicore processors, cluster of workstations or HPC systems. However, large computing systems, such as HPC systems that include hundreds of thousands of computing nodes, have to handle frequent failures of some components. To cope with this issue, FT-GAIA transparently replicates simulation entities and distributes them on multiple execution nodes. This allows the simulation to tolerate crash-failures of computing nodes. Moreover, FT-GAIA offers some protection against Byzantine failures, since interaction messages among the simulated entities are replicated as well, so that the receiving entity can identify and discard corrupted messages. Results from an analytical model and from an experimental evaluation show that FT-GAIA provides a high degree of fault tolerance, at the cost of a moderate increase in the computational load of the execution units.", "venue": "Simul. Model. Pract. Theory", "authors": ["Gabriele  D'Angelo", "Stefano  Ferretti", "Moreno  Marzolla"], "year": 2019, "n_citations": 1}
{"id": 6630089, "s2_id": "0beed6285028e5a58c12cd23077549f8406b5132", "title": "High order concentrated non-negative matrix-exponential functions", "abstract": "Highly concentrated functions play an important role in many research fields including control system analysis and physics, and they turned out to be the key idea behind inverse Laplace transform methods as well. \nThis paper uses the matrix-exponential family of functions to create highly concentrated functions, whose squared coefficient of variation (SCV) is very low. In the field of stochastic modeling, matrix-exponential functions have been used for decades. They have many advantages: they are easy to manipulate, always non-negative, and integrals involving matrix-exponential functions often have closed-form solutions. For the time being there is no symbolic construction available to obtain the most concentrated matrix-exponential functions, and the numerical optimization-based approach has many pitfalls, too. \nIn this paper, we present a numerical optimization-based procedure to construct highly concentrated matrix-exponential functions. To make the objective function explicit and easy to evaluate we introduce and use a new representation called hyper-trigonometric representation. This representation makes it possible to achieve very low SCV.", "venue": "ArXiv", "authors": ["G\u00e1bor  Horv\u00e1th", "Ill\u00e9s  Horv\u00e1th", "Mikl\u00f3s  Telek"], "year": 2019, "n_citations": 1}
{"id": 6630716, "s2_id": "4da85474d24d7131160f34f618e6483451eac490", "title": "An algorithm for calculating steady state probabilities of $M|E_r|c|K$ queueing systems", "abstract": "This paper presents a method for calculating steady state probabilities of M|Er|c|K queueing systems. The infinitesimal generator matrix is used to define all possible states in the system and their transition probabilities. While this matrix can be written down immediately for many other M|PH|c|K queueing systems with phase-type service times (e.g. Coxian, Hypoexponential, ...), it requires a more careful analysis for systems with Erlangian service times. The constructed matrix may then be used to calculate steady state probabilities using an iterative algorithm. The resulting steady state probabilities can be used to calculate various performance measures, e.g. the average queue length. Additionally, computational issues of the implementation are discussed and an example from the field of telecommunication call-center queue length will be outlined to substantiate the applicability of these efforts. In the appendix, tables of the average queueing length given a specific number of service channels, traffic density, and system size are presented.", "venue": "ArXiv", "authors": ["Stefan  Hochrainer", "Ronald  Hochreiter", "Georg Ch. Pflug"], "year": 2014, "n_citations": 0}
{"id": 6634266, "s2_id": "4511f92a779b3bcc3f6ecaaec3d367eec67bc16a", "title": "Analysis of bandwidth measurement methodologies over WLAN systems", "abstract": "WLAN devices have become a fundamental component of nowadays network deployments. However, even though traditional networking applications run mostly unchanged over wireless links, the actual interaction between these applications and the dynamics of wireless transmissions is not yet fully understood. An important example of such applications are bandwidth estimation tools. This area has become a mature research topic with well-developed results. Unfortunately recent studies have shown that the application of these results to WLAN links is not straightforward. The main reasons for this is that the assumptions taken to develop bandwidth measurements tools do not hold any longer in the presence of wireless links (e.g. non-FIFO scheduling). This paper builds from these observations and its main goal is to analyze the interaction between probe packets and WLAN transmissions in bandwidth estimation processes. The paper proposes an analytical model that better accounts for the particularities of WLAN links. The model is validated through extensive experimentation and simulation and reveals that (1) the distribution of the delay to transmit probing packets is not the same for the whole probing sequence, this biases the measurements process and (2) existing tools and techniques point at the achievable throughput rather than the available bandwidth or the capacity, as previously assumed.", "venue": "ArXiv", "authors": ["Marc  Portoles-Comeras", "Albert  Cabellos-Aparicio", "Josep  Mangues-Bafalluy", "Jordi  Domingo-Pascual"], "year": 2009, "n_citations": 0}
{"id": 6635914, "s2_id": "049b1326e26bdff9c21a6847ec91c525e08dfb51", "title": "Energy Saving Strategy Based on Profiling", "abstract": "Constraints imposed by power consumption and the related costs are one of the key roadblocks to the design and development of next generation exascale systems. To mitigate these issues, strategies that reduce the power consumption of the processor are the need of the hour. Techniques such as Dynamic Voltage and Frequency Scaling (DVFS) exist which reduce the power consumption of a processor at runtime but they should be used in such a manner so that their overhead does not hamper application performance. In this paper, we propose an energy saving strategy which operates on timeslice basis to apply DVFS under a user defined performance constraint. Results show energy savings up to 7% when NAS benchmarks are tested on a laptop platform", "venue": "ArXiv", "authors": ["Milan  Yadav", "Kanak  Khanna"], "year": 2019, "n_citations": 0}
{"id": 6636442, "s2_id": "976e11e4c139ba7aad15753ac3ba647e1f88ef1f", "title": "Age of Information in a Decentralized Network of Parallel Queues with Routing and Packets Losses", "abstract": "The paper deals with Age of Information in a network of multiple sources and parallel servers/queues with buffering capabilities, preemption in service and losses in served packets. The servers do not communicate between each other and the packets are dispatched through the servers according to a predefined probabilistic routing. By making use of the Stochastic Hybrid System (SHS) method, we provide a derivation of the average Age of Information of a system of two parallel servers (with and without buffer capabilities) and compare the result with that of a single queue. We show known results of packets delay in Queuing Theory do not hold for Age of Information. Unfortunately, the complexity of computing the Age of Information using the SHS method increases highly with the number of queues. We therefore provide an upper bound of the average Age of Information in a parallel server system of an arbitrary number of M/M/1/(N + 1) queues and its tightness in various regimes. This upper bound allows providing a tight approximation of the Age of Information with a very low complexity.", "venue": "ArXiv", "authors": ["Josu  Doncel", "Mohamad  Assaad"], "year": 2020, "n_citations": 2}
{"id": 6639849, "s2_id": "1bcc8647915320eb73f264395c57dff34634a790", "title": "Utilizing Ensemble Learning for Performance and Power Modeling and Improvement of Parallel Cancer Deep Learning CANDLE Benchmarks", "abstract": "Machine learning (ML) continues to grow in importance across nearly all domains and is a natural tool in modeling to learn from data. Often a tradeoff exists between a model's ability to minimize bias and variance. In this paper, we utilize ensemble learning to combine linear, nonlinear, and tree-/rule-based ML methods to cope with the bias-variance tradeoff and result in more accurate models. Hardware performance counter values are correlated with properties of applications that impact performance and power on the underlying system. We use the datasets collected for two parallel cancer deep learning CANDLE benchmarks, NT3 (weak scaling) and P1B2 (strong scaling), to build performance and power models based on hardware performance counters using single-object and multiple-objects ensemble learning to identify the most important counters for improvement. Based on the insights from these models, we improve the performance and energy of P1B2 and NT3 by optimizing the deep learning environments TensorFlow, Keras, Horovod, and Python under the huge page size of 8 MB on the Cray XC40 Theta at Argonne National Laboratory. Experimental results show that ensemble learning not only produces more accurate models but also provides more robust performance counter ranking. We achieve up to 61.15% performance improvement and up to 62.58% energy saving for P1B2 and up to 55.81% performance improvement and up to 52.60% energy saving for NT3 on up to 24,576 cores.", "venue": "Concurrency and Computation: Practice and Experience", "authors": ["Xingfu  Wu", "Valerie  Taylor"], "year": 2021, "n_citations": 1}
{"id": 6646151, "s2_id": "201d6d512527f5026c659a56cac73fc4e69fba28", "title": "Benchmarking for Metaheuristic Black-Box Optimization: Perspectives and Open Challenges", "abstract": "Research on new optimization algorithms is often funded based on the motivation that such algorithms might improve the capabilities to deal with real-world and industrially relevant optimization challenges. Besides a huge variety of different evolutionary and metaheuristic optimization algorithms, also a large number of test problems and benchmark suites have been developed and used for comparative assessments of algorithms, in the context of global, continuous, and black-box optimization. For many of the commonly used synthetic benchmark problems or artificial fitness landscapes, there are however, no methods available, to relate the resulting algorithm performance assessments to technologically relevant real-world optimization problems, or vice versa. Also, from a theoretical perspective, many of the commonly used benchmark problems and approaches have little to no generalization value. Based on a mini-review of publications with critical comments, advice, and new approaches, this communication aims to give a constructive perspective on several open challenges and prospective research directions related to systematic and generalizable benchmarking for black-box optimization.", "venue": "2020 IEEE Congress on Evolutionary Computation (CEC)", "authors": ["Ramses  Sala", "Ralf  M\u00fcller"], "year": 2020, "n_citations": 0}
{"id": 6648344, "s2_id": "8f1e0626ec1d6d1e3165f8aa1529e35b5cd0a19a", "title": "Application of non-uniform laxity to EDF for aperiodic tasks to improve task utilisation on multicore platforms", "abstract": "This paper proposes a new scheduler applying the concept of non-uniform laxity to Earliest deadline first (EDF) approach for aperiodic tasks. This scheduler improves task utilisation (Execution time / deadline) and also increases the number of tasks that are being scheduled. Laxity is a measure of the spare time permitted for the task before it misses its deadline, and is computed using the expression (deadline - (current time + execution time)). Weight decides the priority of the task and is defined by the expression (quantum slice time / allocated time)*total core time for the task. Quantum slice time is the time actually used, allocated time is the time allocated by the scheduler, and total core time is the time actually reserved by the core for execution of one quantum of the task. Non-uniform laxity enables scheduling of tasks that have higher priority before the normal execution of other tasks and is computed by multiplying the weight of the task with its laxity. The algorithm presented in the paper has been simulated on Cheddar, a real time scheduling tool and also on SESC, an architectural simulator for multicore platforms, for upto 5000 random task sets, and upto 5000 cores. This scheduler improves task utilisation by 35% and the number of tasks being scheduled by 36%, compared to conventional EDF.", "venue": "ArXiv", "authors": ["K. Pradheep Kumar", "A. P. Shanthi"], "year": 2009, "n_citations": 3}
{"id": 6648757, "s2_id": "8951eefd1ffc7ff515ee2bf1509358cbd07ae935", "title": "AI Benchmark: All About Deep Learning on Smartphones in 2019", "abstract": "The performance of mobile AI accelerators has been evolving rapidly in the past two years, nearly doubling with each new generation of SoCs. The current 4th generation of mobile NPUs is already approaching the results of CUDA-compatible Nvidia graphics cards presented not long ago, which together with the increased capabilities of mobile deep learning frameworks makes it possible to run complex and deep AI models on mobile devices. In this paper, we evaluate the performance and compare the results of all chipsets from Qualcomm, HiSilicon, Samsung, MediaTek and Unisoc that are providing hardware acceleration for AI inference. We also discuss the recent changes in the Android ML pipeline and provide an overview of the deployment of deep learning models on mobile devices. All numerical results provided in this paper can be found and are regularly updated on the official project website: http://ai-benchmark.com.", "venue": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)", "authors": ["Andrey  Ignatov", "Radu  Timofte", "Andrei  Kulik", "Seungsoo  Yang", "Ke  Wang", "Felix  Baum", "Max  Wu", "Lirong  Xu", "Luc Van Gool"], "year": 2019, "n_citations": 90}
{"id": 6651204, "s2_id": "ce63c82636203bafcb3624d725f3f7f4b6c3fd92", "title": "QWin: Enforcing Tail Latency SLO at Shared Storage Backend", "abstract": "Consolidating latency-critical (LC) and best-effort (BE) tenants at storage backend helps to increase resources utilization. Even if tenants use dedicated queues and threads to achieve performance isolation, threads are still contend for CPU cores. Therefore, we argue that it is necessary to partition cores between LC and BE tenants, and meanwhile each core is dedicated to run a thread. Expect for frequently changing bursty load, fluctuated service time at storage backend also drastically changes the need of cores. In order to guarantee tail latency service level objectives (SLOs), the abrupt changing need of cores must be satisfied immediately. Otherwise, tail latency SLO violation happens. Unfortunately, partitioning-based approaches lack the ability to react the changing need of cores, resulting in extreme spikes in latency and SLO violation happens. In this paper, we present QWin, a tail latency SLO aware core allocation to enforce tail latency SLO at shared storage backend. QWin consists of an SLO-tocore calculation model that accurately calculates the number of cores combining with definitive runtime load determined by a flexible request-based window, and an autonomous core allocation that adjusts cores at adaptive frequency by dynamically changing core policies. When consolidating multiple LC and BE tenants, QWin outperforms the-state-of-the-art approaches in guaranteeing tail latency SLO for LC tenants and meanwhile increasing bandwidth of BE tenants by up to 31x.", "venue": "ArXiv", "authors": ["Liuying  Ma", "Zhenqing  Liu", "Jin  Xiong", "Dejun  Jiang"], "year": 2021, "n_citations": 0}
{"id": 6652070, "s2_id": "ab8bb5bc0f99f92d851c4402e540dc46c180a436", "title": "Basic Performance Measurements of the Intel Optane DC Persistent Memory Module", "abstract": "Scalable nonvolatile memory DIMMs will finally be commercially available with the release of the Intel Optane DC Persistent Memory Module (or just \"Optane DC PMM\"). This new nonvolatile DIMM supports byte-granularity accesses with access times on the order of DRAM, while also providing data storage that survives power outages. This work comprises the first in-depth, scholarly, performance review of Intel's Optane DC PMM, exploring its capabilities as a main memory device, and as persistent, byte-addressable memory exposed to user-space applications. This report details the technologies performance under a number of modes and scenarios, and across a wide variety of macro-scale benchmarks. Optane DC PMMs can be used as large memory devices with a DRAM cache to hide their lower bandwidth and higher latency. When used in this Memory (or cached) mode, Optane DC memory has little impact on applications with small memory footprints. Applications with larger memory footprints may experience some slow-down relative to DRAM, but are now able to keep much more data in memory. When used under a file system, Optane DC PMMs can result in significant performance gains, especially when the file system is optimized to use the load/store interface of the Optane DC PMM and the application uses many small, persistent writes. For instance, using the NOVA-relaxed NVMM file system, we can improve the performance of Kyoto Cabinet by almost 2x. Optane DC PMMs can also enable user-space persistence where the application explicitly controls its writes into persistent Optane DC media. In our experiments, modified applications that used user-space Optane DC persistence generally outperformed their file system counterparts. For instance, the persistent version of RocksDB performed almost 2x faster than the equivalent program utilizing an NVMM-aware file system.", "venue": "ArXiv", "authors": ["Joseph  Izraelevitz", "Jian  Yang", "Lu  Zhang", "Juno  Kim", "Xiao  Liu", "Amirsaman  Memaripour", "Yun Joon Soh", "Zixuan  Wang", "Yi  Xu", "Subramanya R. Dulloor", "Jishen  Zhao", "Steven  Swanson"], "year": 2019, "n_citations": 244}
{"id": 6653151, "s2_id": "c7213c31d082b39607635ffc7224de0fc966a981", "title": "A Note on Disk Drag Dynamics", "abstract": "The electrical power consumed by typical magnetic hard disk drives (HDD) not only increases linearly with the number of spindles but, more significantly, it increases as very fast power-laws of speed (RPM) and diameter. Since the theoretical basis for this relationship is neither well-known nor readily accessible in the literature, we show how these exponents arise from aerodynamic disk drag and discuss their import for green storage capacity planning.", "venue": "ArXiv", "authors": ["Neil J. Gunther"], "year": 2012, "n_citations": 0}
{"id": 6655477, "s2_id": "64bac81afc7b5517b9535b4308cc2b7b4dbe3b01", "title": "Performance and Power Modeling and Prediction Using MuMMI and Ten Machine Learning Methods", "abstract": "In this paper, we use modeling and prediction tool MuMMI (Multiple Metrics Modeling Infrastructure) and ten machine learning methods to model and predict performance and power and compare their prediction error rates. We use a fault-tolerant linear algebra code and a fault-tolerant heat distribution code to conduct our modeling and prediction study on the Cray XC40 Theta and IBM BG/Q Mira at Argonne National Laboratory and the Intel Haswell cluster Shepard at Sandia National Laboratories. Our experiment results show that the prediction error rates in performance and power using MuMMI are less than 10% for most cases. Based on the models for runtime, node power, CPU power, and memory power, we identify the most significant performance counters for potential optimization efforts associated with the application characteristics and the target architectures, and we predict theoretical outcomes of the potential optimizations. When we compare the prediction accuracy using MuMMI with that using 10 machine learning methods, we observe that MuMMI not only results in more accurate prediction in both performance and power but also presents how performance counters impact the performance and power models. This provides some insights about how to fine-tune the applications and/or systems for energy efficiency.", "venue": "ArXiv", "authors": ["Xingfu  Wu", "Valerie  Taylor", "Zhiling  Lan"], "year": 2020, "n_citations": 1}
{"id": 6657592, "s2_id": "3d677a8afbd66e09889dd878f74f881302562170", "title": "Speed Based Optimal Power Control in Small Cell Networks", "abstract": "Small cell networks promise good quality of service (QoS) even for cell edge users, however pose challenges to cater to the high-speed users. The major difficulty being that of frequent handovers and the corresponding handover losses, which significantly depend upon the speed of the user. It was shown previously that the optimal cell size increases with speed. Thus, in scenarios with diverse users (speeds spanning over large ranges), it would be inefficient to serve all users using common cell radius and it is practically infeasible to design different cell sizes for different speeds. Alternatively, we propose to allocate power to a user based on its speed, e.g., higher power virtually increases the cell size. We solve well known Hamiltonian Jacobi equations under certain assumptions to obtain a power law, optimal for load factor and busy probability, for any given average power constraint and cell size. The optimal power control turns out to be linear in speed. We build a system level simulator for small cell network, using elaborate Monte-Carlo simulations, and show that the performance of the system improves significantly with linear power law. The power law is tested even for the cases, for which the system does not satisfy the assumptions required by the theory. For example, the linear power law has significant improvement in comparison with the 'equal power' system, even in presence of time varying and random interference. We observe good improvement in almost all cases with improvements up to 89\\% for certain configurations.", "venue": "Comput. Commun.", "authors": ["Veeraruna  Kavitha", "Manu K. Gupta", "V\u00e9ronique  Capdevielle", "Rahul  Kishor", "Majed  Haddad"], "year": 2019, "n_citations": 1}
{"id": 6662636, "s2_id": "593999ee5e56d3919c248e3b0a46771eccce59e9", "title": "Toward a Unified Performance and Power Consumption NAND Flash Memory Model of Embedded and Solid State Secondary Storage Systems", "abstract": "This paper presents a set of models dedicated to describe a flash storage subsystem structure, functions, performance and power consumption behaviors. These models cover a large range of today's NAND flash memory applications. They are designed to be implemented in simulation tools allowing to estimate and compare performance and power consumption of I/O requests on flash memory based storage systems. Such tools can also help in designing and validating new flash storage systems and management mechanisms. This work is integrated in a global project aiming to build a framework simulating complex flash storage hierarchies for performance and power consumption analysis. This tool will be highly configurable and modular with various levels of usage complexity according to the required aim: from a software user point of view for simulating storage systems, to a developer point of view for designing, testing and validating new flash storage management systems.", "venue": "ArXiv", "authors": ["Pierre  Olivier", "Jalil  Boukhobza", "Eric  Senn"], "year": 2013, "n_citations": 1}
{"id": 6666998, "s2_id": "5cb6ff1b931ebe65b032cf13f563c0be6cc85697", "title": "C3PO: Computation Congestion Control (PrOactive) - an algorithm for dynamic diffusion of ephemeral in-network services", "abstract": "There is an obvious trend that more and more data and computation are migrating into networks nowadays. Combining mature virtualization technologies with service-centric net- working, we are entering into an era where countless services reside in an ISP network to provide low-latency access. Such services are often computation intensive and are dynamically created and destroyed on demands everywhere in the network to perform various tasks. Consequently, these ephemeral in-network services introduce a new type of congestion in the network which we refer to as \"computation congestion\". The service load need to be effectively distributed on different nodes in order to maintain the funtionality and responsiveness of the network, which calls for a new design rather than reusing the centralised scheduler designed for cloud-based services. In this paper, we study both passive and proactive control strategies, based on the proactive control we further propose a fully distributed solution which is low complexity, adaptive, and responsive to network dynamics.", "venue": "ArXiv", "authors": ["Liang  Wang", "M\u00e1rio  Almeida", "Jeremy  Blackburn", "Jon  Crowcroft"], "year": 2016, "n_citations": 0}
{"id": 6668536, "s2_id": "3f199b134fa58a3f095d0a5dbbb047956e81b754", "title": "Download Time Analysis for Distributed Storage Codes With Locality and Availability", "abstract": "The paper presents techniques for analyzing the expected download time in distributed storage systems that employ systematic availability codes. These codes provide access to hot data through the systematic server containing the object and multiple recovery groups. When a request for an object is received, it can be replicated (forked) to the systematic server and all recovery groups. We first consider the low-traffic regime and present the close-form expression for the download time. By comparison across systems with availability, maximum distance separable (MDS), and replication codes, we demonstrate that availability codes can reduce download time in some settings but are not always optimal. In the high-traffic regime, the system contains multiple inter-dependent Fork-Join queues, making exact analysis intractable. Accordingly, we present upper and lower bounds on the download time, and an M/G/1 queue approximation for several cases of interest. Via extensive numerical simulations, we evaluate our bounds and demonstrate that the M/G/1 queue approximation has a high degree of accuracy.", "venue": "IEEE Transactions on Communications", "authors": ["Mehmet Fatih Akta\u015f", "Swanand  Kadhe", "Emina  Soljanin", "Alex  Sprintson"], "year": 2021, "n_citations": 6}
{"id": 6668594, "s2_id": "3e0e3d432063af9dcf887dadc56ce3c580b77e98", "title": "Astrophysical Particle Simulations on Heterogeneous CPU-GPU Systems", "abstract": "A heterogeneous CPU-GPU node is getting popular in HPC clusters. We need to rethink algorithms and optimization techniques for such system depending on the relative performance of CPU vs. GPU. In this paper, we report a performance optimized particle simulation code \"OTOO\", that is based on the octree method, for heterogenous systems. Main applications of OTOO are astrophysical simulations such as N-body models and the evolution of a violent merger of stars. We propose optimal task split between CPU and GPU where GPU is only used to compute the calculation of the particle force. Also, we describe optimization techniques such as control of the force accuracy, vectorized tree walk, and work partitioning among multiple GPUs. We used OTOO for modeling a merger of two white dwarf stars and found that OTOO is powerful and practical to simulate the fate of the process.", "venue": "ArXiv", "authors": ["Naohito  Nakasato", "Go  Ogiya", "Yohei  Miki", "Masao  Mori", "Ken'ichi  Nomoto"], "year": 2012, "n_citations": 11}
{"id": 6670604, "s2_id": "ddce2f41414d35592dda0d12ea33bfac29fe983f", "title": "Faster and Cheaper: Parallelizing Large-Scale Matrix Factorization on GPUs", "abstract": "Matrix factorization (MF) is used by many popular algorithms such as collaborative filtering. GPU with massive cores and high memory bandwidth sheds light on accelerating MF much further when appropriately exploiting its architectural characteristics. This paper presents cuMF, a CUDA-based matrix factorization library that optimizes alternate least square (ALS) method to solve very large-scale MF. CuMF uses a set of techniques to maximize the performance on single and multiple GPUs. These techniques include smart access of sparse data leveraging GPU memory hierarchy, using data parallelism in conjunction with model parallelism, minimizing the communication overhead among GPUs, and a novel topology-aware parallel reduction scheme. With only a single machine with four Nvidia GPU cards, cuMF can be 6-10 times as fast, and 33-100 times as cost-efficient, compared with the state-of-art distributed CPU solutions. Moreover, cuMF can solve the largest matrix factorization problem ever reported in current literature, with impressively good performance.", "venue": "HPDC", "authors": ["Wei  Tan", "Liangliang  Cao", "Liana L. Fong"], "year": 2016, "n_citations": 47}
{"id": 6671490, "s2_id": "9f9f74402a7cc5197339321baa35762beaf2c29b", "title": "Reducing communication in algebraic multigrid with multi-step node aware communication", "abstract": "Algebraic multigrid (AMG) is often viewed as a scalable O ( n ) solver for sparse linear systems. Yet, AMG lacks parallel scalability due to increasingly large costs associated with communication, both in the initial construction of a multigrid hierarchy and in the iterative solve phase. This work introduces a parallel implementation of AMG that reduces the cost of communication, yielding improved parallel scalability. It is common in Message Passing Interface (MPI), particularly in the MPI-everywhere approach, to arrange inter-process communication, so that communication is transported regardless of the location of the send and receive processes. Performance tests show notable differences in the cost of intra- and internode communication, motivating a restructuring of communication. In this case, the communication schedule takes advantage of the less costly intra-node communication, reducing both the number and the size of internode messages. Node-centric communication extends to the range of components in both the setup and solve phase of AMG, yielding an increase in the weak and strong scaling of the entire method.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Amanda  Bienz", "William D Gropp", "Luke N Olson"], "year": 2020, "n_citations": 4}
{"id": 6672345, "s2_id": "d3dea6b8ace7209d574ada0dee917f112840fe57", "title": "Ber Performance Analysis of WiMAX PHY Layer under different channel conditions", "abstract": "This paper gives an introduction on the IEEE 802.16 standard WIMAX or Worldwide Interoperability for Microwave Access. The different parts give details on the architectural specifications of WiMAX networks and also on the working principle of WiMAX networks including its services provided. It also provides brief descriptions on its salient features of this technology and how it benefits the networking industry. A brief outline of the basic building blocks or equipment of WiMAX architecture is also provided. This paper also evaluates the simulation performance of IEEE 802.16 OFDM PHY layer. The Stanford University Interim (SUI) channel model under varying parameters is selected for the wireless channel in the simulation. The performance measurements and analysis was done in simulation developed in MATLAB.", "venue": "ArXiv", "authors": ["Shantanu  Pathak", "S  Ranjani"], "year": 2013, "n_citations": 2}
{"id": 6672944, "s2_id": "ce9986b04583fd2c3b2be7d440fe3351f27db042", "title": "Delay-Optimal Policies in Partial Fork-Join Systems with Redundancy and Random Slowdowns", "abstract": "We consider a large distributed service system consisting of n homogeneous servers with infinite capacity FIFO queues. Jobs arrive as a Poisson process of rate \u03bbn/kn (for some positive constant kn and integer kn). Each incoming job consists of ???? identical tasks that can be executed in parallel, and that can be encoded into at least ???? ?replicas\" of the same size (by introducing redundancy) so that the job is considered to be completed when any kn replicas associated with it finish their service. Moreover, we assume that servers can experience random slowdowns in their processing rate so that the service time of a replica is the product of its size and a random slowdown. First, we assume that the server slowdowns are shifted exponential and independent of the replica sizes. In this setting we show that the delay of a typical job is asymptotically minimized (as n - \u221e) when the number of replicas per task is a constant that only depends on the arrival rate \u03bb, and on the expected slowdown of servers. Second, we introduce a new model for the server slowdowns in which larger tasks experience less variable slowdowns than smaller tasks. In this setting we show that, under the class of policies where all replicas start their service at the same time, the delay of a typical job is asymptotically minimized (as n - \u221e) when the number of replicas per task is made to depend on the actual size of the tasks being replicated, with smaller tasks being replicated more than larger tasks.", "venue": "SIGMETRICS", "authors": ["Martin  Zubeldia"], "year": 2020, "n_citations": 0}
{"id": 6675328, "s2_id": "489cd9ee6a79e9385eceadb953194e97b84ce61c", "title": "Fast Mixing of Parallel Glauber Dynamics and Low-Delay CSMA Scheduling", "abstract": "Glauber dynamics is a powerful tool to generate randomized, approximate solutions to combinatorially difficult problems. It has been recently used to design distributed carrier-sense multiple-access (CSMA) scheduling algorithms for multihop wireless networks. In this paper, we derive bounds on the mixing time of a generalization of Glauber dynamics where multiple links update their states in parallel and the fugacity of each link can be different. The results are used to prove that the average queue length (and hence, the delay) under the parallel-Glauber-dynamics-based CSMA grows polynomially in the number of links for wireless networks with bounded-degree interference graphs when the arrival rate lies in a fraction of the capacity region. Other versions of adaptive CSMA can be analyzed similarly. We also show that in specific network topologies, the low-delay capacity region can be further improved.", "venue": "IEEE Transactions on Information Theory", "authors": ["Libin  Jiang", "Mathieu  Leconte", "Jian  Ni", "R.  Srikant", "Jean C. Walrand"], "year": 2012, "n_citations": 60}
{"id": 6678602, "s2_id": "579a44ba9520d4f1aefd8d8b082b2a8edb76480a", "title": "A Comparison of Parallel Graph Processing Implementations", "abstract": "The rapidly growing number of large network analysis problems has led to the emergence of many parallel and distributed graph processing systems\u2014one survey in 2014 identified over 80. Determining the best approach for a given problem is infeasible for most developers. We present an approach and associated software for analyzing the performance and scalability of parallel, open-source graph libraries. We demonstrate our approach on five graph processing packages: GraphMat, Graph500, Graph Algorithm Platform Benchmark Suite, GraphBIG, and PowerGraph using synthetic and real-world datasets. We examine previously overlooked aspects of parallel graph processing performance such as phases of execution and energy usage for three algorithms: breadth first search, single source shortest paths, and PageRank.", "venue": "2017 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Samuel  Pollard", "Boyana  Norris"], "year": 2017, "n_citations": 7}
{"id": 6679337, "s2_id": "14165081d55f31f97de21ed38645b39b57a4aca7", "title": "Dynamic load balancing with tokens", "abstract": "Abstract Efficiently exploiting the resources of data centers is a complex task that requires efficient and reliable load balancing and resource allocation algorithms. The former are in charge of assigning jobs to servers upon their arrival in the system, while the latter are responsible for sharing the server resources between their assigned jobs. These algorithms should adapt to various constraints, such as data locality, that restrict the feasible job assignments. In this paper, we propose a token-based algorithm that efficiently balances the load between the servers without requiring any knowledge on the job arrival rates and the server capacities. Assuming a balanced fair sharing of the server resources, we show that the resulting dynamic load balancing is insensitive to the job size distribution. Its performance is compared to that obtained under the best static load balancing and in an ideal system that would constantly optimize the resource utilization. We also make the connection with other token-based algorithms such as Join-Idle-Queue.", "venue": "Comput. Commun.", "authors": ["C\u00e9line  Comte"], "year": 2019, "n_citations": 5}
{"id": 6682039, "s2_id": "54e2dea60ca45bc3fd013f56160fd2317cc90fc4", "title": "Agile Elicitation of Scalability Requirements for Open Systems: A Case Study", "abstract": "Eliciting scalability requirements during agile software development is complicated and poorly described in previous research. This article presents a lightweight artifact for eliciting scalability requirements during agile software development: the ScrumScale model. The ScrumScale model is a simple spreadsheet. The scalability concepts underlying the ScrumScale model are clarified in this design science research, which also utilizes coordination theory. This paper describes the open banking case study, where a legacy banking system becomes open. This challenges the scalability of this legacy system. The first step in understanding this challenge is to elicit the new scalability requirements. In the open banking case study, key stakeholders from TietoEVRY spent 55 hours eliciting TietoEVRY\u2019s open banking project\u2019s scalability requirements. According to TietoEVRY, the ScrumScale model provided a systematic way of producing scalability requirements. For TietoEVRY, the scalability concepts behind the ScrumScale model also offered significant advantages in dialogues with other stakeholders.", "venue": "Journal of Systems and Software", "authors": ["Gunnar  Brataas", "Antonio  Martini", "Geir Kjetil Hanssen", "Georg  R\u00e6der"], "year": 2021, "n_citations": 0}
{"id": 6682893, "s2_id": "7623555f277898e6aadf8dd6665ba4bb0dbbc056", "title": "Consideration for effectively handling parallel workloads on public cloud system", "abstract": "We retrieved and analyzed parallel storage workloads of the FUJITSU K5 cloud service to clarify how to build cost-effective hybrid storage systems. A hybrid storage system consists of fast but low-capacity tier (first tier) and slow but high-capacity tier (second tier). And, it typically consists of either SSDs and HDDs or NVMs and SSDs. As a result, we found that 1) regions for first tier should be assigned only if a workload includes large number of IO accesses for a whole day, 2) the regions that include a large number of IO accesses should be dynamically chosen and moved from second tier to first tier for a short interval, and 3) if a cache hit ratio is regularly low, use of the cache for the workload should be cancelled, and the whole workload region should be assigned to the region for first tier. These workloads already have been released from the SNIA web site.", "venue": "ArXiv", "authors": ["Kazuichi  Oe"], "year": 2020, "n_citations": 0}
{"id": 6686503, "s2_id": "193eff607da7f63b933428fc8322a7b0c09f1194", "title": "Experience with PCIe streaming on FPGA for high throughput ML inferencing", "abstract": "Achieving maximum possible rate of inferencing with minimum hardware resources plays a major role in reducing enterprise operational costs. In this paper we explore use of PCIe streaming on FPGA based platforms to achieve high throughput. PCIe streaming is a unique capability available on FPGA that eliminates the need for memory copy overheads. We have presented our results for inferences on a gradient boosted trees model, for online retail recommendations. We compare the results achieved with the popular library implementations on GPU and the CPU platforms and observe that the PCIe streaming enabled FPGA implementation achieves the best overall measured performance. We also measure power consumption across all platforms and find that the PCIe streaming on FPGA platform achieves the 25x and 12x better energy efficiency than an implementation on CPU and GPU platforms, respectively. We discuss the conditions that need to be met, in order to achieve this kind of acceleration on the FPGA. Further, we analyze the run time statistics on GPU and FPGA and identify opportunities to enhance performance on both the platforms.", "venue": "ArXiv", "authors": ["Piyush  Manavar", "Manoj  Nambiar", "Nupur  Sumeet", "Rekha  Singhal", "Sharod  Choudhary", "Amey  Pandit"], "year": 2021, "n_citations": 0}
{"id": 6689261, "s2_id": "ac60277d9649eb833297e39e249834751a408245", "title": "Tandem Queueing Systems Maximum Throughput Problem", "abstract": "In this paper we consider the problem of maximum throughput for tandem queueing system. We modeled this system as a Quasi-Birth-Death process. In order to do this we named level the number of customers waiting in the first buffer (including the customer in service) and we called phase the state of the remining servers. Using this model we studied the problem of maximum throughput of the system: the maximum arrival rate that a given system could support before becoming saturated, or unstable. We considered different particular cases of such systems, which were obtained by modifying the capacity of the intermediary buffers, the arrival rate and the service rates. The results of the simulations are presented in our paper and can be summed up in the following conclusions: 1. The analytic formula for the maximum throughput of the system tends to become rather complicated when the number of servers increase 2. The maximum throughput of the system converges as the number of servers increases 3. The homogeneous case reveals an interesting characteristic: if we reverse the order of the servers, maximum thruoughput of the system remains unchanged The QBD process used for the case of Poisson arrivals can be extended to model more general arrival processes.", "venue": "ArXiv", "authors": ["Daniel Marian Merezeanu", "Daniela  Andone"], "year": 2015, "n_citations": 0}
{"id": 6689661, "s2_id": "a7ff5bb67702f6c241d4d1ccf5e5733a38177779", "title": "Performance and Scalability Models for a Hypergrowth e-Commerce Web Site", "abstract": "The performance of successful Web-based e-commerce services has all the allure of a roller-coaster ride: accelerated fiscal growth combined with the ever-present danger of running out of server capacity. This chapter presents a case study based on the author's own capacity planning engagement with one of the hottest e-commerce Web sites in the world. Several spreadsheet techniques are presented for forecasting both short-term and long-term trends in the consumption of server capacity. Two new performance metrics are introduced for site planning and procurement: the effective demand, and the doubling period.", "venue": "Performance Engineering", "authors": ["Neil J. Gunther"], "year": 2001, "n_citations": 12}
{"id": 6691584, "s2_id": "1f1e4547c01a9ad584beefe7080a7f08934f96e7", "title": "The Power of d Choices in Scheduling for Data Centers with Heterogeneous Servers", "abstract": "MapReduce framework is the de facto in big data and its applications where a big data-set is split into small data chunks that are replicated on different servers among thousands of servers. The heterogeneous server structure of the system makes the scheduling much harder than scheduling for systems with homogeneous servers. Throughput optimality of the system on one hand and delay optimality on the other hand creates a dilemma for assigning tasks to servers. The JSQ-MaxWeight and Balanced-Pandas algorithms are the states of the arts algorithms with theoretical guarantees on throughput and delay optimality for systems with two and three levels of data locality. However, the scheduling complexity of these two algorithms are way too much. Hence, we use the power of $d$ choices algorithm combined with the Balanced-Pandas algorithm and the JSQ-MaxWeight algorithm, and compare the complexity of the simple algorithms and the power of $d$ choices versions of them. We will further show that the Balanced-Pandas algorithm combined with the power of the $d$ choices, Balanced-Pandas-Pod, not only performs better than simple Balanced-Pandas, but also is less sensitive to the parameter $d$ than the combination of the JSQ-MaxWeight algorithm and the power of the $d$ choices, JSQ-MaxWeight-Pod. In fact in our extensive simulation results, the Balanced-Pandas-Pod algorithm is performing better than the simple Balanced-Pandas algorithm in low and medium loads, where data centers are usually performing at, and performs almost the same as the Balanced-Pandas algorithm at high loads. Note that the load balancing complexity of Balanced-Pandas and JSQ-MaxWeight algorithms are $O(M)$, where $M$ is the number of servers in the system which is in the order of thousands servers, whereas the complexity of Balanced-Pandas-Pod and JSQ-MaxWeight-Pod are $O(1)$, that makes the central scheduler faster and saves energy.", "venue": "ArXiv", "authors": ["Amir  Moaddeli", "Iman Nabati Ahmadi", "Negin  Abhar"], "year": 2019, "n_citations": 2}
{"id": 6695090, "s2_id": "c04a6bde172c35a99fc5c74fc0f5c8e511e32e93", "title": "Heavy-Traffic Insensitive Bounds for Weighted Proportionally Fair Bandwidth Sharing Policies", "abstract": "We consider a connection-level model proposed by Massouli\\'{e} and Roberts for bandwidth sharing among file transfer flows in a communication network, and we study weighted proportionally fair sharing policies where the weights represent the relative importance of flows on different routes. We are interested in characterizing performance in the heavy-traffic regime. Existing work on this problem has focused on diffusion approximations, which were first studied by Kang et al. (2009). However, except for the case where the weights of all the routes are equal, the steady-state distribution of the limiting diffusion process is unknown and thus there are no explicit-form characterizations, even when exponential file size distributions are assumed. For more general file size distributions, the diffusion approximation was derived for the equal-weights case by Vlasiou, Zhang and Zwart (2014), but an interchange-of-limits result was lacking. \nWe take a Lyapunov-drift-based approach that is different from the diffusion approximation approach, where we directly analyze the steady state of the system. We first establish a state-space collapse result in steady state, and then obtain explicit-form bounds on the weighted sum of the expected numbers of flows on different routes, where the weights are the same as those used in the weighted proportionally fair sharing policy. Our bounds hold for a class of phase-type file size distributions; i.e., the bounds are heavy-traffic insensitive to the distributions in this class. For the equal-weights case, the upper and lower bounds coincide, which implies the heavy-traffic insensitivity of the expectation of the total number of flows. Furthermore, our state-space collapse result implies an interchange of limits as a by-product for the diffusion approximation by Vlasiou, Zhang and Zwart (2014).", "venue": "ArXiv", "authors": ["Weina  Wang", "Siva Theja Maguluri", "R.  Srikant", "Lei  Ying"], "year": 2018, "n_citations": 3}